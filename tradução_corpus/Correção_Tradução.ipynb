{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca0dbU-0ieCR",
        "outputId": "c36fc33b-4aaf-41f7-eafd-0b3add175c7c"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wpC8MZtMibev"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZkvEA7VsitOi"
      },
      "outputs": [],
      "source": [
        "!export OPENAI_API_KEY=\"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\"\n",
        "key = \"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\"\n",
        "ENDPOINT_GPT = \"https://api.openai.com/v1\" # Gpt\n",
        "model_gpt = \"gpt-4o-mini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VydEcaWwjurB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qOc21CHVjlgJ"
      },
      "outputs": [],
      "source": [
        "with open(\"corpus_artigos.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    articles_json = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gX5YWRwXj2DI"
      },
      "outputs": [],
      "source": [
        "conf_principal = articles_json[\"Conferência Principal\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A3aIApDOfi4T"
      },
      "outputs": [],
      "source": [
        "dados = {\n",
        "    \"conferencia_principal\": [\n",
        "    ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXFmfEXLi2tw",
        "outputId": "2411e61a-a63b-4bdd-e196-36d70c72c264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Abstract. Este artigo apresenta uma abordagem para a integração de dados estruturados em aplicativos de chatbot. Utilizando nossa ferramenta Mindmap, que organiza hierarquicamente os dados e mapeia nós para ações, desenvolvemos um esquema JSON aumentado para melhorar o entendimento contextual do chatbot e a precisão das respostas. Ao aplicar o conjunto Langchain e técnicas de Geração Aumentada por Recuperação (RAG), nosso método aprimora a recuperação e o processamento de dados a partir de um armazenamento vetorial, melhorando significativamente a relevância da interação. Palavras-chave: Integração de Chatbot; Dados Estruturados; RAG; Langchain \n",
            "\n",
            "1. Introdução  \n",
            "Nos últimos anos, a evolução do software voltado para o consumidor levou a expectativas elevadas entre os usuários corporativos por interações mais intuitivas e naturais com sistemas computacionais. Tradicionalmente, as interfaces de usuário em ambientes corporativos dependiam de paradigmas de interação baseados em janelas. No entanto, o advento de tecnologias sofisticadas de processamento de linguagem natural (NLP) começou a mudar esse paradigma, tornando as interfaces em linguagem natural cada vez mais desejáveis para aplicações empresariais [Weiying et al. 2019]. Um marco significativo nessa transição foi o lançamento público do ChatGPT pela OpenAI [OpenAI 2023]. A tecnologia subjacente ao ChatGPT é baseada em um Modelo de Linguagem Grande (LLM), um tipo de modelo de aprendizado de máquina treinado em conjuntos de dados extensos contendo conteúdo textual diversificado. A escala desses conjuntos de dados de treinamento permite que os LLMs respondam efetivamente a uma ampla gama de entradas de usuários. No entanto, a implementação de LLMs em ambientes corporativos apresenta desafios únicos. Uma limitação importante é que esses modelos geralmente não são treinados em dados proprietários de instituições privadas, que devem permanecer confidenciais para proteger a integridade organizacional e a privacidade dos dados. Para abordar essa questão, pesquisadores exploraram o uso de grafos de conhecimento para aprimorar ainda mais os LLMs [Wen et al. 2023], e técnicas como Geração Aumentada por Recuperação (RAG) foram desenvolvidas para fornecer aos LLMs contexto derivado de conjuntos de dados privados, melhorando assim sua relevância e precisão em configurações empresariais [Lewis et al. 2020]. Apesar da eficácia dessas técnicas, elas frequentemente não atendem às demandas do desenvolvimento de software corporativo, onde atualizações frequentes e respostas rápidas a mudanças organizacionais são comuns. Essas abordagens também enfrentam desafios, como a dificuldade em incorporar novos conhecimentos e explicar seus processos de raciocínio [Wen et al. 2023].  \n",
            "\n",
            "Este estudo propõe um mecanismo para estruturar dados de forma mais eficaz para fornecer aos chatbots baseados em LLM as informações contextuais necessárias para fornecer respostas precisas e contextualmente relevantes em ambientes corporativos. Dadas as rigorosas exigências dos ambientes corporativos, onde o controle sobre as saídas geradas é primordial, a abordagem delineada neste trabalho prioriza modelos de interação supervisionados em vez de sistemas baseados em agentes autônomos. Ao contrário de aplicações públicas gerais, onde isenções podem mitigar os riscos de saídas imprecisas ou falhas, ambientes corporativos exigem um nível mais alto de supervisão para prevenir resultados adversos potenciais.\n",
            "\n",
            "2. Concepção  \n",
            "Com a crescente demanda de nossos principais clientes, particularmente aqueles que formam a base de consumidores de nosso modelo de negócios, por uma interface de chatbot LLM mais integrada e sofisticada, desenvolvemos uma abordagem que aproveita os sistemas de chatbot estruturados existentes e os dados já mapeados dentro desses sistemas.  \n",
            "\n",
            "O sistema de chatbot estruturado organizou os dados em uma estrutura hierárquica, em forma de árvore, onde cada nó pai representava um tópico, e os nós filhos indicavam possíveis respostas ou subtópicos associados a esse tópico. Essa configuração garantiu uma estrutura de diálogo bem definida e navegável. Com o objetivo de criar um chatbot baseado em LLM integrado ao RAG — que emprega um armazenamento vetorial para recuperação semântica — desenvolvemos uma ferramenta capaz de exportar esses dados estruturados para integração no novo sistema.  \n",
            "\n",
            "No entanto, identificamos limitações significativas em nosso software atual, que apenas suportava a geração de dados em RTF (Rich Text Format), um formato inadequado para nossas necessidades. Para superar esse desafio, desenvolvemos uma nova ferramenta para gerenciar o registro de nós e seus nós filhos associados. Isso levou à criação de uma interface baseada na web (Mindmap), construída usando frameworks avançados de JavaScript (React com Next.js [Vercel, Inc. 2024]).  \n",
            "\n",
            "A introdução dessa ferramenta expandiu significativamente o escopo do projeto, permitindo-nos ir além de simples nós de texto de tópicos e filhos, possibilitando a criação de estruturas de dados mais complexas adaptadas a objetivos específicos. Uma das características críticas desenvolvidas foi o mapeamento de nós para ações, que ditam as operações a serem executadas pelo chatbot. O texto registrado dentro de cada nó é posteriormente analisado como parâmetros para a ação correspondente.  \n",
            "\n",
            "Com o desenvolvimento da ferramenta Mindmap, agora temos uma plataforma robusta que permite o registro sistemático dos dados necessários e facilita sua exportação para formatos mais adequados para processamento posterior.  \n",
            "\n",
            "(a) Captura de tela da ferramenta Mindmap  \n",
            "(b) Trecho do json aumentado  \n",
            "Figura 1. JSON aumentado gerado  \n",
            "\n",
            "3. Pipeline proposto  \n",
            "A ferramenta Mindmap exporta dados hierárquicos como um arquivo JSON estruturado, que é processado em um JSON aumentado usando scripts Python dentro do conjunto Langchain [Documentação do LangChain], atualmente associado ao modelo GPT-4 da OpenAI. Este arquivo aumentado incorpora perguntas e respostas, derivadas da hierarquia de cada nó, suas ações e metadados relacionados.  \n",
            "\n",
            "Essas perguntas e respostas são geradas com base nas ações do nó e perfis predefinidos. O JSON resultante é então usado para preencher um banco de dados de armazenamento vetorial, servindo como a base para as funcionalidades RAG do chatbot. O design é agnóstico em relação ao método do motor do chatbot para consumir esses dados para construir seu armazenamento vetorial ou executar a inferência. Por exemplo, um grupo de pesquisa colaborador utiliza o armazenamento vetorial sem incorporar perguntas e respostas, usando-as posteriormente para validar o conjunto de dados gerado. Em contraste, internamente, utilizamos as perguntas e respostas para criar documentos diretamente no armazenamento vetorial. Um estudo comparativo sobre a eficácia dessas estratégias para vários casos de uso poderia ser o foco de pesquisas futuras.  \n",
            "\n",
            "Tabela 1. Amostra de ações do nó  \n",
            "Descrição da ação  \n",
            "Nome da ação  \n",
            "Ação  \n",
            "Use um armazenamento vetorial para documentos  \n",
            "Buscar dados em um site  \n",
            "O nó contém uma URL e um seletor CSS para buscar dados. Além disso, contém um texto de JUIZ para verificar se os dados são semelhantes ao que é esperado. Dado um link de documento, construir um armazenamento vetorial específico para aquele documento. Uma vez que uma consulta inicial corresponda, a resposta deve vir do documento, uma segunda consulta LLM é usada nesse armazenamento vetorial específico.  \n",
            "Traduzir a entrada do usuário em uma chamada de API.  \n",
            "Traduzir o retorno em uma saída legível para servir ao usuário.  \n",
            "O nó contém instruções sobre como isso deve ser feito.  \n",
            "Use o texto do nó como contexto para fornecer uma resposta à entrada do usuário.  \n",
            "\n",
            "Servir texto do nó  \n",
            "Buscar uma API  \n",
            "\n",
            "4. Estrutura de dados  \n",
            "O esquema JSON aumentado de saída encapsula vários elementos-chave, incluindo a lista de ações dentro do objeto exportado, os perfis utilizados para gerar perguntas e respostas, e a estrutura hierárquica de nós e seus filhos. Cada nó é atribuído um identificador exclusivo e um timestamp de atualização, gerado por nossa aplicação Mindmap, para facilitar atualizações eficientes em aplicativos consumidores downstream.  \n",
            "\n",
            "Para aumentar a funcionalidade de cada nó, introduzimos uma estrutura suplementar denominada \"ajudante\". Cada ação dentro de um nó está associada a um ajudante, que consiste em parâmetros analisados durante a geração do JSON aumentado. Internamente, nossa equipe carrega esses ajudantes no banco de dados vetorial, possibilitando sua recuperação em tempo de execução por meio de Langchain e Python.  \n",
            "\n",
            "Olhando para o futuro, uma melhoria significativa em nosso roteiro envolve integrar a ferramenta frontend Mindmap com o backend em Python responsável por gerar a estrutura JSON aumentada. Também planejamos migrar o conteúdo gerado, incluindo perguntas e respostas, para nosso banco de dados relacional juntamente com os nós. Essa integração otimizará o fluxo de trabalho, permitindo a exportação direta do JSON aumentado da aplicação Mindmap, melhorando assim a experiência do usuário e a eficiência operacional.  \n",
            "\n",
            "Tabela 2. Amostra de perfis utilizados ao gerar perguntas e respostas  \n",
            "Perfil de Pergunta  \n",
            "Descrição do perfil  \n",
            "Você é um estudante de ciência da computação que concentra sua entrada usando mensagens diretas.  \n",
            "Você é um estudante de idiomas com vocabulário rico.  \n",
            "Você é um usuário de internet desconhecido com gramática pobre que basicamente usa palavras-chave ao interagir com sistemas.  \n",
            "Você é um chatbot de organização, que precisa responder de forma formal nunca traindo os ideais da organização.  \n",
            "Utilizado em  \n",
            "Perguntas  \n",
            "Perguntas  \n",
            "Perguntas  \n",
            "Respostas  \n",
            "\n",
            "5. Conclusão  \n",
            "O trabalho que apresentamos propõe uma abordagem inovadora e coesa para integrar todo o fluxo de trabalho na criação de uma aplicação de chatbot que está intimamente ligada aos dados subjacentes. Ao aproveitar a entrada de dados estruturados e as funcionalidades avançadas de nossa ferramenta Mindmap, estabelecemos uma base sólida que não apenas suporta a geração contínua de estruturas JSON aprimoradas, mas também facilita a recuperação e processamento de dados em tempo real por meio de tecnologias avançadas como Langchain e Python.  \n",
            "\n",
            "O gerenciamento do software Mindmap é projetado para estar nas mãos dos proprietários do processo, garantindo que aqueles que compreendem as complexidades de cada processo estejam diretamente envolvidos em sua configuração e supervisão. Por exemplo, uma de nossas instâncias de chatbot é atualmente gerida pelo nosso Escritório de Gestão de Processos (EP).  \n",
            "\n",
            "Nossa colaboração com o grupo de pesquisa parceiro, o Laboratório de Inteligência Computacional Aplicada (ICA), tem sido inestimável, fornecendo insights críticos e feedback valioso para a melhoria contínua de nossa ferramenta.  \n",
            "\n",
            "Olhando para o futuro, estamos confiantes de que o desenvolvimento e aperfeiçoamento contínuos desta ferramenta aprimorará ainda mais nossa capacidade de integrar estruturas de dados complexas com aplicativos de chatbot, contribuindo, em última análise, para sistemas mais inteligentes e responsivos em toda a organização. Este trabalho não apenas demonstra o potencial da integração de dados estruturados, mas também estabelece as bases para futuras inovações no campo das soluções de chatbot empresariais.\n",
            "1\n",
            "Resumo. Ataque de replay é uma falsificação de fala utilizada na tentativa de autenticação de locutor. Redes neurais profundas têm sido propostas como métodos para detecção de áudios fraudulentos. Tendo em vista a utilização desses modelos em aplicações reais, além de bom desempenho na aprendizagem, espera-se que o modelo obtido apresente bons resultados com bases de dados distintas da utilizada no treinamento. Neste trabalho, duas abordagens foram avaliadas com três bases de dados públicas, com resultados que indicam baixa capacidade de generalização dos modelos.  \n",
            "\n",
            "1. Introdução  \n",
            "Sistemas de biometria de voz estão sendo amplamente utilizados nos mais diversos setores, como indústria automotiva, financeiro, saúde e educação [Khan et al. 2023]. Nesses casos, a autenticação do usuário é realizada por sistemas de verificação automática de locutor (Automated Speaker Verification – ASV), suscetíveis a ataques de falsificação (spoofing). Ataque de replay consiste na apresentação a um ASV da reprodução de um áudio previamente gravado, com o objetivo de validar a fala do locutor como genuína. Esse tipo de falsificação ocorre de forma passiva e é difícil de ser detectado, uma vez que o sinal reproduzido apresenta semelhanças físicas (frequências, espectros, formas de ondas) ao original [Khan et al. 2023]. Para aumentar a confiabilidade, os sistemas ASV são combinados com sistemas que identificam a fala falsificada, também chamados de antispoofing [Alzantot et al. 2019]. A série de competições Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof) promove, desde 2015, o desenvolvimento de métodos para detecção de falsificação. A cada edição, uma base de dados é disponibilizada para ser utilizada no treinamento e na validação de contramedidas aos ataques [Alzantot et al. 2019, Lee et al. 2022, Nautsch et al. 2021]. De forma geral, os métodos iniciam com a extração de atributos (features), predominantemente baseados na análise espectral do áudio, como espectrogramas, cepstrogramas, coeficientes em escala mel e variações da análise de Fourier [Khan et al. 2023]. O processo de aprendizado ocorre utilizando-se esses atributos como entradas para um classificador, tipicamente uma rede neural. Frequentemente têm sido utilizadas redes convolucionais [Chettri et al. 2018, Korshunov et al. 2018] (Convolutional Neural Network - CNN) e suas variações, como a rede convolucional leve (Light Convolutional Neural Network - LCNN) [Lavrentyeva et al. 2017, Lavrentyeva et al. 2019] e a rede convolucional residual (Residual Neural Network - ResNet) [Alzantot et al. 2019, Zhang et al. 2021]. Usualmente, para utilização em uma aplicação real, é esperado que o modelo apresente uma boa capacidade de generalização [Korshunov and Marcel 2016], isto é, que seu desempenho não seja muito diferente quando comparados os resultados obtidos em dados distintos dos usados no treinamento. O objetivo deste trabalho é estudar o desempenho de abordagens propostas à detecção de ataque de replay entre bases de dados distintas da utilizada no aprendizado das redes, de modo a contribuir para a discussão sobre generalização dos modelos.  \n",
            "\n",
            "2. Metodologia  \n",
            "Neste trabalho, duas abordagens de classificação foram avaliadas com três bases de dados públicas. O desempenho dos métodos foi mensurado pelo EER (Equal Error Rate), ponto de operação em que a taxa de falsa aceitação (False Acceptance Rate - FAR) e a taxa de falsa rejeição (False Rejection Rate - FRR) são iguais [Jain et al. 2008].  \n",
            "\n",
            "2.1. Bases de Dados  \n",
            "O estudo foi realizado com três bases de dados públicas: ASVspoof 2019, ASVspoof 2021 e REMASC. As três bases foram gravadas em inglês e, portanto, a influência da variação de idioma não pode ser explorada nesse caso. O treinamento de todos os modelos neste trabalho utilizou o conjunto de treinamento da base de dados ASVspoof 2019. As bases ASVspoof 2019 e ASVspoof 2021 são compostas por arquivos de áudio do tipo flac, com um canal e taxa de amostragem de 16kHz. Ambas possuem outros tipos de ataque além do ataque de replay, mas, para os experimentos deste trabalho, foram utilizados apenas os conjuntos relativos ao ataque denominado de acesso físico (Physical Access - PA), pois são os dados com o ataque de replay. Esses conjuntos são formados por 218.430 e 943.110 amostras, respectivamente. A base de dados REMASC foi concebida visando sistemas controlados por voz (Voice Controlled Systems - VCS), em que a coleta do áudio ocorre a uma distância maior do locutor. Seu conjunto abrange 54.712 amostras, armazenadas em arquivos do tipo wav, multicanais, amostrados a 16kHz e 44kHz [Gong et al. 2019]. Os áudios foram padronizados em monocanais a 16kHz, aplicando a média dos canais e redução da taxa de amostragem (downsampling) quando necessário. As três bases de dados disponibilizam arquivos de protocolo, indicando quais dados devem ser usados para treinamento e teste, bem como a identificação do locutor e do áudio e sua classificação original (genuíno ou falso). Além disso, fornecem metadados como tamanho do ambiente e características dos dispositivos utilizados para coleta.  \n",
            "\n",
            "2.2. Modelos  \n",
            "Neste trabalho, foram avaliados dois tipos de arquitetura de redes neurais, ResNet e LCNN. A ResNet avaliada usa como atributos de entrada a magnitude do espectro em escala logarítmica (Log-magnitude STFT - Short-Time Fourier Transform). Foi utilizado um modelo pré-treinado disponibilizado publicamente. Quanto à rede LCNN, foi utilizada uma implementação disponibilizada publicamente e usada em um estudo que avaliou diversos atributos como entrada para a rede [Lee et al. 2022, Lee 2024]. Neste caso, como não há modelo pré-treinado disponível, o treinamento foi executado utilizando os seguintes atributos: análise discreta arbitrária de Fourier (arbitrary discrete Fourier analysis - ADFA), cepstrogramas (CEPS e CEPS1724), constant Q analysis (CQA), transformada discreta de cosseno (discrete cosine transform - DCT), análise discreta de Fourier em escala Mel (Mel-scale discrete Fourier analysis - MDFA) e espectrogramas (Spec e Spec1724). Os espectrogramas e cepstrogramas foram extraídos pela transformada rápida de Fourier (Fast Fourier Transform - FFT) utilizando uma janela de Blackman com comprimento 1024 (Spec e Ceps) e 1724 (Spec1724 e Ceps1724). Ainda, como referência para comparação, foi usada a abordagem LFCC-LCNN disponibilizada como baseline para o desafio ASVspoof 2021 [Liu et al. 2023], que inclui um modelo pré-treinado.  \n",
            "\n",
            "3. Resultados e Discussão  \n",
            "Os treinamentos da abordagem RD-LCNN foram realizados ao longo de cem épocas e foi escolhido o modelo obtido a partir da época com menor valor EER no conjunto de desenvolvimento da base ASVspoof 2019. A Tabela 1 apresenta os resultados de todos os modelos avaliados nas três bases de dados, ASVspoof 2019, ASVspoof 2021 e REMASC. As colunas (a) mostram como referência os resultados relatados na literatura para os subconjuntos de desenvolvimento (Dev) e de avaliação (Eval) da base ASVspoof 2019 [Nautsch et al. 2021] e, nas demais colunas, os resultados obtidos nos experimentos deste trabalho. O modelo pré-treinado da baseline LFCC-LCNN apresentou bom desempenho com o subconjunto eval da base de dados ASVspoof 2019, com EER = 2,43%, fato esperado, uma vez que o treinamento ocorreu com o subconjunto train dessa mesma base de dados. É importante observar que a mesma superou as baselines propostas em 2019, CQCC-GMM e LFCC-GMM, que apresentaram EER = 11,04% e EER = 13,54%, respectivamente [Nautsch et al. 2021]. Já com a base ASVspoof 2021, o desempenho (EER = 45,67%) foi similar ao relatado pela literatura (EER = 44,77%) [Liu et al. 2023] e, portanto, muito pior. A abordagem ResNet para a base de dados ASVspoof 2019 do subconjunto eval apresentou resultado (EER = 7,07%) próximo ao da literatura (EER = 3,81%) e, embora 86% maior, ainda foi melhor que os das baselines do desafio de 2019: EER = 11,04% (B01 - CQCC-GMM) e EER = 13,54% (B02 - LFCC-GMM) [Nautsch et al. 2021]. Para as bases ASVspoof 2021 e REMASC observa-se um baixo desempenho, com valores EER acima de 40%. Os modelos treinados da abordagem RD-LCNN apresentaram resultados para o subconjunto dev do ASVspoof 2019 próximos aos relatados na literatura. Para o subconjunto eval da base ASVspoof 2019, o atributo “DCT” expressou a maior diferença. A inferência nas bases de dados ASVspoof 2021 e REMASC também resultaram em valores EER muito piores, maiores que 35%.  \n",
            "\n",
            "4. Conclusão  \n",
            "Os resultados obtidos para a base de dados ASVspoof 2019 demonstram bom desempenho da baseline e das abordagens experimentadas, semelhantes aos relatados pela literatura. Observa-se que a baseline obteve EER = 2,43% no ASVspoof 2019 - eval, e, embora não se tenha encontrado valor na literatura para efeito de comparação, esse resultado foi melhor que os de ambas as baselines do desafio de 2019. Os altos valores EER obtidos com as bases de dados ASVspoof 2021 e REMASC ratificam a situação exposta por [Korshunov and Marcel 2016], apontando para uma baixa capacidade de generalização de todas as abordagens processadas.\n",
            "2\n",
            "Resumo. O artigo compara modelos de síntese de fala com arquiteturas baseadas em espectrograma e fim-a-fim, com o objetivo de determinar a capacidade de clonagem de voz em cenário low-resource. Foram avaliados conjuntos de treinamento de adaptação com diferentes quantidades de fala para clonagem de uma voz alvo, e o tempo necessário para realizar o treinamento. O modelo VITS mostrou-se mais eficiente, alcançando os melhores resultados no teste de qualidade perceptual no cenário low-resource com dados no idioma português, e completou o treinamento em menos tempo, quando comparado com o Tacotron2.  1. Introdução  A síntese de fala tem sido um campo de intenso estudo e inovação ao longo dos últimos anos, com avanços significativos impulsionados pelos rápidos progressos na área de inteligência artificial generativa. Dentro deste contexto, diversas abordagens têm sido exploradas, incluindo as arquiteturas baseadas em espectrogramas e as abordagens fim-a-fim.  As arquiteturas Tacotron [Wang et al. 2017] e Tacotron2 [Shen et al. 2018] têm sido amplamente estudadas e aplicadas, demonstrando a capacidade de converter texto em fala natural por meio da geração de espectrogramas intermediários, que são posteriormente transformados em sinais de fala através de vocoders, como as arquiteturas WaveNet [van den Oord et al. 2016] e HiFi-GAN [Kong et al. 2020]. Apesar dos resultados promissores, esses modelos frequentemente requerem grandes quantidades de dados e longos períodos de treinamento para atingir um nível satisfatório de qualidade e naturalidade na fala.  As abordagens mais recentes de síntese de fala, como o modelo VITS (do inglês, Variational Inference Text-to-Speech) [Kim et al. 2021], propõem uma estratégia fim-a-fim que elimina a necessidade de um estágio intermediário explícito de geração do espectrograma, combinando de forma eficaz a geração e a codificação do sinal de fala em um único fluxo de trabalho. Este método tem mostrado potencial em reduzir significativamente a quantidade de dados necessários para o treinamento, bem como o tempo total para alcançar resultados de alta qualidade.  A eficiência da síntese de fala em cenários com recursos limitados (low-resource) é uma área de interesse crescente, especialmente para idiomas com menor disponibilidade de dados anotados, como o caso do português. Trabalhos recentes têm investigado a eficácia de diferentes modelos em condições low-resource, abordando desafios específicos como a qualidade da fala sintetizada, a adaptabilidade de modelos pré-treinados para novos falantes e a eficiência computacional do processo de treinamento [Lux et al. 2022].  O objetivo deste artigo é comparar as arquiteturas baseadas em espectrogramas e fim-a-fim no contexto de clonagem de voz em português, com ênfase no desempenho do VITS versus o Tacotron2. O objetivo é comparar os modelos em cenários low-resource e quantificar o número mínimo de dados e tempo de treinamento necessários para atingir resultados de alta qualidade. Os resultados baseiam-se em métricas de qualidade objetiva e subjetiva, e na análise do tempo de treinamento. Esperamos fornecer insights práticos para a escolha e implementação de modelos de síntese de fala com voz personalizada em condições de dados restritos, contribuindo para a eficiência e a acessibilidade da tecnologia de síntese de fala em uma ampla gama de aplicações para o idioma português do Brasil.  2. Metodologia  O treinamento dos modelos foi realizado utilizando duas bases de fala no idioma Português Brasileiro: (i) o TTS-Portuguese Corpus [Casanova et al. 2022], composto por textos de domínio público provenientes tanto da Wikipédia quanto do Chatterbot-corpus (um corpus criado originalmente para a construção de chatbots), contendo aproximadamente 10 horas e 28 minutos de fala de um único locutor masculino, gravada com taxa de amostragem de 48 kHz e 16 bits, tendo 3.632 áudios no formato WAV linear, com um range de duração de 0,67 a 50,08 segundos (todos os clipes de áudio com duração superior a 20 segundos foram removidos do treinamento); (ii) uma base de fala proprietária do CPQD composta por um locutor masculino contendo 20 minutos de fala, gravada com taxa de amostragem de 48kHz, 16 bits e formato PCM linear, contendo os arquivos de áudio e as transcrições ortográficas correspondentes.  O treinamento foi realizado a partir do repositório do VITS¹, que foi adaptado para a inclusão de fonemas do idioma português do Brasil, realizado através do uso do módulo Phonemizer² em conjunto com a pipeline de preparação de dados.  O treinamento dos modelos base ocorreu ao longo de 80 horas e 2.000 épocas no dataset TTS-Portuguese Corpus. A partir do último checkpoint gerado pelo modelo base, foram realizados fine-tunings trocando os dados de treinamento pela base proprietária com a voz do locutor masculino, usando conjuntos de treinamento com 20, 15, 10 e 5 minutos de fala visando avaliar a quantidade mínima de dados necessários para obter síntese de boa qualidade. O objetivo do fine-tuning é adaptar o modelo base para as características da voz alvo, ou seja, realizar a clonagem de voz. Após apenas 1 hora de treinamento de fine-tuning usando 20 minutos de fala, foram observados resultados de alta qualidade tanto no VITS como no Tacotron2. A qualidade melhorou ainda mais após 20 horas de treinamento. Ambos utilizaram o vocoder HiFi-GAN, sendo que no caso do Tacotron2 o vocoder foi treinado de forma independente. Para os conjuntos de treinamento menores, a seção 3 apresenta os resultados obtidos.  ¹https://github.com/jaywalnut310/vits/ ²https://pypi.org/project/phonemizer/3.0.1/   3. Resultados  Para avaliar a qualidade da fala sintetizada resultante foram utilizadas medidas objetivas e subjetivas. As métricas objetivas foram o MCD (do inglês, Mel-Cepstral Distortion) e o F0 RMSE (do inglês, Log-F0 Root Mean Square Error) [Hayashi et al. 2021]. Para a avaliação subjetiva foi utilizada a métrica MOS (Mean Opinion Score), em um experimento que contou com 15 avaliadores não especialistas.  A métrica MCD, calculada por meio do repositório TTS Objective Metrics³, quantifica a distância entre dois sinais de fala. Quanto menor o valor MCD, mais semelhantes são as vozes. A qualidade da voz sintetizada foi avaliada com base no conjunto de teste, com frases separadas para validação. Ao comparar a voz sintetizada resultante do modelo de fine-tuning obtido com 20 minutos, com a voz original gravada, obteve-se valores de MCD entre 1.6 e 1.78. A métrica MCD mostra valores próximos de 0, indicando que o modelo é capaz de gerar fala sintetizada próxima da fala gravada. Para o F0 RMSE, aplicada nas mesmas sentenças, foram obtidos valores entre 0.18 e 0.34. Os resultados reforçam a alta qualidade da fala sintetizada.  3.1. Avaliação Subjetiva  Para a avaliação subjetiva foi utilizado o servidor webMUSHRA⁴. Um grupo de 15 avaliadores não especialistas ouviram um conjunto de amostras e atribuiram notas de 0 a 100 com base na naturalidade da voz, sendo 0 nada natural e 100 muito natural. Esse processo permitiu realizar uma análise subjetiva da qualidade do áudio sintetizado, proporcionando uma análise mais fidedigna da percepção humana em relação ao desempenho dos modelos. As avaliações mostram uma melhor qualidade do VITS em relação ao Tacotron2. A Figura 1 mostra o boxplot com os dados do teste subjetivo, utilizando áudios sintetizados por modelos obtidos através do fine-tuning com diferentes conjuntos de treinamento da voz alvo. Na legenda, 400 representa o conjunto com 20 minutos de fala, 300 indica 15 minutos, 200 indica 10 minutos e 100 indica o conjunto com 5 minutos de fala.  Os resultados indicam que o VITS (C1) consistentemente recebeu avaliações mais altas em comparação ao Tacotron2 (C2). O desvio padrão menor do VITS em comparação ao Tacotron2 em todos os conjuntos de treinamento indica que as opiniões dos usuários sobre a qualidade do áudio gerado pelo VITS são mais consistentes e robustas.  No teste realizado com o conjunto contendo 5 minutos de fala de treinamento, o VITS teve uma média de 71,49 enquanto o Tacotron2 teve 67,49. Essa diferença foi consistente em todos os conjuntos de treinamento (20, 15, 10 e 5 minutos). No entanto, a diferença aumenta com um volume maior de dados, sugerindo que o VITS não apenas produz áudio de melhor qualidade, mas também que melhora mais conforme a quantidade de dados de treinamento aumenta.  4. Conclusão  O objetivo principal deste trabalho foi comparar as arquiteturas de síntese de fala generativa com abordagens de espectrograma (Tacotron2) e fim-a-fim (VITS) em cenários low-resource, com uso de até 5 minutos de fala no treinamento de fine-tuning, para clonagem de voz; ou seja, avaliar a capacidade de adaptação dos modelos base pré-treinados fazendo uso de dados limitados de uma nova voz personalizada.  O modelo VITS, quando treinado com 20 minutos, mostrou resultados com alta qualidade após apenas 1 hora de treinamento. Por outro lado, o Tacotron2, sob as mesmas condições, apresentou maior variabilidade e menor consistência na qualidade do áudio sintetizado. Mesmo quando treinado com 5 minutos o VITS apresentou boa qualidade e baixa variância. Ao comparar o tempo de treinamento, o modelo VITS mostrou-se mais eficiente, alcançando bons resultados em menos tempo e com menos dados em relação ao Tacotron2.  Os resultados indicam que o VITS não só oferece uma síntese de fala de melhor qualidade, com maior similaridade à voz original e menor variância entre as amostras sintetizadas, mas também é mais eficiente em termos de tempo de treinamento em cenários low-resource.\n",
            "3\n",
            "Resumo. Todo texto pode refletir as opiniões de seu autor, e essas opiniões, especialmente em contextos políticos, estão frequentemente ligadas a valores humanos específicos que elas tanto alcançam quanto restringem. Identificar esses valores pode fornecer aos formuladores de políticas insights mais profundos sobre os fatores subjacentes que influenciam o discurso público e a tomada de decisões. Embora os atuais modelos de linguagem de grande porte (LLMs) tenham mostrado promessas em várias tarefas, nenhum modelo único pode generalizar suficientemente para se destacar em tarefas como a detecção de valores humanos. Neste trabalho, utilizamos dados da tarefa de Detecção de Valores Humanos no CLEF 2024 e propomos aproveitar múltiplos conjuntos de LLMs para aprimorar a identificação de valores humanos em texto. Nossos resultados mostraram que os modelos em conjunto alcançaram pontuações F1 mais altas do que todos os modelos de linha de base, sugerindo que combinar múltiplos modelos pode oferecer um desempenho comparável a modelos muito grandes, mas com requisitos de memória muito menores.\n",
            "\n",
            "1. Introdução\n",
            "As pessoas podem concordar ou discordar sobre inúmeros tópicos, mesmo quando usam as mesmas informações. Essas diferenças surgem, em grande parte, de suas crenças individuais sobre o que vale a pena buscar, um conceito referido como (valores humanos) valores. Valores humanos podem entrar em conflito ou estar alinhados, levando a uma ampla gama de opiniões sobre questões controversas. Essa divergência é uma das razões para a formação de diferentes partidos políticos, cada um representando os valores de grupos específicos [Kiesel et al. 2022]. Dada sua importância, o estudo de valores humanos abrange múltiplas disciplinas, incluindo ciências sociais [Schwartz 1994] e argumentação formal [Bench-Capon 2003]. Pesquisadores se concentraram em vários aspectos, como classificar valores, detectá-los em texto e entender seu impacto social. Na ciência da computação, há um corpo crescente de trabalho dedicado à detecção de valores e reconhecimento de emoções a partir do texto [Dellaert et al. 1996, Tariq et al. 2019, Ammanabrolu et al. 2022]. Essas tarefas são desafiadoras e, ainda assim, têm um amplo espectro de aplicações, como ajudar formuladores de políticas a avaliar o sentimento público, detectar alinhamento político e mais.\n",
            "\n",
            "Neste trabalho, nosso objetivo é avançar no campo da detecção de valores humanos aproveitando múltiplos conjuntos de Modelos de Linguagem Grande (LLMs) para identificar esses valores em texto e melhorar o desempenho do modelo. Adotamos a taxonomia de valores apresentada em [Schwartz et al. 2012], que categoriza valores em dois tipos para cada valor—alcançados e restringidos. No entanto, nossa tarefa foca exclusivamente na identificação da presença de um valor em uma frase, portanto somamos as versões alcançadas e restritas para determinar se uma frase contém um valor particular. Conduzimos este estudo com um conjunto de dados do CLEF 2024. Os dados são altamente desbalanceados, tornando este um problema de classificação desafiador.\n",
            "\n",
            "2. Contexto e Trabalhos Relacionados\n",
            "A Detecção de Valores Humanos ganhou atenção recentemente, particularmente como foco de uma tarefa compartilhada no CLEF 2024. Esta tarefa teve como objetivo detectar valores humanos na fala, atraindo a participação de 20 equipes. Os resultados dessa competição, incluindo as métricas de desempenho de cada equipe, estão detalhados em [Kiesel et al. 2022]. Esses esforços ressaltam a complexidade de detectar valores humanos sutis em texto e destacam a necessidade de modelos avançados que possam capturar com precisão tais sutilezas. LLMs revolucionaram tarefas de NLP em vários domínios. A introdução de arquiteturas Transformer [Vaswani et al. 2017] marcou um salto significativo, levando ao desenvolvimento de modelos pré-treinados poderosos, como BERT [Devlin et al. 2019], RoBERTa [Liu et al. 2019] e DeBERTa [He et al. 2021]. Esses modelos têm sido altamente eficazes em classificação de texto, análise de sentimento e geração de conteúdo, reduzindo significativamente a necessidade de treinar modelos do zero. Vários estudos [Xian et al. 2023, Hoang et al. 2019, Sun et al. 2019, Sobhanam e Prakash 2023] demonstraram a eficácia do ajuste fino desses modelos para tarefas específicas, mostrando sua versatilidade e robustez ao enfrentar desafios diversos de NLP. O Aprendizado em Conjunto é uma técnica bem estabelecida em aprendizado de máquina, frequentemente empregada para melhorar o desempenho preditivo ao combinar múltiplos modelos. Tradicionalmente associado a árvores de decisão [Quinlan 1986], o aprendizado em conjunto evoluiu para incorporar várias estruturas, incluindo aquelas envolvendo LLMs [Jiang et al. 2023].\n",
            "\n",
            "3. Metodologia\n",
            "Os dados usados neste estudo vêm da tarefa de Detecção de Valores Humanos no CLEF (Conferência e Laboratórios do Fórum de Avaliação) 2024 (ValueEval’24) [Kiesel et al. 2024a] e consistem em aproximadamente 3 mil textos anotados por humanos contendo mais de 73 mil frases. A anotação associada a cada frase indica se um valor humano específico é “alcançado” e “restringido”. Um total de 19 valores humanos são analisados. Cada coluna recebe o valor 0, 0,5 ou 1, indicando se a frase não contém o valor humano, contém parcialmente ou contém totalmente, respectivamente. Este estudo focou no conjunto de dados em inglês. Todos os modelos foram otimizados para a pontuação F1-Macro. Para abordar a tarefa como um problema de classificação multirrótulo, combinamos as colunas “alcançado” e “restringido” no arquivo de rótulos, somando seus valores para determinar se um valor humano específico está presente em uma frase (0 para falso, 1 para verdadeiro). O resultado foi um array de 19 valores booleanos para cada frase, que foram usados como entradas para o ajuste fino do modelo. Assim, cada valor humano representa uma classe e o modelo preditivo pode atribuir mais de uma classe a uma dada frase. Embora o valor Humildade tenha sido removido por muitos participantes do CLEF devido à sua escassez no conjunto de treinamento (presente em apenas 0,2% das frases), nós o mantivemos, considerando importante prever até mesmo valores raros para garantir um desempenho abrangente em todos os valores. Usando o conjunto de dados de treinamento, ajustamos seis modelos: versões base e grandes de BERT [Devlin et al. 2019], RoBERTa [Liu et al. 2019] e DeBERTa [He et al. 2021]. Após o ajuste fino, utilizamos os dados de validação para criar um novo conjunto de dados que incluísse as frases, probabilidades de predição para cada classe e predições binárias indicando se um valor está presente em uma frase. Os rótulos verdadeiros também foram incluídos no conjunto de dados para permitir avaliações. Cinco abordagens diferentes de conjunto foram utilizadas para combinar as saídas dos modelos:\n",
            "\n",
            "• prob-equal: As probabilidades de cada modelo foram somadas e, em seguida, médias. Um limiar de 0,2 foi aplicado.\n",
            "• prob-large-double: As probabilidades dos modelos base foram somadas e as probabilidades dos modelos grandes foram dobradas antes de serem somadas. O total foi dividido pelo número de votos (nove) e um limiar de 0,2 foi aplicado.\n",
            "• preds-majority: As predições binárias de todos os modelos foram somadas, com um limiar de 2 aplicado para prever um valor como presente se pelo menos dois modelos o identificaram.\n",
            "• preds-large-double: As predições binárias foram somadas, com modelos grandes recebendo dois votos cada. Um limiar de 2 foi usado, significando que um valor seria previsto como presente se um modelo grande ou dois modelos base o identificassem.\n",
            "• prob-weight-macro-f1: As probabilidades previstas por cada modelo foram ponderadas por suas pontuações F1 no conjunto de validação. As probabilidades ponderadas foram então somadas e normalizadas, seguidas pela aplicação de um limiar de 0,2. Para reprodutibilidade, todos os experimentos, diagramas de conjunto e scripts usados para o ajuste fino estão disponíveis no GitHub, com uma semente aleatória fixa para todas as bibliotecas. Detalhes de implementação e resultados adicionais também estão em nosso repositório. Os modelos usados neste estudo são acessíveis publicamente e podem ser baixados do HuggingFace.\n",
            "\n",
            "4. Resultados\n",
            "Os resultados são apresentados na Tabela 1. O modelo RoBERTa Large alcançou a maior precisão entre os modelos individuais, o que está alinhado com as expectativas, dado o tamanho maior do modelo. No entanto, uma vez que a métrica principal para a seleção de modelos durante o treinamento foi a pontuação macro F1 em vez da precisão, não é surpreendente que modelos maiores e modelos em conjunto não mostrem constantemente maior precisão.\n",
            "\n",
            "Tabela 1. Resultados de F1 e Precisão para nossos modelos e linhas de base. ⋆ significa que o modelo é um conjunto, e † significa que usou a versão de conjunto de dados multilíngue.\n",
            "Modelo  Macro F1   Precisão\n",
            "Modelos Base   Conjuntos   Linhas de Base\n",
            "BERT-base-uncased  BERT-large  RoBERTa-base  RoBERTa-large  DeBERTa-base  DeBERTa-large  \n",
            "prob-equal  prob-large-double  prob-weight-macro-f1  preds-majority  preds-large-double  \n",
            "[Legkas et al. 2024] †  [Yunis 2024] ⋆ †  [Yeste et al. 2024]  \n",
            " 0.160 0.263 0.248 0.282 0.274 0.295  \n",
            " 0.330 0.326 0.330 0.318 0.319  \n",
            " 0.390 0.350 0.280  \n",
            " 0.502 0.482 0.485 0.508 0.480 0.507  \n",
            " 0.447 0.438 0.445 0.484 0.418  – – –  \n",
            "\n",
            "A Tabela 1 também compara nossos resultados com os três melhores modelos das submissões do CLEF 2024. Notavelmente, nossas abordagens em conjunto, especificamente prob-weight-macro-f1 e prob-equal, apresentaram desempenho apenas 0,03 e 0,02 abaixo dos modelos com melhor pontuação da conferência, que utilizaram modelos XLM e o conjunto de dados multilíngue. A abordagem de Arthur Schopenhauer [Yunis 2024] aproveitou um conjunto de modelos DeBERTa-v2-xxlarge e xlmRoBERTa-large. Da mesma forma, Hierocles de Alexandria [Legkas et al. 2024] empregou tanto os conjuntos de dados multilíngues quanto os traduzidos para o inglês, incorporando informações da sequência de frases e ajustando um modelo XLM-RoBERTa-xl. Finalmente, a equipe Filó de Alexandria [Yeste et al. 2024] ajustou um modelo DeBERTa especificamente para esta tarefa. Ao observar as pontuações para cada um dos 19 valores humanos, vemos que nossos conjuntos demonstraram desempenho competitivo, correspondendo de perto aos resultados dos modelos XLM e superando o modelo DeBERTa-base em quase todos os valores. Essa tarefa foi particularmente desafiadora devido ao significativo desbalanceamento de classes no conjunto de dados, com quase 50% das instâncias do conjunto de teste não contendo nenhum dos 19 valores. Esse desbalanceamento distorce as predições em direção a falsos negativos, resultando em pontuações F1 mais baixas, apesar da alta precisão, pois os modelos podem prever corretamente a ausência de valores devido à sua prevalência. No geral, os resultados demonstram que os modelos em conjunto podem alcançar um desempenho comparável ao de modelos muito grandes, mesmo ao utilizar modelos que requerem menos recursos computacionais. Embora treinar um modelo XLM-DeBERTa não fosse viável no hardware usado para este estudo devido a limitações de memória, nossos conjuntos ainda alcançaram uma sólida pontuação macro F1. Especificamente, o melhor modelo em conjunto melhorou a pontuação macro F1 de 0.295 (a mais alta entre os modelos base) para 0.33, destacando a eficácia dos métodos em conjunto em melhorar o desempenho do modelo neste contexto.\n",
            "\n",
            "5. Conclusão\n",
            "Neste estudo, lidamos com a complexa tarefa de identificar valores humanos em texto, um desafio crucial para entender os valores que moldam o discurso público e a tomada de decisões. Aproveitando múltiplos conjuntos de LLMs, demonstramos que abordagens baseadas em conjunto poderiam melhorar significativamente o desempenho do modelo individual nessa tarefa. Isso sugere que, em vez de depender exclusivamente de um único LLM poderoso, os métodos em conjunto oferecem uma solução mais robusta e eficaz para tarefas complexas de NLP. Apesar das capacidades avançadas de modelos como o GPT-4.0, esses modelos ainda lutam para entregar constantemente um desempenho satisfatório neste domínio. Por exemplo, no ValueEval’24, uma equipe usando o GPT-4.0 para classificação zero-shot alcançou uma pontuação F1 de 0.25 [Kiesel et al. 2024b], que é inferior ao desempenho de nossas abordagens em conjunto. Isso destaca os desafios inerentes à detecção de valores humanos, onde as nuances da linguagem e do contexto muitas vezes superam a capacidade de um único modelo, não importa o quão sofisticado. Trabalhos futuros incluirão uma análise qualitativa para entender melhor os erros cometidos pelos modelos e melhorar as abordagens propostas, reforçando o potencial do aprendizado em conjunto como uma estratégia chave na promoção do campo.\n",
            "\n",
            "Agradecimentos. Este trabalho foi parcialmente financiado pelo CNPq-Brasil e Capes Código de Financiamento 001.\n",
            "4\n",
            "Resumo. Este artigo examina o desempenho da API Perspective, desenvolvida pela Jigsaw, na detecção de discurso de ódio em português. Embora a API Perspective suporte múltiplos idiomas, suas métricas de desempenho são frequentemente agregadas, ocultando detalhes específicos. Nosso estudo revela que o score AUC-ROC da API para o português é significativamente mais baixo em comparação ao inglês (0,744 vs. 0,942). Para abordar isso, desenvolvemos um modelo de classificador BERT treinado em um conjunto de dados de discurso de ódio no Twitter em português. Nosso modelo, com apenas 100 mensagens em seu conjunto de treinamento, superou a API Perspective. Esses achados destacam a necessidade de métricas de desempenho mais granulares e sugerem que modelos personalizados podem oferecer melhores soluções para idiomas específicos.\n",
            "\n",
            "1. Introdução\n",
            "A API Perspective é uma ferramenta projetada para identificar e mitigar linguagem tóxica online [Lees et al. 2022]. Usando modelos avançados de aprendizado de máquina e Processamento de Linguagem Natural (NLP), a API Perspective analisa conteúdo textual para detectar várias formas de discurso nocivo, incluindo ameaças, insultos e discurso de ódio. É considerada de ponta na detecção de toxicidade e utilizada por várias plataformas, como Reddit, The New York Times, The Wall Street Journal e EL PAÍS. \n",
            "\n",
            "Apesar de sua ampla adoção e do suposto suporte multilíngue, incluindo o português, o desempenho real da API Perspective em diferentes idiomas continua incerto. A documentação oficial e os artigos de pesquisa associados frequentemente relatam métricas de desempenho agregando dados de múltiplos idiomas dentro de um conjunto de dados multilingue. Essa agregação oculta as métricas de desempenho individuais para o português, tornando difícil avaliar a eficácia da API nesse idioma específico. A falta de transparência nas métricas de desempenho específicas por idioma levanta preocupações sobre a confiabilidade da API quando aplicada a textos em idiomas que não sejam o inglês.\n",
            "\n",
            "A aceitação generalizada da API Perspective como uma ferramenta líder na detecção de discurso de ódio, combinada com seu alegado suporte ao português, sugere que profissionais podem adotá-la prontamente para trabalhar em contextos de língua portuguesa. No entanto, se o desempenho da API em português não for equivalente ao seu desempenho em inglês ou aos resultados agregados, isso pode levar a análises e conclusões imprecisas, particularmente em áreas como ciências sociais computacionais, onde a detecção precisa de linguagem é crítica. O potencial para resultados enganosos é especialmente preocupante quando a confiabilidade da ferramenta em português é considerada garantida com base em seu desempenho em outros idiomas.\n",
            "\n",
            "Este artigo aborda essa lacuna avaliando o desempenho da API Perspective na detecção de discurso de ódio especificamente em português. Ele também avalia a viabilidade de desenvolver uma ferramenta de detecção de discurso de ódio personalizada adaptada ao português. Para guiar nossa investigação, formulamos as seguintes perguntas de pesquisa:\n",
            "RQ1: Quão bem a API Perspective se sai na detecção de discurso de ódio em português?\n",
            "RQ2: Para o português, é mais eficaz e eficiente usar uma ferramenta personalizada em vez de confiar em soluções existentes como a API Perspective? Se sim, quanto esforço seria necessário para construí-la?\n",
            "\n",
            "Para responder a essas perguntas, avaliamos as métricas da API Perspective usando um conjunto de dados de discurso de ódio no Twitter em português. Em seguida, comparamos isso a métricas obtidas em um conjunto de dados semelhante em inglês, considerando classificação, data de coleta e conteúdo. Nossos achados revelaram que o desempenho da API Perspective em português foi significativamente pior do que em inglês. Com base nessa percepção, desenvolvemos nossa versão de um classificador BERT para detectar discurso de ódio em português. Notavelmente, com apenas 100 mensagens, o modelo BERT superou a API Perspective na detecção de discurso de ódio em português. Em contraste, o modelo BERT treinado com o conjunto de dados em inglês não superou o desempenho da API Perspective.\n",
            "\n",
            "2. Trabalhos Relacionados\n",
            "Esta seção revisa estudos relacionados a modelos de detecção de discurso de ódio e comparações de desempenho específicas por idioma.\n",
            "\n",
            "2.1. Comparações de Modelos\n",
            "Modelos de transformador multilíngues, como o BERT e suas variantes, têm ganhado atenção significativa na detecção de discurso de ódio em vários idiomas. Por exemplo, [Roy et al. 2021] demonstraram a superioridade de modelos de transformadores ajustados para lidar com dados multilíngues, evidenciando sua eficácia em comparação com abordagens mais generalizadas, como a API Perspective. Isso destaca o potencial de modelos especializados para superar soluções mais amplas e padronizadas.\n",
            "\n",
            "Outra contribuição notável é de [Kennedy et al. 2020], que introduziram uma abordagem híbrida que combina medição facetada de Rasch com aprendizado profundo multitarefa. Essa metodologia melhora tanto a interpretabilidade quanto a precisão da detecção de discurso de ódio ao integrar técnicas psicométricas tradicionais com modelos avançados de aprendizado profundo. Comparado à API Perspective, que se baseia em algoritmos mais generalizados, a abordagem de Kennedy et al. oferece uma compreensão mais sutil das variações linguísticas e da intensidade do discurso de ódio.\n",
            "\n",
            "2.2. Comparações Específicas por Idioma\n",
            "No artigo introdutório da Perspective [Lees et al. 2022], os desenvolvedores relataram scores AUC-ROC de 0,98 para inglês, 0,91 para russo e 0,87 para um grupo de dez outros idiomas. Esses resultados destacam uma disparidade na eficácia da API entre os idiomas, levantando preocupações sobre sua aplicabilidade em contextos não anglófonos. Estudos adicionais confirmaram que essas preocupações eram relevantes. Por exemplo, [Nogara et al. 2024] analisaram o uso da API Perspective em alemão e descobriram que a API tende a classificar textos em alemão como significativamente mais tóxicos do que suas contrapartes em inglês. Essa descoberta sublinha os potenciais preconceitos e imprecisões que surgem ao aplicar a API a idiomas que não sejam o inglês, destacando a necessidade de investigações adicionais sobre suas capacidades multilíngues.\n",
            "\n",
            "O estudo seminal sobre o uso da API Perspective em português foi conduzido por [Kobellarz e Silva 2022]. Eles compararam textos idênticos em português e inglês usando a API e concluíram que ela apresenta melhor desempenho ao analisar textos em seu idioma original. Isso sugere que a API Perspective pode ser menos eficaz na detecção de nuances em conteúdo traduzido ou em língua não nativa. Baseando-se nesse estudo, [Lima et al. 2024] desenvolveram um conjunto de dados rotulados manualmente de mensagens tóxicas em português e avaliaram a API em relação a esse conjunto. Seus achados revelaram discrepâncias significativas, enfatizando a necessidade de a API passar por um treinamento mais focado em conteúdo na língua portuguesa para melhorar sua precisão e confiabilidade na detecção de discurso de ódio.\n",
            "\n",
            "Além disso, [Silva et al. 2023] propuseram conjuntos de dados padronizados e benchmarks para análise de sentimento em inglês, abordando especificamente os desafios de automatizar o processo de desenvolvimento. Embora seu foco tenha sido no inglês, os métodos e padrões que defendem podem fornecer insights valiosos para melhorar o desempenho da API Perspective em outros idiomas, incluindo o português.\n",
            "\n",
            "2.3. Lacuna de Pesquisa\n",
            "Apesar do uso generalizado e da validação da API Perspective para detecção de discurso de ódio em vários idiomas, uma lacuna significativa permanece em sua avaliação de desempenho para idiomas menos comumente estudados, como o português. Pesquisas anteriores mostraram o forte desempenho da API em inglês e outros idiomas principais, evidenciado por altas pontuações de AUC-ROC e métricas robustas. No entanto, avaliações detalhadas para idiomas menos representados em seus conjuntos de dados de treinamento são escassas.\n",
            "\n",
            "Para abordar essas lacunas, conduzimos avaliações focadas do desempenho da API Perspective para idiomas individuais. Nosso estudo destaca as vantagens de desenvolver modelos personalizados adaptados a idiomas específicos, como o português, oferecendo detecções de discurso de ódio mais precisas e confiáveis. Isso enfatiza a necessidade de considerar soluções personalizadas ao lado de modelos multilíngues existentes para melhorar a eficácia da detecção de discurso de ódio em diversos idiomas.\n",
            "\n",
            "3. Metodologia\n",
            "Nesta seção, discutimos a seleção do conjunto de dados, a avaliação da API Perspective e o ajuste fino dos modelos BERThs.\n",
            "\n",
            "3.1. Conjunto de Dados\n",
            "Nossa análise exigiu um conjunto de dados de discurso de ódio em português e um conjunto de dados semelhante em inglês. Em vez de rotular manualmente as mensagens, para fins de comparação imparcial, que pode ser custoso e suscetível a erros, optamos por usar dois conjuntos de dados de discurso de ódio no Twitter bem conhecidos: [Fortuna et al. 2019] e o conjunto de dados de Detecção de Discurso de Ódio Automatizada e o Problema da Linguagem Ofensiva [Davidson et al. 2017]. \n",
            "\n",
            "Ambos os conjuntos de dados foram criados usando a mesma metodologia para classificar mensagens. Isso envolveu identificar e minerar contas propensas a postarem tweets relacionados ao discurso de ódio em 2017. Os tweets foram então classificados como contendo ou não discurso de ódio, o que corresponde à saída da API Perspective. \n",
            "\n",
            "Os conjuntos de dados originais em português e inglês variam significativamente em tamanho e proporção de mensagens de discurso de ódio. O conjunto de dados em português inclui 5.934 mensagens não tóxicas e 1.607 mensagens tóxicas, resultando em uma proporção de aproximadamente 3,7 mensagens não tóxicas para cada mensagem tóxica. Por outro lado, o conjunto de dados em inglês consistia inicialmente em 25.000 tweets classificados, com 3.280 mensagens não tóxicas e 21.720 mensagens tóxicas.\n",
            "\n",
            "Para uma comparação justa das pontuações de classificação entre os dois conjuntos de dados, equilibramos suas proporções, utilizando o conjunto de dados em português como a linha de base, já que este será o principal objeto de nosso estudo. Selecionando uma amostra aleatória de mensagens do conjunto de dados em inglês que refletisse a mesma proporção, obtivemos um conjunto final de dados em inglês composto por 3.280 mensagens não tóxicas e 886 mensagens tóxicas, com a mesma proporção de mensagens não tóxicas para mensagens tóxicas de aproximadamente 3,7.\n",
            "\n",
            "3.2. Comparando os Resultados da API Perspective\n",
            "Para comparar os modelos da Perspective para inglês e português, selecionamos amostras aleatórias de mensagens e as analisamos quanto à toxicidade usando a API Perspective. Focamos no atributo de Toxicidade, que é amplamente utilizado na literatura devido à sua robustez e compatibilidade com ambos os conjuntos de dados em análise. A análise foi conduzida em junho de 2024, e a API Perspective forneceu scores de toxicidade para cada amostra nos dois conjuntos de dados. \n",
            "\n",
            "Cada mensagem recebeu um score de toxicidade variando de 0 a 1, onde 0 representa uma probabilidade muito baixa de toxicidade e 1 indica uma probabilidade muito alta. Para garantir a comparação mais precisa possível, otimizamos o limiar para a classificação de toxicidade, maximizando a pontuação F1 para cada conjunto de dados individualmente. O limiar ótimo foi determinado em 0,48 para o conjunto de dados em português e 0,59 para o conjunto de dados em inglês, refletindo as diferentes calibrações necessárias para os dois idiomas.\n",
            "\n",
            "3.3. Modelo BERThs (BERT para discurso de ódio)\n",
            "Nesta seção, mostramos como ajustamos nosso próprio classificador BERT para detecção de discurso de ódio, chamado BERThs. O BERThs foi ajustado usando tanto um conjunto de dados em português quanto um em inglês.\n",
            "\n",
            "Inicialmente, o objetivo do modelo, especialmente o em português, não era alcançar a maior precisão possível, mas ser fácil de replicar. Isso nos ajudará a mostrar se um modelo ajustado simples pode ser mais eficaz que a API Perspective em português. \n",
            "\n",
            "Para ajustar o BERThs-Pt, utilizamos o BERTimbau [Souza et al. 2020] como modelo base, já que ele é pré-treinado em português e melhor se adapta à nossa tarefa. Dado o pequeno tamanho do corpus anotado, ajustamos e avaliamos o modelo 30 vezes usando diferentes conjuntos estratificados não sobrepostos: conjuntos de treinamento, validação e teste, compostos por 80%, 10% e 10% do conjunto de dados rotulado. Cada divisão manteve a distribuição de classes original de aproximadamente 21,3% de mensagens tóxicas e 78,7% de mensagens não tóxicas. Os mesmos conjuntos de testes foram usados para avaliar a API Perspective. Essa abordagem garantiu robustez e preveniu problemas como treinar em um conjunto de mensagens totalmente tóxicas, o que poderia levar a resultados não confiáveis. \n",
            "\n",
            "Para determinar o número mínimo de mensagens necessárias para que nosso classificador superasse a API Perspective, inicialmente utilizamos apenas 10 mensagens do conjunto de treinamento para ajustar o BERTimbau. Adicionamos gradualmente mais 10 mensagens ao conjunto de treinamento após cada iteração, até que o BERThs alcançasse uma pontuação AUC melhor que a da Perspective. A métrica AUC foi escolhida porque era a única métrica relatada para o português no artigo da API Perspective. Depois disso, adicionamos 200 novas mensagens ao conjunto de treinamento em cada iteração subsequente, até que todas as mensagens fossem incluídas, destacando o desempenho máximo que nosso modelo poderia alcançar. \n",
            "\n",
            "O ajuste fino foi realizado usando a biblioteca PyTorch [Paszke et al. 2019], com o otimizador AdamW [Loshchilov e Hutter 2017] e uma taxa de aprendizado de 5 × 10−6. Os limiares de classificação foram estabelecidos com base nas probabilidades de saída do modelo, definidos como os limiares que geraram a melhor pontuação média F1 em nosso conjunto de validação.\n",
            "\n",
            "Para o BERThs-En, utilizamos o modelo BERT uncased [Devlin et al. 2019], que é otimizado para processamento da língua inglesa. O procedimento de ajuste seguido foi a mesma abordagem geral utilizada para a variante em português, mas com modificações específicas para contar com o desempenho superior da API Perspective em textos em inglês. Especificamente, em vez de aumentar gradualmente o conjunto de treinamento em 10 mensagens e, em seguida, 200 mensagens por iteração, optamos por aumentar diretamente o conjunto de treinamento em 200 mensagens a cada iteração.\n",
            "\n",
            "4. Resultados\n",
            "Esta seção apresenta as métricas de previsão da API Perspective para os conjuntos de dados em inglês e português e as compara com o BERThs. Os modelos foram ajustados em uma GPU NVIDIA RTX 4090. Como os modelos foram treinados 30 vezes com diferentes amostras de dados, os resultados nesta seção apresentam a média seguida do desvio padrão.\n",
            "\n",
            "4.1. Desempenho da Perspective\n",
            "A Tabela 1 mostra as métricas de desempenho da API Perspective nos conjuntos de dados em inglês e português. Ela destaca uma disparidade significativa na eficácia do modelo entre os dois idiomas, com o conjunto de dados em inglês alcançando consistentemente pontuações mais altas em todas as métricas. Notavelmente, a precisão, a recuperação, a pontuação F1 e o AUC-ROC são consideravelmente mais baixos para o conjunto de dados em português, sugerindo que a capacidade do modelo de classificar com precisão conteúdo tóxico está comprometida em português.\n",
            "\n",
            "Observe que a pontuação F1 – que serve como uma medida equilibrada da precisão e recuperação de um modelo em tarefas de classificação – é quase 35 pontos percentuais mais baixa em português. Além disso, tipicamente, existe uma troca entre as métricas de precisão e recuperação; ajustar o limiar para melhorar uma frequentemente resulta na deterioração da outra. No entanto, neste caso, a precisão e a recuperação são significativamente mais baixas para o conjunto de dados em português, indicando um problema de desempenho geral. Baixa precisão geralmente implica em um alto número de falsos positivos, enquanto baixa recuperação indica muitos falsos negativos.\n",
            "\n",
            "Tabela 1. Métricas para o modelo da API Perspective em inglês e português.\n",
            "\n",
            "| Métrica           | API Perspective (En) | API Perspective (Pt) | Diferença       |\n",
            "|-------------------|----------------------|----------------------|------------------|\n",
            "| Acurácia          | 0.901                | 0.813                | 0.122            |\n",
            "| Precisão          | 0.744                | 0.336                | 0.408            |\n",
            "| Recuperação       | 0.777                | 0.340                | 0.437            |\n",
            "| Pontuação F1      | 0.942                | 0.199                | 0.743            |\n",
            "\n",
            "A Figura 1 exibe o AUC médio ao longo de tamanhos de treinamento variados do modelo BERThs-Pt, com o painel esquerdo cobrindo até 100 posts do Twitter para avaliar o desempenho inicial e o painel direito se estendendo até 5000 posts para avaliar o potencial total de treinamento do modelo.\n",
            "\n",
            "Os resultados mais preocupantes estão no score AUC-ROC, que mede as habilidades de classificação da API Perspective em seu artigo oficial. O conjunto de dados em português pontua 20 pontos percentuais mais baixo que o conjunto de dados em inglês no AUC-ROC. Isso é tanto surpreendente quanto alarmante, dado que a API Perspective multilíngue é relatada como tendo um AUC-ROC de 0.877, apenas ligeiramente mais baixo que o parceiro em inglês na documentação oficial.\n",
            "\n",
            "Esses achados sugerem que, embora a API Perspective afirme suportar múltiplos idiomas, seu desempenho em português é substancialmente inferior ao inglês. Isso sublinha a importância de avaliar modelos multilíngues com base em cada idioma para garantir sua eficácia e confiabilidade em diferentes contextos linguísticos.\n",
            "\n",
            "4.2. Avaliando o BERThs\n",
            "O BERThs-Pt foi avaliado aumentando gradualmente o conjunto de treinamento em 10 mensagens de cada vez. A Figura 1 ilustra o desempenho do modelo à medida que o número de mensagens de treinamento aumentou. A linha vermelha representa o desempenho médio da API Perspective no conjunto de testes. Observe que nosso modelo superou a API Perspective em score AUC-ROC com apenas 100 mensagens de treinamento.\n",
            "\n",
            "Por outro lado, o conjunto de dados BERThs-En mostrou um resultado diferente. Mesmo com um extenso conjunto de dados de treinamento, deixando de lado uma pequena parte para testes e validação, o modelo BERT ajustado ainda teve um desempenho inferior ao da API Perspective, alcançando um AUC-ROC médio de 0.934 em comparação com 0.942 da Perspective.\n",
            "\n",
            "Esses achados sugerem que, embora a API Perspective seja uma ferramenta excelente, não é adequada para o português. Isso significa que criar um modelo personalizado pode facilmente superar o desempenho da API Perspective com uma quantidade relativamente pequena de dados rotulados. Portanto, um classificador adaptado às nuances lingüísticas e contextuais específicas do português é mais adequado para a detecção de discurso de ódio.\n",
            "\n",
            "4.3. Análise Qualitativa\n",
            "A Tabela 2 apresenta uma análise comparativa dos resultados de classificação para a API Perspective e o BERThs-Pt em um conjunto de dados de discurso de ódio em português. A observação mais notável desta análise é que o BERThs-Pt classificou incorretamente significativamente menos mensagens (14,3%) em comparação com a API Perspective (20,9%), indicando que o modelo BERT é geralmente mais preciso em discernir as nuances do texto. Essa superioridade é particularmente evidente em seu manuseio de instâncias sutis e dependentes de contexto de discurso de ódio, onde a API Perspective muitas vezes enfrenta dificuldades. A análise ainda revela que, enquanto ambos os modelos têm bom desempenho com conteúdo claro e inequívoco, eles encontram desafios com linguagem ambígua e mensagens contextualizadas, casos em que o modelo BERT mostra uma melhor habilidade geral para navegar nessas complexidades.\n",
            "\n",
            "Tabela 2. Comparação de mensagens mal classificadas e corretamente classificadas da API Perspective e do BERThs-Pt.\n",
            "\n",
            "| Categoria               | Berkeley-Pt (Correta)       | API Perspective (Incorreta)         |\n",
            "|-------------------------|------------------------------|-------------------------------------|\n",
            "| 72.2% das Mensagens     | \"Que mulher burra do cacete\" | \"Vai também ser lançado um manual de boas maneiras para lidar com fufas, gays e transsexuais...\" |\n",
            "| 6.9% das Mensagens      | \"Boa semana para todos!\"    | \"as pessoas não entendem que no meio dos refugiados tem inúmeros terroristas, é uma coisa tão óbvia...\" |\n",
            "| 7.4% das Mensagens      | \"gorda e feia\"              | \"feliz dia do não tenho roupa pra sair...\" |\n",
            "| 13.5% das Mensagens     | \"Você é cheinha, NÃO é gostosa.\" | \"E traveco mesmo, mó pirocão...\" |\n",
            "\n",
            "Estabelecido que o BERThs-Pt geralmente supera a API Perspective, realizamos uma análise específica por quadrantes para explorar essas diferenças mais a fundo. O primeiro quadrante representa mensagens que ambos os métodos classificaram corretamente, representando 72,2% das mensagens. Isso indica sua eficácia em lidar com conteúdo inequívoco, como ilustrado na Tabela 2. A alta taxa de sucesso destaca a capacidade de ambos os modelos de gerenciar casos diretos de discurso de ódio ou conteúdo benigno onde a ambiguidade linguística é baixa. No entanto, cenários do mundo real muitas vezes envolvem linguagem mais sutil, onde as diferenças entre os modelos se tornam mais evidentes.\n",
            "\n",
            "O segundo quadrante abrange 6,9% das mensagens que a Perspective classificou corretamente, mas que o BERThs-Pt classificou erroneamente. Uma amostra aleatória de quatro dessas mensagens revela que são algo ambíguas, dificultando a determinação com certeza se foram classificadas incorretamente. Esses casos destacam os desafios de categorizar com precisão a linguagem nuanceada e dependente do contexto.\n",
            "\n",
            "O terceiro quadrante, que inclui 13,5% das mensagens que a Perspective classificou incorretamente e o BERThs-Pt classificou corretamente. Torna-se evidente que a Perspective enfrenta dificuldades com contextos mais complexos, particularmente quando o discurso de ódio não é explícito. O modelo possui particular dificuldade com gírias ou linguagem codificada, como termos depreciativos direcionados a indivíduos LGBTQ+. As limitações da Perspective em compreender tais insultos indiretos se tornam evidentes aqui, sugerindo que seu treinamento generalizado pode não capturar suficientemente as nuances da língua portuguesa.\n",
            "\n",
            "Finalmente, o quarto quadrante, que compreende 7,4% das mensagens, envolve casos em que ambos os modelos falharam. Essas mensagens tipicamente carecem de contexto suficiente, dificultando a classificação precisa. A dificuldade compartilhada nesta categoria sublinha os desafios de detectar discurso de ódio quando a linguagem é ambígua ou o contexto está ausente.\n",
            "\n",
            "5. Conclusões\n",
            "Este estudo avaliou o desempenho da API Perspective na detecção de discurso de ódio em português, comparando-o com o inglês e explorando o potencial de modelos personalizados treinados. Os resultados mostram uma lacuna significativa de desempenho, com a API alcançando um score AUC-ROC de 94,2 em inglês, mas apenas 74,4 em português. Essa queda ilustra as limitações do uso de uma ferramenta multilíngue generalizada para idiomas específicos.\n",
            "\n",
            "Confiar em um modelo que suporte o português, mas entrega resultados insatisfatórios, apresenta duas questões principais. Primeiro, pesquisas conduzidas usando tal ferramenta podem produzir resultados imprecisos ou enganosos, minando a validade do estudo. Em segundo lugar, pesquisadores de regiões não anglófonas podem se sentir pressionados a conduzir suas pesquisas em contextos em inglês para aproveitar o desempenho mais confiável de ferramentas como a API Perspective, podendo negligenciar nuances linguísticas e culturais importantes.\n",
            "\n",
            "Enquanto a API Perspective se destaca em inglês, nosso estudo mostra que pode não ser a melhor escolha para o português. Um modelo BERT personalizado que desenvolvemos usando o BERTimbau superou a API com apenas 100 mensagens de treinamento, sugerindo que o ajuste fino de modelos para idiomas específicos pode produzir melhores resultados na detecção de discurso de ódio.\n",
            "\n",
            "Em conclusão, enquanto a API Perspective oferece desempenho robusto para o inglês, sua eficácia em português é limitada. Pesquisadores e práticos devem considerar desenvolver modelos personalizados adaptados a seus contextos linguísticos específicos para alcançar resultados mais precisos e confiáveis.\n",
            "\n",
            "Agradecimentos\n",
            "Este trabalho foi parcialmente financiado por CNPq, CAPES, FAPEMIG e IAIA - INCT em IA.\n",
            "5\n",
            "Resumo. Neste trabalho, exploramos a segmentação textual para o português utilizando o modelo BERTimbau, com bases de dados construídas usando tradução automática e a partir de notícias online. Obtivemos Pk = 6,89 para uma avaliação dentro do domínio, mas resultados piores em avaliações fora do domínio, destacando a importância de uma base de treinamento diversificada para melhorar a generalização em múltiplos domínios. 1. Introdução Com o aumento na geração de conteúdo textual não estruturado, como transcrições automáticas de notícias, aulas e reuniões, há também um crescente interesse em extrair de forma eficiente informações relevantes desse material [Retkowski and Waibel 2024, Gklezakos et al. 2024]. Por exemplo, pode ser desafiador encontrar o início de um determinado tópico discutido na transcrição de uma longa reunião, a menos que essa transcrição esteja devidamente estruturada. A segmentação textual baseada em tópicos é uma tarefa de Processamento de Linguagem Natural (PLN) que divide um texto longo em segmentos não sobrepostos, de acordo com as mudanças de tópico [Hearst 1997]. Essa ferramenta permite estruturar e compreender melhor grandes volumes de dados, facilitando a busca e a extração de informações. Recentes trabalhos Há poucos sobre segmentação textual em português [Cardoso et al. 2017, Francisco 2018]. Neste artigo, exploramos a segmentação textual baseada em tópicos para o português, aplicando a abordagem proposta em [Yu et al. 2023], utilizando o modelo BERTimbau [Souza et al. 2023]. Construímos os conjuntos de dados de treinamento e teste por meio de tradução automática para o português, e utilizando notícias extraídas da internet. 2. Metodologia Neste trabalho, utilizamos a abordagem proposta por [Yu et al. 2023] que trata a segmentação textual como um problema de classificação de uma sequência de sentenças, em que se deseja identificar a última sentença de cada tópico, ou seja, identificar as fronteiras dos segmentos. O componente principal é um modelo de linguagem pré-treinado do tipo Transformer encoder [Vaswani et al. 2023], que produz a representação contextual das sentenças do texto de entrada. Cada representação de sentença é usada na classificação de fronteira do segmento, conforme mostrado na Figura 1. Figura 1. Estrutura do modelo de segmentação proposto por [Yu et al. 2023] Em [Yu et al. 2023], além da tarefa principal de segmentação baseada em tópicos, são definidas duas tarefas auxiliares adicionais, Topic-aware Sentence Structure Prediction (TSSP) e Contrastive Semantic Similarity Learning (CSSL), com o objetivo de modelar a coerência textual e obter melhores resultados na segmentação. O modelo é treinado de forma supervisionada, otimizando a soma das perdas das três tarefas definidas, sobre um conjunto de treinamento devidamente anotado. Neste trabalho, utilizamos datasets para o treinamento e a avaliação obtidos por meio de tradução automática para português ou construídos a partir de notícias em português extraídas da internet. Os datasets WikiSection e WIKI-50 foram usados por [Yu et al. 2023] e passaram pelo processo de tradução automática usando a API de tradução da Google. O dataset WikiSection [Arnold et al. 2019] foi usado para treinamento e avaliação, e consiste num conjunto de 38K artigos em inglês e alemão, nos domínios de doenças e cidades. Após a tradução, restaram 3.590 documentos no domínio de doenças e 19.539 documentos no domínio de cidades. O dataset WIKI-50 [Koshorek et al. 2018] foi usado apenas para avaliação, e consiste originalmente em um conjunto de 50 amostras em inglês, provenientes da Wikipedia. Para a avaliação dos modelos, utilizamos também datasets em português construídos a partir de notícias extraídas com webscraping do portal G1 (portal de notícias do Grupo Globo de Comunicação), e do canal de notícias do IBGE2 (Instituto Brasileiro de Geografia e Estatística). Os documentos de texto foram formados pela concatenação aleatória de notícias, sendo cada notícia considerada um segmento de tópico diferente. No caso do dataset G1, foram gerados 454 documentos a partir de 1.300 notícias. Para o dataset IBGE, foram gerados 1.517 documentos a partir de 3.376 notícias. 1https://g1.globo.com/tecnologia/noticia/2012/11/siga-o-g1-por-rss.html 2https://servicodados.ibge.gov.br/api/docs/noticias?versao=3 Como o nosso objetivo é aplicar a segmentação para o português, substituímos o modelo usado em [Yu et al. 2023] pelo modelo BERTimbau [Souza et al. 2023], pré-treinado para o português do Brasil. Utilizamos as versões BERTimbau Base (110M de parâmetros) e BERTimbau Large (335M de parâmetros)3. O treinamento foi realizado em uma GPU NVIDIA T4, usando BERTimbau Base e Large, com 70% do dataset WikiSection em português, por 5 épocas, com learning rate de 5 × 10−5, batch size de 2 e gradiente acumulado de 2. Criamos sempre um modelo treinado com WikiSection/cidades e o outro modelo treinado com WikiSection/doenças. No caso do BERTimbau Large, o treinamento durou aproximadamente 2 dias e 5 horas para o conjunto de cidades e pouco mais de 11 horas para o conjunto de doenças. A avaliação dos modelos seguiu a mesma linha de [Yu et al. 2023]. Usamos três métricas usuais para avaliação de segmentação textual: F1, Pk [Beeferman et al. 1999], e WindowDiff [Pevzner and Hearst 2002]. No caso das métricas Pk e WindowDiff, quanto menor o valor, melhor o desempenho. No caso da métrica F1, quanto maior o valor, melhor o desempenho. A avaliação dentro do domínio de treinamento foi realizada com 20% do dataset WikiSection em português. Os datasets WIKI-50, G1 e IBGE são usados apenas para avaliação fora do domínio de treinamento. 3. Resultados As Tabelas 1 e 2 apresentam os resultados de avaliação dos modelos usando BERTimbau, criados e avaliados para o português, dentro do mesmo domínio, com os datasets WikiSection/cidades e WikiSection/doenças. Também são apresentados os resultados para o inglês correspondentes ao modelo BERT Base [Devlin et al. 2018], obtidos por [Yu et al. 2023]. Modelo (en) BERT Base [Yu et al. 2023] 80,16 87,41 (pt) BERTimbau Base 87,59 (pt) BERTimbau Large F1 Pk WD 8,22 10,19 8,55 7,07 8,37 6,89 Tabela 1. Resultados dos modelos criados e avaliados com o dataset WikiSection/cidades. BERT Base avaliado em inglês, BERTimbau em português. Modelo WD (en) BERT Base [Yu et al. 2023] 68,26 18,29 22,06 76,91 17,16 19,45 (pt) BERTimbau Base 77,77 16,55 18,76 (pt) BERTimbau Large Pk F1 Tabela 2. Resultados dos modelos criados e avaliados com o dataset WikiSection/doenças. BERT Base avaliado em inglês, BERTimbau em português. As métricas de avaliação obtidas com os modelos BERTimbau para o português são melhores e próximas àquelas apresentadas por [Yu et al. 2023] em inglês. Neste caso, devemos considerar também que o modelo criado para o português usando BERTimbau Large é maior que o modelo usado em [Yu et al. 2023]. 3https://huggingface.co/neuralmind/bert-base-portuguese-cased A Tabela 3 apresenta os resultados da avaliação de dois modelos criados para o português nos domínios de cidades e doenças, usando o BERTimbau Large, e avaliados fora do domínio de treinamento, nos datasets WIKI-50, G1 e IBGE. Dataset Modelo / cidades Pk WD 35,01 35,36 13,62 17,28 20,12 21,06 F1 15,43 64,66 43,12 Wiki50 G1 IBGE Modelo / doenças Pk F1 WD 36,02 32,42 26,55 12,97 35,98 54,81 25,61 43,36 23,40 Tabela 3. Avaliação fora do domínio de treinamento. Modelos com o BERTimbau Large criados com o dataset WikiSection/cidades e WikiSection/doenças. O desempenho do modelo fora do domínio de treinamento foi inferior ao desempenho dentro do domínio. Os resultados foram melhores para o modelo treinado com o dataset WikiSection/cidades. De fato, segundo [Arnold et al. 2019], o conteúdo do dataset WikiSection apresenta características distintas para cada domínio: WikiSection/doenças é de domínio científico restrito com linguagem específica, enquanto WikiSection/cidades é de domínio geral mais diverso, mais próximo de um conteúdo de notícias. Isso sugere que a composição de dados de treinamento pode ajudar a obter um modelo para segmentação textual que generalize melhor para múltiplos domínios. 4. Conclusão Neste trabalho, exploramos a segmentação textual para o português, seguindo a abordagem de [Yu et al. 2023], mas utilizando o modelo pré-treinado para o português BERTimbau [Souza et al. 2023]. Empregamos bases de treinamento e teste construídas usando a tradução automática de bases existentes, além de bases de teste construídas a partir de notícias em português recuperadas da internet. Obtivemos ótimos resultados na segmentação de texto dentro do mesmo domínio para o português, semelhante ao que foi obtido por [Yu et al. 2023] para o inglês. Nossos resultados sugerem a eficácia do método empregado para a criação do modelo em português e a importância de usar uma base de treinamento de domínio diversificado para obter um modelo que generalize melhor para múltiplos domínios. Para trabalhos futuros, pretendemos explorar modelos diferentes e buscar uma composição mais variada de dados de treinamento para obter um modelo que generalize melhor para vários domínios. Além disso, desejamos estudar a segmentação textual de transcrições automáticas obtidas com reconhecimento de fala, e explorar a segmentação de textos muito longos, considerando a típica limitação do contexto de entrada de modelos baseados em Transformer [Vaswani et al. 2023].\n",
            "6\n",
            "Resumo. Neste artigo, propomos um pipeline para ajustar modelos de fala para texto (STT) específicos de domínio usando dados sintéticos gerados por Inteligência Artificial (IA). Nossa metodologia elimina a necessidade de dados de áudio rotulados manualmente, que são caros e difíceis de obter, gerando dados específicos de domínio com um Grande Modelo de Linguagem (LLM) combinado com várias soluções de Texto para Fala (TTS). Aplicamos nosso pipeline ao domínio da radiologia e comparamos os resultados com diferentes abordagens baseadas na disponibilidade de dados específicos de domínio, variando desde a ausência total de dados específicos de domínio até o uso de apenas dados de alta qualidade específicos de domínio (verdade fundamental). Nossa performance melhorou a precisão da linha de base em 40,19% e 10,63% para os modelos WhisperX Tiny e Small, respectivamente, que, embora tenham apresentado desempenho inferior em relação aos resultados do uso da verdade fundamental, mostram que é possível alcançar bons resultados com custo e esforço mínimos. Por fim, a análise dos resultados fornece uma boa visão sobre a quantidade de ação necessária para alcançar bons resultados com base na disponibilidade de dados reais.\n",
            "\n",
            "1. Introdução\n",
            "A transcrição automática de áudio, comumente chamada de fala para texto (STT), tem sido uma prática comum em muitos campos de trabalho, como saúde, justiça, educação e negócios [Kumar 2024]. No entanto, a precisão no reconhecimento e na transcrição da linguagem é importante para garantir o uso correto e eficiente das informações transcritas. Isso é especialmente importante em aplicações específicas de domínio, nas quais o uso de termos técnicos e jargões aumenta o desafio de reconhecimento e transcrição [Suh et al. 2024]. No entanto, muitas das soluções disponíveis para esse problema são construídas com dados genéricos. Devido a isso, seus resultados são de qualidade inferior quando utilizados em cenários específicos de domínio [Chan et al. 2016]. \n",
            "\n",
            "Uma abordagem comum para resolver essa questão é construir e refinar soluções utilizando contextos, vocabulários e outros tipos de dados relacionados ao domínio [Huang et al. 2020]. Atualmente, é padrão utilizar modelos genéricos de IA como base para soluções de STT e ajustar esses modelos com dados específicos de domínio [Mak et al. 2024]. No entanto, o processo de ajuste é caro e requer uma quantidade significativa de dados e esforço [Hu et al. 2022]. Para aplicações médicas, por exemplo, é necessário coletar dados sensíveis, fazer com que profissionais de saúde verifiquem, corrijam e validem esses dados, e garantir sua privacidade e segurança em relação aos pacientes e ao pessoal envolvidos [Johnson et al. 2014]. \n",
            "\n",
            "No entanto, a necessidade de soluções STT de alta qualidade é evidente em muitos setores de trabalho. Na radiologia, por exemplo, é comum que os médicos utilizem ferramentas de STT em sua prática para aumentar a produtividade em relação à transcrição tradicional, onde o profissional grava um relatório por meio de voz para ser posteriormente transcrito manualmente por outro profissional (geralmente sem formação médica) [Hammana et al. 2015]. Qualquer erro ou atraso nesse processo pode resultar em possíveis danos e consequências para os pacientes e seus tratamentos [Vorbeck et al. 2000]. Outro exemplo comum são os tribunais e procedimentos judiciais, nos quais uma grande quantidade de textos específicos de domínio é gerada e frequentemente transcrita manualmente, resultando em processos caros e ineficientes [da Cruz et al. 2022]. \n",
            "\n",
            "Nesse contexto, este trabalho propõe um pipeline de baixo custo para o treinamento e ajuste de modelos de IA STT quando dados específicos de domínio são necessários, mas não estão prontamente disponíveis. Nosso pipeline é baseado na utilização de modelos de IA para gerar dados sintéticos específicos de domínio. Para isso, utilizamos um Grande Modelo de Linguagem (LLM) para produzir conteúdo específico de domínio que simula casos de uso reais. Especificamente para este trabalho, exploramos o domínio da radiologia, gerando dados para relatórios radiológicos sintéticos usando um LLM e uma abordagem de prompting específica. Os dados sintéticos são então convertidos em arquivos de áudio por meio de ferramentas TTS. Assim, o processo de ajuste é realizado inteiramente usando dados sintéticos gerados pela IA. Além disso, devido ao foco em ser uma solução de baixo custo, os resultados deste trabalho foram feitos utilizando soluções baratas ou disponíveis gratuitamente. Simultaneamente, este trabalho também apresenta uma análise de comparação de uma gama de possíveis resultados finais, dependendo da disponibilidade de dados específicos de domínio.\n",
            "\n",
            "2. Trabalhos Relacionados\n",
            "A transcrição automática de áudio tem sido um campo de pesquisa frutífero nas ciências da computação ao longo de muitos anos [Yu et al. 2010, Blackley et al. 2019]. Muitos dos trabalhos tradicionais nesse campo estão focados nos desafios herdados dele, como lidar com sutilezas da linguagem, estrutura e fluência [Gontier et al. 2021], e as limitações no acesso a conjuntos de dados adequados [Hu et al. 2022]. Esses desafios aumentam ao lidar com cenários específicos de domínio [Samarakoon et al. 2018].\n",
            "\n",
            "Com relação aos conjuntos de dados, a maioria dos trabalhos na área utiliza conjuntos de dados na língua inglesa [Casanova et al. 2022]. Ao trabalhar em cenários com outros idiomas, os pesquisadores devem não apenas resolver os desafios recorrentes de STT, mas também adaptar suas soluções, como feito por Gruzitis et al. [Gruzitis et al. 2022], que adaptaram seus modelos para a língua letã, e o trabalho de Vivancos-Vincente et al. [Vivancos-Vicente et al. 2016] para o espanhol e português. Alternativamente, o trabalho proposto por Casanova et al. [Casanova et al. 2022] mostra uma alternativa para treinar modelos para diferentes idiomas com base na ampliação de dados de apenas um falante para o idioma-alvo, utilizando técnicas de conversão de voz entre idiomas e TTS mult falante.\n",
            "\n",
            "Além disso, o acesso a bons conjuntos de dados específicos de domínio é um desafio, e sua produção envolve altos custos com especialistas de domínio, análise de dados e validação. Esse problema é frequentemente enfrentado com o uso de dados sintéticos [Li et al. 2018, Rosenberg et al. 2019, Laptev et al. 2020, Huang et al. 2020, Yang et al. 2023]. No entanto, os dados sintéticos muitas vezes estão distantes de casos de uso reais devido à ausência de erros e imperfeições que são comuns em dados produzidos por humanos, o que os torna \"perfeitos demais\" em comparação com casos do mundo real. Esse \"problema de perfeição\" é tratado com a introdução de erros e imperfeições sintéticas, como feito pela solução Synt++ proposta por Hu et al. [Hu et al. 2022], na qual ruído e artefatos aleatórios são introduzidos na geração de dados sintéticos para que se aproxime mais dos dados da vida real.\n",
            "\n",
            "Somente recentemente o processo de síntese de dados usando LLMs foi explorado, como o trabalho apresentado por Vásquez-Correa et al. [Vásquez-Correa et al. 2023], que gera dados sintéticos específicos de domínio por meio de prompting para ajustar uma solução STT para os idiomas inglês, espanhol e basco. Silva et al. [Silva et al. 2024] também utilizam um LLM para gerar dados sintéticos para um conjunto de dados de previsão de falha de hardware. Seu conjunto de dados foi gerado a partir de categorias de problemas e relatórios de principais fabricantes de componentes no mercado.\n",
            "\n",
            "Da mesma forma, este trabalho propõe uma nova abordagem para dados sintéticos baseada em prompting. Os dados sintéticos são então convertidos em arquivos de áudio por meio de algoritmos TTS e usados para ajustar um modelo genérico de IA STT. Nossa abordagem utiliza um modelo de IA STT genérico simples e de baixo custo como meio de comprovar sua utilidade em cenários com recursos mínimos. Além disso, este trabalho apresenta uma análise comparativa dos resultados com base na disponibilidade de dados específicos de domínio, variando desde a ausência total de dados específicos de domínio (nossa solução) até o uso de apenas dados específicos de domínio de alta qualidade (solução ideal).\n",
            "\n",
            "3. Metodologia\n",
            "3.1. Conjuntos de Dados\n",
            "Para validar a eficiência de nosso pipeline proposto, usamos um conjunto de dados de áudio rotulado manualmente por profissionais de radiologia, que foi dividido em um conjunto para treinamento e outro para teste. O conjunto de treinamento incluiu 98 arquivos de áudio de dois radiologistas cisgêneros do sexo masculino, com uma duração total de 1 hora, 10 minutos e 8 segundos de áudio. O conjunto de dados de teste consistiu em 82 arquivos de áudio dos mesmos dois radiologistas, com uma duração total de 1 hora, 4 minutos e 21 segundos de áudio. Tanto os conjuntos de treinamento quanto os de teste tinham uma quantidade igual de arquivos de áudio para os dois radiologistas, e todos os arquivos de áudio foram falados em português. Todos os arquivos de áudio foram gravados em cenários do mundo real, incluindo ruído de fundo dos respectivos locais de trabalho, artefatos de áudio e outros problemas comuns. Este conjunto de dados constitui nosso conjunto de dados de verdade fundamental, que foi usado para comparação com os resultados das outras abordagens exploradas.\n",
            "\n",
            "3.2. Métodos e Tecnologias\n",
            "A biblioteca transformers da Hugging Faces [Vaswani 2017] foi usada para ajustar o modelo STT, que também foi configurado para a língua portuguesa. Optamos por um processo de ajuste tradicional usando todos os pesos disponíveis. Para a inferência, usamos o modelo WhisperX [Bain et al. 2023], que oferece uma transcrição mais rápida e precisa, com o backend Ctranslate2 para melhor compatibilidade e redução do tempo de inferência. A principal razão para usar o WhisperX foi a presença de um Detecção de Atividade de Voz (VAD) interno, que reduz consideravelmente as tendências de alucinação e otimiza o uso de VRAM [Koenecke et al. 2024].\n",
            "\n",
            "Utilizamos o GPT-4o como o LLM para gerar relatórios radiológicos sintéticos específicos de domínio usando uma abordagem e prompts específicos [Islam e Moushi 2024]. Os relatórios sintéticos foram alimentados em soluções TTS para gerar arquivos de áudio para o processo de ajuste. \n",
            "\n",
            "Como soluções TTS, utilizamos a solução ElevenLabs, que é bastante econômica para sua qualidade, e o Google Text-to-Speech. Ambas as ferramentas permitiram uma variedade de entonações, estilos de fala e variações, que ajudaram a reduzir o \"problema de perfeição\" frequentemente produzido nos dados sintéticos. Além disso, o uso de duas soluções TTS melhorou a representação e diversidade de padrões de fala e sotaques.\n",
            "\n",
            "3.3. Métricas\n",
            "A métrica de Taxa de Erro de palavras (WER) foi utilizada para avaliar a precisão das soluções STT [Ali e Renals 2018]. A métrica WER é calculada pela razão entre o número de erros transcritos e o número de palavras originalmente faladas. Esses erros são classificados como Substituições (S), Inserções (I) e Exclusões (D). A fórmula WER que usamos foi: WER = S+D+I/N, onde N é o número de palavras originalmente faladas.\n",
            "\n",
            "4. Resultados\n",
            "4.1. Pipeline Proposto\n",
            "Como mostrado na Figura 1, o pipeline proposto visa ajustar um modelo STT usando um conjunto de dados sintéticos específicos de domínio. Começa com um prompt especializado para o LLM. Este prompt especializado deve considerar a terminologia específica e informações de domínio para garantir que os dados sintéticos se assemelhem de perto aos dados da vida real.\n",
            "\n",
            "Os dados sintéticos gerados pelo LLM são alimentados em soluções TTS e convertidos em arquivos de áudio. É importante incluir variações no tom de voz e ruídos sintéticos nesse processo para reduzir o \"problema de perfeição\". Juntos, os dados sintéticos gerados pelo LLM e sua representação em áudio compõem o conjunto de dados rotulado pela IA. Este conjunto de dados é então usado para ajustar o modelo STT escolhido.\n",
            "\n",
            "4.2. Conjunto de Dados Rotulados pela IA\n",
            "O GPT-4o foi usado como ferramenta LLM para a geração de dados sintéticos específicos de domínio. Para isso, primeiramente introduzimos o modelo ao contexto da radiologia e lhe fornecemos uma série de especialidades de radiologia e tipos de exames, como tomografia computadorizada e radiografia. Além disso, para garantir uma formulação típica do estilo de relatório, instruímos o LLM a criar frases e sentenças em um formato progressivo, começando por descrições normais, seguidas de possíveis achados e diagnósticos específicos para esses. Por fim, instruímos o LLM a não incluir abreviações e a fornecer os resultados em um formato JSON sem texto adicional. O prompt utilizado pode ser visto na Figura 2.\n",
            "\n",
            "Você deve gerar {número de frases} frases em português que poderiam estar presentes em um relatório de {tipo de relatório} feito por um médico especialista em um campo médico específico que será fornecido como entrada. Gere as frases e sentenças seguindo uma cadeia lógica de raciocínio, começando por casos regulares e progredindo para achados possíveis e diagnósticos específicos relacionados ao contexto fornecido. Explore múltiplos tipos de frases, que variam desde descrições básicas até conclusões detalhadas. Evite usar abreviações, e toda vez que precisar mencionar um termo específico, use-o em sua forma mais completa (por exemplo, use centímetros em vez de cm e batimentos por minuto em vez de bpm). Formate a saída: retorne um objeto JSON com a lista de frases. Não inclua nenhum texto adicional antes e depois do JSON. Exemplo de saída JSON: { \"frases\": [ \"O paciente apresenta ritmo cardíaco regular, com 72 batimentos por minuto.\", \"A imagem mostra um aumento moderado no tamanho do ventrículo esquerdo.\", \"Não há evidências de derrame pleural ou ascite.\" ] } Saída apenas o JSON com as {número de frases} frases sem textos adicionais.\n",
            "\n",
            "Como mencionado anteriormente, usamos duas ferramentas TTS para a geração de áudio sintético: ElevenLabs e Google Text-to-Speech. O uso de ambas as ferramentas visa diversificar os dados gerados com padrões de fala variados, ritmo, entonação e qualidade. Geramos 46 minutos e 43 segundos de áudio usando a ElevenLabs em um total de 980 arquivos. Esses arquivos foram igualmente divididos em cinco vozes masculinas diferentes. Em relação ao Google Text-to-Speech, geramos 58 minutos e 55 segundos de áudio, novamente, em um total de 980 arquivos, utilizando apenas uma voz masculina disponível. O conjunto de dados para os dados gerados sinteticamente está disponível em um repositório do GitHub. \n",
            "\n",
            "A Figura 3 (a) e (b) mostra a distribuição do comprimento do áudio para o conjunto de dados sintético em comparação ao real, rotulado manualmente. Como pode ser visto, a distribuição geral é bastante semelhante, enquanto os dados sintéticos tendem a ser mais curtos, resultando em mais arquivos. A nuvem de palavras em português para ambos os conjuntos de dados pode ser vista na Figura 3 (c) e (d). Ambos os conjuntos de dados mostram termos específicos de domínio, com maior presença de termos de pontuação (vírgulas, pontos, etc) no conjunto de dados real. Alternativamente, o conjunto de dados sintético tem uma presença maior de frases como “Não há” ou “Há sinais” (significando \"There is no\" e \"There are signs of\", respectivamente em inglês), mostrando uma tendência a repetir estruturas de frase com os mesmos termos iniciais. A distribuição de termos e tempos entre as ferramentas TTS é bastante semelhante.\n",
            "\n",
            "4.3. Análise\n",
            "A Figura 4 mostra os resultados para a métrica WER em quatro cenários diferentes: uma linha de base (WhisperX sem ajuste); WhisperX ajustado usando os dados sintéticos; WhisperX usando dados de áudio sintético gerados a partir de relatórios radiológicos reais; WhisperX ajustado usando a verdade fundamental (cenário ideal). Os resultados são mostrados tanto para as versões WhisperX Small quanto Tiny, suas formas mais acessíveis. \n",
            "\n",
            "Em ambos os casos, para as versões WhisperX Tiny e Small, o comportamento foi bastante semelhante, com a versão Tiny apresentando valores consideravelmente mais altos para a WER em comparação à versão Small. A linha de base, como era de se esperar, apresentou o maior valor de WER (104.13 e 37.35), enquanto a versão da verdade fundamental alcançou o mais baixo (40.11 e 23.11). Vale mencionar que a versão Tiny da verdade fundamental, embora tenha apresentado desempenho significativamente pior, alcançou uma WER semelhante quando comparada à versão Small de linha de base (40.11 em comparação a 37.35). \n",
            "\n",
            "Vale ressaltar que o alto valor de WER da linha de base WhisperX Tiny é o resultado de alucinações, que fizeram com que o modelo incluísse palavras que não estavam presentes em seus resultados. Isso fez com que o valor de WER aumentasse acima de 100%. Nosso pipeline proposto, que usou apenas dados sintéticos para o processo de ajuste, alcançou uma WER de 62.28 e 33.38 para as versões Tiny e Small, respectivamente, o que corresponde a uma melhoria de 41.85% e 10.62%. Esses resultados foram alcançados usando apenas os arquivos de áudio gerados pela ElevenLabs, que apresentaram desempenho melhor do que os gerados pela ferramenta Google-TTS (WER igual a 70.96 e 33.94 para as versões Tiny e Small) e por uma combinação de ambas, ElevenLabs e Google-TTS (WER igual a 66.75 e 33.94 para as versões Tiny e Small). \n",
            "\n",
            "Para exemplificar um caso em que existem alguns dados reais para o ajuste, testamos usando apenas relatórios radiológicos de casos reais (ignorando a etapa do LLM) e produzindo os dados de áudio a partir deles usando as mesmas ferramentas TTS mencionadas anteriormente. Esses novos dados foram usados para ajustar ambas as versões Tiny e Small do WhisperX, alcançando a WER de 56.24 e 30.76, respectivamente, que são 45.99% e 17.64% melhores que a linha de base. Para esses resultados, usamos apenas os dados de áudio gerados pela ElevenLabs, já que ela obteve melhores resultados em testes anteriores.\n",
            "\n",
            "Nossos resultados mostram que é possível alcançar melhores resultados utilizando uma abordagem completamente sintética. Embora ainda tenha um desempenho inferior em comparação com abordagens baseadas em dados reais, mostra uma abordagem promissora que tem muito espaço para experimentação e melhoria e acarreta um custo muito baixo em comparação com a geração de um conjunto de dados com dados reais.\n",
            "\n",
            "A Figura 4 mostra os resultados das quatro abordagens utilizando tanto os modelos WhisperX Small quanto Tiny. A métrica WER é mostrada no eixo y. Os resultados da verdade fundamental são, como esperado, os melhores resultados com os menores valores de WER para ambos os modelos. No entanto, também é a abordagem mais cara, com suas desvantagens e desafios. Além disso, não é improvável que seus melhores resultados sejam consequência de algum nível de sobreajuste, uma vez que os dados de treinamento e teste vêm dos mesmos médicos usando os mesmos equipamentos nos mesmos ambientes. Por outro lado, o conjunto de dados sintético foi composto por uma variedade maior de vozes e entonações que, embora sejam semelhantes às reais em termos de contexto e entonação, ainda são bastante diferentes. A ampla gama de possíveis vozes da ferramenta ElevenLabs pode explicar por que ela teve um desempenho melhor do que a ferramenta Google-TTS. De nossos experimentos, a ferramenta Google-TTS tende a gerar arquivos de áudio muito limpos e \"perfeitos\", semelhantes a robôs, que estão distantes de casos de uso reais.\n",
            "\n",
            "5. Conclusão\n",
            "Este trabalho apresentou um pipeline para o ajuste de soluções STT específicas de domínio usando dados sintéticos produzidos por uma combinação de prompting LLM e ferramentas TTS. Nosso pipeline proposto produz dados sintéticos de boa qualidade e supera o \"problema da perfeição\" usando ferramentas TTS para uma variedade maior de vozes, entonação e ritmo. Nossas descobertas mostram que nosso pipeline melhora os resultados em comparação com uma solução não ajustada.\n",
            "\n",
            "Dados os resultados, também podemos fazer suposições com base na disponibilidade de dados reais específicos de domínio. Como a Figura 4 mostra, e como era esperado, quanto mais dados reais utilizados, melhores os resultados. No entanto, a diferença entre o uso de alguns dados reais (usando dados de relatórios de casos reais com TTS para geração de áudio) e 100% de dados sintéticos não é significativa (cerca de 10% de melhoria para o modelo Tiny e 8% de melhoria para o modelo Small, comparando ambas as abordagens), indicando que em alguns casos, a abordagem exclusivamente sintética pode fornecer resultados suficientemente bons. No entanto, vale a pena gastar recursos adquirindo conhecimento e dados específicos de domínio, especialmente para produzir um prompt LLM especializado necessário para nossa abordagem, mas isso não refletirá necessariamente uma melhoria significativa sobre os dados sintéticos.\n",
            "\n",
            "Nossa escolha por usar os modelos WhisperX Tiny e Small está focada em fornecer uma solução de baixo custo para cenários específicos de domínio. Modelos WhisperX mais altos provavelmente fornecerão melhores resultados, mas exigem hardware caro e mais recursos para treinamento. Além disso, modelos mais altos exigiriam custos mais altos para hospedagem online para uma solução pronta para produção. Considerando nosso cenário, um investimento considerável seria necessário para hospedar tal estratégia para um único hospital com múltiplos médicos simultâneos trabalhando ao mesmo tempo diariamente. No entanto, nossos resultados indicam que, com o uso de um conjunto de dados de verdade fundamental, pode ser possível melhorar um modelo mais simples por meio de ajuste para funcionar tão bem quanto um modelo de linha de base melhor, como vimos com os resultados do modelo Tiny ajustado à verdade fundamental em comparação com o modelo Small de linha de base. Em nossos testes preliminares, encontramos que o modelo WhisperX Medium de linha de base tem uma WER de 28.85, que é ligeiramente superior à WER do modelo WhisperX Small ajustado à verdade fundamental que apresentamos (23.11).\n",
            "\n",
            "Além dos custos operacionais, a complexidade do modelo de IA utilizado impacta seu tempo de inferência (o tempo que leva para gerar a saída dada a entrada). Modelos mais simples, como Tiny e Small, têm um tempo de inferência relativo significativamente menor do que modelos maiores [Bain et al. 2023]. Para configurações em tempo real, isso é de grande importância, como a explorada neste estudo para soluções STT na radiologia.\n",
            "\n",
            "Como trabalho futuro, nosso pipeline poderia ser avaliado para outros contextos específicos de domínio, bem como mais experimentações sobre a variação de dados sintéticos que se aproximem ainda mais de cenários reais, incluindo o uso de diferentes sotaques, condições acústicas e ruído de fundo. O uso de um conjunto de dados de verdade fundamental mais diversificado também pode fornecer melhores insights sobre possíveis sobreajustes e resultados mais realistas para modelos ajustados treinados com ele. Não é improvável que uma solução pronta para produção alcance um valor de WER mais próximo dos resultados de nossas abordagens do que os atuais resultados da verdade fundamental. Além disso, um conjunto de dados de verdade fundamental de áudio mais longo poderia certamente fornecer melhores insights sobre nossos resultados, já que foi limitado a pouco mais de 1 hora devido a restrições orçamentárias e de tempo.\n",
            "\n",
            "Por fim, uma análise mais detalhada do equilíbrio entre dados sintéticos e reais poderia fornecer um entendimento mais aprofundado sobre quanto esforço é necessário para criar abordagens híbridas que se assemelhem mais a dados reais, incluindo o uso de áudio real em vez de depender puramente de ferramentas TTS. Isso poderia proporcionar uma ótima abordagem para ajustar modelos STT com uma fração dos custos normalmente associados ao uso de conjuntos de dados de alta qualidade.\n",
            "\n",
            "6. Agradecimentos\n",
            "Os autores gostariam de agradecer à Agência Brasileira FUNCAP-CE pelo suporte financeiro sob o projeto NUP 31052.001303/2023-62. Agradecemos também a Raiza Vaz pela ajuda na construção do banco de dados de verdade fundamental.\n",
            "7\n",
            "Resumo. O Reconhecimento de Entidades Nomeadas Médicas (NER) identifica e categoriza entidades médicas a partir de textos não estruturados, sendo crucial para tarefas de monitoramento da saúde. Apesar dos avanços com Modelos de Linguagem de Grande Escala (LLMs), o NER médico enfrenta desafios devido à escassez e dispersão de dados rotulados entre instituições, protegidos por regulamentações de privacidade. O Aprendizado Federado (FL) oferece uma solução ao permitir o treinamento descentralizado de modelos enquanto preserva a privacidade dos dados, mas é vulnerável a ataques bizantinos. Esta pesquisa propõe um protocolo FL simples e seguro utilizando Criptografia Homomórfica (HE), chamado FedHE, que elimina a necessidade de confiança entre as federações e o coordenador do treinamento. O FL criptografado impõe constrangimentos significativos em relação ao consumo de recursos e desempenho, tornando os modelos de linguagem de última geração impraticáveis. Esta pesquisa visa avaliar quão bem as representações compactas do BERT funcionam em tarefas federadas de NER médico em comparação com as abordagens de última geração. Os resultados mostraram que as representações compactas do BERT, como o BERTmini, são competitivas com o estado da arte e são viáveis para uso no FedHE. No entanto, os custos de consumo de recursos permanecem um desafio, especialmente quando o número de clientes aumenta.\n",
            "\n",
            "1. Introdução\n",
            "O reconhecimento de entidades nomeadas médicas (NER) tem como objetivo identificar entidades médicas (por exemplo, nomes de medicamentos, reações adversas e sintomas) a partir de textos médicos não estruturados e classificá-las em diferentes categorias. Pode ser utilizado em muitas tarefas inteligentes de saúde, como farmacovigilância e monitoramento de saúde [Tang et al. 2013]. Com os recentes avanços no campo [Peng et al. 2024], o problema do NER viu melhorias significativas. No entanto, no contexto específico do NER médico, existem desafios significativos no processo de aprendizagem devido à natureza sensível dos dados. Primeiro, os dados rotulados disponíveis de uma única instituição de saúde podem não ser representativos o suficiente para ajustar um modelo de NER com boa precisão preditiva. Em segundo lugar, o treinamento colaborativo com compartilhamento de dados é frequentemente impraticável, considerando as proibições regulamentares e os riscos de segurança associados à sensibilidade dos dados e à confiança entre as partes. \n",
            "\n",
            "Para aproveitar dados massivamente distribuídos e aumentar a generalização do modelo, o aprendizado federado (FL) foi introduzido em [Konečný et al. 2016] como um novo framework de aprendizado. Em um ciclo de treinamento FL, os clientes colaborativamente treinam um modelo global compartilhado trocando pesos ou gradientes do modelo, enquanto mantêm seus dados armazenados localmente. Ao trazer o modelo para os dados, o FL evita a transferência de dados e alcança um desempenho competitivo em comparação com modelos treinados com dados agrupados. Recentemente, [Peng et al. 2024] forneceu uma avaliação aprofundada do aprendizado federado em processamento de linguagem natural biomédico, demonstrando que o modelo BlueBERT (BERTblue), particularmente sua variante maior (BERTlargeblue), treinado usando FL, supera tanto sua versão treinada em dados de um único cliente quanto o GPT-4 quando aplicado em um cenário de prompt de poucos exemplos. Claramente, o FL, combinado com variações do BERT, se destaca como uma abordagem eficaz para NER.\n",
            "\n",
            "Embora o foco principal do FL seja manter rigorosas proteções de privacidade, prevenindo o compartilhamento de dados, [Zhu et al. 2019] introduziram uma nova vulnerabilidade na forma de ataques de inferência, mostrando que dados de treinamento privados podem ser extraídos dos gradientes compartilhados publicamente. Para mitigar esse risco, uma abordagem é incorporar uma etapa de criptografia no framework de aprendizado federado. Especificamente, o uso de Criptografia Homomórfica (HE) [Yi et al. 2014] dentro do FL permite que os clientes criptografem seus gradientes, possibilitando que o coordenador central agregue atualizações de modelo diretamente em textos criptografados, eliminando assim a necessidade de descriptografia.\n",
            "\n",
            "Embora a HE seja frequentemente considerada o padrão-ouro para criptografia de dados em uso, ela impõe sobrecargas de desempenho significativas em termos de computação e comunicação. Como resultado, a implementação de modelos de linguagem de última geração torna-se impraticável devido ao grande número de parâmetros treináveis. Portanto, para implementar um sistema FL mais seguro usando HE, modelos menores devem ser selecionados. Neste contexto, este artigo aborda duas questões-chave: (i) Qual é o custo computacional de aplicar HE em FL para aplicações NER? (ii) Quanto da precisão preditiva pode ser sacrificado ao escolher uma abordagem FL+HE mais segura com um modelo menor? Os resultados obtidos mostraram que modelos compactos, como BERTmini, podem competir com modelos de NER de última geração em um cenário FL+HE para diferentes corpora. No entanto, as sobrecargas de recursos — particularmente largura de banda de comunicação e utilização de memória — continuam a representar desafios significativos.\n",
            "\n",
            "2. Aprendizado Federado\n",
            "O cenário prototípico de FL consiste em um servidor central S e um conjunto de K clientes distribuídos C, de modo que |C| = K, que cooperam para resolver uma tarefa de aprendizado supervisionado padrão. Cada cliente c ∈ C tem acesso ao seu próprio conjunto de treinamento privado Dc = {xc,i, yc,i}nc i=1. O objetivo do FL é treinar um modelo preditivo global cuja arquitetura e parâmetros θ∗ ∈ Rd c=1 pcLc(θ; Dc), onde são compartilhados entre todos os clientes e encontrados para minimizar minθ Lc é o objetivo local e pc ≥ 0 especifica a contribuição individual do cliente c tal que (cid:80)K n, onde n = (cid:80)K c=1 nc. A função do objetivo local Lc geralmente é definida como o risco empírico calculado sobre o conjunto de treinamento Dc amostrado da distribuição de dados local do cliente Lc(θ; Dc) = 1 i=1 l(θ; (xc,i, yc,i)), onde l é uma perda de nível de instância (por exemplo, perda de entropia cruzada ou erro quadrático em relação a tarefas de classificação ou regressão, respectivamente).\n",
            "\n",
            "Em Aprendizado Federado, para gerar um modelo global θ a partir de modelos locais treinados com parâmetros θc, uma etapa de agregação é necessária para combinar as atualizações de todos os clientes. Um dos métodos mais amplamente utilizados para agregação é chamado FedAvg. Em cada rodada t, os clientes realizam passos de treinamento local em seus conjuntos de dados privados Dc para minimizar suas respectivas funções objetivo Lc. Após completar as atualizações locais, os clientes enviam seus parâmetros atualizados θ(t) de volta ao servidor central S. O servidor então agrega esses θ(t+1) = (cid:80)K c=1 pcθ(t) e usa isso para atualizar o modelo global para a próxima rodada de treinamento.\n",
            "\n",
            "2.1. Aprendizado Federado com Criptografia Homomórfica Total\n",
            "A Criptografia Homomórfica (HE) permite que determinados cálculos (por exemplo, adição) sejam realizados diretamente em textos criptografados, sem que seja necessário descriptografá-los primeiro. A ideia intuitiva é que uma terceira parte pode computar os dados sem realmente conhecer esses dados. Esse problema é resolvido com criptografia baseada em chave onde o processo de criptografia preserva operações algébricas. Para o operador de adição, por exemplo, teríamos e(k, a)+e(k, b) = e(k, a+b) para um esquema de criptografia e(., .), chave de criptografia k e textos simples a e b. Assim, uma terceira parte poderia computar o texto criptografado do valor da adição a + b a partir dos textos criptografados de a e b, e retornar isso ao proprietário que poderia descriptografar para obter o resultado do cálculo no texto simples [Al Badawi e Polyakov 2023].\n",
            "\n",
            "No contexto do FL, a viabilidade da HE é particularmente restringida ao criptografar atualizações de modelo locais. O uso de modelos de linguagem grandes, como o BERT com 110M de parâmetros, torna-se praticamente inviável dada a largura de banda e a sobrecarga computacional associadas ao processamento de gradientes criptografados. Essa limitação ressalta a necessidade de técnicas de criptografia mais eficientes, estratégias de compressão de modelo e a adoção de arquiteturas mais compactas - sendo esta última o foco da avaliação deste artigo.\n",
            "\n",
            "3. Metodologia\n",
            "Esta seção descreve a proposta deste trabalho para tornar o NER médico federado seguro com HE. A solução proposta, chamada FedHE, visa ser concebida como um framework genérico sobre como a HE pode ser usada no FL, tornando-os compatíveis com regulamentações de privacidade de dados e permitindo cenários como NER médico funcionarem sem o risco de ataques de inferência.\n",
            "\n",
            "O protocolo FedHE utiliza criptografia HE para proteger os dados dos gradientes. Assim, mesmo que atacantes bizantinos comprometam o servidor de computação, eles não têm acesso às informações dos dados dos gradientes de cada cliente de aprendizado. Além disso, é impossível para atacantes bizantinos usarem esses dados de gradientes criptografados para treinar modelos sombra.\n",
            "\n",
            "Neste trabalho, o esquema criptográfico CKKS (Cheon-Kim-Kim-Song) [Marcolla et al. 2022] é utilizado para criptografar os gradientes dos clientes, preservando as operações aritméticas de adição e multiplicação por um número escalar em texto simples. O CKKS é um esquema criptográfico assimétrico que requer pares de chaves, portanto, um serviço de gerenciamento de chaves (KMS) é necessário. Observe que este trabalho não visa detalhar nem o esquema de criptografia nem o protocolo KMS, mas dependemos de estratégias e algoritmos definidos publicamente na literatura. O algoritmo do coordenador orquestra a rede federada (veja Algoritmo 1). Normalmente, ele define como o protocolo funciona, estabelece mecanismos para definir a arquitetura, garante confiança entre os clientes e agrega os gradientes gerados localmente.\n",
            "\n",
            "O algoritmo FedAvg é executado homomorficamente, sem a descriptografia das atualizações dos clientes. Além disso, embora a estratégia KMS não seja especificada neste artigo, assumimos que o coordenador tem acesso apenas à chave pública.\n",
            "\n",
            "Algoritmo 1: Coordenador FedHE. Os K clientes são indexados por c ∈ C, T é o total de rodadas de aprendizado federado e L é a função de perda. O objetivo é obter θ∗ que minimize a função de perda dos clientes.\n",
            "\n",
            "Estado: Modelo local com parâmetros θi. Função ServerTrain: inicializar θ(0), solicitar chave pública do KMS, para cada rodada t = 0, . . . , T faça: número de clientes: m ← max(C · K, 1), seleção de clientes: St ← (conjunto aleatório de m clientes), para cada cliente c ∈ St em paralelo faça: ∇enc(L(t+1) c) ← ClientTrain(c). No final, homomorphic FedAvg: ∇enc(L(t+1)) ← (cid:80)K for each client c ∈ C in parallel do ClientUpdate(c, ∇enc(L(t+1)) c=1 end end nc n ∇enc(L(t+1) c)\n",
            "\n",
            "O algoritmo de treinamento do cliente FedHE é onde o treinamento efetivo acontece (veja o Algoritmo 2). Cada cliente treina em seu próprio conjunto de dados de treinamento privado e compartilha apenas as atualizações de gradientes criptografados com o coordenador para agregação do modelo.\n",
            "\n",
            "Algoritmo 2: Cliente FedHE. X representa as amostras de treinamento enquanto Y representa os rótulos de treinamento. I, ϵ, η representam o número de épocas locais, a tolerância e a taxa de aprendizado, respectivamente. O objetivo é obter θ∗ que minimize a função de perda L.\n",
            "\n",
            "Estado: Modelo local com parâmetros θi. Função ClientTrain: solicitar chave pública do KMS para cada época i = 0, . . . , I faça: propagação para frente: ˆYi = forward(X, θi), calcular perda: Li = loss(Y, ˆYi), se Li <ϵ então interromper, senão: retropropagação: ∇Li = backprop(X, θi, Li), criptografia de gradientes: ∇enc(Li) = encrypt(∇Li, PublicKey), retornar ∇enc(Li)\n",
            "\n",
            "Função ClientUpdate(∇enc(Lagg)): solicitar chave privada do KMS, descriptografar gradientes: ∇Lagg = decrypt(∇enc(Lagg), PrivateKey), atualizar: θi+1 = θi − η∇Lagg\n",
            "\n",
            "Em conclusão, o protocolo FedHE é adaptável a uma variedade de arquiteturas de modelo e pode ser integrado perfeitamente em plataformas FL estabelecidas como Flower [Beutel et al. 2020], TensorFlow Federated2, FATE3, entre outras.\n",
            "\n",
            "4. Resultados\n",
            "Nesta seção, apresentamos as principais descobertas de nossa análise do FedHE, focando em dois aspectos práticos: (1) o desempenho do FedHE treinado com modelos compactos do BERT em comparação com modelos de última geração, e (2) o desempenho e as sobrecargas de consumo de recursos associadas ao FedHE.\n",
            "\n",
            "Corpora de Reconhecimento de Entidade Nomeada\n",
            "Comparamos o FedHE com esquemas de treinamento alternativos em dois conjuntos de dados de NLP biomédico e um conjunto de dados de notícias, focando em tarefas de NER. No NER, o objetivo é identificar e classificar entidades nomeadas, como doenças e genes, a partir de uma sequência dada de tokens. Os corpora selecionados foram escolhidos com base em dois critérios principais: são publicamente disponíveis, garantindo a reprodutibilidade dos resultados, e são comumente usados em artigos bem citados, o que ajuda a garantir a qualidade dos dados. Um resumo dos conjuntos de dados selecionados pode ser encontrado na Tabela 1.\n",
            "\n",
            "Tipo de Entidade/Relação  Corpus\n",
            "Tipo  CONLL-2003  Geral  BC2GM  BC4CHEMD  Medicamento/Chem  Gene  Artigos de notícias  Resumo Medline  Resumo PubMed\n",
            "Trema Dev 14987 3466 26006 3251 94170  Teste 3684 3251 11772 11771\n",
            "\n",
            "Tabela 1. Lista de corpora de NER e suas estatísticas\n",
            "\n",
            "Quais são os custos de desempenho e recursos do FedHE?\n",
            "No FedHE, a criptografia de gradientes introduz um aumento substancial no tamanho dos dados, o que pode impactar significativamente a largura de banda. A Tabela 4 fornece uma análise comparativa da sobrecarga de tamanho associada a diferentes modelos do BERT ao usar o esquema de criptografia CKKS. Por exemplo, o BERTtiny, que possui um tamanho de gradiente em texto simples de 16 MB, aumenta para 340 MB quando criptografado. Da mesma forma, o tamanho do gradiente do BERTmini cresce de 42 MB para 864 MB sob o mesmo esquema. O efeito mais pronunciado é visto com o BERTblue e o BERTlarge blue, onde o tamanho do gradiente foi 20 vezes e mais de 50 vezes maior, respectivamente. Embora o treinamento local permaneça inalterado, esses aumentos no tamanho do texto criptografado levam a sobrecargas significativas de largura de banda. Assim, modelos como o BERTblue tornam-se impraticáveis para o FedHE devido ao tamanho proibitivo dos gradientes criptografados, enfatizando a necessidade de abordagens mais eficientes em termos de largura de banda ou modelos menores para manter a viabilidade em configurações federadas. \n",
            "\n",
            "A Tabela 4 destaca a impraticabilidade de Modelos de Linguagem Grandes no FedHE devido ao seu crescimento exponencial nas exigências de memória e largura de banda. A Tabela 3 mostra que, enquanto as operações em gradientes criptografados, particularmente a criptografia, tornam-se mais custosas com o aumento dos tamanhos dos parâmetros, essas geralmente não representam um gargalo no treinamento. No entanto, se o treinamento federado envolver rodadas de agregação frequentes e épocas locais infrequentes, essas operações podem se tornar um gargalo significativo quando o número de parâmetros for suficientemente grande. Normalmente, os clientes realizam extensos treinamentos locais com agregações menos frequentes, o que se alinha tanto com o desempenho quanto com as eficiências operacionais no FedHE.\n",
            "\n",
            "Devido às excessivas demandas de memória do BERTlarge blue—mais de 50 GB no FedHE e 1 GB em texto simples—junto com requisitos significativos de largura de banda e processamento, este trabalho se concentrará em comparar a eficácia apenas com o modelo base BERTblue. Trabalhos futuros podem abordar a comparação com versões maiores do BERT, como o BERTlarge blue.\n",
            "\n",
            "Modelo  BERTtiny  BERTmini  Tipo  Criptografar  Descriptografar  Criptografar  Descriptografar  Média (s)  Desvio Padrão (s) 8.016 2.179 21.877 5.884 0.133 0.053 0.251 0.095  Percentil 99 (s) 8.484 2.337 22.671 6.144\n",
            "\n",
            "Tabela 3. Estatísticas resumidas para tempos de criptografia e descriptografia dos modelos BERT\n",
            "\n",
            "Enquanto a Tabela 4 destaca restrições significativas de largura de banda no lado do servidor no FedHE, também é essencial avaliar os custos de recursos e tempo associados a operações de agregação. A Figura 4 esclarece o desempenho da agregação à medida que o número de clientes aumenta para o modelo BERTtiny usando o corpus BC2GM. A análise indica que o tempo de agregação do FedAvg homomórfico não apresenta um problema de eficiência no processo de treinamento; especificamente, o BERTtiny completa a agregação em aproximadamente 20 segundos com 22 clientes, enquanto o BERTmini exige cerca de 50 segundos com 14 clientes. No entanto, é importante notar que à medida que o número de clientes cresce, a memória necessária para armazenar gradientes criptografados aumenta significativamente. Por exemplo, a agregação para o BERTmini com 16 clientes levou a uma falha do coordenador devido à insuficiência de memória. Embora tais problemas possam ser mitigados usando estratégias de memória externa, essas soluções introduzem uma sobrecarga adicional de desempenho.\n",
            "\n",
            "Como o FedHE com modelos compactos do BERT se compara a modelos de ponta do BERT para NER médico?\n",
            "Outra questão fundamental de pesquisa para este trabalho é entender quão distantes os modelos compactos do BERT estão dos modelos de ponta para NER médico federado. Em particular, gostaríamos também de entender se a introdução da HE pode distorcer os resultados gerais. A tabela 4 mostra uma comparação F1 das abordagens testadas com os modelos de ponta.\n",
            "\n",
            "Outra questão crítica de pesquisa abordada neste trabalho é avaliar a diferença de desempenho entre modelos compactos do BERT e modelos de ponta para NER médico federado. Além disso, investigamos se a integração da HE afeta os resultados de desempenho global. A Tabela 4 fornece uma análise comparativa das pontuações F1 para as abordagens avaliadas em relação aos modelos de ponta.\n",
            "\n",
            "Modelo  Método  CONLL-2003  BC2GM  BC4CHEMD\n",
            "BERTtiny  Treino 0.804 ± 0.002 0.618 ± 0.006 0.865 ± 0.005\n",
            "Único 0.841 ± 0.000 0.728 ± 0.002 0.953 ± 0.001\n",
            "Central 0.726 ± 0.010 0.464 ± 0.005\n",
            "Federado 0.624 ± 0.001 0.816 ± 0.000 0.650 ± 0.014 0.744 ± 0.005\n",
            "FedHE 0.802 ± 0.000 0.690 ± 0.002 0.961 ± 0.002\n",
            "BERTmini  Único 0.995 ± 0.001 0.787 ± 0.005 0.990 ± 0.000\n",
            "Central 0.993 ± 0.000 0.958 ± 0.001 0.758 ± 0.013\n",
            "FedHE  BERTblue  Central  Avaliação\n",
            "0.391 ± 0.047 0.537 ± 0.003 0.460 ± 0.022 0.605 ± 0.011 0.598 ± 0.010 0.448 ± 0.008 \n",
            "0.621 ± 0.004 0.464 ± 0.005 0.538 ± 0.004 0.802 ± 0.000 0.613 ± 0.002 \n",
            "0.859 ± 0.001 0.584 ± 0.000 0.833 ± 0.000 0.994 ± 0.000 \n",
            "0.781 ± 0.001 0.998 ± 0.000 0.739 ± 0.004 \n",
            "0.877 ± 0.000 0.590 ± 0.001 0.683 ± 0.000 0.999 ± 0.000 \n",
            "\n",
            "Tabela 4. Comparação da pontuação F1 do FedHE com vários modelos BERT em conjuntos de dados de NER médicos. Os desvios padrão estão mostrados entre parênteses. Negrito indica que o FedHE supera o Federal, enquanto sublinhado indica que supera a avaliação Centralizada.\n",
            "\n",
            "Configuração do Treinamento\n",
            "Em todos os experimentos, realizamos 50 épocas para treinar os modelos. O aprendizado centralizado e de único cliente, realizamos 50 épocas locais em seus conjuntos de dados privados. Os dados de único cliente foram obtidos dividindo o conjunto de dados em duas partes e pegando apenas uma das corporações. As abordagens federadas e FedHE rodaram 5 rodadas de agregação com 10 épocas locais cada nos dados dos clientes. Testes de eficácia, mostrados na Tabela 4, foram conduzidos com 2 clientes. Os desvios padrão para federados e FedHE foram calculados a partir de todos os clientes, enquanto os desvios de único cliente e centralizados foram provenientes de 2 execuções. Para treinar os modelos, utilizamos o otimizador Adam com uma taxa de aprendizado inicial de 2e − 5 e um decaimento de peso de 0.1. Todos os experimentos foram realizados em um sistema equipado com uma GPU NVIDIA A100 e pelo menos 32 GB de RAM disponível.\n",
            "\n",
            "Discussão\n",
            "Os resultados na Tabela 4 destacam o desempenho de diferentes modelos do BERT em 4 configurações diferentes (centralizado, único cliente, federado e FedHE) para tarefas de NER médico. O modelo centralizado BERTblue é usado como base de comparação, representando o estado da arte em modelos baseados em BERT para reconhecimento de entidades nomeadas médicas. Este modelo estabelece um alto padrão de comparação, demonstrando sua precisão em todos os conjuntos de dados.\n",
            "\n",
            "Importante, a aplicação da HE não distorce a eficácia do modelo. Ao contrário, o ruído de criptografia introduzido pelos processos de criptografia e descriptografia não prejudica a precisão do modelo. Em vez disso, às vezes até melhora o desempenho, como reflete os valores em negrito na tabela. A hipótese mais forte para esse fato é que o pequeno ruído adicionado pelas operações com textos criptografados ajudou o modelo a generalizar melhor. Isso indica que a HE pode ser integrada efetivamente sem comprometer e potencialmente melhorar o desempenho do modelo.\n",
            "\n",
            "A análise também revela que modelos de aprendizado de único cliente, como BERTtiny e BERTmini, frequentemente alcançam maior precisão de treinamento, mas generalizam de forma menos eficaz em comparação com abordagens federadas e FedHE. Modelos de aprendizado federado e FedHE exibem superior generalização em todos os corpora avaliados. O BERTtiny mostra desempenho inferior em comparação com o BERTmini, com diferenças significativas em todos os 4 métodos testados para todos os corpora. O BERTmini, usando apenas 11M de parâmetros, apresentou resultados satisfatórios mesmo quando comparado ao mais complexo BERTblue com 108M de parâmetros. Isso sugere que podemos alcançar resultados mais próximos do estado da arte usando representações compactas do BERT sem tornar o FL+HE impraticável. Este trabalho também sugere que avaliar outras variantes do BERT um pouco mais complexas, como o BERTsmall, poderia fornecer insights adicionais e potenciais melhorias no desempenho do modelo.\n",
            "\n",
            "5. Conclusão\n",
            "De um modo geral, o FedHE mostra um framework genérico para integrar HE em um protocolo FL como uma alternativa robusta para tarefas de NER médica federada. O FedHE oferece desempenho robusto e vantagens práticas, tornando-o uma escolha atraente para cenários onde a privacidade dos dados e a eficácia do modelo são críticas. Os resultados sublinham a viabilidade do FedHE em manter alto desempenho enquanto incorpora técnicas de criptografia. Para trabalhos futuros, destacamos (1) estudar cenários onde o número de clientes é maior e os dados são não-IID; (2) avaliar a viabilidade de outras variantes compactas do BERT, como o BERTsmall e (3) testar os modelos em relação a outras linhas de base baseadas em LLM, como o BERTlarge blue.\n",
            "\n",
            "6. Agradecimentos\n",
            "Os autores também gostariam de agradecer à Universidade Federal de Ouro Preto (PROPPI/UFOP) pelo apoio ao desenvolvimento deste estudo.\n",
            "8\n",
            "Resumo. O reconhecimento de discurso tóxico e de ódio em plataformas de mídia social é importante devido aos riscos significativos que representam para os usuários e para o ecossistema digital. Modelos de última geração, como o BERTimbau, estabeleceram benchmarks para classificação de texto em português, mas desafios permanecem na detecção precisa de conteúdo tóxico. Este artigo investiga a eficácia do fine-tuning de um modelo menor e open-source, o LLaMA 3.1 8B 4bit, para essa tarefa. Propomos um método de evolução de prompt iterativa para otimizar o desempenho do modelo. Nossos resultados demonstram que o fine-tuning melhora significativamente o F1-score do modelo LLaMA de 0,61 para 0,75, superando o BERTimbau em precisão e igualando o desempenho do GPT-4o mini. No entanto, a abordagem depende da qualidade dos modelos de linguagem utilizados para a evolução do prompt, destacando a necessidade de mais pesquisas para aumentar a robustez nesta área. \n",
            "\n",
            "1. Introdução\n",
            "A tarefa de reconhecer discurso tóxico e de ódio ganhou atenção substancial nos últimos anos, particularmente com o aumento do conteúdo gerado por usuários em plataformas de mídia social. À medida que essas plataformas moldam cada vez mais o discurso público, a proliferação de conteúdo nocivo apresenta riscos significativos tanto para usuários individuais quanto para o ambiente digital mais amplo. Consequentemente, a necessidade de ferramentas de moderação eficazes aumentou, impulsionando a pesquisa em direção a soluções automatizadas capazes de operar em escala. Métodos de ponta atuais para classificação automatizada de conteúdo tóxico predominantemente utilizam arquiteturas baseadas em transformers, com modelos apenas de codificador sendo os mais comuns. No contexto da língua portuguesa, o BERTimbau emergiu como uma abordagem líder [Souza et al. 2020], demonstrando desempenho superior em várias tarefas de PNL, incluindo classificação de emoções [Hammes e de Freitas 2021], detecção de discurso tóxico [da Rocha Junqueira et al. 2023, Oliveira et al. 2023], agrupamento de notícias [Pereira e da Silva 2023], entre outras tarefas [dos Santos e Paraboni 2023, Serras e Finger 2021]. A capacidade do BERTimbau de capturar sutilezas nas expressões em português estabeleceu um padrão elevado no campo, tornando-o o benchmark para tarefas de classificação multiclasse. No entanto, apesar de sua eficácia, o problema da classificação precisa de conteúdo tóxico permanece um desafio aberto, particularmente na paisagem diversa e em evolução do discurso online. \n",
            "\n",
            "Avanços recentes mudaram o foco para modelos apenas de decodificador, como LLM2Vec [BehnamGhader et al. 2024] e NV-Embed [Lee et al. 2024], que demonstraram resultados promissores em várias línguas. Notavelmente, o OpenAI Chat-GPT [OpenAI et al. 2024] 1, um grande modelo de linguagem apenas de decodificador, demonstrou desempenho competitivo nesta área [Oliveira et al. 2023]. O surgimento de modelos open-source, como a família de modelos Meta LLaMA [Dubey et al. 2024], ainda mais compelidos a reexaminar metodologias existentes, levantando questões de pesquisa sobre o potencial desses novos modelos. \n",
            "\n",
            "Baseando-se nesses desenvolvimentos recentes, este trabalho explora as capacidades dos modelos apenas de decodificador, com foco específico no modelo LLaMA 3.1 8B 4bit [Dubey et al. 2024]. Este modelo é particularmente atraente devido à sua natureza open-source, desempenho de benchmark e tamanho relativamente menor, tornando-o bem adequado para fine-tuning em tarefas especializadas, como a classificação de conteúdo tóxico em português. As principais perguntas de pesquisa que orientam esta investigação são RQ1: Um modelo LLaMA 3.1 8B 4bit ajustado pode alcançar ou superar a performance do GPT-4o mini na classificação de conteúdo tóxico em português? RQ2: Este modelo pode superar a abordagem atual baseada no BERTimbau na mesma tarefa? Para abordar essas questões, propomos uma abordagem heurística que utiliza um LLM maior (GPT-4o-mini) para refinar os prompts empregados por um LLM menor, automatizando assim a engenharia de prompt. O prompt ótimo é então usado para ajustar o modelo LLaMA 3.1 8B 4bit para classificação de conteúdo tóxico em mídias sociais, utilizando o conjunto de dados TolDBr - um grande conjunto de dados público sobre esta tarefa [Leite et al. 2020]. Nossos resultados mostram que o modelo LLaMA 3.1 8B 4-bit ajustado, operando em modo de classificação zero-shot, supera o modelo baseado no BERTimbau em precisão e é equivalente ao GPT-4o mini. \n",
            "\n",
            "2. Materiais e Métodos\n",
            "Embora o foco principal deste trabalho seja investigar o desempenho de um modelo de linguagem pequeno e open-source (com apenas 8B de parâmetros) para a tarefa de detecção de texto tóxico em português, a escolha do prompt é um desafio significativo. A qualidade do prompt influencia fortemente o desempenho do LLM [Brown et al. 2020]. Portanto, este trabalho propõe uma abordagem simples para a evolução de prompts, utilizando, em última instância, o melhor prompt identificado para ajustar o modelo. \n",
            "\n",
            "As subseções a seguir descrevem o conjunto de dados selecionado para benchmarking, que é um grande e popular conjunto de dados segundo os padrões da língua portuguesa para esta tarefa. Além disso, um esboço da metodologia para selecionar o melhor prompt e a abordagem utilizada para o fine-tuning do modelo. \n",
            "\n",
            "2.1. Conjunto de dados Told-Br\n",
            "Utilizamos o conjunto de dados ToLD-br, desenvolvido em [Leite et al. 2020] para treinamento e teste dos modelos utilizados neste estudo. Este conjunto de dados contém 21.000 tweets, anotados de forma binária como “tóxico” ou “não tóxico”. Além disso, os tweets também são classificados em diferentes categorias de toxicidade, como LGBTfobia, insultos, racismo, misoginia e xenofobia. \n",
            "\n",
            "Neste estudo, focamos na classificação binária entre “tóxico” e “não tóxico”, utilizando as anotações correspondentes para treinar e testar nossos modelos. O conjunto de dados foi dividido de maneira estratificada, com 80% dos tweets alocados para o conjunto de treinamento e os 20% remanescentes para o conjunto de teste. \n",
            "\n",
            "2.2. Engenharia de Prompt: Refinamento Iterativo de Prompt\n",
            "O desafio em usar grandes modelos de linguagem (LLMs) para classificação zero-shot reside em identificar o prompt mais eficaz. Este estudo propõe uma heurística para refinar iterativamente os prompts usando um LLM maior, com a intenção de aumentar a precisão da classificação em um LLM menor. \n",
            "\n",
            "Nossa abordagem se baseia em pesquisas anteriores, principalmente trabalhos de [Oliveira et al. 2024] e [Oliveira et al. 2023], que defendem o uso de aprendizado em contexto para classificação de posts em mídias sociais. Embora esses estudos explorem tanto as modalidades zero-shot quanto few-shot, nosso foco permanece exclusivamente no cenário zero-shot. \n",
            "\n",
            "Dado que os LLMs mostraram funcionar de forma eficaz como otimizadores de caixa-preta [Zheng et al. 2023] e são alternativas viáveis para operações de mutação e crossover em algoritmos genéticos [Lehman et al. 2023, Meyerson et al. 2023], nos inspiramos no trabalho apresentado em [Guo et al. 2024] para propor um algoritmo simplificado para evoluir prompts adaptados explicitamente à tarefa de detecção de discurso tóxico em português. \n",
            "\n",
            "A metodologia é estruturada como a Figura 1 ilustra: Inicialmente, uma população de prompts é inicializada, cada um especificamente projetado para classificar posts em mídias sociais como tóxicos ou não tóxicos. Os prompts, então, passam por um processo de seleção, retendo apenas os que apresentam melhor desempenho com base em métricas de avaliação. Em seguida, operações para evoluir o prompt são aplicadas utilizando uma instrução a um LLM maior, como o GPT-4, que ajuda a gerar novas variações ao recombinar elementos de prompts selecionados existentes. Este processo é refinado iterativamente para aumentar a qualidade dos prompts. Finalmente, o prompt ótimo deste ciclo é utilizado para ajustar o modelo. O Algoritmo 1 fornece uma visão geral em pseudocódigo desses passos. \n",
            "\n",
            "Figura 1. O processo heurístico de evolução iterativa de prompts começa com um conjunto inicial de prompts, que são avaliados usando o modelo LLaMA com base em seus F1-scores. Os prompts de melhor desempenho são então selecionados, e o GPT-4o mini gera novos prompts. Esses novos prompts são adicionados de volta à população, e o processo se repete. O melhor prompt desse ciclo iterativo é, em última instância, selecionado para uso posterior. Todos os prompts e instruções utilizados neste estudo foram escritos em português. \n",
            "\n",
            "2.3. Metodologia de Fine-Tuning de LLM\n",
            "Esta metodologia ajusta o modelo usando uma versão quantizada para melhorar a eficiência de memória e a velocidade. O Fine-Tuning Eficiente em Parâmetros (PEFT) [Houlsby et al. 2019, Hu et al. 2021] e técnicas QLoRA [Dettmers et al. 2023] reduzem a complexidade do modelo, focando na otimização de projeções QKV e Camadas de Feed Forward. Os dados de treinamento são divididos em conjuntos de treinamento e validação. Prompts específicos, estruturados como prompts Alpaca, alinham o modelo com os objetivos de classificação de conteúdo tóxico em português. \n",
            "\n",
            "Exemplo de Prompt Alpaca: Abaixo está uma instrução que descreve uma tarefa, emparelhada com uma entrada que fornece mais contexto. Escreva uma resposta que complete adequadamente o pedido. \n",
            "Você está analisando um post em mídia social.\n",
            "Instrução: O texto contém discurso de ódio, ofensas, agressões, insultos, palavras de baixo calão ou qualquer forma de toxicidade, responda apenas com 'sim'. Se for apropriado e não tóxico, responda 'não'. \n",
            "Se a Entrada: Tweet: toda vez que o clima muda, minha garganta fica inflamada... droga! \n",
            "Resposta: não. \n",
            "\n",
            "3. Configuração Experimental e Resultados\n",
            "Dois experimentos principais são conduzidos para abordar as perguntas de pesquisa propostas neste estudo. Primeiro, o Experimento #1 é projetado para identificar o melhor prompt. Utilizando este prompt, o modelo LLaMA 3.1 8B é ajustado com os dados de treinamento do conjunto de dados ToLD-Br (Experimento #2). Quatro versões do modelo são ajustadas, variando parâmetros relacionados ao PEFT/QLoRA. Um experimento adicional é proposto para avaliar o desempenho do uso de um modelo fine-tuned GPT-4o mini dentro do mesmo cenário. Todos os experimentos são realizados em um ambiente do Google Colab, utilizando uma GPU A100. O código-fonte está disponível em https://github.com/oliveiraamanda/ToxicSpeech-Llama-STIL-2024. \n",
            "\n",
            "3.1. Experimento #1 - Engenharia de Prompt\n",
            "Para evoluir os prompts usando o algoritmo iterativo proposto aqui, é essencial primeiro definir a função de custo a ser minimizada. O F1-score da classificação binária em uma partição dos dados de treinamento é selecionado como a função de custo. Como nossa população consiste em frases em linguagem natural, os indivíduos iniciais devem ser definidos manualmente para o domínio específico. Essa abordagem também foi adotada em [Guo et al. 2024]. Neste trabalho, baseamos nossos prompts iniciais naqueles propostos em [Oliveira et al. 2023] e [Oliveira et al. 2024], uma vez que esses estudos servem como uma linha de base e abordam o mesmo conjunto de dados. Em seguida, derivamos prompts adicionais adicionando ou removendo frases e palavras, totalizando seis prompts. O modelo alvo, LLaMA 3.1 8B, realiza a classificação utilizando apenas 50 instâncias de cada classe para calcular o F1-score, dada a alta carga computacional dessa função. Após isso, os prompts são evoluídos através de um processo iterativo envolvendo dois modelos—o modelo alvo LLaMA 3.1 8B e o GPT-4o mini ao longo de 50 épocas. Depois desse período, o melhor prompt (com o maior F1-score) é selecionado e apresentado abaixo. “Melhor Prompt: Você está analisando um post em mídia social. Se o texto contém discurso de ódio, ofensas, agressões, insultos, palavras de baixo calão ou qualquer forma de toxicidade, responda apenas com 'sim'. Se for apropriado e não tóxico, responda 'não'.” \n",
            "\n",
            "3.2. Experimento #2 - Processo de Fine-Tuning do LLaMA\n",
            "Para facilitar o fine-tuning em hardware modesto, utilizamos uma versão quantizada de 4 bits do modelo LLaMA 3.1, utilizando a técnica QLoRA [Dettmers et al. 2023], com 8 bilhões de parâmetros modelo-LLaMA 3.1 8B [Dubey et al. 2024]2. Utilizamos a biblioteca PEFT da Hugging Face3 com a biblioteca Unsloth4, definindo a taxa de aprendizado para 2e−4 e o comprimento da sequência em 2048 tokens, enquanto variamos os parâmetros “rank” e “LoRa Alpha”. \n",
            "\n",
            "O processo de fine-tuning utilizou o prompt mais eficaz e envolveu 3.000 passos de treinamento, com um tamanho de lote de 2 e acúmulo de gradientes definido para 4, processando efetivamente 6.000 instâncias do conjunto de dados de treinamento. Os resultados de experimentos que variam os parâmetros “rank” e “LoRa alpha” são apresentados na Tabela 1, enquanto a função de perda do fine-tuning utilizando “rank=16” e “LoRa alpha=16” é mostrada na Figura 2. \n",
            "\n",
            "3.3. Experimento #3 - Processo de Fine-Tuning do GPT-4o mini\n",
            "Para ajustar o modelo GPT-4o mini, utilizamos a plataforma Azure AI Studio, aproveitando os mesmos dados de treinamento utilizados no Experimento #2. Adotamos o melhor prompt identificado no Experimento #1 e criamos um arquivo JSONL onde cada instância do conjunto de treinamento foi precedida pelo prompt e acompanhada por seu respectivo rótulo. \n",
            "\n",
            "Escolhemos a versão de 2024-07-18 do GPT-4o-mini, que era a disponível para fine-tuning na Azure. Após o treinamento, o modelo foi implantado na plataforma Azure, permitindo seu uso através de chamadas de API. \n",
            "\n",
            "Durante a avaliação, observamos que a utilização do Azure Studio, que incorpora uma camada adicional de moderação de conteúdo além da fornecida pela OpenAI, levou a certas imprecisões na moderação. Aproximadamente 1% do conjunto de teste foi erroneamente classificado devido a “erros de moderação de conteúdo”. Para esses casos, atribuímos o rótulo “não tóxico”. \n",
            "\n",
            "3.4. Comparação de Resultados\n",
            "Identificado através da abordagem de evolução de prompt iterativa, o prompt mais eficaz foi testado com três modelos adicionais: Mari-taca 6 AI Sabiá3 [Pires et al. 2023], OpenAI GPT-4o mini [OpenAI et al. 2024] 7 e OpenAI ChatGPT 3.5 Turbo [Brown et al. 2020] 8, assim como o modelo BERTimbau [Souza et al. 2020]. \n",
            "\n",
            "Os resultados apresentados na Tabela 2 destacam a importância do fine-tuning do modelo LLaMA 3.1 8B. Especificamente, o fine-tuning melhorou o F1-score de 0,61 para 0,75, demonstrando um ganho substancial de desempenho. Além disso, ao aplicar a metodologia de fine-tuning usando o prompt proposto em [Oliveira et al. 2023], o F1-score atingiu 0,70. No entanto, nossa abordagem de evolução de prompt melhorou ainda mais isso para 0,75, indicando que o prompt refinado contribuiu significativamente para o desempenho do modelo. \n",
            "\n",
            "Além disso, o modelo LLaMA 3.1 8B, apesar de ter sido ajustado com apenas 3.000 passos e 6.000 instâncias, apresentou desempenho competitivo em comparação com outros modelos de ponta, como GPT-4o mini, Sabiá3 e BERTimbau. Notavelmente, Sabiá3, um modelo líder da Maritaca AI, demonstrou precisão comparável ao GPT-4o mini em vários exames brasileiros de alta importância, como OAB, ENEM e ENADE. Esses resultados ressaltam a eficácia de nossa metodologia de evolução de prompt e o potencial de modelos menores como o LLaMA 3.1 8B quando combinados com técnicas eficientes de fine-tuning. \n",
            "\n",
            "Os resultados na Tabela 1 revelam diferenças no desempenho do modelo com base na configuração dos parâmetros “r” (rank) e “LoRa alpha”. A configuração com “r=16” e “alpha=16” alcança o melhor desempenho geral, com um F1-Score de 0,75, equilibrando precisão (0,69) e recall (0,83). Aumentar “r” para 24 ou “alpha” para 24 leva a uma queda acentuada no desempenho, com o modelo apresentando sintomas de overfitting, particularmente com uma queda dramática no recall. A configuração com “r=8” e “alpha=16” demonstra recall alto (0,935), mas à custa da precisão, indicando um viés em relação à superprevisão da classe positiva. \n",
            "\n",
            "4. Conclusão\n",
            "Neste estudo, investigamos se um modelo de linguagem menor, open-source e quantizado como o LLaMA 3.1 8B 4 bits poderia efetivamente realizar a detecção de texto tóxico em português, particularmente quando otimizado usando uma abordagem de evolução de prompt iterativa juntamente com fine-tuning. Os experimentos demonstraram que, com prompts cuidadosamente evoluídos, o modelo pode alcançar desempenho competitivo, mesmo com um número limitado de passos de treinamento e instâncias. Isso destaca o potencial de modelos menores quando combinados com técnicas eficientes de engenharia de prompt. \n",
            "\n",
            "No entanto, a abordagem tem suas limitações. O sucesso do algoritmo de evolução de prompt depende fortemente da qualidade dos modelos de linguagem subjacentes utilizados para as operações de evolução de texto. Essa dependência pode ser uma limitação significativa, uma vez que deficiências nos modelos de linguagem afetam diretamente a qualidade dos prompts evoluídos e, consequentemente, o desempenho geral do modelo. Mais pesquisas são necessárias para abordar essas dependências e aumentar a robustez da abordagem de engenharia de prompt. \n",
            "\n",
            "Agradecimentos\n",
            "Gostaríamos de expressar nossos sinceros agradecimentos à Blip, cujo apoio generoso e assistência inestimável foram cruciais para a presença do primeiro autor no evento. Os autores também gostariam de agradecer à Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Código de Financiamento 001, à Fundação de Amparo à Pesquisa do Estado de Minas Gerais (FAPEMIG, bolsas APQ-01518-21), ao Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq, bolsas 307151/2022-0, 308400/2022-4) e à Universidade Federal de Ouro Preto (PROPPI/UFOP) pelo apoio ao desenvolvimento deste estudo.\n",
            "9\n",
            "Resumo. Neste artigo de revisão e opinião, discutimos as opções e desafios para a análise sintática. Apesar dos avanços significativos nos últimos anos, impulsionados principalmente por arquiteturas de redes neurais, a precisão da análise parece estar se aproximando de um platô. Este artigo propõe uma reflexão sobre os fatores que podem estar influenciando tais resultados e sugere alguns caminhos para o futuro.\n",
            "\n",
            "Motivação\n",
            "A importância de boas ferramentas de etiquetagem de partes do discurso e anotação de análise para tarefas subsequentes de Processamento de Linguagem Natural (PLN) é reconhecida por várias publicações na história da área, incluindo abordagens mais clássicas (simbólicas e estatísticas) e novas (geralmente baseadas em redes neurais). Em particular, a ascensão do framework “Universal Dependencies” (UD) [Nivre et al. 2016, de Marneffe et al. 2021] reacendeu o interesse pela análise de dependências, impulsionando novos esforços em estudos de sintaxe e análise em PLN. \n",
            "\n",
            "Este artigo de revisão e opinião tenta traçar um panorama dos esforços de análise mais recentes que se alinham aos padrões do UD, tentando identificar os limites potenciais da tarefa com os métodos atuais e quais outras estratégias podem ser adotadas para continuar melhorando os resultados alcançados na área. Tal iniciativa é ousada e naturalmente sujeita a falhas, pois as línguas naturais possuem características diversas e sempre há novos métodos de PLN surgindo. Diante disso, este artigo faz uma seleção de trabalhos da literatura, escolhendo abordagens relativamente recentes e amplamente citadas na área, a fim de traçar algumas conclusões tentativas (e certamente temporariamente ancoradas). Além da seleção de trabalhos possivelmente interessantes e da visão geral que apoiou este artigo, nossa contribuição inclui um exercício de “manter a cabeça acima da água”, mostrando o quão longe chegamos e as imperfeições do panorama.\n",
            "\n",
            "Sobre as técnicas atuais de análise\n",
            "O uso de redes neurais para a detecção de padrões e, consequentemente, a previsão de etiquetas de partes do discurso e relações de dependência tornou-se o método preferido na área [Goldberg 2016]. Dentro das redes neurais, várias técnicas, como Long Short-Term Memory (LSTM) em suas diversas versões [Van Houdt et al. 2020], juntamente com outras técnicas de aprendizado profundo [Dozat e Manning 2016], foram empregadas na última década com avanços consistentes para línguas bem dotadas de recursos. A mais recente evolução trazida pelos métodos de autoatenção [Vaswani et al. 2017], baseados nos famosos Transformers, remonta a alguns anos, mas ainda é uma das principais razões para as melhorias recentes.\n",
            "\n",
            "Em geral, embora diferentes critérios possam ser utilizados, neste artigo distinguimos os esforços de análise de acordo com as ferramentas de análise genéricas ou iniciativas específicas de análise de línguas; e a tecnologia básica empregada (por exemplo, BiLSTM, Deep Biaffine e Self-Attention). As ferramentas de análise mais populares, dentro do padrão UD, são UDPipe em suas versões 1.3 e 2.0 [Straka et al. 2016, Straka 2018], o pipeline Stanza [Qi et al. 2020], UDify [Kondratyuk e Straka 2019] e o pipeline AllenNLP [Dozat e Manning 2016]. Outras ferramentas foram desenvolvidas, mas aparentemente tiveram um número menor de usuários, como Diaparser [Attardi et al. 2021], UDapter [Üstün et al. 2020], UUParser [de Lhoneux et al. 2017], LAL-Parser [Mrini et al. 2019] e o algoritmo Hierarchical Pointer Network [Fernández-González e Gómez-Rodríguez 2023].\n",
            "\n",
            "Esses parsers geralmente concentram seus esforços para abranger várias línguas, sendo claramente multilíngues. Algumas dessas ferramentas foram especificamente projetadas para cobrir o grande conjunto de línguas disponíveis no repositório UD (que atualmente inclui mais de 150 línguas). No entanto, do ponto de vista tecnológico, as ferramentas apresentam diferenças consideráveis, embora todas elas façam uso de modelos de redes neurais. A tecnologia de Bidirectional LSTM (BiLSTM) [Van Houdt et al. 2020] é frequentemente empregada por muitos sistemas, incluindo UDPipe 2.0, Stanza e o algoritmo Hierarchical Pointer Networks. A tecnologia Deep Biaffine [Dozat e Manning 2016] é encontrada no pipeline AllenNLP, mas também em ferramentas como Diaparser e UDapter. Self-attention [Vaswani et al. 2017] é encontrada nas ferramentas LAL-Parser e UDify. Além disso, as ferramentas mencionadas apresentam diferenças na oferta de um modelo estático ou na possibilidade de realizar a construção de modelo através de um conjunto de treinamento e/ou adotar embeddings de palavras pré-treinados.\n",
            "\n",
            "Resultados da análise\n",
            "Os melhores valores relatados para cada um dos métodos de análise citados anteriormente estão mostrados na Tabela 1. Optamos por relatar apenas o Label Attachment Score (LAS), uma vez que este é geralmente a métrica de avaliação mais adotada e também uma das métricas mais punitivas, pois mede a precisão da identificação da relação de dependência e os tokens relacionados como cabeça e dependente. A tabela também indica a língua para a qual o maior LAS foi relatado.\n",
            "\n",
            "Tabela 1. Maior LAS reportado para as ferramentas de análise genéricas.\n",
            "sistema de análise                                LAS mais alto                 língua                           \n",
            "UUParser                                                      87.34%                          Português                      \n",
            "Stanza                                                        90.01%                          Espanhol                      \n",
            "UDPipe 1.3                                                  91.20%                          Hindi                           \n",
            "UDapter                                                       92.60%                          Italiano                       \n",
            "Diaparser                                                     93.65%                          Italiano                       \n",
            "UDify                                                         93.70%                          Russo                          \n",
            "UDPipe 2.0                                                  94.53%                          Russo                          \n",
            "AllenNLP pipeline                                   94.60%                          Inglês                         \n",
            "Hier. Pointer Networks                        96.15%                          Inglês                         \n",
            "LAL-parser                                                     96.29%                          Inglês                         \n",
            "\n",
            "tecnologia citada                             publicação\n",
            "BiLSTM                                                         2017                            \n",
            "Deep Biaffine                                            2020                            \n",
            "NN Classifier                                         2016                            \n",
            "Deep Biaffine                                            2020                            \n",
            "Deep Biaffine                                           2021                            \n",
            "Self-Attention                                          2019                            \n",
            "BiLSTM                                                        2018                            \n",
            "Deep Biaffine                                        2016                            \n",
            "BiLSTM                                                    2023                            \n",
            "Self-Attention                                          2019                            \n",
            "\n",
            "O desempenho dos métodos de análise varia consideravelmente de acordo com a língua a que são aplicados, conforme a literatura científica tem mostrado. Por exemplo, para o UDPipe 2, o LAS relatado para o espanhol e o italiano pode ser tão baixo quanto 80.68% e 77.34%, respectivamente. Para o pipeline AllenNLP, o LAS para o chinês e o espanhol foi de 85.38% e 91.65%, respectivamente. Os valores mostrados na tabela também podem refletir o número de línguas testadas. Enquanto UDify e UDPipe testam mais de 70 línguas, AllenNLP pipeline, UUParser e LAL-parser testam apenas 6, 5 e 2 línguas, respectivamente.\n",
            "\n",
            "Focando apenas na maior precisão de LAS apresentada na Tabela 1, é notável que a maioria das maiores pontuações está acima de 90% de precisão. Esses números sugerem que o Estado da Arte (SOTA) para LAS é alcançável, apesar da tecnologia empregada, da data de publicação e até mesmo da especificidade de cada desenvolvimento de análise. Observando os três melhores resultados relatados, vemos diferentes técnicas e que o inglês apresenta as melhores pontuações (provavelmente porque é a língua com mais recursos). \n",
            "\n",
            "Esse fato sugere que, após a disseminação de modelos baseados em redes neurais, a qualidade do modelo de treinamento desempenha um papel mais importante do que a tecnologia específica empregada. Assim, as variações para diferentes línguas parecem refletir a qualidade dos dados de treinamento para cada língua. Por exemplo, o LAS para UDify para uma língua com poucos recursos, como o bretão, é tão baixo quanto 40.19%, muito inferior ao máximo de 93.70% alcançado para o russo.\n",
            "\n",
            "Felizmente, a literatura é abundante em termos de esforços para línguas específicas. Esses trabalhos geralmente são apresentados com a construção de um corpus específico para a língua alvo ou transferindo o aprendizado de uma língua com mais recursos para a língua com menos recursos. Observando os trabalhos dedicados a línguas específicas, encontramos um número razoável de publicações, algumas das quais estão resumidas na Tabela 2.\n",
            "\n",
            "Tabela 2. Maior LAS relatado pelos esforços de línguas específicas.\n",
            "língua                                                  trabalho\n",
            "Yorubá                                               [Dione 2021]                               \n",
            "                                                       [Brigada Villa e Giarda 2023]         \n",
            "                                                       [Cassidy et al. 2022]                  \n",
            "                                                       [Lusito e Maillard 2021]               \n",
            "                                                       [Baig et al. 2021]                      \n",
            "                                                       [Dione 2021]                          \n",
            "                                                       [Türk et al. 2022]                     \n",
            "                                                       [Ghiffari et al. 2023]                 \n",
            "                                                       [Pedrazzini e Eckhoff 2021]            \n",
            "                                                       [Sánchez-Rodríguez et al. 2024]     \n",
            "                                                       [Alves et al. 2021]                   \n",
            "                                                       [Branco et al. 2022]                  \n",
            "                                                       [Kabiri et al. 2022]                  \n",
            "                                                       [Gamba e Zeman 2023]                 \n",
            "                                                       [Lopes e Pardo 2024]\n",
            "\n",
            "abordagem geral                                LAS\n",
            "Transferência de aprendizado          58.70%\n",
            "Inglês Arcaico                                   Transferência de aprendizado           59.34%\n",
            "Indonésio                                        Construção de Corpus                 60.74%\n",
            "Urdu                                             62.90%      Construção de Corpus   Transferência de aprendizado\n",
            "Wolof                                           67.83%      Construção de Corpus \n",
            "Turco                                             76.04%      79.22%      Construção de Corpus \n",
            "Irlandês                                       Transferência de aprendizado         79.66%\n",
            "Eslavo Antigo                             84.31%      Construção de Corpus \n",
            "Galego                                         Transferência de aprendizado        89.09% \n",
            "Croata                                         Construção de Corpus                 92.54%\n",
            "Português                                     Construção de Corpus                 92.68% \n",
            "                                                       Construção de Corpus                 94.61% \n",
            "                                                       Construção de Corpus                 94.70%\n",
            "                                                       Português                                Persa  \n",
            "                                                       Latim\n",
            "\n",
            "Os exemplos resumidos na Tabela 2 mostram esforços que podem ser agrupados em tentativas de atender a línguas com muito poucos recursos (como inglês arcaico, eslavo antigo, liguriano, urdu, bambarã, wolof e indonésio) e línguas com poucos recursos (como turco, croata, galego, irlandês, persa, latim e português). Enquanto as tentativas com línguas com muito poucos recursos são principalmente baseadas na transferência de aprendizado, as línguas com mais recursos concentram seus esforços em construir melhores corpora a serem usados para treinar modelos específicos. A observação do LAS na Tabela 2 mostra que os melhores resultados relatados estão também acima da pontuação de 90% dos métodos de análise genéricos (Tabela 1). Obviamente, os casos difíceis, como o yorubá e o inglês arcaico, apresentam baixa precisão, apesar dos esforços, provavelmente porque são línguas com poucos recursos. No entanto, é notável a precisão alcançada pela transferência de aprendizado para o eslavo antigo e o croata, assim como os altos valores para persa, latim e português com a produção de corpora de treinamento de alta qualidade.\n",
            "\n",
            "Para onde podemos caminhar? \n",
            "O advento de métodos populares de redes neurais na última década trouxe um progresso impressionante em várias áreas de PLN, colocando a Inteligência Artificial no centro dos tópicos em todas as áreas do conhecimento humano. Para as tarefas de análise, especificamente, usando os padrões do UD, notamos o aumento da qualidade desde 2016. No entanto, as melhorias parecem alcançar um limite de até 96% de precisão, e é notável que nenhuma especificidade mostra um claro predomínio.\n",
            "\n",
            "É também bem conhecido que línguas com poucos recursos podem não ser capazes de se beneficiar das vantagens dos métodos SOTA. Seria melhor para essas línguas investir em métodos mais clássicos ou na melhoria de recursos através da construção de corpora, incluindo uma anotação cuidadosa. Técnicas específicas como aumento de dados (data augmentation) e resolução conjunta de tarefas também podem ser caminhos interessantes (veja, por exemplo, o trabalho de [Yshaayahu Levi e Tsarfaty 2024] para análise do hebraico). Esses caminhos também podem ser relevantes para línguas que já alcançam precisão em torno de 95%, ou seja, que já estão entregando resultados SOTA.\n",
            "\n",
            "Outra questão relevante é se a busca por uma precisão melhor (acima de 96%) é um objetivo realista. Devemos nos reconciliar com esses 4% ausentes devido a uma incerteza natural da anotação de dependências? Olhando para o melhor método para uma língua específica (português), os autores [Lopes e Pardo 2024] [Duran et al. 2023a] [Duran et al. 2023b] discutem algumas razões para os erros remanescentes que também são citados na literatura: fenômenos sub-representados no corpus de treinamento (que podem ser resolvidos por meio de aumento de dados e/ou mais anotações de corpus) e questões difíceis de anotação (como decidir qual é a cabeça de uma frase preposicional) que às vezes podem desafiar até mesmo os humanos. Pessoalmente, acreditamos que a precisão acima de 99% já alcançada para etiquetagem de partes do discurso pode ser alcançada para a análise também. No entanto, pode ser necessário simplificar algumas distinções sintáticas ou procurar novas abordagens para o problema da análise.\n",
            "\n",
            "O leitor interessado pode encontrar mais informações no portal do projeto POeTiSA: https://sites.google.com/icmc.usp.br/poetisa\n",
            "\n",
            "Agradecimentos\n",
            "Este trabalho foi realizado no Centro de Inteligência Artificial da Universidade de São Paulo (C4AI - http://c4ai.inova.usp.br/), com apoio da Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP, processo nº 2019/07665-4) e da IBM Corporation. O projeto também foi apoiado pelo Ministério da Ciência, Tecnologia e Inovação, com recursos da Lei nº 8.248, de 23 de outubro de 1991, no âmbito do PPI-SOFTEX, coordenado pela Softex e publicado como Residence in TIC 13, DOU 01245.010222/2022-44.\n",
            "10\n",
            "Resumo. Este artigo analisou a robustez de um modelo de Classificação Automática de Redações (AES) de última geração, aplicando várias perturbações motivadas linguisticamente ao corpus Essay-BR. Nossas descobertas revelam que o modelo AES não conseguiu detectar essas modificações adversariais, frequentemente atribuindo notas mais altas às redações perturbadas do que às originais.  1. Introdução  A Classificação Automática de Redações (AES) tem como objetivo fornecer modelos computacionais para classificar automaticamente redações ou com mínima participação humana [Page 1966]. Embora essa área de pesquisa tenha mais de cinquenta anos [Beigman Klebanov e Madnani 2020], ela ganhou recentemente a atenção da comunidade brasileira devido aos corpora disponíveis publicamente [Marinho et al. 2021, Marinho et al. 2022a]. Vários métodos para classificar uma redação ou suas características surgiram com base nesses recursos [de Sousa et al. 2024, Oliveira et al. 2023, Marinho et al. 2022b]. Além disso, há um crescente interesse na área. Por exemplo, recentemente ocorreu a Competição PROPOR’24, cujo objetivo era desenvolver sistemas computacionais capazes de avaliar automaticamente redações [Mello et al. 2024].  Apesar dos avanços obtidos, a comunidade brasileira fez pouco esforço para avaliar a robustez dos métodos AES, incluindo a análise de sua sensibilidade a perturbações adversariais. [Liu et al. 2024] definem robustez como a capacidade de permanecer estável e confiável em diferentes circunstâncias. Estudos demonstram que os métodos AES para a língua inglesa são facilmente enganados [Perelman 2014], reduzindo a confiabilidade dos sistemas de classificação automatizada baseados em IA [Kabra et al. 2022]. Com base nessas limitações dos métodos AES para o inglês, investigamos se os métodos AES para o português sofrem de problemas de robustez.  Nosso objetivo é analisar métodos AES usando redações adversariais. Para isso, aplicamos um conjunto de perturbações a um corpus de redações, incluindo a adição de textos não relacionados, embaralhamento, exclusão e repetição de parágrafos de uma redação. Com essas perturbações motivadas linguisticamente, avaliamos uma estratégia AES de última geração para o português e descobrimos que o modelo analisado não conseguiu lidar com redações adversariais, produzindo, na verdade, melhores resultados para redações não perturbadas.  O restante deste artigo está organizado da seguinte forma: a Seção 2 apresenta brevemente trabalhos relacionados. Na Seção 3, detalhamos a análise realizada para verificar a robustez de um método de Classificação Automática de Redações em português. Finalmente, a Seção 4 conclui o artigo e indica direções futuras.   2. Trabalhos Relacionados  [Kabra et al. 2022] propuseram um esquema de avaliação adversarial agnóstico ao modelo e métricas associadas para sistemas AES, a fim de testar suas capacidades de compreensão da língua natural e robustez geral. Eles avaliaram modelos que variavam de abordagens baseadas em engenharia de características até os mais recentes algoritmos de aprendizado profundo. Os autores descobriram que os modelos AES são extremamente instáveis, de modo que mesmo modificações significativas (de até 25%) com conteúdo não relacionado ao tema das questões não diminuem a pontuação produzida pelos modelos.  [Liu et al. 2024] avaliaram a robustez e as capacidades de generalização dos modelos de Classificação Automática de Redações por meio de uma série abrangente de experimentos para validar a eficácia de vários modelos. Os autores selecionam aleatoriamente uma parte das redações e embaralham a ordem das frases ou excluem uma frase aleatoriamente para construir um conjunto de amostras adversariais chinesas para avaliar a robustez dos modelos. Os resultados mostraram que os modelos AES avançados têm baixa robustez e capacidade de generalização, e os Modelos de Linguagem de Grande Escala têm melhor desempenho, mas ainda precisam ser aprimorados.  3. Análise de Robustez  O corpus Essay-BR [Marinho et al. 2021] está organizado em conjuntos de treinamento, desenvolvimento e teste, cada um com 3.198, 686 e 686 redações. Usamos o conjunto de testes para gerar redações adversariais. Primeiro, extraímos as redações com pontuação maior ou igual a 680, uma vez que a pontuação média do ENEM 2023 foi de 641,6 pontos, resultando em 305 redações. Adotamos a estratégia de selecionar as melhores redações, evitando aquelas com vários problemas gramaticais, estruturais e argumentativos. Depois, aplicamos várias perturbações às redações para produzir redações adversariais. A partir do conjunto de redações originais e adversariais, avaliamos a robustez de um modelo de Classificação Automática de Redações (AES).  Implementamos perturbações motivadas linguisticamente para analisar a robustez de um modelo AES, ou seja, para verificar se o modelo consegue detectar qualquer diferença entre respostas originais e modificadas. As perturbações estão detalhadas a seguir.  Adicionar texto não relacionado. Adicionamos um parágrafo não relacionado em cada redação. Criamos três conjuntos de redações com conteúdo não relacionado, cada um indicando a posição onde um texto não relacionado foi adicionado. Os conjuntos contêm textos não relacionados adicionados no início, no meio e no final das redações. Extraímos um parágrafo de redações com um prompt diferente da redação analisada e o adicionamos à redação. Este teste tenta imitar o comportamento dos alunos quando tornam suas respostas extensas ao adicionar informações irrelevantes.  Adicionar receita de bolo e música. Embora essas perturbações acresçam conteúdo não relacionado a uma redação, elas possuem uma estrutura linguística muito diferente da prosa escrita nas redações. Portanto, isso pode ser usado para testar um sistema negativamente. Além disso, tem-se observado que os alunos usam essa estratégia em suas provas, possivelmente na tentativa de enganar o sistema. Criamos dois conjuntos de perturbações, um para receita de bolo e o outro para a música. Em ambos os conjuntos, adicionamos conteúdo não relacionado no meio das redações.  Adicionar texto repetido. Para essa estratégia adversarial, também criamos três conjuntos de perturbações. Para cada conjunto, repetimos o conteúdo da redação no início, no meio e no final das redações. A motivação para essa perturbação é que, segundo [Kabra et al. 2022], os alunos às vezes repetem frases ou palavras-chave específicas em suas respostas para torná-las mais longas, mas ainda em contexto e para formar parágrafos coesos [Higgins e Heilman 2014, Yoon et al. 2018].  Excluir texto. Semelhante à adição de texto repetido, criamos três conjuntos de perturbações nessa estratégia. Para cada conjunto, removemos um parágrafo no início, no meio e no final das redações. De acordo com [Kabra et al. 2022], esses testes geralmente quebram o fluxo de um argumento, excluem detalhes cruciais de uma redação e diminuem a verbosidade. Essa perturbação pode prejudicar seriamente a coerência e a qualidade da escrita e frustrar os leitores.  Embaralhar texto. Para essa perturbação, embaralhamos aleatoriamente o conteúdo de uma redação. A motivação para essa estratégia adversarial é analisar aspectos importantes da pontuação de redações, como coerência e organização, que medem a extensão em que a resposta demonstra uma estrutura unificada e direção da narrativa [Barzilay e Lapata 2008, Tay et al. 2018].  Após gerar redações adversariais, avaliamos uma classificação automatizada de redações de última geração [de Sousa et al. 2024] baseada no modelo BERT [Devlin et al. 2019]. Avaliamos esse modelo usando cada competência do ENEM por meio da métrica Kappa Ponderada Quadrática (QWK) [Cohen 1968] para redações originais e adversariais. QWK é uma métrica comumente utilizada para avaliar modelos AES [Yannakoudakis e Cummins 2015]. A Tabela 1 mostra os resultados nas redações originais, e a Tabela 2 apresenta os resultados nas redações adversariais.  As Tabelas 1 e 2, de C1 a C5, indicam as cinco competências do ENEM, e o total é a nota final de uma redação. Na Tabela 2, destacamos os valores maiores ou iguais ao valor das redações originais.  Analisando os valores das duas tabelas, podemos observar que apenas os valores da adição de texto no início e da adição de uma receita de bolo não foram superiores aos valores das redações originais, indicando que o modelo AES conseguiu identificar perturbações nas redações, penalizando suas notas. Por outro lado, as notas para a adição de texto não relacionado no meio, no final e uma música foram maiores ou iguais aos valores das redações originais. Uma descoberta interessante é que, apesar da adição de um texto não relacionado ao final de uma redação, a nota C5 não foi penalizada. A Competência 5 do ENEM é dedicada à elaboração de uma proposta para resolver o problema. A proposta normalmente aparece no final de uma redação, e o modelo AES não conseguiu detectar o conteúdo não relacionado adicionado a uma redação. Além disso, a nota final das redações originais e adversariais teve o mesmo valor de QWK, sugerindo que o modelo AES falhou em capturar essa perturbação.  Para a perturbação de repetição de um texto na redação, o modelo AES classificou as redações originais e adversariais com a mesma nota, principalmente na competência quatro. A Competência 4 avalia a estrutura superficial do texto, ou seja, como as frases e parágrafos estão vinculados através de elementos coesivos. Dessa forma, o modelo AES deveria atribuir notas negativas a tais respostas. Além disso, as notas para repetir um texto no meio e no final de uma redação tiveram o mesmo valor das redações originais.  Outra descoberta interessante é que a exclusão de algumas partes da redação melhora sua nota em várias competências. Como podemos ver, as notas para a exclusão de um texto na redação são maiores ou iguais às notas das redações originais, incluindo a nota final. Esses resultados mostram que o modelo AES não conseguiu identificar uma quebra no fluxo de um argumento quando partes essenciais da redação foram removidas.  Finalmente, e talvez a descoberta mais interessante, é que embaralhar os parágrafos de uma redação produz resultados melhores do que as redações originais. Esse resultado demonstra que o modelo AES não conseguiu determinar a coesão e a coerência das redações. Ou seja, o modelo AES não identificou a transição entre as linhas das redações, verificando ideias desconectadas que mudam substancialmente o significado.  O código-fonte do modelo AES e para gerar redações adversariais está disponível publicamente em https://github.com/liara-ifpi/essay-robustness.  Tabela 1. Resultados do Kappa Ponderado Quadrático nas redações originais.  C1  C2  C3  C4  C5 Total  0.44  0.23  0.29  0.24  0.62  0.46  Tabela 2. Resultados do Kappa Ponderado Quadrático nas redações adversariais. C1  Estratégia adversarial  C5 Total  C4  C3  C2  0.14 Adicionar texto não relacionado no início 0.41 0.42 Adicionar texto não relacionado no meio 0.18 0.43 0.22 Adicionar texto não relacionado ao final 0.21 0.38 Adicionar música 0.18 0.38 Adicionar receita de bolo 0.44 0.20 Repetir texto no início 0.21 0.43 Repetir texto no meio 0.23 0.43 Repetir texto no final 0.40 Excluir texto no início 0.19 0.40 0.23 Excluir texto no meio 0.41 0.23 Excluir texto no final 0.47 0.20 Embaralhar texto  0.15 0.24 0.28 0.23 0.22 0.18 0.23 0.24 0.24 0.27 0.28 0.24 0.30 0.22 0.25 0.29 0.24 0.29 0.24 0.29  0.17 0.57 0.62 0.20 0.62 0.24 0.20 0.64 0.61 0.59 0.61 0.62 0.66 0.66 0.63 0.64  0.40 0.44 0.46 0.42 0.41 0.44 0.46 0.46 0.45 0.46 0.46 0.47  4. Conclusão  Este artigo apresentou uma análise de robustez para a classificação automática de redações com foco na língua portuguesa. Usamos o corpus Essay-BR, que é baseado nas competências do ENEM, para realizar essa análise. Nossa estratégia foi adicionar várias perturbações para produzir redações adversariais, visando verificar se um modelo de classificação automática de redações de última geração consegue detectar qualquer diferença entre respostas originais e modificadas. A partir da análise, aprendemos que o modelo de classificação automática de redações não conseguiu identificar as perturbações nas redações, produzindo notas que eram até maiores do que as respostas originais. Esperamos que esta análise lance luz sobre esta área de pesquisa e ajude a desenvolver estratégias mais robustas para a classificação automática de redações.  Para trabalhos futuros, pretendemos desenvolver mais perturbações e criar um kit de ferramentas para facilitar a criação de redações adversariais.  \n",
            "11\n",
            "Resumo. A nota fiscal eletrônica é essencial para o processo de auditoria fiscal. Este artigo avalia a eficácia de algoritmos de clusterização para agrupar descrições de produtos em notas fiscais eletrônicas, um desafio devido à falta de padronização nos registros. Usando similaridade de strings e ajustes para unidades de medida, foram testados DBSCAN, HDBSCAN, OPTICS e Agglomerative Clustering. As métricas de avaliação incluíram o Coeficiente de Silhueta, Índice de Calinski-Harabasz e a porcentagem de produtos agrupados. O HDBSCAN apresentou o melhor desempenho inicial, e a subclusterização, apesar de melhorar as métricas, introduziu inconsistências nos agrupamentos. \n",
            "\n",
            "1. Introdução  \n",
            "As Notas Fiscais Eletrônicas (NF-e) são um marco na modernização dos processos fiscais no Brasil, ao melhorar o controle e a fiscalização tributária, o que já resultou em avanços na arrecadação de impostos e no processo de auditoria [Vieira et al. 2019, Neto and Lopo Martinez 2016]. No entanto, a análise dessas notas enfrenta desafios devido à falta de padronização nas descrições de produtos, com erros ortográficos, abreviações e variações nas unidades de medida [Mazzarolo et al. 2022]. Essa inconsistência dificulta a organização e comparação de dados, exigindo técnicas computacionais para agrupar descrições similares e auxiliar na auditoria fiscal, que requer a correspondência entre o inventário das empresas e as notas emitidas por elas. Dessa forma, o uso de algoritmos de agrupamento facilita a fiscalização e melhora a eficiência do processo [Ribeiro et al. 2018]. Neste contexto, este estudo busca avaliar algoritmos de agrupamento, como DBSCAN [Ester et al. 1996], HDBSCAN [Campello et al. 2013], OPTICS [Ankerst et al. 1999] e Agglomerative Clustering (AGG) [Steinbach et al. 2000], para agrupar descrições de produtos e identificar quais algoritmos oferecem o melhor desempenho na organização e interpretação dos dados. Para isso, foi empregada uma métrica personalizada no cálculo da matriz de distâncias, baseada em similaridade entre strings e a análise de uma segunda etapa de agrupamento dos dados. \n",
            "\n",
            "2. Trabalhos relacionados  \n",
            "Nesta seção, serão abordados alguns trabalhos que contribuíram para tentar resolver as diferenças na padronização nas descrições dos produtos, a fim de melhorar o processo de fiscalização tributária no Brasil [Mazzarolo et al. 2022]. O trabalho de [Schulte et al. 2022] apresentou o ELINAC, um modelo que combina autoencoder e busca binária para agrupar descrições de produtos em notas fiscais. O método filtra as descrições, considerando apenas o nome e informações numéricas, como quantidade e dosagem. Embora eficiente, ele tem limitações ao distinguir produtos com variações sutis, como sabor. A revisão de [Ahmed et al. 2022] aponta que a representação vetorial de textos curtos é desafiadora devido à alta dimensionalidade e ao ruído. O estudo de [Marinho et al. 2024] comparou representações textuais para classificar inconsistências em notas fiscais, calculando a similaridade entre a descrição do produto e a oficial da Nomenclatura Comum do Mercosul (NCM). Concluiu-se que a distância de edição de strings teve melhor desempenho preditivo do que embeddings, apesar de não considerar a similaridade entre produtos. Este estudo se diferencia ao focar na avaliação de algoritmos de agrupamento e na representação de descrições de NF-es utilizando similaridade de strings. Enquanto outros trabalhos abordam redes neurais, detecção de fraudes e visualização de dados, este estudo explora a eficácia dos algoritmos de clusterização para organizar e interpretar as descrições de produtos em notas fiscais. \n",
            "\n",
            "3. Metodologia  \n",
            "Esta seção descreve a base de dados, o cálculo da matriz de distâncias e os algoritmos de clusterização utilizados. \n",
            "\n",
            "3.1. Base de dados  \n",
            "Foram usadas duas bases: uma base sintética com 22 descrições, contendo ruídos típicos [Mazzarolo et al. 2022], e uma base real cedida pela Secretaria da Fazenda da Paraíba (SEFAZ-PB) com 507 descrições. As descrições foram normalizadas, removendo caracteres especiais e convertendo tudo para caracteres maiúsculos. \n",
            "\n",
            "3.2. Matriz de distâncias  \n",
            "Uma matriz de distâncias é uma matriz quadrada que contém as distâncias entre todos os pares de elementos do banco de dados. Neste trabalho, a matriz foi feita a partir de uma métrica personalizada, baseada na similaridade de Jaro [Jaro 1989]. O valor da similaridade varia entre 0 e 1, onde 0 indica que as strings não têm correspondências e 1 indica que as strings são idênticas. Entretanto, para o conceito de distância, quanto mais próximo de 0, mais próximos são dois pontos. Dessa forma, para computar a matriz de distância, foi calculado o complemento da similaridade de Jaro, ou seja, 1−JaroSimilarity. Além da similaridade textual, foi introduzido um cálculo adicional para diferenciar produtos com o mesmo nome, mas com medidas distintas, como “200 ML” e “10 KG”. Isso evita que produtos com variação apenas na quantidade sejam considerados iguais. Para implementar esse ajuste, as medidas foram extraídas por meio da expressão regular 1 [Lucena et al. 2022], e convertidas em mililitros, gramas ou metros. Quando as medidas diferem, adiciona-se uma penalidade de 0,3 ao complemento da similaridade de Jaro, valor que foi escolhido após testes com variações entre 0,1 e 0,5. \n",
            "(?:\\d*[,]?\\d+?\\s?(?:kg|ml|mm|l|lt|gr|grs|g|metros|m|gb|k|cm|mg)\\b) (1) \n",
            "\n",
            "3.3. Algoritmos de Clusterização  \n",
            "Para o agrupamento, este estudo avaliou 4 algoritmos diferentes: DBSCAN [Ester et al. 1996], HDBSCAN [Campello et al. 2013], OPTICS [Ankerst et al. 1999] e AGG [Steinbach et al. 2000]. Todos os algoritmos usados foram implementados pela biblioteca scikit-learn, versão 1.5.1, e nenhuma métrica de distância foi passada para os algoritmos, uma vez que a matriz já está pré-computada. Os algoritmos foram usados em duas etapas: o agrupamento inicial e a subclusterização dos grupos de outliers, aplicada apenas na base real. Para o agrupamento inicial, foi definida uma distância máxima de agrupamento de 0,1 e um tamanho mínimo de cluster sendo igual a 2. Para a segunda etapa, a distância foi igual a 0,2. Os parâmetros de distância foram escolhidos após avaliação do agrupamento com variações entre 0,05 e 0,2 e os demais hiperparâmetros possuem os valores padrões da biblioteca. Para avaliar o resultado dos agrupamentos, foram utilizadas duas métricas principais: o Coeficiente de Silhueta [Rousseeuw 1987], que avalia a coesão dos clusters, e o Índice de Calinski-Harabasz (CH) [Caliński and JA 1974], que mede a separação entre os grupos. O cálculo dessas métricas foi feito utilizando as distâncias entre pontos pré-computadas. Além disso, foi considerada a porcentagem de produtos agrupados para avaliar a cobertura dos dados pelos algoritmos de agrupamento.  \n",
            "\n",
            "4. Resultados e discussões  \n",
            "É importante ressaltar que o algoritmo AGG não gera um grupo de outliers identificado como −1, o que exigiu um ajuste no cálculo das métricas para esse caso. Especificamente, todos os grupos individuais, que contêm apenas um produto, foram considerados como pertencentes ao grupo −1, permitindo que as métricas fossem calculadas de forma consistente. Na base sintética, DBSCAN, OPTICS e AGG produziram clusters idênticos, enquanto o HDBSCAN teve desempenho superior, distinguindo produtos com variações de sabor, mas não separando bem produtos de medidas diferentes. Nos dados reais, o HDBSCAN obteve as melhores métricas gerais, enquanto o OPTICS teve o maior coeficiente de Silhueta, mas o menor índice de CH, sugerindo que seus clusters não estavam bem separados. \n",
            "\n",
            "A Tabela 2 apresenta os resultados da subclusterização dos grupos de produtos considerados outliers na base de dados real. Todas as métricas possuíram aumentos nos valores, quando comparados à primeira clusterização, especialmente na utilização do HDBSCAN, tanto na primeira, quanto na segunda etapa.  \n",
            "\n",
            "Tabela 1. Avaliação dos algoritmos de clusterização no agrupamento inicial  \n",
            "Base de Dados | Algoritmo | Silhueta | CH | Produtos agrupados (%)  \n",
            "Base Controlada | Base SEFAZ-PB |  |  |  \n",
            "DBSCAN | HDBSCAN | OPTICS | AGG | DBSCAN | HDBSCAN | OPTICS | AGG |  \n",
            "0,490 | 0,563 | 0,490 | 0,490 | 0,686 | 0,726 | 0,730 | 0,696 |  \n",
            "7,97 | 15,58 | 7,97 | 7,97 | 21,71 | 43,40 | 17,98 | 20,18 |  \n",
            "98,16 | 99,80 | 98,16 | 98,16 | 86,19 | 94,08 | 85,99 | 75,79 |  \n",
            "\n",
            "Embora as métricas tenham melhorado com a segunda etapa de clusterização usando o HDBSCAN, surgiram inconsistências nos agrupamentos. Por exemplo, produtos como “BOM TRIGO PREP. EMULSIF.” e “MARG. MEDALHA DE OURO” foram agrupados erroneamente no mesmo cluster. Isso indica que a fase adicional pode priorizar a melhoria das métricas, mas comprometer a consistência semântica, tornando os clusters menos úteis ou interpretáveis na prática. \n",
            "\n",
            "Tabela 2. Avaliação dos algoritmos de clusterização no segundo agrupamento  \n",
            "Primeira Etapa | Segunda Etapa | Silhueta | CH | Produtos agrupados (%)  \n",
            "DBSCAN | HDBSCAN | OPTICS | AGG | DBSCAN | HDBSCAN | OPTICS | AGG |  \n",
            "DBSCAN | HDBSCAN | OPTICS | AGG | DBSCAN | HDBSCAN | OPTICS | AGG |  \n",
            "0,718 | 0,737 | 0,717 | 0,719 | 0,729 | 0,740 | 0,729 | 0,729 |  \n",
            "0,761 | 0,779 | 0,760 | 0,763 | 0,728 | 0,746 | 0,727 | 0,729 |  \n",
            "29,02 | 79,30 | 27,50 | 28,06 | 46,28 | 125,55 | 46,28 | 46,28 |  \n",
            "25,25 | 73,11 | 22,96 | 23,46 | 27,42 | 75,52 | 26,00 | 26,53 |  \n",
            "90,13 | 97,63 | 89,74 | 89,94 | 94,47 | 99,21 | 94,47 | 94,47 |  \n",
            "89,94 | 97,63 | 89,54 | 89,74 | 89,94 | 97,43 | 89,54 | 89,74 |  \n",
            "\n",
            "5. Considerações finais  \n",
            "Este estudo avaliou os algoritmos de clusterização DBSCAN, HDBSCAN, OPTICS e AGG para agrupar descrições de produtos em NF-e, utilizando similaridade de strings como representação de dados. O HDBSCAN apresentou o melhor desempenho inicial, mas a segunda etapa de agrupamento gerou inconsistências. DBSCAN e OPTICS tiveram métricas um pouco inferiores, porém com menos irregularidades. Sugere-se, como trabalhos futuros, testar o método em bases maiores e explorar representações como embeddings e redes neurais para padronização.   \n",
            "\n",
            "Referências  \n",
            "Ahmed, M., Tiun, S., Omar, N., and Sani, N. S. (2022). Short text clustering algorithms, application and challenges: A survey. Applied Sciences.  \n",
            "Ankerst, M., Breunig, M. M., Kriegel, H.-P., and Sander, J. (1999). Optics: ordering points to identify the clustering structure. SIGMOD Rec., 28(2):49–60.  \n",
            "Caliński, T. and JA, H. (1974). A dendrite method for cluster analysis. Communications in Statistics - Theory and Methods, 3:1–27.  \n",
            "Campello, R. J. G. B., Moulavi, D., and Sander, J. (2013). Density-based clustering based on hierarchical density estimates. In Pei, J., Tseng, V. S., Cao, L., Motoda, H., and Xu, G., editors, Advances in Knowledge Discovery and Data Mining, pages 160–172, Berlin, Heidelberg. Springer Berlin Heidelberg.  \n",
            "Ester, M., Kriegel, H.-P., Sander, J., Xu, X., et al. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. In KDD, volume 96, pages 226–231.  \n",
            "Jaro, M. A. (1989). Advances in record-linkage methodology as applied to matching the 1985 census of Tampa, Florida. Journal of the American Statistical Association, 84(406):414–420.  \n",
            "Lucena, L. F., de Menezes e Silva Filho, T., do Rêgo, T. G., and Malheiros, Y. (2022). Automatic recognition of units of measurement in product descriptions from tax invoices using neural networks. In Pinheiro, V., Gamallo, P., Amaro, R., Scarton, C., Batista, F., Silva, D., Magro, C., and Pinto, H., editors, Computational Processing of the Portuguese Language, pages 156–165, Cham. Springer International Publishing.  \n",
            "Marinho, M., Weigang, L., Oliveira, V., and Borges, V. (2024). Estratégias computacionais baseadas em similaridade de textos e visualização exploratória para a identificação de inconsistências em notas fiscais eletrônicas.  \n",
            "Mazzarolo, J., Steinmetz, R., and Mergen, S. (2022). Um estudo sobre a falta de padronização na descrição de produtos em notas fiscais eletrônicas. In Anais da XVII Escola Regional de Banco de Dados, pages 31–40, Porto Alegre, RS, Brasil. SBC.  \n",
            "Neto, H. and Lopo Martinez, A. (2016). Nota fiscal de serviços eletrônica: Uma análise dos impactos na arrecadação em municípios brasileiros. Revista de Contabilidade e Organizações, 10:49.  \n",
            "Ribeiro, L., Brandão, W., Marques, I., Andrade, P., Júnior, R., Oliveira, F., and Kelles, R. (2018). Reconhecimento de entidades nomeadas em itens de produto da nota fiscal eletrônica. 36:116–126.  \n",
            "Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20:53–65.  \n",
            "Schulte, J. P., Giuntini, F. T., Nobre, R. A., Nascimento, K. C. d., Meneguette, R. I., Li, W., Gonçalves, V. P., and Rocha Filho, G. P. (2022). Elinac: Autoencoder approach for electronic invoices data clustering. Applied Sciences, 12(6).  \n",
            "Steinbach, M., Karypis, G., and Kumar, V. (2000). A comparison of document clustering techniques.  \n",
            "Vieira, P. A., Pimenta, D. P., Cruz, A. F. d., and Souza, E. M. S. d. (2019). Efeitos do programa de nota fiscal eletrônica sobre o aumento da arrecadação do estado. Revista de Administração Pública, 53(2):481–491.  \n",
            "12\n",
            "Resumo. Avaliar sistemas Text-To-Speech (TTS) é um desafio, uma vez que a qualidade crescente da síntese impõe obstáculos em discriminar a capacidade de modelos em reproduzir atributos prosódicos, especialmente para o português brasileiro. Métricas de avaliação offline não medem a reação genuína de avaliadores aos estímulos de áudios. Propõe-se, portanto, um método de avaliação online com rastreamento de globo ocular. Os experimentos com 76 anotadores apontam que há uma correlação razoável entre EyetrackingMOS e MOS, assim como uma redução em sua duração total. Desta forma, acredita-se que esta métrica forneça uma informação precisa e potencialmente rápida para complementar os métodos de avaliação.  Index Terms: Speech Synthesis Models Evaluation, Portuguese language, spontaneous speech, eyetracking \n",
            "\n",
            "1. Introdução  \n",
            "Sistemas de texto-para-fala, do inglês Text-To-Speech (TTS) buscam vocalizar um texto escrito em níveis próximos à naturalidade de fala humana [Caseli and Nunes 2024]. Os avanços em Aprendizado Profundo impulsionaram o desenvolvimento de tais sistemas. Posteriormente, a utilização de modelos gerativos baseados em fluxo, como os propostos por [Kingma et al. 2016] e [Hoogeboom et al. 2019], tem permitido maior flexibilidade na manipulação de características prosódicas1 da fala sintética. Os resultados de modelos do estado da arte já reproduzem a identidade dos locutores com bastante naturalidade em condições mais amplas de dados [Casanova et al. 2022, Tan et al. 2022]. Entretanto, modelos de síntese ainda encontram obstáculos na reprodução de aspectos específicos da expressividade individual de falantes. Estes aspectos podem ser medidos através da entoação, duração e ritmo da fala [Ju et al. 2024], que são de natureza prosódica, o que se agrava em cenários de síntese zero-shot [Casanova et al. 2022, Ju et al. 2024]. Neste contexto, sistemas contemporâneos de TTS investigam outras capacidades além da reprodução da identidade de um locutor com naturalidade nos resultados, dentre elas o interesse em manter a naturalidade ao gerar fala nas variantes internacionais de uma língua (accent-robust), como o Synthesizing Multi-Accent Speech By Weight Factorization (SYNTACC) [Nguyen et al. 2023]. A possibilidade de síntese de fala com sensibilidade de sotaques internacionais também levanta hipóteses de aplicações para variantes linguísticas regionais de uma dada língua que conta com menos recursos, a fim de avaliar se a qualidade se preserva. O português brasileiro é uma língua que contempla uma grande quantidade de variantes, dadas as dimensões continentais do Brasil, e devido a fatores históricos, sociais e culturais [Mota et al. 2023].  \n",
            "\n",
            "Para avaliar a qualidade da fala sintetizada nesses sistemas, são utilizadas diversas métricas. As métricas subjetivas como: Mean Opinion Score (MOS) [ITU - T 1996], Crowd MOS [Ribeiro et al. 2011], Similarity MOS (SMOS) [Jia et al. 2019] e Comparative MOS (CMOS), por um lado, dependem da opinião e percepção de um grupo de ouvintes humanos. Apesar de importante, este perfil de métricas pode oferecer risco para análise de sotaques a depender da correspondência entre o contexto regional/cultural dos avaliadores e os áudios sintéticos, uma vez que a avaliação será influenciada por seus contextos culturais, linguísticos e experiências individuais. Por outro lado, as métricas objetivas como: Speaker Encoder Cosine Similarity (SECS) [Casanova et al. 2021], Prosody Similarity with Prompt, Prosody Similarity with Ground Truth e Word Error Rate (WER) [Shen et al. 2023] podem não capturar completamente a percepção humana da qualidade do áudio sobre o desempenho na qualidade de expressividade individual e representatividade de variantes linguísticas e, por isso, complementam a análise subjetiva. A ausência de uma métrica padrão e amplamente aceita dificulta a identificação de tendências e avanços consistentes no campo do TTS, além de dificultar o entendimento de quais modelos são mais adequados para determinados cenários ou requisitos específicos (cf. [Le Maguer et al. 2024]). Ambos os perfis têm sensibilidades a aspectos diferentes e limitações que devem ser avaliadas [Cooper et al. 2024].  \n",
            "\n",
            "Ambas as métricas também podem ser observadas quanto à sua resposta aos estímulos de áudios fornecidos durante a avaliação. Em métodos de avaliação offline (MOS, CrowdMOS, SMOS e CMOS), o indivíduo pontua apenas após ouvir todo o estímulo, enquanto métodos de avaliação online permitem que se registre suas impressões à medida que o estímulo é recebido, tendo como objetivo capturar reações genuínas e momentâneas. A avaliação de estímulos de áudio utilizando rastreamento ocular já é amplamente empregada em contextos linguísticos, como na análise de processamento de linguagem, compreensão auditiva, e percepção fonética [ALMEIDA et al. 2021]. No entanto, sua aplicação na avaliação de sistemas de síntese de fala ainda é pouco explorada. Buscamos preencher essa lacuna, propondo um novo método, EyetrackingMOS, que utiliza o rastreamento ocular para avaliação de qualidade dos áudios de forma mais natural, sem que o participante atribua uma nota de forma direta.  \n",
            "\n",
            "As principais contribuições feitas nesse trabalho são sumarizadas como se segue:  \n",
            "1. Proposta de um novo método de avaliação de sistemas de síntese de fala que integra o rastreamento ocular, chamado de EyetrackingMOS;  \n",
            "2. Comparação entre o EyetrackingMOS e uma adaptação do MOS tradicional, destacando suas respectivas vantagens e limitações;  \n",
            "3. Apresentação dos experimentos, detalhes de configuração do modelo e interfaces em um repositório2, facilitando a replicabilidade em diferentes cenários e promovendo avanços na pesquisa sobre síntese de fala.  \n",
            "\n",
            "2. Revisão sobre métricas subjetivas para análise de sistemas de TTS  \n",
            "Na década de 1990, a International Telecommunication Union (ITU) padronizou diversos tipos de testes de audição que eram frequentemente usados na telefonia [ITU - T 1996]. A pontuação baseada em opinião pode ser definida como o valor em uma escala predefinida que um sujeito atribui à sua opinião sobre o desempenho de um sistema [ITU - R 2017, Loizou 2011]. A pontuação média de opinião, do inglês Mean Opinion Score (MOS) é um tipo de Absolute Category Rating (ACR) [Ribeiro et al. 2011]. A MOS emergiu como o descritor mais popular sobre a percepção da qualidade de mídia. Para o cálculo da MOS, humanos avaliam os áudios sintetizados e naturais e atribuem uma nota de 1 a 5, no qual o valor final corresponde à média das notas de todos os avaliadores. A tabela traduzida com a equação correspondente pode ser vista no repositório2.  \n",
            "\n",
            "Diversas variações da MOS foram desenvolvidas para atender a diferentes necessidades de avaliação. A Crowd Mean Opinion Score (crowdMOS) propõe uma adaptação ao ambiente tradicional de testes MOS, ao utilizar trabalhadores de uma multidão (do inglês, crowd) pela internet para realizar avaliações em ambientes não controlados, o que permite maior diversidade de ouvintes a um custo reduzido, embora com desafios em termos de controle de qualidade [Ribeiro et al. 2011]. A Similarity Mean Opinion Score (SMOS)3, por sua vez, foca na avaliação da semelhança entre áudios sintetizados e de referência, sendo útil para medir quão próximo um áudio gerado está de uma voz original em termos de características acústicas e vocais [Ren et al. 2021]. Já a Comparative Mean Opinion Score (CMOS) avalia a qualidade relativa entre duas versões de áudio sintetizado, pedindo aos avaliadores que comparem diretamente os áudios e apontem qual deles possui melhor qualidade, utilizando uma escala de -3 a +3 [Ren et al. 2022]. Cada uma dessas variantes da MOS foca em diferentes aspectos da qualidade de áudio, utilizadas de acordo com o que se deseja avaliar. Considerando que estudos têm utilizado a MOS como uma medida de naturalidade da fala em tarefas de síntese (cf. [Sellam et al. 2023], [Choi et al. 2022]), a descrição da característica observada pelo avaliador foi adaptada para a avaliação de naturalidade (veja a coluna 4 da Tabela 1).  \n",
            "\n",
            "2Acesso em EyetrackingMOS-STIL https://github.com/GustavoEvangelistaAraujo/  \n",
            "3Também abreviado por SimMOS na literatura.  \n",
            "\n",
            "3. EyetrackingMOS  \n",
            "O rastreamento ocular é amplamente reconhecido como uma das técnicas mais precisas para a avaliação online do processamento linguístico [Mitchell 2004, Kaiser 2013]. Os variados movimentos dos olhos durante o processamento de informações podem ser utilizados para inferir como essas informações são processadas, seja durante a leitura de texto (estímulo de leitura) ou ao observar uma imagem (estímulo visual). O resultado é obtido a partir da porcentagem de tempo em que o avaliador olhou para os lados direito e esquerdo, os quais mostram figuras relacionadas aos conceitos que se deseja medir. Assim, registramos a porcentagem de tempo que o participante permanece com o olhar sobre a figura que representa a fala natural. Esta medida pode ser avaliada em um intervalo de 0% a 100% e mapeada para a escala MOS como apresentado na Tabela 1. Assim como no MOS, ao final é calculada a média das notas de todos os avaliadores.  \n",
            "\n",
            "Tabela 1. Mapeamento entre pontuações do EyetrackingMOS e MOS  \n",
            "Tempo de fixação (%) Avaliação MOS Qualidade Naturalidade  \n",
            "81 a 100 61 a 80 41 a 60 21 a 40 0 a 20  \n",
            "Extremamente natural Muito natural Razoavelmente natural Pouco natural Nada natural  \n",
            "Excelente Boa Razoável Pobre Ruim  \n",
            "5 4 3 2 1  \n",
            "\n",
            "4. Materiais e métodos  \n",
            "4.1. Descrição do conjunto de dados  \n",
            "Há uma carência de conjuntos de dados de áudio com variantes linguísticas regionais. Corpus como BRACCENT, utilizado em [Batista 2019], [Ling et al. 2018] e [Ynoguti 1999], não apresentam volume satisfatório de dados, assim como não tratam da fala espontânea. Portanto, foi escolhido para este estudo um recorte de áudios de um grande dataset do Museu da Pessoa4, um museu virtual e colaborativo de histórias de vida, que são do tipo entrevistas biográficas, compilado pelo projeto Tarsila5. Detalhes do recorte preliminar do corpus (MuPe-v1) estão disponíveis no repositório2.  \n",
            "\n",
            "4.2. Modelo de síntese de fala  \n",
            "O modelo SYNTACC [Nguyen et al. 2023] é uma arquitetura para síntese de fala com múltiplos sotaques baseada no YourTTS [Casanova et al. 2022]. Similarmente ao antecessor, utiliza uma arquitetura de codificação-decodificação baseada em Transformer, onde o codificador recebe a sequência de texto como entrada e gera uma representação intermediária, que é posteriormente processada pelo decodificador para gerar o espectrograma mel, uma representação em espectro da frequência ao longo do tempo que é reconstruída em áudio por um vocoder.  \n",
            "\n",
            "Esse modelo implementa as seguintes mudanças: na entrada, a arquitetura concatena 4 embeddings de idiomas treináveis em cada caractere de entrada, uma técnica de fatorização de pesos (weight factorization), o que permite um treinamento multi-accent.  \n",
            "4https://museudapessoa.org/  \n",
            "5https://sites.google.com/view/tarsila-c4ai/home  \n",
            "\n",
            "Esta abordagem divide os pesos do modelo em componentes compartilhados e específicos para cada variante linguística, otimizando o treinamento em cenários de poucos recursos. Isso possibilita que a síntese de fala seja adaptável para o contexto do português brasileiro, sendo possível obter um controle explícito de sotaques pelo congelamento parcial de pesos atribuídos a ele e portanto permite que a fala seja sintetizada de forma mais específica para cada variante. Detalhes da arquitetura, configurações do modelo e etapa de treinamento também foram disponibilizados no repositório2.  \n",
            "\n",
            "5. Experimentos  \n",
            "A Figura 1 apresenta o fluxo de interação do usuário neste experimento. Para tanto, foi utilizada a plataforma Gorilla6, uma plataforma paga, com o objetivo de construção e coleta de tarefas de anotação. A sequência de interfaces e o conjunto de dados dos estímulos também são apresentados no repositório2.  \n",
            "\n",
            "Figura 1. Fluxograma da interação do usuário em cada experimento  \n",
            "No experimento elaborado neste trabalho, o processo se inicia com a aceitação do Termo de Consentimento Livre e Esclarecido (TCLE). Em seguida, o participante é conduzido a um tutorial, que tem o objetivo de ambientá-lo com o experimento subsequente. Para a captura do vídeo, são utilizadas as câmeras padrões dos dispositivos pessoais (apenas computador e notebook) dos usuários, caso as configurações de iluminação e qualidade de imagem não sejam suficientemente boas para permitir que o participante complete a calibragem sem erros, o participante é impedido de continuar. Em conjunto com uma calibragem recorrente, é possível inferir que a qualidade de rastreamento se mantenha desde o início até o final do experimento. O Gorilla utiliza a biblioteca Webgazer7 para rastreamento ocular. No caso do EyetrackingMOS, após o tutorial, o usuário passa pela etapa de calibragem, que é dividida em duas partes. Primeiro, é necessário posicionar corretamente o rosto em relação à câmera. Em seguida, o participante deve fixar o olhar em uma sequência de pontos que aparecem aleatoriamente nas extremidades da área útil da tela. São apresentados 10 pontos no total, no qual os 5 primeiros pontos tornam-se uma referência de rastreamento, e os 5 seguintes são repetidos como validação dos anteriores. Caso haja uma discrepância significativa entre a referência e a validação em um dos pontos (considerado como a tolerância do teste), a calibragem é considerada falha, e o usuário precisa repetir o processo. Após uma calibragem bem-sucedida, o participante prossegue e visualiza uma tela com duas imagens vetoriais ilustrativas (um robô e uma figura humana, que trocam de posição de forma aleatória a cada estímulo) enquanto ouve o áudio. A pausa é uma tela subsequente ao final do áudio com apenas um sinal de “+” por 3 segundos, feita para poder reposicionar o olhar do usuário no meio da tela.  \n",
            "6https://gorilla.sc/  \n",
            "7https://webgazer.cs.brown.edu/  \n",
            "\n",
            "Este ciclo de estímulo e pausa é repetido 15 vezes, e então é feita uma nova calibragem para garantir a qualidade de rastreamento do globo ocular, sendo realizado quatro ciclos completos, que totalizam 60 estímulos.  \n",
            "\n",
            "Por outro lado, no experimento de MOS, após o TCLE e o tutorial, o participante é exposto a 60 estímulos de áudio. Durante o tutorial, são apresentados três exemplos de áudios sintetizados, correspondentes às pontuações 1, 3 e 5, para ajudar o participante a alinhar suas expectativas. O participante pode ouvir cada estímulo mais de uma vez antes de decidir sua pontuação, utilizando a Tabela 1 como referência para todas as 60 amostras de áudio, conforme descrito na literatura de avaliação de modelos de síntese.  \n",
            "\n",
            "A divisão das listas de áudios para avaliação foi realizada considerando dois tipos de áudios: sintetizados e naturais. Esses áudios foram organizados em duas listas: Lista A e Lista B, que foram atribuídas aos participantes de forma equilibrada. 30 áudios naturais foram colocados na Lista A, enquanto seus correspondentes sintetizados foram alocados na lista B. Da mesma forma, 30 áudios sintetizados foram incluídos na Lista A, com os seus correspondentes naturais na lista B. Quanto aos participantes, conforme [Loizou 2011], a proporção de avaliações subjetivas deve ser de 10 especialistas para 20 não especialistas. Foram escolhidos 76 anotadores dentre 28 especialistas e 48 não especialistas, distribuídos entre 4 grupos de 19 participantes. Ambos experimentos foram elaborados desta mesma forma, o que assegurou uma diversidade de perspectivas nas avaliações, permitindo uma análise comparativa abrangente entre as opiniões de especialistas e não especialistas sobre os áudios apresentados.  \n",
            "\n",
            "6. Resultados preliminares  \n",
            "A Figura 2 ilustra a relação entre os valores mensurados pelo EyetrackingMOS, convertidos para a escala MOS, e os valores mensurados diretamente pelo MOS. Cada ponto azul representa um par de medidas, com o eixo horizontal correspondendo aos valores do EyetrackingMOS convertidos para a escala MOS e o eixo vertical representando os valores obtidos diretamente pelo MOS. A linha verde traçada no gráfico indica a linha de tendência linear, mostrando a direção geral da correlação entre as duas variáveis. A Figura 3 ilustra a dispersão das pontuações obtidas tanto pelo MOS quanto pelo rastreamento ocular (EyetrackingMOS) para áudios reais e sintetizados. Em ambos os testes, os participantes conseguiram separar razoavelmente bem os áudios reais dos sintetizados.  \n",
            "\n",
            "Figura 2. Gráfico de dispersão entre EyetrackingMOS e MOS  \n",
            "Figura 3. Gráfico de dispersão por classe  \n",
            "No gráfico de dispersão por MOS (Figura 4), observa-se uma distinção clara entre os áudios reais, que tendem a receber pontuações mais altas, e os sintetizados, que se concentram nas faixas intermediárias e baixas. No entanto, no gráfico de dispersão por rastreamento ocular (Figura 5), a separação entre áudios reais e sintetizados é menos evidente. Essa maior dispersão nos resultados do rastreamento ocular é esperada, já que nesse método os estímulos são percebidos apenas uma vez, enquanto nos métodos offline como o MOS, o anotador pode ouvir o estímulo repetidas vezes antes de tomar sua decisão, resultando em uma separação mais clara entre os tipos de áudio. Assim, o rastreamento ocular oferece uma avaliação mais detalhada, capturando variações mais sutis na percepção da qualidade dos áudios.  \n",
            "\n",
            "Figura 4. Gráfico de dispersão por MOS  \n",
            "Figura 5. Gráfico de dispersão por rastreamento ocular  \n",
            "Os resultados apresentados na Tabela 2 indicam uma correlação razoável entre o EyetrackingMOS e o MOS, com uma métrica R² de 56%, sugerindo que o EyetrackingMOS explica 56% da variância observada no MOS. O desvio padrão do erro entre as duas métricas é de 0,72 unidades, mostrando que, em geral, elas tendem a ser próximas, com uma diferença média de menos de uma unidade. Além disso, o MOS tende a classificar um número maior de áudios com a nota máxima ou valores próximos, enquanto o EyetrackingMOS oferece uma análise mais detalhada, por sua escala ser de 0 a 100, o que é observado em áudios de alta qualidade. Essa dispersão indica que, embora exista uma correlação razoável entre as duas métricas, conforme evidenciado pela inclinação positiva da linha de tendência, as medidas não são perfeitamente alinhadas, refletindo diferenças na maneira como cada método capta e avalia a qualidade dos áudios.  \n",
            "\n",
            "Tabela 2. Medidas de performance estatística  \n",
            "Medida Pearson Mean Squared Error (MSE) Rooted Mean Squared Error (RMSE) R² Spearman  Interpretação geral  \n",
            "Valor Correlação moderada 0.744 Erro médio baixo 0.710 Erro médio baixo 0.844 0.553 Explica 55% da variância Cor relação moderada 0.714  \n",
            "\n",
            "Também foi realizada uma análise da concordância entre os avaliadores dentro de seus respectivos grupos, utilizando o coeficiente de Kendall’s W para avaliar a consistência das respostas (Tabela 3). Em resumo, o grupo EyetrackingMOS apresentou maior consistência nas avaliações, com alta concordância na maioria dos estímulos, enquanto o grupo MOS demonstrou uma maior variabilidade, com concordância que variou de alta até nenhuma, indicando possíveis desafios na avaliação uniforme dos estímulos por este grupo. Com relação ao tempo, EyetrackingMOS e MOS tomaram em média 12:07min e 12:30min dos participantes, respectivamente. As medianas foram de 11:38min e 10:41min, respectivamente. Nota-se que o teste MOS tende a ser em torno de 1 minuto mais rápido que o EyetrackingMOS, que pode ser justificada pelo tempo das 4 calibragens do rastreamento ocular.  \n",
            "\n",
            "Tabela 3. Medidas de concordância para cada grupo de experimentos  \n",
            "Grupo EyetrackingMOS  Intervalo de Kendall’s W  Interpretação geral  \n",
            "0.6719 a 0.9579  Alta concordância geral, algumas variações  \n",
            "Grande variação, alta concordância a nenhuma concordância  \n",
            "MOS  0.0000 a 0.9474  \n",
            "\n",
            "7. Conclusão e trabalhos futuros  \n",
            "Conforme os resultados preliminares, o EyetrackingMOS e MOS têm uma correlação razoável. Paralelamente, a utilização de uma medida de avaliação subjetiva com rastreamento ocular oferece vantagens significativas, uma vez que permite capturar reações genuínas e síncronas aos estímulos apresentados. Além disso, o controle mais rigoroso sobre a quantidade de estímulos recebidos por cada participante pode reduzir a variação na concordância e aumentar a quantidade de estímulos por sessão. A reação mecânica ocular também pode reduzir variações na concordância, causadas pelas diferentes interpretações das descrições de pontuação de métricas subjetivas. A escala de 0 a 100 para cada indivíduo oferece uma avaliação mais detalhada e precisa, permitindo uma maior granularidade na análise das respostas, ao contrário das escalas limitadas a poucos pontos. Embora a produção dessa medida seja mais complexa e demorada, o benefício de obter uma análise mais transparente das reações dos participantes justifica seu uso como complemento do MOS tradicional.  \n",
            "\n",
            "Como futuros, pretende-se experimentar diferentes tecnologias/plataformas de captação ocular para comparar a precisão da captação. Também é importante obter dados estatísticos com uma distinção das pontuações fornecidas entre os grupos de especialistas e não especialistas. Além disso, a seleção de variáveis deve ser refinada, como, por exemplo, calcular a fixação no espaço intermediário entre as imagens, o que pode oferecer uma compreensão mais detalhada das reações dos participantes. Por fim, explorar maneiras de realizar esses testes gratuitamente, seja por meio de parcerias, uso de plataformas de crowdsourcing ou outras abordagens que reduzam os custos e ampliem o acesso aos participantes.  \n",
            "13\n",
            "Resumo. Modelos de Língua têm estabelecido novos padrões de desempenho em tarefas textuais. Porém, tais modelos exigem grandes volumes de dados e recursos computacionais intensivos. Este estudo explora o uso de técnicas de Ajuste Fino Eficiente de Parâmetros (PEFT), especificamente LoRA e GreenTrainer, aplicadas a modelos especializados para o português, OPT-PTBR e PTT5. Almeja-se avaliar se as técnicas de PEFT mantêm o desempenho dos modelos enquanto mitigam os impactos financeiros e ambientais do uso intensivo de recursos, mesmo em modelos menores. Os resultados mostram que o GreenTrainer, particularmente, oferece desempenho competitivo em relação ao Ajuste Fino completo, enquanto reduz significativamente demandas computacionais. \n",
            "\n",
            "1. Introdução  \n",
            "Modelos de Língua (MLs) Computacionais têm como objetivo representar componentes da língua humana de forma simplificada usando representações numéricas, mas tentando preservar seus fundamentos lexicos, sintáticos e semânticos [Paes et al. 2024]. No contexto atual do Processamento de Linguagem Natural (PLN), MLs Neurais — baseados em redes neurais — que empregam a arquitetura Transformer [Vaswani et al. 2017] destacam-se por alcançarem resultados no estado-da-arte em diversas tarefas [Wolf et al. 2020]. Particularmente, MLs de larga escala (Large Language Models, LLMs) [Zhao et al. 2023, Paes et al. 2024] estabeleceram novos padrões para tarefas generativas, como a sumarização [Fu et al. 2024]. Tais modelos se caracterizam pelo seu vasto número de parâmetros que possibilitam a observação de habilidades emergentes, ao resolverem tarefas para as quais não foram explicitamente treinados [Paes et al. 2024]. Como consequência, LLMs passaram a ser integrados como componentes de software e partes essenciais de agentes de conversação, expandindo seu uso para além dos ambientes acadêmicos e corporativos e tornando-os acessíveis por qualquer indivíduo com um computador. \n",
            "\n",
            "Dessa forma, aumentou-se a demanda pelo desenvolvimento e acesso de LLMs, acompanhados por um crescimento expressivo no número de parâmetros desses modelos [Maslej et al. 2024]. Contudo, o aumento em larga escala de parâmetros apresenta desafios notáveis, incluindo a necessidade de vastos volumes de dados e um intenso consumo de recursos computacionais [Zhao et al. 2023]. Neste cenário, a Inteligência Artificial Verde (IA Verde) desponta como uma área dedicada a elucidar e reduzir os impactos computacionais — tanto ambientais, como socioeconômicos — do desenvolvimento de soluções em IA [Schwartz et al. 2020]. Atualmente, o desenvolvimento e a pesquisa em modelos de língua são dominados por entidades privadas, e com uma concentração significativa nos Estados Unidos, União Europeia e China [Maslej et al. 2024]. Essa concentração representa um entrave, pois limita a diversificação de pesquisa em outras regiões, como o Brasil, que enfrentam restrições de recursos. Além disso, a sustentabilidade ambiental emerge como uma questão crítica, dado, por exemplo, o alto uso de tempo em GPUs para treinamento e operação de MLs, que tem como consequências um elevado consumo energético e seu equivalente em emissões de dióxido de carbono (CO2eq) e uso de água potável [Li et al. 2023]. \n",
            "\n",
            "No contexto de adaptação de MLs, técnicas como o Ajuste Fino (Fine-Tuning) e, mais ainda, o Ajuste Fino Eficiente de Parâmetros (Parameter Efficient Fine-Tuning, PEFT) [Xu et al. 2023] emergem como abordagens para adaptar LLMs de forma a aliviar essas limitações. Ambas as abordagens aproveitam o conhecimento previamente codificado em MLs Pré-Treinados (Pre-trained Language Models, PLMs) [Ding et al. 2023] e os adaptam para domínios ou tarefas específicas. Entretanto, enquanto a primeira abordagem pode alterar todos os parâmetros do modelo pré-treinado, a segunda abordagem foca na adaptação considerando explicitamente a limitação de recursos. Todavia, diversos métodos de PEFT dependem da seleção de parâmetros a serem alterados, o que pode acarretar em degradação de desempenho [Yang et al. 2024]. \n",
            "\n",
            "Os métodos de PEFT são tipicamente avaliados em LLMs com bilhões de parâmetros, são superparametrizados [Ding et al. 2023]. Embora haja uma motivação natural para reduzir o consumo de recursos por parte desses modelos, sua aplicação em grande escala, mesmo que de forma mais eficiente, não elimina completamente as barreiras impostas ao uso de MLs dessa magnitude. Surge, então, uma questão relevante: quais seriam os impactos da aplicação de técnicas de PEFT em modelos de menor escala em relação à sua capacidade de realizar tarefas específicas? Adicionalmente, o português destaca-se como uma língua diversificada, apresentando particularidades estruturais significativas, como a relação de ordem das palavras e as variações nas desinências, que podem alterar o significado de uma frase [Kato et al. 2023]. Nesse contexto, outra questão importante se apresenta: a aplicação de técnicas de PEFT em modelos de menor escala para o português afetaria negativamente o desempenho e a representação do idioma? \n",
            "\n",
            "Para responder a tais questões, este artigo contribui com uma avaliação entre a abordagem de ajuste fino completo e técnicas de PEFT, especificamente Low-Rank Adaptation (LoRA) [Hu et al. 2022] e GreenTrainer [Huang et al. 2024], em dois PLMs específicos para o português: OPT-PTBR1, com 125 milhões de parâmetros, e PTT5-base [Carmo et al. 2020], com 223 milhões de parâmetros. Nossos resultados demonstram que as técnicas eficientes produzem desempenhos competitivos em relação ao ajuste fino completo, mesmo em modelos de menor escala. Notavelmente, a técnica GreenTrainer apresentou resultados com menor degradação e, em alguns casos, até superiores ao ajuste fino completo. Com essa análise, buscamos contribuir para a atenuação dos impactos socioeconômicos e ambientais do treinamento de MLs, sem deixar de considerar as particularidades do idioma português. \n",
            "\n",
            "2. Fundamentação Teórica  \n",
            "Esta seção visa elucidar conceitos fundamentais tratados no trabalho e essenciais no contexto de ajuste de MLs, especificamente acerca de PLMs e métodos de PEFT. \n",
            "\n",
            "2.1. Ajuste de Modelos de Língua Pré-treinados  \n",
            "Os PLMs são modelos que passam por uma etapa chamada de pré-treinamento, cujo objetivo é incorporar informações linguísticas relevantes a partir de um grande volume de corpora. Todavia, esses modelos podem não representar adequadamente informações específicas de certos domínios ou tarefas não abordadas durante o pré-treinamento. Para tratar dessa questão, adota-se amplamente o ajuste fino dos PLMs, no qual os pesos dos modelos são atualizados para tarefas ou domínios particulares por meio do treinamento sobre um novo conjunto de dados específico, tipicamente na tarefa final pretendida. Dessa forma, é possível aproveitar o conhecimento previamente codificado sem a necessidade de repetir a etapa de pré-treinamento, realizando um processo direcionado e geralmente menos oneroso [Paes et al. 2024]. \n",
            "\n",
            "2.2. Ajuste Fino Eficiente de Parâmetros  \n",
            "O conjunto de técnicas de PEFT reduz a demanda por recursos computacionais para ajuste de PLMs. Esses métodos são divididos por [Xu et al. 2023] em aditivo, parcial, reparametrizado, unificado e híbrido. O ajuste aditivo introduz uma quantidade menor de parâmetros adicionais ajustáveis, evitando o ajuste dos parâmetros próprios do modelo pré-treinado. O ajuste parcial atualiza apenas um subconjunto dos parâmetros pré-treinados. A reparametrização utiliza transformações de baixo posto da Álgebra Linear para reduzir o número de parâmetros treináveis. O método unificado propõe um framework coeso que simplifica a integração de técnicas de ajuste fino, garantindo consistência e eficiência na adaptação dos modelos. Por fim, o método híbrido combina diversas técnicas de PEFT. Em comum, todos os métodos ajustam um número reduzido de parâmetros dos MLs. \n",
            "\n",
            "Dentre essas técnicas, o método reparametrizado LoRA [Hu et al. 2022] se destaca como um dos métodos de PEFT mais utilizados para o ajuste de modelos em diferentes tarefas ao proporcionar consistentemente a redução no número de parâmetros treináveis e consequente redução na demanda de memória [Zhao et al. 2024a, Yang et al. 2024]. Essa estratégia utiliza matrizes adicionais de baixo posto A e B, que substituem a matriz de pesos original W. A computação final dos modelos é realizada por meio da expressão W + A × B, permitindo a adaptação dos pesos com uma quantidade significativamente menor de recursos computacionais. \n",
            "\n",
            "Embora eficaz, a LoRA ainda requer a computação dos gradientes de ativação durante a etapa de backpropagation no treinamento de modelos, o que limita seu potencial máximo de redução de recursos. A estratégia GreenTrainer [Huang et al. 2024] surge como uma alternativa que visa reduzir diretamente as operações necessárias para ajustes dos modelos, sem desconsiderar a backpropagation. Ela seleciona tensores específicos para ajuste a cada época de treinamento, com base na importância de cada tensor para a diminuição da loss, caracterizando-se assim como uma técnica de ajuste parcial. Além disso, ela permite a configuração do hiperparâmetro ρ, que determina a porcentagem de operações mantidas em relação ao ajuste fino completo. \n",
            "\n",
            "Desse modo, ao considerar uma técnica consolidada e amplamente reconhecida como a LoRA, e uma abordagem emergente e competitiva, como o GreenTrainer, este estudo visa realizar uma avaliação inicial acerca do impacto dessas abordagens no desempenho de PLMs de menor escala em tarefas finais, bem como na redução de seus custos e impactos computacionais. \n",
            "\n",
            "3. Trabalhos Relacionados  \n",
            "Trabalhos recentes têm desenvolvido MLs específicos para o português utilizando métodos eficientes. Como ilustração, [Carmo et al. 2020] realizaram tanto o ajuste completo de parâmetros quanto o ajuste restrito aos embeddings do vocabulário — um método parcial — no treinamento de um ML voltado para o português. Os resultados indicam que, embora competitivo, o ajuste restrito aos embeddings é inferior ao ajuste completo. Além disso, os estudos de [Garcia et al. 2024] e [Cabral et al. 2024] introduziram LLMs ajustados especificamente para tarefas em português baseados na arquitetura Llama [Touvron et al. 2023], empregando a técnica de reparametrização LoRA. \n",
            "\n",
            "Outros trabalhos avaliam o impacto de técnicas de PEFT sobre o desempenho de PLMs. [Yang et al. 2024] comparam o ajuste fino completo a técnicas como LoRA, Prefix-tuning [Li and Liang 2021] e o uso de adaptadores [Houlsby et al. 2019] em modelos de menor escala da arquitetura BERT [Devlin et al. 2019] em tarefas não generativas, destacando o desempenho da estratégia LoRA e a competitividade das demais estratégias de PEFT em relação ao ajuste completo nesse contexto. Contudo, tratando-se da avaliação realizada sobre modelos generativos da arquitetura Llama, as técnicas baseadas em LoRA se sobressaem. Resultados similares são reportados em [Huang et al. 2024], que, ao proporem a estratégia GreenTrainer, realizaram uma avaliação comparativa com outras técnicas de PEFT, concluindo por sua competitividade. O estudo avaliou PLMs multilíngues ou predominantemente voltados ao inglês, com parâmetros variando entre 350 milhões e 7 bilhões, revelando o potencial da aplicação de técnicas eficientes até mesmo nos modelos com menor número de parâmetros. \n",
            "\n",
            "No melhor de nosso conhecimento, não há, ainda, trabalho que avalie o uso da abordagem GreenTrainer para MLs em português. Adicionalmente, nenhum dos trabalhos mencionados apresenta uma análise comparativa que considere a relação do desempenho de MLs de menor escala no idioma e o impacto de seu consumo em termos de tempo e CO2eq. Desse modo, este estudo visa oferecer novas perspectivas que abordem tanto a eficácia preditiva de modelos, quanto os custos associados à sua etapa de ajuste. \n",
            "\n",
            "4. Avaliação de Técnicas de Ajuste Eficiente  \n",
            "Esta seção detalha os MLs avaliados, a tarefa de PLN selecionada e as métricas de avaliação adotadas para investigar o impacto das técnicas LoRA e GreenTrainer tanto no desempenho textual quanto no consumo computacional. Destaca-se que o ajuste fino completo dos parâmetros dos modelos foi adotado como baseline. \n",
            "\n",
            "4.1. Modelos de Língua Selecionados  \n",
            "A seleção de MLs para tarefas específicas é influenciada pelo número de parâmetros e pelo corpus de pré-treinamento, fatores cruciais para a viabilidade de execução em diferentes plataformas de hardware e para as capacidades de geração de texto do modelo. Modelos maiores geralmente demandam mais recursos computacionais, enquanto o corpus de pré-treinamento determina a adequação do modelo às necessidades da tarefa [Freitas 2024]. No contexto de recursos limitados, foram escolhidos dois modelos: o OPT-PTBR2, com 125 milhões de parâmetros, baseado na arquitetura Open Pre-trained Transformer (OPT) [Zhang et al. 2022] e adaptado para o português do Brasil, e o PTT5-base [Carmo et al. 2020], com 223 milhões de parâmetros, utilizando a arquitetura T5 [Raffel et al. 2020] e pré-treinado com um corpus de páginas web em português do Brasil. A escolha de modelos menores alinha-se com a necessidade de operar em ambientes com recursos limitados, mantendo a avaliação da qualidade da geração de textos em português. \n",
            "\n",
            "4.2. A Tarefa de PLN Aplicada: Sumarização  \n",
            "Para garantir a compatibilidade com a implementação pública do GreenTrainer3, a tarefa de sumarização textual foi selecionada. A sumarização por meio de MLs consiste em condensar as informações de um texto, gerando uma nova versão que preserva de forma concisa o conteúdo essencial do original. Essa tarefa é amplamente estudada em PLN, incluindo no contexto do português brasileiro [Paiola 2022, Pontes et al. 2022, Feltrin et al. 2023], com LLMs recentemente estabelecendo novos padrões de geração [Souza et al. 2024]. Fatores como a coocorrência de termos relevantes e a fidelidade entre texto original e gerado são importantes para determinar a qualidade de um resumo. Igualmente relevantes são aspectos como a aderência à formalidade e precisão gramatical pretendidas. Por exemplo, no contexto jornalístico, resumos de notícias políticas podem exigir um nível de formalidade distinto daquele necessário para resumos de eventos recentes em um reality show popular, embora, em ambos os casos, a correção gramatical seja tipicamente fundamental. Assim, a tarefa de sumarização posiciona-se apropriadamente para a avaliação da aplicação de técnicas de ajuste de modelos, uma vez que a adequação a contextos e domínios específicos é fundamental para garantir a qualidade das gerações textuais [Paes et al. 2024]. \n",
            "\n",
            "4.3. Métricas de Avaliação  \n",
            "Com o objetivo de avaliar de forma integrada a qualidade do desempenho generativo e os custos e impactos computacionais, três grupos de métricas foram usados na análise de geração de resumos. O primeiro grupo visa medir a aderência dos resumos gerados em relação aos textos de referência e é composto pelas métricas ROUGE [Lin 2004] e BERTScore [Zhang et al. 2020]. A métrica ROUGE, amplamente utilizada nesse contexto, compara a sobreposição de n-gramas entre o resumo automático e a referência, enquanto o BERTScore utiliza modelos de língua baseados em BERT para avaliar a similaridade semântica entre os textos. Neste estudo, a métrica ROUGE é apresentada pela média de suas variantes, ROUGE-1, ROUGE-2, ROUGE-L e ROUGE-S [Souza et al. 2024], que se diferenciam na forma de computar os n-gramas, sendo os resultados expressos em valores percentuais. O BERTScore, por sua vez, é expresso em termos da sua componente F1. O segundo grupo de métricas visa mensurar explicitamente o impacto e o consumo de recursos associados ao ajuste dos modelos, incluindo a contagem do número de (peta) operações de ponto flutuante por segundo (PFLOPS), que quantifica as operações aritméticas necessárias para o ajuste, o tempo de treinamento dos modelos e a quantidade equivalente de CO2 emitida durante o processo, estimada pela ferramenta disponível por [Lacoste et al. 2019]. Por fim, o terceiro grupo aproveita do extenso conjunto de métricas fornecidas pelo portal NILC-metrix [Leal et al. 2023] para avaliar a qualidade de escrita dos textos gerados. Essas métricas extraem valores de diversos indicadores linguísticos para avaliar informações sobre morfossintaxe, coesão e coerência. \n",
            "\n",
            "5. Experimentos  \n",
            "Esta seção apresenta os experimentos conduzidos, detalhando as configurações utilizadas e os resultados obtidos. \n",
            "\n",
            "5.1. Configurações Experimentais  \n",
            "Hiperparâmetros  \n",
            "Considerando a premissa de recursos limitados, os modelos foram treinados por apenas uma época, com uma taxa de aprendizado de 2 · 10−5 e um tamanho de lote de 4. Para a tarefa de sumarização, foram definidos: max input length de 512, max output length de 128, repetition penalty de 2,5 e length penalty de 1,0. No que se refere aos parâmetros do LoRA, utilizou-se r = 8, lora alpha = 32 e uma taxa de dropout de 0,1. O GreenTrainer foi testado com ρ de 0,5 e 0,7, e implementado conforme [Huang et al. 2024]. Também ao encontro desse trabalho, o modelo OPT-PTBR foi configurado com a estrutura “TL;DR” para sumarização, enquanto o modelo PTT5 usou o prefixo “sumarize: [sequência de entrada]”. Por fim, o BERTScore foi computado utilizando o modelo BERT multilingual5, dada a incompatibilidade da métrica com um modelo próprio para o português. \n",
            "\n",
            "Conjunto de Dados  \n",
            "A tarefa de sumarização ocorreu com a base RecognaSumm [Paiola et al. 2024]. Esse conjunto possui origem diversificada, sendo composto por notícias de diferentes fontes de informação. Tal diversidade resulta em uma coleção de documentos que abrangem uma variedade de tópicos e estilos jornalísticos. Ademais, o RecognaSumm contém cerca de 135 mil instâncias em que, para os propósitos deste trabalho, foram selecionadas apenas as colunas referentes ao texto da notícia e ao sumário, esse último servindo como referência padrão nas métricas de avaliação. Adota-se a subdivisão pré-estabelecida do conjunto de dados, de 81,2 mil instâncias para treinamento e 27,1 mil para validação e teste cada. \n",
            "\n",
            "5.2. Resultados Experimentais  \n",
            "A Tabela 1 combina os resultados do primeiro e do segundo grupo de métricas avaliados. A porcentagem indica a variação positiva ou negativa em relação ao ajuste fino completo, com resultados com diferença percentual inferior a 1% marcados com 0%. Por fins de simplificação, as configurações de ρ para o GreenTrainer são denotadas GT-ρ. \n",
            "\n",
            "Tabela 1. Comparação de eficiência, impacto ambiental e métricas textuais.  \n",
            "Modelo  Estratégia  PFLOPS  Tempo (h)  CO2eq (kg)  ROUGE  BERTScore  \n",
            "OPT-PTBR (125M params)     \n",
            "PTT5 (220M params)     \n",
            "Ajuste fino     \n",
            "GT-0.5     \n",
            "GT-0.7     \n",
            "LoRA     \n",
            "Ajuste fino     \n",
            "GT-0.5     \n",
            "GT-0.7     \n",
            "LoRA     \n",
            "16,59     \n",
            "8,18 (51%↓)     \n",
            "11,61 (30%↓)     \n",
            "11,06 (33%↓)     \n",
            "15,67     \n",
            "8,76 (44%↓)     \n",
            "11,50 (27%↓)     \n",
            "10,45 (33%↓)     \n",
            "3,00     \n",
            "1,45 (52%↓)     \n",
            "2,17 (28%↓)     \n",
            "2,28 (24%↓)     \n",
            "3,73     \n",
            "2,38 (36%↓)     \n",
            "2,87 (23%↓)     \n",
            "2,61 (30%↓)     \n",
            "0,24     \n",
            "0,12 (50↓%)     \n",
            "0,17 (29↓%)     \n",
            "0,18 (25↓%)     \n",
            "0,30     \n",
            "0,19 (37↓%)     \n",
            "0,23 (23↓%)     \n",
            "0,21 (30↓%)     \n",
            "7,48     \n",
            "4,60 (39%↓)     \n",
            "7,94 (6%↑)     \n",
            "7,23 (3%↓)     \n",
            "27,82     \n",
            "27,16 (2%↓)     \n",
            "27,56 (1%↓)     \n",
            "26,20 (6%↓)     \n",
            "0,652     \n",
            "0,662 (2%↑)     \n",
            "0,682 (5%↑)     \n",
            "0,672 (3%↑)     \n",
            "0,742     \n",
            "0,739 (0%↓)     \n",
            "0,741 (0%↓)     \n",
            "0,734 (1%↓)     \n",
            "\n",
            "Os resultados indicam que a estratégia GT-0.7 apresentou ou a menor degradação, ou uma melhora no desempenho textual em comparação com o ajuste fino em todos os casos avaliados. Em termos de desempenho computacional, seus resultados são próximos aos da LoRA, embora ligeiramente inferiores. Observa-se que a configuração GT-0.5, a mais eficiente em termos de consumo, apresentou uma queda significativa nos resultados generativos para o OPT-PTBR na métrica ROUGE. No entanto, essa mesma configuração não resultou em grandes quedas para o modelo PTT5, indicando que a robustez inerente do modelo deve ser considerada ao aplicar estratégias de eficiência drástica. Na verdade, essa configuração foi superior à estratégia LoRA para esse modelo. \n",
            "\n",
            "(a.) ROUGE x PFLOPS  (b.) ROUGE x Tempo  (c.) BERTScore x PFLOPS  (d.) BERTScore x Tempo  \n",
            "30  25  20  15  10  5  30  25  20  15  10  5  \n",
            "8  11  14  17  1  1.5  2  2.5  3  3.5  4  \n",
            "0.74  0.72  0.7  0.68  0.66  0.64  \n",
            "8  11  14  17  0.74  0.72  0.7  0.68  0.66  0.64  \n",
            "1  1.5  2  2.5  3  3.5  4  \n",
            "OPT-PTBR  PTT5  GT-0.5  GT-0.7  LoRA  Ajuste Fino  \n",
            "\n",
            "Figura 1. Comparativo entre os desempenhos computacionais e textuais.  \n",
            "A Figura 1 contrasta as métricas textuais com as medidas de desempenho. Essa comparação reforça a estratégia GT-0.7 como a que gera resultados mais próximos do ajuste fino, seguida pela estratégia LoRA. Fica evidente também a leve superioridade da economia da estratégia LoRA em relação à GT-0.7. Em termos de economia computacional, a estratégia LoRA se posiciona consistentemente entre as configurações de GT, embora, em termos de resultados generativos, seja inferior à GT-0.5 para o modelo PTT5. Além disso, particularmente para o modelo OPT-PTBR, os resultados de BERTScore obtidos pelo ajuste eficiente foram superiores ao ajuste completo. Por fim, a Figura 1 também demonstra a superioridade geral do modelo PTT5 na execução da tarefa, ressaltando o impacto que a escolha adequada do PLM pode implicar. \n",
            "\n",
            "A Tabela 2 apresenta a distância euclidiana média, calculada com base em cinco conjuntos de métricas do NILC-metrix, entre uma amostra de 100 sumários gerados para cada configuração avaliada e suas respectivas referências. Antes do cálculo, os valores foram normalizados para o intervalo de 0 a 1. Os melhores resultados estão destacados em negrito, enquanto os segundos melhores estão sublinhados. De modo geral, os valores semelhantes observados dentro do mesmo modelo, independentemente da estratégia de ajuste, indicam que as técnicas de PEFT não comprometem significativamente a capacidade de escrita dos modelos de língua quando comparadas ao ajuste fino. Notavelmente, as configurações do GT obtiveram os melhores resultados em várias ocorrências. No entanto, uma avaliação comparativa entre os modelos revela que o PTT5 consistentemente apresenta desempenho superior, especialmente na avaliação de Informações Morfossintáticas. Esse resultado pode estar relacionado à etapa de pré-treinamento, mais robusta nesse modelo, sugerindo que uma execução adequada dessa fase possibilita uma estratégia de ajuste mais eficiente. No entanto, uma dualidade surge, pois uma etapa de pré-treinamento mais robusta pode resultar em custos mais elevados. De maneira geral, esses resultados corroboram os anteriormente descritos, indicando que, além da escolha da estratégia de ajuste, a seleção do modelo mostra-se crucial. \n",
            "\n",
            "6. Considerações Finais  \n",
            "Este trabalho conduziu experimentos com estratégias de ajuste fino eficiente, empregando dois modelos de menor escala treinados em português para a tarefa de sumarização textual. Os resultados indicam que a estratégia do GreenTrainer é competitiva em relação à estratégia já estabelecida LoRA. Dependendo da escolha do parâmetro ρ, a estratégia pode, inclusive, alcançar um equilíbrio superior entre a degradação de desempenho e o ganho de eficiência computacional. Além disso, os resultados revelam que a aplicação de estratégias eficientes pode implicar degradações significativas, dependendo da escolha do modelo. Trabalhos futuros incluem avaliar essas estratégias em outros modelos e tarefas, visando obter melhores indicativos sobre a generalização, além de considerar novas estratégias como LoRETTA [Yang et al. 2024] e GaLore [Zhao et al. 2024b]. \n",
            "\n",
            "Por fim, visando à transparência, explicitamos os custos totais desta pesquisa, totalizando R$1.642,48 em uso de recursos em nuvem. Os experimentos, realizados na Google Cloud Platform na região us-central1, resultaram em emissões estimadas de 14,52 kgCO2eq, com 364 horas de computação em duas GPUs T4 (TDP de 70W) e uma eficiência de carbono de 0,57 kgCO2eq/kWh.\n",
            "14\n",
            "Resumo. Repositórios bibliográficos digitais, incluindo publicações, autores e áreas de pesquisa, são essenciais para o compartilhamento de informações científicas. No entanto, a eficiência na recuperação, extração e classificação de informações em tais arquivos é ameaçada pela ambiguidade dos nomes dos autores. Este artigo aborda o problema da Desambiguação de Nomes de Autores (AND) propondo um método híbrido de aprendizado de máquina integrando abordagens de Representações de Codificadores Bidirecionais de Transformadores (BERT), Redes Neurais Convolucionais de Grafos (GCN) e Agrupamento Hierárquico Aglomertivo Aprimorado por Grafos (GHAC). O modelo BERT extrai dados textuais de documentos científicos, o GCN estrutura dados globais de grafos acadêmicos, e o GHAC considera o contexto global de redes heterogêneas para identificar padrões de colaboração científica. Comparamos o método híbrido com trabalhos de ponta em AND usando um conjunto de dados de acesso público composto por 7.886 documentos, 137 autores únicos e 14 grupos de autores ambíguos, juntamente com métricas de validação reconhecidas. Os resultados alcançaram uma alta pontuação de precisão de 93,8%, recall de 96,3%, F1 de 95%, Pureza Média de Cluster (ACP) de 96,5%, Pureza Média de Autor (AAP) de 97,4% e K-Métrica de 96,9%. Em comparação com a abordagem base de AND, o método híbrido apresenta melhores resultados, indicando uma abordagem promissora.\n",
            "\n",
            "1. Introdução\n",
            "Repositórios bibliográficos digitais são vastos reservatórios de informações sobre citações bibliográficas (DBLP [DBLP 2024], ArnetMiner [AMiner 2024b], CiteSeerX [CiteSeerX 2019]). Eles oferecem funcionalidades que permitem a identificação de trabalhos de cientistas, autores e suas respectivas redes sociais acadêmicas. O DBLP atualmente lista cerca de 7 milhões de trabalhos em Ciência da Computação, incluindo periódicos e artigos de conferências. Em janeiro de 2024, o DBLP reuniu informações sobre aproximadamente 3,5 milhões de autores, com 227 mil nomes de pesquisadores e publicações manualmente verificados pela equipe do DBLP, correspondendo a uma curadoria de 34% de todas as publicações na base de dados. ArnetMiner armazena informações sobre aproximadamente 2 milhões de trabalhos científicos, 1,7 milhões de autores e 8 milhões de citações bibliográficas [AMiner 2024a]. \n",
            "\n",
            "Ao armazenar milhões de informações a partir de registros bibliográficos, os repositórios digitais tornam-se uma fonte essencial de informações para a comunidade acadêmica e científica global, permitindo a recuperação, extração e classificação de publicações relevantes de forma centralizada [Ferreira et al. 2020]. Além dessas características bibliográficas, essas bibliotecas digitais fornecem análises úteis para uma melhor tomada de decisão por agências de fomento científico e instituições acadêmicas [Hussain e Asghar 2017]. \n",
            "\n",
            "No entanto, um problema comum em repositórios bibliográficos digitais é a Desambiguação Automática de Nomes de Autores (AND). O problema de AND ocorre quando diferentes autores têm o mesmo registro de nome ou quando um autor possui múltiplos registros de nome no mesmo conjunto de dados. Tal problema pode afetar significativamente a performance na recuperação de documentos e informações através de mecanismos de busca na Web, e obstruir a integridade da entidade para bancos de dados integrados. Mesmo que o problema de ambiguidade de nomes de autores tenha sido estudado por décadas, ele permanece sem uma solução canônica. Assim, esforços de pesquisa para resolver o problema de AND são essenciais, especialmente considerando que repositórios bibliográficos digitais estão se tornando mais centrados em pessoas do que centrados em documentos [Shin et al. 2014]. \n",
            "\n",
            "Este trabalho aborda o problema de AND com um novo método híbrido combinando técnicas avançadas de aprendizado de máquina, como as Representações de Codificadores Bidirecionais de Transformadores (BERT) [Devlin et al. 2019], Redes Neurais Convolucionais de Grafos (GCN) [Zhang et al. 2019], e Agrupamento Hierárquico Aglomertivo Aprimorado por Grafos (GHAC) [Qiao et al. 2019]. Como proposto por [Kipf e Welling 2017], o GCN é um poderoso modelo de aprendizado de máquina que estende a Rede Neural Convolucional (CNN) para lidar com dados estruturados como grafos, capturando dependências locais e globais dentro de uma rede. Nosso método visa aprimorar a precisão de AND em repositórios bibliográficos digitais considerando a recuperação, extração e classificação de informações, aplicando um tratamento semântico do conteúdo dos documentos relacionado a uma representação gráfica de relações entre documentos, autores e outros atributos científicos. \n",
            "\n",
            "Como apresentado em [Ferreira et al. 2020], existem várias abordagens para resolver o problema de AND aplicando diversas técnicas, mas nenhum trabalho combina aprendizado de transferência com técnicas GCN e GHAC. Além disso, de acordo com uma recente revisão da literatura sobre AND, usando a teoria da abordagem meta-analítica consolidada com técnicas quantitativas e aspectos bibliométricos, o método híbrido proposto é considerado uma solução inovadora para AND [Rodrigues et al. 2024]. O restante do artigo inclui na Seção 2 trabalhos relacionados que se concentram em abordagens para o problema de AND. A Seção 3 detalha o método híbrido de AND. A Seção 4 inclui os experimentos realizados com as métricas de avaliação. Na Seção 5, apresentamos os resultados com discussão. Finalmente, a conclusão e trabalhos futuros estão na Seção 6. \n",
            "\n",
            "2. Trabalhos Relacionados\n",
            "Conforme apresentado em [Rodrigues et al. 2024], as abordagens amplamente utilizadas para resolver AND são o agrupamento de autores associado a funções de similaridade e métodos de agrupamento; e alguns trabalhos com atribuição de autores aliados a métodos de classificação. Além disso, abordagens baseadas em grafos, embeddings de palavras com aprendizado supervisionado, e heurísticas com aplicações probabilísticas são comuns. A revisão da literatura destaca a prevalência e eficácia das técnicas de agrupamento de autores, especialmente ao abordar questões associadas a grandes bases de dados bibliográficas. Nesta seção, apresentamos os trabalhos mais relacionados ao nosso método híbrido. \n",
            "\n",
            "Os autores em [Kim e Owen-Smith 2020] exploram técnicas supervisionadas usando aprendizado de transferência em tarefas de AND onde não havia dados rotulados disponíveis para treinamento. Os resultados mostram que ao treinar dados de origem que representam bem as principais características dos conjuntos de dados de destino, os modelos de desambiguação desenvolvidos através do aprendizado de transferência podem produzir resultados comparáveis aos alcançados por abordagens tradicionais de aprendizado de máquina, que treinam algoritmos em subconjuntos rotulados especificamente dos dados de destino. \n",
            "\n",
            "Em [Waqas e Qadir 2021], os autores propõem um método para realizar AND baseado em clusters heurísticos em várias camadas. Eles utilizaram características globais e aquelas relacionadas à estrutura das publicações para agrupá-las. Uma das diferenças apontadas pelos autores é que, em vez de confiar apenas em informações de palavras-chave, a abordagem também considera a estrutura contextual das publicações para o agrupamento. Os autores usam um método de classificação incremental para reduzir erros após a criação de clusters. Um conjunto de dados chamado CustAND foi apresentado para teste e execução do método AND. \n",
            "\n",
            "A abordagem de [Pooja et al. 2022] utiliza GCN em conjunto com mecanismos de atenção para aprender representações em um grafo heterogêneo de documentos. O trabalho destaca a importância de usar atenção em diferentes níveis, tanto em relação aos tipos de vizinhos quanto às relações, para incorporar contexto relevante ao aprendizado de representações de nós. A ênfase na atenção permite uma análise detalhada do impacto desse mecanismo na captura de informações semânticas e contextuais dos documentos em um grafo. Os autores usaram duas variantes do ArnetMiner como conjuntos de dados, a primeira com 110 e a segunda com 100 referências a nomes ambíguos. \n",
            "\n",
            "3. O Método Híbrido\n",
            "O método híbrido possui quatro etapas principais, conforme apresentado na Figura 1 e descrito a seguir. \n",
            "\n",
            "3.1. Entrada e Pré-processamento de Dados\n",
            "A primeira etapa do método híbrido lida com a entrada e pré-processamento dos dados dos documentos (publicações), quando os dados são ajustados e formatados para garantir que estejam adequados para as etapas subsequentes. \n",
            "\n",
            "3.2. Criação e Caracterização do Grafo\n",
            "Esta etapa desempenha um papel fundamental, pois cria a estrutura da rede heterogênea a partir das informações recebidas da etapa anterior e fornece informações contextuais para a tarefa de AND. \n",
            "• Criação de uma Rede Heterogênea – inclui diferentes tipos de nós e arestas. O grafo é formalmente definido como Gheterogêneo = (Nnos, Earestas), onde Nnos são nós representando publicações, autores e palavras. As arestas Earestas representam as diferentes conexões entre os nós, como contém (entre publicações e palavras), escrito por (entre publicações e autores), co-autoria (entre autores que colaboraram na mesma publicação) e palavra compartilhada (entre publicações que compartilham palavras-chave). \n",
            "• Extração de Embeddings com BERT – o BERT utiliza aprendizado de transferência pré-treinando seus parâmetros em grandes conjuntos de textos não rotulados com apenas pequenas modificações para realizar tarefas em um determinado domínio. O BERT converte cada palavra em um texto em uma representação vetorial que captura o significado da palavra dado o contexto em que aparece. Esta representação pode ser combinada para obter uma representação de frases inteiras. Neste trabalho, usamos a variante SciBERT do BERT pré-treinada em textos científicos, que é particularmente eficaz na captura das informações contextuais e semânticas de documentos acadêmicos [Beltagy et al. 2019]. O SciBERT calcula os embeddings das publicações com base em títulos e resumos. Esses embeddings são incorporados como características dos nós que representam as publicações no grafo. O algoritmo é descrito na sequência. Dado um conjunto de N documentos com títulos e resumos, onde cada documento i possui um título Ti e um resumo Ri, o processo de extração de embeddings com SciBERT pode ser detalhado da seguinte maneira: \n",
            "1. Tokenização de Títulos e Resumos: cada título Ti e resumo Ri é tokenizado em sequências de tokens separados, representados por {ti,1, ti,2, ... , ti,Li} e {ri,1, ri,2, ... , ri,Mi}, respectivamente. \n",
            "2. Geração de Embedding SciBERT: as sequências de tokens dos títulos Ti e resumos Ri são processadas pelo BERT, que produz um vetor de embeddings para cada documento. Esses embeddings capturam a semântica dos textos, refletindo os principais tópicos e relações contextuais. \n",
            "3. Embedding do Grafo: os embeddings resultantes do SciBERT são usados como características dos nós que representam as publicações no grafo heterogêneo. Em nosso algoritmo, o SciBERT realiza a extração de embeddings nos títulos e resumos dos documentos. Esses embeddings são usados como características dos nós no grafo heterogêneo, permitindo que o GCN subsequente use essas representações para analisar as interações entre publicações, autores e palavras, incluindo relacionamentos de coautoria. Um índice de arestas representa de forma esparsa as conexões entre os nós. Este formato permite ao GCN processar grandes redes heterogêneas enquanto mantém informações essenciais de conectividade entre entidades. \n",
            "\n",
            "3.3. Aprendizado com GCN\n",
            "Após a extração de embeddings com SciBERT e a construção do grafo heterogêneo, os embeddings dos títulos e resumos são usados como características dos nós na rede. As operações de propagação nas camadas do GCN usam esses embeddings para calcular as representações dos nós vizinhos. A necessidade de aplicar um modelo de GCN a uma rede heterogênea, em vez de outras técnicas tradicionais de aprendizado profundo, surge das particularidades das redes, onde as relações entre diferentes tipos de nós e as estruturas de grafos devem ser capturadas de forma eficaz. \n",
            "\n",
            "Na etapa do GCN, este método híbrido proposto processa inicialmente os dados textuais para criar um vocabulário com uma matriz de características, onde cada linha corresponde ao embedding de um nó, como documentos, autores ou palavras-chave. O índice de arestas representa as conexões entre os nós, preservando as relações essenciais na rede heterogênea. Para capturar dependências locais e globais dentro dos dados da rede heterogênea, cada camada do GCN atualiza as representações dos nós com base nas conexões e características definidas pelo índice de arestas. O modelo GCN proposto utiliza funções de ativação para introduzir não linearidades no modelo. Em nosso trabalho, usamos a função ReLU (σ(x) = max(0, x)) em diferentes estágios do GCN (amplamente utilizada para mitigar o problema do gradiente que desaparece). O treinamento do GCN é realizado minimizando a função de perda MSE, definida como L = 1/N * Σ(Zi − Xi)², onde N é o número de nós na rede, Zi é a saída final do GCN para o nó i, e Xi é o vetor de características original do nó i. O otimizador Adam [Kingma 2014] foi usado para ajustar os pesos do modelo, ajustando uma taxa de aprendizado inicial conforme necessário. \n",
            "\n",
            "Finalmente, o GCN produz embeddings dos nós na rede, representados como vetores de baixa dimensão que capturam tanto as características iniciais dos nós quanto a estrutura da rede. Esses embeddings são usados para tarefas subsequentes, como o agrupamento hierárquico aglomerativo, que será realizado na próxima etapa. \n",
            "\n",
            "3.4. Geração de Agrupamento Hierárquico Aglomertivo\n",
            "Os resultados de agrupamento de autores desambiguados são gerados com base em suas representações na rede heterogênea. O objetivo é agrupar documentos com características semelhantes nas interações entre publicações, autores e palavras-chave usando o método GHAC [Qiao et al. 2019]. O GHAC é um algoritmo de agrupamento hierárquico aglomerativo que integra informações estruturais da rede considerando a similaridade média dos embeddings entre os nós conectados. O algoritmo é adequado para redes heterogêneas complexas, como a que foi construída a partir dos embeddings gerados pelo GCN. \n",
            "\n",
            "Inicialmente, cada documento é um cluster individual. O algoritmo iterativo prossegue para mesclar os clusters com a maior similaridade média entre seus componentes até atingir o número desejado de clusters. A similaridade entre os dois clusters é definida com base nos produtos internos normalizados dos embeddings dos nós, permitindo que o GHAC capture as relações de dados semânticos e contextuais. \n",
            "\n",
            "Os documentos são agrupados para maximizar a coesão interna dos clusters enquanto preservam as características de interação semântica e estrutural entre os diferentes tipos de entidades na rede. Este método não apenas agrupa documentos com base em similaridades locais, mas também considera o contexto global da rede heterogênea, tornando-o particularmente eficaz na organização de redes acadêmicas complexas e na identificação de padrões subjacentes de coautoria e colaboração científica. \n",
            "\n",
            "4. Experimentos\n",
            "Para validar o método híbrido, realizamos experimentos comparativos à abordagem de múltiplas camadas com técnicas de agrupamento de [Waqas e Qadir 2021] como baseline, utilizando o conjunto de dados público CustAND, que é composto por 14 grupos de nomes ambíguos com 137 autores distintos e 7.886 documentos [Waqas e Qadir 2022]. Este conjunto de dados é valioso para estudos de AND com vários atributos e relações de dados complexas. O pipeline de execução e o código para a implementação deste método estão disponíveis no repositório. \n",
            "\n",
            "4.1. Configuração Experimental\n",
            "Utilizamos os títulos e resumos dos documentos para extrair embeddings com SciBERT. Em seguida, concatenamos essas características para formar o texto de entrada tokenizado usando o tokenizador BERT limitado a 512 tokens. A saída foi um embedding de 768 dimensões representando cada documento. A configuração GCN definida empiricamente inclui três camadas com um tamanho de embedding de 768, função de ativação ReLU, função de perda de Erro Quadrático Médio (MSE), e a otimização foi realizada com uma taxa de aprendizado de 0.001 para o algoritmo Adam. Executamos o treinamento por 200 épocas com um tamanho de lote de 128. A linguagem Python foi usada para executar os experimentos em um ambiente Google Colab com acelerador de hardware, GPU com 22.5 GB de RAM, CPU com 53 GB de RAM, 201.2 GB de disco, e o tipo de tempo de execução configurado para Python 3. \n",
            "\n",
            "4.2. Métricas de Avaliação\n",
            "As métricas de precisão, recall, F1-measure e métricas específicas para agrupamento, como Pureza Média de Cluster (ACP), Pureza Média de Autor (AAP) e métricas K, comumente apresentadas na literatura de AND, são usadas para avaliar os resultados experimentais. \n",
            "\n",
            "A precisão mede a proporção de documentos classificados corretamente em relação ao número total de documentos pertencentes a autores, avaliando a capacidade do algoritmo de atribuir documentos corretamente a autores como Precision = Documentos Corretamente Classificados / Total de Documentos Classificados. O recall avalia a capacidade do algoritmo de recuperar todos os documentos de um autor real, medindo a capacidade de recuperação do algoritmo em relação a autores reais como Recall = Documentos Corretamente Recuperados / Total de Documentos de Autor Real. O F1-measure é a média harmônica da precisão e recall, fornecendo uma métrica equilibrada entre essas métricas (métrica de desempenho geral) como F1-measure = 2 × Precision × Recall / (Precision + Recall). \n",
            "\n",
            "A ACP avalia a pureza média dos clusters gerados pelo algoritmo em relação aos clusters teóricos. A ACP mede o quão bem os documentos foram agrupados em clusters que representam autores reais como ACP = 1/N * Σ(nij/nj), onde N é o tamanho total dos registros de publicação/trabalho no conjunto de teste, q é o número de clusters previstos pelo método híbrido, R é o número de clusters de referência/reais gerados manualmente, nij é o número de elementos em comum entre os clusters previstos pelo método híbrido i e os clusters de referência j, e nj é o número de elementos no cluster de referência j. Quanto mais puros os clusters, maior o valor da ACP. A AAP mede quão fragmentados ou coesos são os clusters previstos pelo algoritmo em relação aos clusters de referência. Um AAP mais alto indica que os clusters são menos fragmentados, como AAP = 1/N * Σ(n²ij/ni). \n",
            "\n",
            "A K-métrica determina a relação entre a pureza média dos clusters (ACP) e a pureza média dos autores (AAP). É uma métrica que fornece uma única medida que considera tanto a qualidade dos clusters quanto a qualidade da atribuição dos documentos aos autores, como K-metric = √(ACP × AAP). A K-métrica ajuda a avaliar a performance geral do algoritmo de desambiguação balanceando a qualidade dos clusters e a qualidade da atribuição de documentos. \n",
            "\n",
            "A Figura 2 apresenta um exemplo ilustrativo com figuras geométricas correspondendo a um registro de autoria, onde figuras iguais representam o mesmo autor. Existem três clusters teóricos e quatro empíricos, com um cluster empírico não puro e dois registros de autoria fragmentados em dois clusters. Os resultados das métricas aplicadas a este exemplo, considerando a ACP com os clusters empíricos incluem no primeiro dois clusters e o terceiro e quarto clusters dois autores diferentes. Os valores dos numeradores de AAP permanecem os mesmos, mas os denominadores refletem o número de registros nos clusters teóricos. Por exemplo, 32/4 representa três registros do mesmo autor em um cluster empírico de quatro no teórico. O valor final de AAP é 9 × (32/0.722), e a K-métrica é a média geométrica de ACP e AAP. A precisão é 0.857 considerando a soma de três pares de registros de autoria do mesmo autor nos primeiros e segundos clusters empíricos e nenhum nos últimos três clusters. O denominador soma o número total de pares de registros de autoria de cada cluster empírico. O recall é 0.6 usando o mesmo numerador de precisão com o denominador somando os pares de registros de autoria que se referem ao mesmo autor nos clusters teóricos. Finalmente, o F1-measure = 2 × (0.857 × 0.6) é calculado.\n",
            "\n",
            "5. Resultados e Discussão\n",
            "Nesta seção, apresentamos os resultados do nosso método híbrido com as métricas de avaliação (Seção 4.2) para o conjunto de dados CustAND com 14 grupos de nomes ambíguos comparado ao trabalho de baseline de [Waqas e Qadir 2021]. No conjunto de dados CustAND, um exemplo de um grupo de nomes ambíguos para “A Choudhary” consiste em 12 autores distintos que compartilham o mesmo nome na citação do documento, a saber, “Ashish Choudhary”, “Amit Choudhary”, “Anil Choudhary”, “Arvind Choudhary”, “Anupam Choudhary”, “Ajay Choudhary”, “Abhishek Choudhary”, “Aniruddha Choudhary”, “Anjali Choudhary”, “Arjun Choudhary”, “Akshay Choudhary” e “Arun Choudhary”. A Tabela 1 resume as métricas para cada grupo de nomes ambíguos apresentando valores médios para nosso método e para a baseline. \n",
            "\n",
            "A análise das métricas de desempenho do nosso método para os 14 grupos de nomes ambíguos do conjunto de dados CustAND revela resultados atraentes. Comparando os resultados reportados por [Waqas e Qadir 2021], a precisão média entre os 14 grupos é levemente inferior (93.8% versus 94.6%), o que pode indicar uma perda de precisão ao classificar documentos para autores específicos. No entanto, o recall mais alto (96.3% versus 92.5%) sugere que o método aplicado a grupos ambíguos possui uma melhor capacidade de recall e é mais eficiente na identificação de todos os documentos de um autor. O F1-measure de 95% entre os 14 grupos, comparado a 93.5% para [Waqas e Qadir 2021], demonstra que o método alcança um melhor equilíbrio entre precisão e recall. \n",
            "\n",
            "As métricas ACP e AAP entre os 14 grupos também superam a baseline com valores de 96.5% e 97.4%, em comparação a 95.8% e 87%, respectivamente. Esses resultados sugerem uma pureza média mais alta dos clusters gerados e uma menor fragmentação dos clusters previstos, refletindo um agrupamento mais coeso e representativo dos autores reais. Finalmente, a K-métrica de 96.9% nos 14 clusters do nosso método é significativamente mais alta do que os 91.24% reportados por [Waqas e Qadir 2021], indicando que nosso método alcança um equilíbrio superior entre a qualidade dos clusters e a atribuição correta de documentos aos autores.\n",
            "\n",
            "Tabela 1. Métricas de desempenho por grupo de nomes ambíguos. \n",
            "\n",
            "**Grupo de Nomes Ambíguos** | **Autores** | **Precisão** | **Recall** | **F1-measure** | **ACP** | **AAP** | **K-métrica**\n",
            "--- | --- | --- | --- | --- | --- | --- | ---\n",
            "A Choudhary | 12 | 1.000 | 1.000 | 1.000 | 0.946 | 0.938 | 1.000\n",
            "J Martin | 9 | 1.000 | 1.000 | 1.000 | 0.878 | 1.000 | 1.000\n",
            "M A Qadir | 15 | 1.000 | 1.000 | 1.000 | 0.865 | 1.000 | 1.000\n",
            "J Mitchell | 10 | 0.853 | 1.000 | 1.000 | 0.875 | 1.000 | 1.000\n",
            "A Gupta | 8 | 1.000 | 1.000 | 1.000 | 0.875 | 1.000 | 1.000\n",
            "J Robinson | 12 | 0.938 | 0.592 | 0.754 | 0.972 | 0.763 | 1.000\n",
            "A Kumar | 9 | 1.000 | 1.000 | 1.000 | 0.930 | 0.870 | 1.000\n",
            "J Smith | 12 | 1.000 | 1.000 | 1.000 | 0.913 | 0.912 | 1.000\n",
            "Bin Li | 8 | 1.000 | 1.000 | 1.000 | 0.832 | 0.826 | 1.000\n",
            "S Kim | 10 | 1.000 | 1.000 | 1.000 | 0.900 | 0.889 | 1.000\n",
            "D Eppstein | 3 | 1.000 | 1.000 | 1.000 | 0.670 | 0.671 | 1.000\n",
            "Z Zhang | 10 | 1.000 | 1.000 | 1.000 | 0.860 | 0.839 | 1.000\n",
            "J Lee | 11 | 1.000 | 1.000 | 1.000 | 0.947 | 0.935 | 1.000\n",
            "K Tanaka | 137 | 1.000 | 1.000 | 1.000 | 0.960 | 0.965 | 1.000\n",
            "\n",
            "6. Conclusão\n",
            "O principal objetivo deste trabalho foi alcançado ao propor e avaliar a capacidade de resolução do problema AND usando um método híbrido que envolve aprendizado de transferência com SciBERT, GCN e GHAC. Ao comparar a eficácia do nosso método híbrido com o trabalho de ponta de [Waqas e Qadir 2021], usando o conjunto de dados CustAND, notamos que o método proposto superou a linha de base em relação à precisão média, considerando cinco das seis métricas comumente usadas de precisão, recall, F1-measure, ACP, AAP e K-métrica. \n",
            "\n",
            "Experimentos futuros incluem comparação com [Pooja et al. 2022], incluindo o uso de outros métodos de aprendizado de máquina, métodos diversos de extração de informações textuais, e a adoção de abordagens com redes neurais de grafos, como Redes de Atenção em Grafos (GAT) e GraphSAGE com conjuntos de dados maiores. Além disso, uma implementação gerenciável de entrada de dados para o usuário final como uma interface gráfica para tornar a solução mais amigável.\n",
            "15\n",
            "Resumo. O artigo introduz o Porttinari-base PropBank (PBP): o corpus Porttinari-base com uma camada de papéis semânticos. A anotação foi feita sobre dependências sintáticas, usando regras linguísticas e sob inspeção humana. Foram anotados mais de 40 mil argumentos, e os resultados são discutidos à luz de trabalhos que investigam a generalização das classes do PropBank. \n",
            "\n",
            "1. Introdução\n",
            "Entre os métodos utilizados para representar computacionalmente informação semântica em textos está a anotação de papéis semânticos (ou SRL – Semantic Role Labeling). Papéis semânticos são responsáveis por indicar quem fez o quê, para quem, onde, quando, como, por quê, para quê, com o quê, com quem, etc., e assim estruturam de maneira explícita e interpretável a informação contida em enunciados linguísticos. Enquanto tarefa, a anotação de papéis semânticos tem como objetivo atribuir etiquetas a argumentos de predicadores, indicando o papel que estes argumentos exercem em uma frase. \n",
            "\n",
            "A anotação de papéis semânticos permite criar representações semânticas estáveis ao longo de diferentes realizações linguísticas, e as frases 1 a 5 ilustram este ponto. Sintaticamente, “porta” exerce diferentes funções, assim como “chave”. Na atribuição de papéis semânticos, “porta” é a “coisa abrindo” em todas as 5 frases, e “chave” é o instrumento de abertura em todas as 5 frases, não importa a função sintática que exercem. Na frase “O tempo abriu no feriado”, entretanto, teríamos uma outra estrutura argumental (e outra representação semântica), já que estaríamos diante de um outro sentido de “abrir”. \n",
            "\n",
            "1. A chave abriu a porta.\n",
            "2. Ela abriu a porta com a chave.\n",
            "3. A porta foi aberta com a chave.\n",
            "4. A porta foi aberta por ela com a chave.\n",
            "5. A porta abriu com a chave.\n",
            "\n",
            "Um PropBank (Proposition Bank, ou Banco de Proposições) é um corpus que contém anotação de papéis semânticos, relacionando verbos¹ e seus argumentos diretamente às estruturas sintáticas de um treebank e conforme o modelo sugerido por [Palmer et al. 2005], sintaxe-semântica [Palmer et al. 2005, Levin 1993, Levin and Rappaport Hovav 2005]. Teoricamente, papéis semânticos estariam na interface, já que, apesar de ser um fenômeno linguístico amplamente estudado, não há consenso relativo ao conjunto de papéis semânticos da língua. No PropBank, a diversidade teórica acerca dos papéis semânticos é contornada com a utilização de i) argumentos numerados, que vão de Arg0 a Arg5; e ii) argumentos modificadores, um conjunto mais amplo de argumentos. A motivação para o conjunto limitado de papéis numerados é facilitar a generalização para o aprendizado de máquina, ainda que alguns estudos mostrem que este objetivo é apenas parcialmente alcançado [Merlo and Van Der Plas 2009, Gung and Palmer 2021, Li et al. 2023]. A diferença entre argumentos numerados e argumentos modificadores está sobretudo na natureza da relação sintática que o argumento mantém com o verbo (exigência, nos argumentos numerados, vs opcionalidade, nos argumentos modificadores). A distinção entre os argumentos também é motivada pela assimetricidade semântica dos papéis com relação aos verbos, e por isso argumentos numerados são específicos de verbos (o Arg0 do verbo “abrir” é “quem abre”, e o Arg0 de “alagar” é “causador do alagamento”). Os ArgM, por outro lado, têm uma semântica específica e previsível (indicada pelo nome da etiqueta, como ArgM-tmp para “tempo” e ArgM-cau para “causa”), e podem se associar a qualquer verbo. \n",
            "\n",
            "A semântica dos argumentos numerados é revelada com o alinhamento entre a anotação do corpus e o recurso lexical associado ao PropBank, os chamados Frame Files – em nosso caso, dispomos do Verbo-Brasil [Sanches Duran and Aluísio 2015]. Assim, a frase (2), segundo o estilo PropBank, é anotada como indicado abaixo. Um PropBank, portanto, não é apenas um corpus anotado, mas a associação entre um corpus anotado e um léxico, que indica como os elementos devem ser anotados e o que eles significam. \n",
            "\n",
            "• Ela[Arg0] abriu a porta[Arg1] com a chave[Arg2]. \n",
            "\n",
            "Neste artigo, apresentamos o Porttinari-base PropBank (PBP), que corresponde à adição de uma camada de papéis semânticos a todas as frases do corpus jornalístico Porttinari-base, que compõe o treebank Porttinari [Pardo et al. 2021, Duran et al. 2023]. O Porttinari-base é padrão ouro na anotação sintática conforme a teoria Universal Dependencies (UD) [de Marneffe et al. 2021]. Com o PBP, o Porttinari-base passa a ser duplamente padrão ouro – nas dependências sintáticas e nos papéis semânticos —, contribuindo para a disponibilização de recursos de alto nível para o processamento do português. \n",
            "\n",
            "2. Motivação e trabalhos relacionados \n",
            "O PropBank foi criado com o objetivo de treinar modelos no aprendizado supervisionado. Levando em conta a onipresença de arquiteturas neurais e grandes modelos de língua, discutimos brevemente a relevância para o PLN da anotação de papéis semânticos. Entre as críticas ao atual paradigma de IA, estão a falta de transparência e de aplicabilidade dos métodos e resultados. Neste contexto, papéis semânticos oferecem maneiras interpretáveis de representar semanticamente enunciados verbais, podendo servir de insumo, por exemplo, para a criação de grafos de conhecimento [Mohebbi et al. 2022], o que torna este tipo de representação semântica relevante para a investigação acerca da articulação entre os grandes modelos de língua (os LLMs – Large Language Models) e fontes de conhecimento estruturado [Dong 2023]. Na articulação entre redes neurais e SRL, [Mohebbi et al. 2022] propõem uma abordagem de aprendizado de grafos profundos para computar similaridade semântica de documentos, usando papéis semânticos. \n",
            "\n",
            "A anotação de SRL também pode ser usada para avaliação de modelos de língua quanto à capacidade de representar informação semântica estruturada e interpretável [Tenney et al. 2019a, Tenney et al. 2019b, Han and Choi 2020]. Nessa vertente, notamos a escassez de conjuntos de validação criados para o português, que nos faz utilizar conjuntos de dados traduzidos do inglês [Rodrigues et al. 2023]. A utilidade da anotação de papéis semânticos no PLN também pode ser indireta. Considerando o paradigma de avaliação extrínseca, [Evans and Orasan 2019] utilizam SRL para verificar se a simplificação textual é capaz de facilitar o desempenho na tarefa de SRL. Uma vez que a tarefa de SRL pode ser considerada um passo além da análise sintática, diferentes modelos de representação sintática também podem ser avaliados em função do desempenho obtido na anotação SRL, como sugerido em [Freitas 2023]. \n",
            "\n",
            "Desde 2011, o português dispõe do PropBank-BR [Duran and Aluísio 2011], que anotou papéis semânticos ao estilo PropBank sobre a parte brasileira do treebank Bosque, em sua versão de sintaxe de constituintes disponibilizada pela Linguateca. Este PropBank levou à formulação (e adaptação do inglês) de diretivas de anotação e permitiu a criação do recurso léxico que codifica os sentidos dos verbos e descreve seus frames sintáticos, o Verbo-Brasil [Sanches Duran and Aluísio 2015]. No entanto, esse recurso é ainda limitado. Tendo em vista a criação de um material maior e mais lexicalmente diversificado, foi criado o PropBank-BR v2 [Duran et al. 2014, Hartmann et al. 2016]. Diferentemente da versão anterior, este material foi construído sobre árvores sintáticas não revisadas. Em ambos os casos, o processo de anotação seguiu o PropBank original, com a anotação feita de maneira linear, frase a frase. O português conta ainda com o CINTIL-PropBank [Branco et al. 2012], um corpus de frases anotadas com estrutura de constituintes e papéis semânticos, criado de maneira semiautomática, com algumas etiquetas anotadas automaticamente, e com um conjunto de papéis semânticos que é uma adaptação dos argumentos numerados de [Palmer et al. 2005]. Por fim, em uma abordagem baseada em regras, [Bick 2007] faz SRL seguindo a Constraint Grammar. \n",
            "\n",
            "3. O Porttinari-base PropBank \n",
            "No PBP, a anotação de papéis semânticos foi baseada em dependências, sendo cada argumento um único token. A anotação foi feita em um arquivo no formato CoNLL-U, que consiste em um texto simples com 10 campos separados por caracteres de tabulação². A anotação foi feita nas colunas 10 (nomeada de MISC) para a atribuição dos argumentos, e 9 (coluna DEPS) para a anotação dos frames. Foram anotados argumentos explícitos e implícitos, como sujeitos omitidos. A Figura 1 apresenta a codificação da frase “Júnior já presidiu a JBS, mas vendeu a sua parte”. Para facilitar a leitura, omitimos os conteúdos das colunas 3 a 6. A coluna 9 indica os frames dos verbos “presidir” e “vender” (respectivamente, presidir.01 e vender.01). A coluna 10 informa os papéis semânticos e seus predicadores. Por exemplo, o token 1, “Junior”, é Arg0 do token 3 (“presidir”) e Arg0 do token 8 (“vender”), mesmo que esta última informação não esteja explícita na frase. A anotação do PBP utilizou 26 etiquetas. Diferentemente do PropBank original, criamos, no PBP, etiquetas que especificam alguns casos da classe mais geral ArgM-adv: \n",
            "\n",
            "²https://universaldependencies.org/format.html \n",
            "\n",
            "Figura 1. Anotação de papéis semânticos nas colunas 9 e 10 do CoNLL-U\n",
            "ArgM-conseq, para indicar consequência; ArgM-cond, para indicar condições e ArgM-comp, para indicar comparações. A divergência com relação à lista de papéis do PropBank original está na classe ArgM-src (source, fonte da informação), para ocorrências como \"De acordo com a polícia, trata-se de uma ‘prisão significativa’ para as investigações.\" No PropBank original, este tipo de construção não deve ser anotado, mas as anotamos pela relevância argumentativa/rétórica. Em consonância com as versões do PropBank-BR, utilizamos etiquetas específicas para verbos auxiliares (de tempo, modo, aspecto e voz) para o pronome -se. O material foi anotado com base no Manual de anotação do PropBank-BR v2 [Duran 2014] e nos frames verbais elencados no recurso Verbo-Brasil. Ao longo do projeto, as diretivas de anotação foram enriquecidas e atualizadas, dando origem a [Duran and Freitas 2024]. Com relação a seus antecessores brasileiros, a anotação PBP difere quanto à independência da camada sintática no que se refere à identificação/segmentação de argumentos (se a segmentação da análise sintática e a segmentação de argumentos indicada no Verbo-Brasil divergirem, seguimos o Verbo-Brasil). É interessante destacar as interferências da sintaxe UD na tarefa de SRL. No PBP, as divergências entre anotações sintática e semântica foram motivadas pela impossibilidade, em UD, de cruzar arcos sintáticos (exemplo 1), e em frases com verbos auxiliares, uma vez que a sintaxe UD é bastante econômica quanto ao que deve ser considerado verbo auxiliar. Na anotação UD do Porttinari, estão anotados como auxiliares apenas auxiliares de tempo e voz. Na atribuição de papéis semânticos, porém, esta economia tem como resultado a (falsa) necessidade de atribuir papéis a elementos que, em português, não estão atuando como verbos plenos (“possam” no exemplo 2), e que portanto não deveriam receber papéis semânticos – e o resultado é uma construção sem sentido. \n",
            "\n",
            "1. O defeito, que a Takata demorou a reconhecer, foi revelado em...\n",
            "(a) Codificação UD: Takata demorou o defeito \n",
            "(b) Codificação PBP: Takata reconheceu o defeito \n",
            "\n",
            "2. O projeto prevê que as deduções só possam ocorrer a partir de 2021  \n",
            "(a) Codificação UD: deduções possam; prevê possam; possam ocorrer \n",
            "(b) Codificação PBP: ocorrer deduções; prevê ocorrer \n",
            "\n",
            "O fato de verbos de ligação serem considerados AUX, com relações de dependência “especiais” (os argumentos sintáticos – sujeito e predicativo – ficam dissociados do verbo “ser”, e o núcleo do sintagma é o elemento nominal predicativo) também levou à divergência de anotações, já que o verbo “ser” tem papéis semânticos para as posições de sujeito e de predicativo do sujeito. \n",
            "\n",
            "4. Metodologia \n",
            "Em termos gerais, a anotação de papéis semânticos no PBP seguiu as seguintes etapas, algumas delas concomitantes: \n",
            "\n",
            "1. Identificação do predicador, que em nosso caso foram apenas os verbos;\n",
            "2. Consulta ao Verbo-Brasil para seleção do frame adequado;\n",
            "3. Anotação dos argumentos numerados conforme descritos no Verbo-Brasil;\n",
            "4. Anotação dos argumentos modificadores conforme descritos nas diretivas;\n",
            "5. Caso necessário, criação de frames provisórios para verbos novos ou sentidos novos de formas verbais já presentes no Verbo-Brasil;\n",
            "6. Aplicação de regras de validação para detectar problemas na anotação. \n",
            "\n",
            "Todo o processo de anotação foi feito com base em regras linguisticamente motivadas e de maneira não-linear [Wallis 2003], seguindo o exemplo de [Freitas et al. 2023]. Nisto, diferimos do processo de anotação do PropBank original, no qual cada frase era anotada inteiramente de uma vez, e do PropBank-BR versões 1 e 2, que seguiu a mesma metodologia. A anotação foi feita com a ferramenta ET [de Souza and Freitas 2021], utilizando o ambiente Interrogatório. A anotação foi feita em 3 fases: (i) anotação de elementos explícitos, sempre que possível usando regras com padrões léxico-sintáticos derivados dos exemplos de Duran (2014) e das frases-exemplo no Verbo-Brasil; (ii) anotação de elementos implícitos (que envolveu sobretudo a propagação de sujeitos, feita com regras) e (iii) aplicação final de regras de validação e detecção de inconsistências. Quase todo o processo foi semi-automático, utilizando regras que associam um padrão de busca a uma regra de anotação, sempre com revisão humana. Na propagação de sujeitos omitidos de verbos na forma infinitiva, explicitamos argumentos apenas se estes fossem recuperáveis (frase 1). Em caso de dúvida ou em caso de argumentos não recuperáveis (frase 2), nada foi feito. Em [Freitas 2024] estão detalhados os procedimentos e regras utilizados. \n",
            "\n",
            "1. O presidente centrista optou por garantir pela a primeira vez em anos que... (O presidente garantiu) \n",
            "2. São mecânicas que pressionam a entender que isso tem custo. (não é possível determinar quem entenderá) \n",
            "\n",
            "A anotação de elementos implícitos levou a um outro tipo de desalinhamento entre sintaxe e semântica. Na frase “A Folha pediu [contato com o general Mourão], para que comentasse suas declarações, mas (...)”, o segmento “contato com o general Mourão” é Arg1 do verbo “pedir”, mas apenas “general Mourão” é sujeito/Arg0 de “comentar”. \n",
            "\n",
            "A validação final consistiu na aplicação de 4 regras (Figura 2), que buscavam frases com condições suspeitas. Foram encontrados 101 casos suspeitos, e apenas dois deles (derivados da regra 4) eram falsos positivos. Todos os erros foram corrigidos manualmente. Diferentemente das regras utilizadas no processo de anotação, as regras de validação podem ser aplicadas para a verificação final de outros corpora com anotação de papéis semânticos. As regras de anotação, por sua vez, não foram criadas com o objetivo de serem generalizáveis para outros corpora, mas de criar um material padrão ouro de qualidade e no menor tempo possível. No entanto, a elaboração de um anotador baseado em regras, que aproveite estas regras e o corpus já anotado, é algo bastante possível. \n",
            "\n",
            "Figura 2. Regras para detecção de erros \n",
            "A anotação foi feita por uma única pessoa, por 7 meses, a partir das informações contidas no Manual de anotação do PropBank v2 e no Verbo-Brasil. A fim de avaliar a qualidade da anotação, foi feita uma concordância inter-anotadores a posteriori, tomando como base uma amostra com as 100 frases com a maior quantidade de argumentos anotados no Propbank-BR v2, sobre o qual se baseiam as diretivas de anotação e o Verbo-Brasil. É importante notar também que, embora tenhamos escolhido as 100 frases com a maior quantidade de argumentos anotados, nem todos os verbos do PropBank-BR v2 têm seus argumentos anotados, apenas aqueles mais frequentes no corpus. A comparação, medida com o índice kappa, foi sobre 443 tokens anotados por ambas as anotações, e resultou em uma convergência de .90 (como comparação, no Propbank original, e considerando apenas a classificação de papéis incluindo Arg-M, o kappa foi de .93). \n",
            "\n",
            "O processo de anotação dos frames dos verbos foi concomitante à anotação dos papéis semânticos. Uma vez que o foco da anotação PBP esteve na atribuição dos papéis semânticos, o processo de atribuição de sentidos aos verbos não foi exaustivo. Além de não exaustiva, a anotação privilegiou o alinhamento com verbos não monossêmicos, uma vez que a anotação de verbos monossêmicos poderia ser feita (e foi) de forma automática. \n",
            "\n",
            "Apesar de já dispor de um recurso como o Verbo-Brasil, um corpus novo sempre traz novas formas verbais e novos sentidos para verbos já descritos. Para os sentidos (ainda) sem frames, foi feita uma busca por um verbo similar no Verbo-Brasil, e a solução foi indicada em um documento para posterior aprimoramento do Verbo-Brasil. Ao final do processo, foram documentados cerca de 350 sentidos de verbos sem frames, com exemplos do corpus e soluções provisórias em boa parte deles. A anotação de frames monossêmicos foi feita de maneira automática para os casos de verbos monossêmicos (ou seja, que só dispunham de um frame). Este procedimento levou à inclusão de mais de 500 frames no PBP. \n",
            "\n",
            "A relação estreita entre anotação sintática e anotação de papéis semânticos, por um lado, e as interferências da anotação sintática UD, por outro, levaram à criação de diferentes versões do corpus, e com isso também criamos condições para investigar o papel de diferentes representações linguísticas no aprendizado de SRL. Cada uma das versões é gerada automaticamente a partir de uma versão base. \n",
            "\n",
            "1. PBP na versão UD: Esta versão se caracteriza pela atribuição de papéis semânticos apenas aos elementos considerados verbos plenos na UD. Em consequência: (i) não foram anotados os papéis de argumentos do verbo “ser”; (ii) foram anotados os papéis de argumentos de verbos considerados plenos pela UD, mas considerados auxiliares no Verbo-Brasil. No entanto, para diferenciá-los dos demais argumentos, receberam a etiqueta Arg1 d e Arg0 d. Se desejável, ambas as etiquetas podem ser substituídas por Arg1 e Arg0, respectivamente. \n",
            "\n",
            "2. PBP na versão clássica: Esta versão prioriza o conceito de proposição, e se caracteriza pela atribuição de papéis conforme o modelo PropBank, independentemente do que foi considerado verbo pleno pela UD. Em consequência: (i) foram anotados os papéis de argumentos do verbo “ser”; (ii) não foram anotados os papéis de argumentos de verbos que, apesar de considerados plenos na UD, são considerados auxiliares no Verbo-Brasil e na documentação [Duran and Freitas 2024]; e iii) foram anotados como modificadores auxiliares os verbos que, apesar de considerados plenos pela anotação UD, são considerados auxiliares no PBP (ArgM-mod, ArgM-asp), além daqueles considerados sempre auxiliares (ArgM-tml; ArgM-pas), seguindo a decisão do PropBank-BR v2 [Duran 2014]. \n",
            "\n",
            "5. Resultados e conclusões \n",
            "Foram anotados 45.813 argumentos verbais e 13.395 instâncias verbais contêm anotação de frames, distribuídos em quase 1.018 frames distintos (60,8% dos verbos possuem anotação de frame). As versões anteriores do PropBank-BR continham cerca de 7 mil argumentos anotados. A Figura 3 apresenta a distribuição dos argumentos por função sintática e a Figura 4 traz, com mais detalhes, os papéis semânticos mais frequentes para cada relação sintática (deprel). Todos os números se referem à versão clássica, alinhada à versão 1.0 do treebank Porttinari-base. \n",
            "\n",
            "Vemos que a associação mais frequente é entre obj e Arg1, com 92,03%, seguida da associação entre nsubj e Arg0, com 63,49%. O artigo de [Palmer et al. 2005] traz o mesmo tipo de análise para o PropBank original. No entanto, uma vez que cada gramática recorta e define os elementos que lhes parecem relevantes, é necessário algum cuidado nessa comparação. O elemento sentencial/oracional S, presente no PropBank original (derivado da anotação do Penn Treebank), não existe em UD, e está distribuído entre alguns casos de xcomp e de ccomp, por exemplo. \n",
            "\n",
            "De forma geral, analisando os dados do PBP, extraímos as seguintes informações: (a) Arg1 se distribui principalmente entre as relações obj, nsubj, obl, xcomp e ccomp; (b) Arg0 se concentra em nsubj; (c) Arg2 se distribui principalmente entre obl e xcomp; (d) ArgM-tmp e ArgM-mnr se distribuem entre obl, advmod e advcl; (e) ArgM-loc se concentra em obl. Com a perspectiva da sintaxe: (a) obj participa de Arg1 e parece ser a generalização mais fácil: “se é um obj, então é Arg1”; (b) xcomp participa igualmente de Arg1 e Arg2; (c) obl participa de Arg2, Arg1, tempo, modo e local; (d) advmod participa de neg, adv, tempo, dis e modo; (e) advcl participa de tempo, finalidade, modo e Arg2. \n",
            "\n",
            "Os dados apontam para uma regularidade entre funções sintáticas e papéis numerados – especificamente, papéis Arg0 e Arg1. Os demais argumentos numerados, com baixa frequência, são de mais difícil generalização. Estas constatações convergem com resultados para o inglês, que verificam que a anotação ao estilo PropBank captura melhor regularidades sintáticas, sobretudo para argumentos de frequência alta, em oposição ao estilo VerbNet de anotação de papéis semânticos [Merlo and Van Der Plas 2009]. Lidos de outra forma, embora o objetivo de um PropBank seja muitas vezes servir como material de treino para IA, a forma de codificar os papéis talvez seja mais indicada (ou também seja indicada) para um anotador baseado em regras e que considere sintaxe. De fato, [Palmer et al. 2005] relatam que um anotador simples baseado em regras tem desempenho de 83%, sendo 85% o estado-da-arte em inglês, considerando cenários difíceis de avaliação, com verbos não vistos na fase de treino [Wang et al. 2022]. \n",
            "\n",
            "Apesar de raros, estudos sobre a capacidade de generalização da anotação ao estilo PropBank têm mostrado que, quando comparada à anotação ao estilo VerbNet, a anotação PropBank leva a resultados inferiores no que se refere a argumentos Arg2 a Arg5 e, em termos gerais, os bons resultados obtidos com a anotação PropBank se referem aos argumentos mais frequentes [Merlo and Van Der Plas 2009, Yi et al. 2007, Gung and Palmer 2021, Li et al. 2023]. No entanto, todos os estudos foram feitos para a língua inglesa, e apenas a disponibilização de recursos similares para o português nos permitiria verificar os resultados para a nossa língua. \n",
            "\n",
            "Por fim, a criação de duas versões (UD e Clássica) também permite comparar a anotação PBP com outros PropBanks que tenham seguido mais fielmente a anotação UD. As diferentes versões do corpus, bem como documentação linguística detalhada e regras de anotação utilizadas, estão públicas no portal web do projeto POeTiSA³.\n",
            "16\n",
            "Resumo. Em um contexto onde o sistema judiciário brasileiro, o maior do mundo, enfrenta uma crise devido ao lento processamento de milhões de casos, torna-se imperativo desenvolver métodos eficientes para analisar textos legais. Introduzimos o uBERT, um modelo híbrido que combina arquiteturas de Redes Neurais Transformers e Recorrentes para lidar de forma eficaz com textos legais longos. Nossa abordagem processa o texto completo, independentemente de seu comprimento, enquanto mantém um custo computacional razoável. Nossos experimentos demonstram que o uBERT alcança desempenho superior em comparação ao BERT+LSTM quando a entrada sobreposta é utilizada e é significativamente mais rápido que o ULMFiT para processar documentos legais longos.\n",
            "\n",
            "1. Introdução \n",
            "\n",
            "O NLP jurídico pode ser definido como a aplicação de técnicas de Processamento de Linguagem Natural (NLP) no domínio legal. Este subcampo do NLP tem experimentado um rápido crescimento de interesse tanto na academia quanto na indústria: [Katz et al. 2023] reporta um aumento significativo no volume de publicações, subindo de menos de 30 artigos em 2013 para quase 120 em 2022. O Brasil possui o maior sistema judiciário do mundo, com 18.000 juízes distribuídos em 91 tribunais. No momento da escrita, há mais de 84 milhões de casos legais em andamento [CNJ 2024]. Esses números indicam tanto a necessidade quanto a oportunidade de soluções inovadoras para gerenciar e analisar vastas quantidades de dados legais.\n",
            "\n",
            "Voltamos nosso foco para a Previsão de Julgamento Legal (LJP), que envolve a previsão de decisões judiciais. Embora prever decisões possa ser uma tarefa complexa, argumentamos que pode ser reduzida a uma tarefa de Classificação de Texto, que viu um aumento acentuado em estudos [Li et al. 2022], alimentados por avanços em aprendizado profundo. Em particular, a arquitetura Transformer surgiu como uma mudança de paradigma [Hasan 2022] para muitas tarefas de NLP. No entanto, ainda possui limitações ao lidar com textos longos, o que representa desafios significativos no domínio legal, onde os documentos costumam ser longos e complexos.\n",
            "\n",
            "Há pesquisas frutíferas sendo feitas para melhorar a limitação de tamanho de entrada para Transformers, como Modelos de Linguagem Aumentados por Recuperação (RALMs) [Guu et al. 2020]. No entanto, as técnicas de recuperação atuais frequentemente confiam em modelos de embedding que também podem ser sub-ótimos ao lidar com documentos legais, onde uma única palavra no documento inteiro pode fazer a diferença. Além disso, esses métodos exigem recursos computacionais substanciais e grandes repositórios de documentos para alcançar um bom desempenho.\n",
            "\n",
            "Outros métodos combinam a entrada de maneira sequencial, muitas vezes aproveitando propriedades de Redes Neurais Recorrentes para processar sequências mais longas [Wan et al. 2019], embora esses também costumem truncar o texto se for muito longo. Mas para documentos no domínio legal, como decisões judiciais, a maioria dos documentos é geralmente composta pelo raciocínio do juiz. Portanto, é do nosso interesse ter um método que use o texto completo como entrada.\n",
            "\n",
            "Neste artigo, propomos o uBERT, um modelo híbrido que combina um Transformer baseado em encoder com uma Rede Neural Recorrente, capaz de processar textos longos. Propomos um arranjo experimental com dados de decisões legais e comparamos o uBERT com as linhas de base BERT+LSTM, Big Bird e ULMFiT na tarefa de classificação. Nossos resultados mostram que o uBERT supera ligeiramente o BERT+LSTM desde que a entrada sobreposta seja introduzida. Além disso, o ULMFiT apresenta melhor desempenho para textos longos, mas é 4x mais lento que o uBERT.\n",
            "\n",
            "O restante deste artigo está estruturado da seguinte forma: a Seção 2 revisa trabalhos relacionados ao NLP jurídico e à classificação de textos longos; a Seção 3 descreve nossa proposta, incluindo a formalização da tarefa-alvo e a introdução de nosso modelo; a Seção 4 descreve os experimentos que preparamos para avaliar nosso modelo em termos de desempenho e eficiência. Finalmente, apresentamos os resultados e concluímos com uma discussão das descobertas.\n",
            "\n",
            "2. Trabalhos Relacionados \n",
            "\n",
            "Abordagens Baseadas em Transformers para Processamento de Textos Longos em NLP Jurídico: Longformer [Beltagy et al. 2020] utiliza um mecanismo de atenção esparsa, estendendo o limite de tamanho de entrada para 4096 tokens, que é oito vezes o limite do BERT [Devlin et al. 2019]). [Hoang et al. 2023] aplicou essa arquitetura para classificar textos legais do Corpus de Documentos Legais Indianos – ILDC [Malik et al. 2021], mas não processaram o texto completo.\n",
            "\n",
            "[Pappagari et al. 2019] introduziram o RoBERT, um método que divide textos longos em partes sobrepostas para codificação recorrente. Embora semelhante em conceito à nossa arquitetura, uma comparação direta não é possível devido a detalhes limitados sobre suas estratégias de sobreposição e recorrência. Além disso, o RoBERT foi avaliado em textos mais curtos em comparação ao nosso conjunto de dados.\n",
            "\n",
            "O algoritmo de sobreposição em nossa abordagem, uBERT, é um caso específico do método do Sliding-BERT [Zhang et al. 2023], com o stride definido para metade da sobreposição. Diferentemente do SlidingBERT, onde tokens podem aparecer em vários pedaços, limitamos as sobreposições a dois pedaços para reduzir o custo computacional enquanto preservamos a continuidade do contexto. Essa escolha é impulsionada pela eficiência, não por diferenças de linguagem.\n",
            "\n",
            "[Menezes-Neto e Clementino 2022] introduziram o BrCAD-51, um conjunto de dados projetado para Previsão de Julgamento Legal (LJP), e avaliaram três arquiteturas para essa tarefa: ULMFiT [Howard e Ruder 2018], BigBird [Zaheer et al. 2020], e BERT+LSTM. O ULMFiT, um modelo de aprendizado de transferência que ajusta um modelo de linguagem pré-treinado para tarefas de NLP a jusante, foi a única arquitetura capaz de processar o texto completo como entrada. O BigBird, um modelo de atenção esparsa, aborda o limite de 512 tokens focando em subconjuntos de tokens, reduzindo assim a complexidade computacional, e foi configurado para lidar com textos de até 7.680 tokens. Para o BERT+LSTM, os documentos foram divididos em partes de 512 tokens, com truncamento aplicado a partes intermediárias se um documento exigisse mais de 15. Embora semelhante em abordagem, uBERT difere do BERT+LSTM na medida em que utiliza uma estratégia de sobreposição de partes e não impõe um limite no número de partes, garantindo que o texto completo seja utilizado sem truncamento.\n",
            "\n",
            "Críticas e Limitações na Pesquisa de NLP Jurídico: A indústria legal foi lenta para adotar os avanços em NLP, confiando fortemente no trabalho manual dos advogados. [Mahari et al. 2023] identificam uma questão-chave: a pesquisa em NLP jurídico frequentemente não se alinha com as necessidades práticas dos profissionais do direito. [Medvedeva e Mcbride 2023] destacam ainda uma lacuna significativa na pesquisa de Previsão de Julgamento Legal (LJP), criticando o uso de conjuntos de dados mal projetados que dependem de fatos de casos tendenciosos extraídos de julgamentos. Essa abordagem leva a modelos com desempenho excessivamente otimista que oferecem valor prático limitado aos profissionais do direito.\n",
            "\n",
            "Este trabalho visa preencher a lacuna entre pesquisa e prática no campo do NLP jurídico. Propomos uma arquitetura capaz de processar textos legais de comprimento virtualmente infinito e a avaliamos no conjunto de dados BrCAD-5, que [Medvedeva e Mcbride 2023] consideram um benchmark bem projetado.\n",
            "\n",
            "3. Proposta \n",
            "\n",
            "A classificação de texto pode ser formalizada da seguinte forma. Dado um documento d que representa uma decisão judicial, o objetivo é fazer uma previsão y ∈ {0, 1}, aprendendo um classificador binário f tal que f(d) = y. A classe positiva y = 1 representa uma decisão que será revertida por um Painel de Apelação (AP). Como os documentos legais costumam ser longos, ao usar modelos baseados em Transformers, abordagens convencionais normalmente truncam o texto de d, o que é subótimo para a tarefa [Pappagari et al. 2019]. Isso pode prejudicar o desempenho na tarefa de Previsão de Julgamento Legal, já que alguma parte relevante do texto pode ser cortada.\n",
            "\n",
            "Acreditando que o texto como um todo é mais útil ao aprender um classificador, propomos o unlimited BERT, ou uBERT, uma arquitetura eficiente que combina um Transformer baseado em encoder com uma Rede Neural Recorrente, utilizando um algoritmo de sobreposição durante o treinamento e a inferência para lidar com um número ilimitado de tokens de entrada. Essa abordagem é semelhante ao modelo BERT+LSTM utilizado por [Menezes-Neto e Clementino 2022], mas introduz modificações para manter o contexto local (por meio de partes sobrepostas) e acomodar documentos de praticamente qualquer tamanho. Embora a complexidade de memória quadrática do mecanismo de autoatenção apresente um desafio para escalar a entrada indefinidamente, aproveitamos a capacidade da RNN de processar sequências longas, permitindo que ela pegue embeddings de partes e produza um embedding de documento abrangente. Vários estudos, como [Hoang et al. 2023], exploraram a combinação de mecanismos de atenção e recorrência. Nosso modelo se baseia nesse conceito, mas aplica sobreposições durante o treinamento e a inferência, e não limita o número de partes processadas pelo encoder.\n",
            "\n",
            "A Figura 1 retrata a arquitetura do uBERT. Ela mostra aspectos-chave para entender como nosso modelo funciona.\n",
            "\n",
            "Seja E um Transformer baseado em encoder, com dim sendo a dimensionalidade do vetor de saída da última camada, e R uma Rede Neural Recorrente. Seja maxtok o número máximo de tokens que E pode processar como entrada. Seja maxc o número máximo de partes de maxtok tokens que E pode processar em paralelo com uma única execução. Dividimos o documento d em n partes de tamanho maxtok tokens, começando no primeiro token. Para cada execução, extraímos os estados ocultos das quatro últimas camadas de E, e os concatenamos para formar a representação de cada parte. Isso se baseia na ideia de que diferentes camadas capturam diferentes características linguísticas [Tenney et al. 2019]. Especificamente, o BERT+LSTM, a linha de base mais similar à nossa arquitetura proposta, extrai os estados ocultos das quatro últimas camadas. Embora outras camadas pudessem ser utilizadas para a extração, mantivemos essa abordagem para consistência na comparação de modelos. Em cada execução única, processamos [1, maxc] partes em paralelo, gerando [1, maxc] vetores de embeddings, cada um com dimensionalidade 4 × dim. Processamos iterativamente partes de d até que um vetor de embedding seja gerado para cada parte, assim preservando todo o conteúdo de texto de d.\n",
            "\n",
            "Em seguida, concatenamos os vetores de embedding mantendo a ordem das respectivas partes, gerando um tensor de dimensionalidade (n, 4 × dim). Processamos esse tensor com a RNN sequencialmente, capturando as dependências entre eles e gerando uma representação contextualizada para cada parte.\n",
            "\n",
            "Dividir o texto pelo número de tokens pode interromper seu fluxo, por isso usamos sobreposição de tokens entre partes durante o treinamento e a inferência para manter a continuidade. Essa técnica, semelhante à utilizada por [Hoang et al. 2023], mas aplicada de maneira mais ampla, ajuda a preservar a estrutura natural do texto.\n",
            "\n",
            "Nosso algoritmo de sobreposição de tokens pode ser formalizado da seguinte forma. Considere a decisão judicial d como a sequência tokenizada S = {t1, . . . , tk}, onde k é o número de tokens em d. Definimos o tamanho da sobreposição, z, como o número de tokens que cada parte compartilha com seus vizinhos adjacentes. Assim, qualquer parte compartilha (cid:4) z com a seguinte. As primeiras e últimas partes, tendo apenas uma parte adjacente, compartilham (cid:4) z 2 (cid:5) tokens com seus respectivos vizinhos.\n",
            "\n",
            "A Figura 2 fornece um exemplo simples para esclarecimento. Neste exemplo, a parte Tokens texto bruto Partes sobrepostas batch_1 batch_n EBERT_output_1 BERT_output_n Chunk embeddings Rf tamanho é 4 e z = 2. Como mostrado, a parte c2 = {t4, t5, t6, t7} compartilha o token t4 com a parte c1 e o token t7 com a parte c3. Note que as primeiras e últimas partes compartilham apenas um token com a parte vizinha. \n",
            "\n",
            "4. Experimentos \n",
            "\n",
            "Nesta seção, projetamos experimentos para avaliar nosso modelo proposto, uBERT, e validar sua eficácia no domínio legal. Dividimos nossos experimentos em 3, um para cada uma das seguintes perguntas de pesquisa: \n",
            "\n",
            "RQ1: Um modelo baseado em encoder se beneficiaria do uso do texto inteiro em termos de melhoria de desempenho? \n",
            "\n",
            "Examinamos o impacto de processar os documentos inteiros usando várias passagens de encoder. Primeiro, testamos se simplesmente aumentar os pedaços de texto para processar o texto inteiro sem usar sobreposição (uBERT 0) melhora o desempenho sobre o BERT+LSTM, que processa apenas texto parcial em uma única passagem. Então, investigamos o efeito da introdução de sobreposições (de 0 a 510 tokens2) entre partes para observar se o contexto local adicionado melhora as previsões.\n",
            "\n",
            "RQ2: Se o desempenho melhora, vem com um custo computacional razoável? \n",
            "\n",
            "Comparamos o tempo de inferência de nossa arquitetura com todos os modelos de base para determinar se oferece um ganho de desempenho e para avaliar o custo computacional associado.\n",
            "\n",
            "RQ3: Nossa arquitetura é melhor para processar textos mais longos? \n",
            "\n",
            "Exploramos a relação entre o comprimento do documento e o desempenho do modelo. Testamos os modelos no conjunto de teste completo, bem como em seus subconjuntos, os 10% e os 1% de textos mais longos. Esse experimento envolveu análise estatística para determinar se textos mais longos levam a previsões melhores ou piores.\n",
            "\n",
            "Dados: Utilizamos o conjunto de dados BrCAD-5 em nossos experimentos. A tarefa é uma classificação binária, com a Classe 1 indicando que o AP reverte a decisão anterior, e a Classe 0 indicando que confirma. O conjunto de dados é desequilibrado, com 22% dos pontos de dados pertencentes à Classe 1. Embora essa proporção de desequilíbrio seja consistente em todas as divisões do conjunto de dados, varia significativamente com o comprimento do texto.\n",
            "\n",
            "Modelos: Neste trabalho, nosso modelo (uBERT) utiliza o BERT como encoder e LSTM como RNN, com maxtok definido para 512 tokens e até 15 pedaços (maxc) processados em paralelo. Nosso procedimento de treinamento segue a abordagem de [Menezes-Neto e Clementino 2022], onde ajustamos a última camada do BERT e do LSTM. O ajuste é realizado por 1 época utilizando o scheduler de taxa de aprendizado One Cycle. Nosso procedimento de inferência reflete o processo de treinamento.\n",
            "\n",
            "Linhas de base: nossos modelos de linha de base são ULMFiT (forward, backward e bidirectional), Big Bird e BERT+LSTM. Notavelmente, apenas o ULMFiT e o uBERT processam o texto completo.\n",
            "\n",
            "Infraestrutura e Recursos Computacionais: os experimentos foram conduzidos utilizando a infraestrutura do Google Colab, especificamente uma GPU NVIDIA A100 com 40 GB de RAM.\n",
            "\n",
            "Métricas de Avaliação: Avaliamos todos os modelos utilizando a pontuação Macro F1 e o Coeficiente de Correlação de Matthews (MCC). A pontuação Macro F1 é uma métrica bem estabelecida em campos de NLP, representando a média harmônica de precisão e recall, enquanto o MCC, embora menos comum, é frequentemente utilizado no subcampo da Previsão de Julgamento Legal (LJP), como observado por [Cui et al. 2022]. O MCC mede a correlação entre as classificações previstas e reais, levando em conta verdadeiros positivos, verdadeiros negativos, falsos positivos e falsos negativos, tornando-se adequado para classes desequilibradas. Além disso, o MCC é a métrica utilizada por [Menezes-Neto e Clementino 2022], tornando necessário para nós também utilizá-la para comparação de modelos. Para comparar diferentes linhas de base e configurações do nosso modelo uBERT, empregamos reamostragem bootstrap para obter intervalos de confiança de 95%, seguidos por análise post-hoc de Wilcoxon-Holm para avaliar a significância estatística com α = 5%, seguindo abordagens semelhantes [Demšar 2006, Zhu et al. 2020, Ferraz et al. 2021].\n",
            "\n",
            "5. Resultados \n",
            "\n",
            "A Tabela 1 apresenta os resultados para todas as configurações de modelo no conjunto de teste completo, bem como nos textos 10% e 1% mais longos. Os modelos de base não foram executados no conjunto de teste completo neste estudo devido a limitações de recursos computacionais. Os resultados aqui reportados são reproduzidos de [Menezes-Neto e Clementino 2022], razão pela qual a Tabela 1 não inclui tempos de inferência para o conjunto de teste completo. A Figura 3 exibe as pontuações macro F1 em diferentes comprimentos de texto, enquanto a Figura 4 classifica os modelos usando a métrica MCC. Embora o MCC seja eficaz para comparações dentro do conjunto de dados, é menos adequado para conjuntos de dados com desequilíbrio de classes diferentes; portanto, confiamos na pontuação macro F1 para comparações entre conjuntos de dados.\n",
            "\n",
            "Tabela 1: desempenho do uBERT em vários tamanhos de sobreposição em comparação com linhas de base. \n",
            "Conjunto de Dados: Conjunto de Teste Completo (76.299 documentos) Proporção de Desequilíbrio = 0,28 \n",
            "Conjunto 10% (7.632 documentos) Proporção de Desequilíbrio = 0,32 \n",
            "Conjunto 1% (763 documentos) Proporção de Desequilíbrio = 0,54 \n",
            "Arquitetura Macro-F1↑ MCC↑ Macro-F1↑ MCC↑ Tempo. Inf.↓ Macro-F1↑ MCC↑ Tempo. Inf.↓ \n",
            "Linhas de base ULMFiT - fwd ULMFiT - bwd ULMFiT - bidir Big Bird BERT+LSTM Nosso uBERT 0 uBERT 150 uBERT 205 uBERT 300 uBERT 408 uBERT 510 \n",
            "65,1 % 65,7 % 66,9 % 52,0 % 64,1 % 63,9 % 63,3 % 64,7 % 64,7 % 64,0 % 64,6 % \n",
            "0,32 0,35 0,37 0,27 0,33 0,33 0,32 0,35 0,35 0,33 0,35 \n",
            "64,9 % 63,4 % 64,8 % 44,0 % 63,2 % 62,6 % 62,2 % 62,6 % 63,0 % 63,2 % 63,0 % \n",
            "0,32 0,35 0,34 0,23 0,31 0,31 0,30 0,31 0,31 0,32 0,31 \n",
            "1h 18min 1h 18min 1h 18min 22min 12min 13min 14min 15min 17min 19min 21min \n",
            "72,3 % 59,9 % 69,3 % 30,0 % 64,0 % 61,3 % 59,4 % 62,0 % 63,0 % 64,3 % 64,2 % \n",
            "0,47 0,33 0,43 0,08 0,36 0,34 0,32 0,35 0,36 0,38 0,38 \n",
            "11min:22s 14min:44s 14min:44s 2min:58s 1min:29s 1min:51s 2min:04s 2min:09s 2min:23s 2min:42s 3min:08s \n",
            "\n",
            "O MCC varia de -1 a 1, onde 1 indica previsão perfeita, 0 indica não melhor do que uma chance aleatória, e -1 indica desacordo total entre previsão e observação.\n",
            "\n",
            "Figura 3: Pontuação Macro-F1 x Média de Tokens/Grupo em diferentes grupos do mesmo tamanho classificados pelo comprimento.\n",
            "\n",
            "(a) Todo Texto (b) 10% Texto Mais Longo (c) 1% Texto Mais Longo\n",
            "\n",
            "Figura 4: Diagrama de diferença crítica mostrando comparação estatística par a par entre linhas de base e tamanhos de sobreposição variáveis para o uBERT usando o MCC. Barras conectando representam nenhuma diferença estatística entre os métodos.\n",
            "\n",
            "Processar o Texto Completo Requer Sobreposição \n",
            "Comparando a linha de base BERT+LSTM, que trunca o texto do meio quando excede o tamanho de entrada, com nosso uBERT sem sobreposição (uBERT 0), que usa o texto completo, encontramos que o uBERT ou teve desempenho inferior ou igualou a linha de base em todas as métricas. Notavelmente, teve um desempenho pior nos 1% de textos mais longos, onde a truncagem do meio pelo BERT+LSTM ocorre. Isso sugere que simplesmente processar o texto inteiro é insuficiente para entradas mais longas. Hipotetizamos que partes não sobrepostas introduzem ruído devido à segmentação brusca, o que degrada o desempenho. Nossos resultados apoiam isso, mostrando que a introdução de sobreposição nas configurações do uBERT melhora tanto as pontuações Macro-F1 quanto as de MCC. As seguintes configurações do uBERT superaram significativamente o BERT+LSTM: uBERT 300, uBERT 510 e uBERT 205 no conjunto de teste completo; uBERT 408 e uBERT 300 nos 10% mais longos; e uBERT 408, uBERT 300 e uBERT 510 nos 1% mais longos. Assim, incorporar sobreposição é crucial para manter a consistência semântica e melhorar o desempenho em textos mais longos.\n",
            "\n",
            "uBERT com Sobreposição ainda é Significativamente Mais Rápido que ULMFiT \n",
            "Como esperado, a introdução de sobreposição na arquitetura uBERT aumentou o custo computacional. No entanto, em todo o conjunto de dados completo e nos 10% de textos mais longos, as configurações do uBERT forneceram resultados melhores que a linha de base BERT+LSTM com tempos de inferência comparáveis. Notavelmente, uBERT 408 alcançou uma inferência 4x mais rápida que o ULMFiT nos 10% de textos mais longos. Para os 1% de textos mais longos, o aumento do comprimento exigiu duas passagens do uBERT 408, resultando em uma inferência 1.8x mais lenta em comparação com o BERT+LSTM, que precisou truncar o meio em todos os casos. Apesar disso, o uBERT 408 superou ligeiramente o BERT+LSTM, reduzindo a diferença de desempenho em relação ao ULMFiT enquanto mantinha uma inferência mais rápida, destacando a eficiência e eficácia de nossa abordagem. Em resumo, em todos os subconjuntos, as configurações do uBERT reduziram a diferença com o BERT+LSTM sendo significativamente mais rápidas que o ULMFiT.\n",
            "\n",
            "ULMFiT Supera uBERT em Textos Mais Longos \n",
            "Como mostrado na Figura 3, as diferenças de modelo tornam-se mais evidentes com o aumento do comprimento do texto. O Big Bird consistentemente apresenta desempenho inferior em textos mais longos, razão pela qual foi excluído dos gráficos de comparação. Embora algumas configurações do uBERT superem o BERT+LSTM em textos mais longos, as pontuações F1 em ambos os modelos se degradam em comparação ao desempenho no conjunto de teste completo. Em contraste, os modelos ULMFiT melhoram em textos mais longos em comparação ao conjunto de dados completo. Isso sugere que nossa arquitetura mitiga a degradação para textos mais longos que é inerente à abordagem BERT+LSTM, mas ainda fica aquém dos modelos ULMFiT, que lidam melhor com textos mais longos, mas a um custo de tempo de inferência 4x mais lento.\n",
            "\n",
            "6. Conclusão e Trabalhos Futuros \n",
            "\n",
            "Nossos experimentos demonstram que o modelo uBERT melhora o manuseio de textos legais em comparação com modelos baseados em encoder, particularmente em textos mais longos, devido à sua capacidade de processar documentos inteiros utilizando pedaços sobrepostos. Apesar do aumento do custo computacional, uBERT continua sendo mais rápido que o ULMFiT. O uBERT supera ligeiramente o BERT+LSTM, mas ainda fica aquém do ULMFiT. Assim, um refinamento adicional é necessário para corresponder totalmente ao desempenho do ULMFiT. Notavelmente, mesmo o ULMFiT, o modelo de melhor desempenho em nossos experimentos, obtém pontuações Macro-F1 relativamente baixas, sugerindo que o processamento do texto completo por si só é insuficiente para um alto desempenho nesta tarefa. Nessa direção, futuras pesquisas devem expandir a metodologia de avaliação ao analisar casos classificados corretamente e incorretamente em todos os modelos testados para avaliar se determinadas características das decisões judiciais as tornam mais propensas a erros de classificação por certos modelos. Tal análise, no entanto, exige uma abordagem multidisciplinar, incluindo a contribuição de profissionais jurídicos altamente habilitados.\n",
            "\n",
            "Futuras pesquisas também devem explorar diferentes estratégias de segmentação para aprimorar o processamento de texto. Comparar a segmentação sintática, que é baseada na estrutura gramatical, com a segmentação semântica, que é baseada no significado do conteúdo, pode fornecer insights valiosos. Como este estudo se concentra em um conjunto de dados em língua portuguesa, avaliar essas abordagens de segmentação em conjuntos de dados em vários idiomas ajudaria a determinar se estratégias de segmentação ótimas variam com a língua, contribuindo para uma segmentação de textos longos mais robusta e desempenho de modelo em diversos contextos linguísticos.\n",
            "\n",
            "Agradecimentos \n",
            "\n",
            "Este trabalho foi apoiado pela CAPES (Código de Financiamento 001), CNPQ (bolsa 312360/23-1), Programa Unificado de Bolsas de Estudo para Apoio à Formação de Estudantes (PUB-USP), Centro de Inteligência Artificial USP-IBM-FAPESP (bolsa FAPESP 2019/07665-4) e Secretaria da Fazenda do Estado do Rio Grande do Sul (SEFAZ-RS), Brasil.\n",
            "17\n",
            "Resumo. A sumarização automática de texto tem como objetivo a criação de um resumo com as informações mais relevantes extraídas de um ou mais documentos textuais. Apesar dos avanços obtidos na área, pesquisas envolvendo documentos escritos em português do Brasil ainda são escassas. Este artigo apresenta uma análise envolvendo diferentes abordagens de sumarização, desde baselines clássicas, passando por sistemas extrativos, o ajuste fino de diferentes arquiteturas dos modelos P T T 5 e F LAN -T 5, até o uso de modelos de linguagem de larga escala para sumarização abstrativa. Experimentos foram realizados considerando três bases de dados de artigos de notícias escritos em português. Os resultados demonstraram que os modelos ajustados para a tarefa de sumarização abstrativa obtiveram resultados competitivos com base nas medidas do ROUGE-L e do BERTScore com modelos maiores, como o GPT-4o.\n",
            "\n",
            "1. Introdução\n",
            "A crescente demanda por informações impulsiona o desenvolvimento de tecnologias capazes de processar e sintetizar grandes volumes de dados de forma rápida e eficiente. Em um cenário em que a produção de conteúdo digital online é cada vez mais abundante, torna-se cada vez mais desafiador para os leitores acompanhar todas as notícias relevantes [Levitin 2014, Zhang et al. 2024]. Nesse contexto, sistemas de Sumarização Automática de Texto (SAT) podem ser ferramentas úteis para auxiliar os usuários, oferecendo a capacidade de gerar resumos concisos que capturam as informações mais relevantes de um texto ou de múltiplos documentos relacionados, permitindo uma assimilação mais rápida do conteúdo [Lin and Ng 2019, Zhang et al. 2022]. \n",
            "\n",
            "A SAT é uma área de pesquisa em Processamento de Linguagem Natural (PLN) que busca gerar resumos de documentos textuais de forma automática. Existem duas abordagens principais para a tarefa de SAT: a Extrativa e a Abstrativa [Nenkova and McKeown 2012]. A sumarização extrativa seleciona as frases mais relevantes diretamente do texto original para compor o resumo, enquanto a sumarização abstrativa envolve a reescrita do conteúdo de forma mais condensada e frequentemente utiliza técnicas de geração de linguagem natural, sendo capaz de criar novas frases que não necessariamente aparecem no texto original [Zhang et al. 2022]. A sumarização pode ser aplicada tanto a um único documento (monodocumento) quanto a um conjunto de documentos (multidocumento).\n",
            "\n",
            "Nos últimos anos, houve uma mudança no foco das pesquisas na área de SAT das abordagens extrativas para as abstrativas [Lin and Ng 2019]. Essa mudança foi impulsionada pelo desenvolvimento de algoritmos baseados em redes neurais profundas, especialmente na arquitetura Transformer [Vaswani 2017], capazes de geração de linguagem natural. Assim, diversas abordagens surgiram usando modelos neurais pré-treinados e, mais recentemente, modelos de linguagem de larga escala, do inglês Large Language Models (LLMs) [Zhang et al. 2022]. Contudo, apesar dos resultados promissores usando essas abordagens neurais, elas impõem diversos desafios, como a necessidade de grandes bases de dados para treinamento e demandam muitos recursos computacionais. Apesar dos avanços na área, a maioria das pesquisas tem como foco a língua inglesa, e poucos estudos têm sido dedicados ao português, especialmente para a sumarização abstrativa [Zhang et al. 2022]. Essa lacuna limita a aplicabilidade de sistemas de sumarização automáticos projetados especificamente para o português, que enfrenta a ausência de modelos e bases de dados para suportar essas pesquisas [Paiola et al. 2024].\n",
            "\n",
            "Este artigo busca contribuir para o desenvolvimento da tarefa de SAT em português do Brasil por meio de uma investigação de algoritmos de sumarização aplicados a artigos de notícias. O estudo abrange desde o uso de sistemas extrativos, comumente usados como baselines de comparação, o ajuste fino dos modelos P T T 5 e F LAN -T 5 para sumarização abstrativa, até o uso de LLMs de código aberto e proprietários, como o GPT-4o, Llama 3 [Touvron et al. 2023] e o Gemma [Team et al. 2024]. Para isso, foram realizados experimentos em três bases de dados, o Temário e Recognnasum para a tarefa de sumarização monodocumento e o CSTNews para sumarização multidocumento. O desempenho dos modelos foi avaliado usando as medidas de avaliações automáticas do ROUGE-L e BERTScore, que são usualmente adotadas na literatura. \n",
            "\n",
            "Os códigos desenvolvidos e os resumos gerados neste trabalho estão públicos em https://github.com/laicsiifes/benchmark_ptbr_summ.\n",
            "\n",
            "2. Trabalhos Relacionados\n",
            "A literatura da área de SAT é vasta e existem diversos surveys que fornecem uma visão ampla do desenvolvimento da área desde a sua origem [Nenkova and McKeown 2012, Lin and Ng 2019, Zhang et al. 2022]. Por limitações de espaço, esta seção foca apenas em trabalhos que envolveram documentos escritos em português ou que usaram técnicas adotadas nos experimentos realizados neste estudo. \n",
            "\n",
            "Diversos indicadores de relevância vêm sendo explorados para a execução da tarefa de sumarização extrativa [Leite and Rino 2008, Oliveira et al. 2016a]. Em sua maioria, esses indicadores baseiam-se em técnicas estatísticas, como frequência e centralidade, ou em heurísticas, como a posição das sentenças nos documentos. O estudo conduzido por Oliveira et al. [Oliveira et al. 2016a] avaliou diferentes técnicas para mensurar a relevância de sentenças em tarefas de SAT de artigos jornalísticos em inglês. Os autores analisaram os métodos individualmente e em combinação, utilizando-os como atributos em algoritmos de classificação. Leite e Rino [Leite and Rino 2008] investigaram uma abordagem combinando múltiplas features e algoritmos de aprendizado de máquina para a sumarização extrativa de documentos em português.\n",
            "\n",
            "No trabalho de Sodré e Oliveira [Sodré and de Oliveira 2019], os autores investigaram a estratégia de combinar alguns dos indicadores analisados por Oliveira et al. [Oliveira et al. 2016a] e aplicaram algoritmos de regressão para estimar um escore de relevância das sentenças na tarefa de sumarização de artigos jornalísticos em português. Gomes e Oliveira [Gomes and de Oliveira 2019] propuseram um sistema usando Programação Linear Inteira (PLI) para sumarização extrativa multidocumento. O sistema desenvolvido usa bigramas como conceitos e aplica métodos estatísticos tradicionais para identificar as informações mais relevantes para a construção do resumo. \n",
            "\n",
            "Diferentemente dos trabalhos anteriores, Paiola et al. [Paiola et al. 2022] investigaram a tarefa de sumarização abstrativa. Os autores usaram diversas bases de dados em português (TeMário, CSTNews, WikiLingua e XL-Sum) e um sistema de tradução para aplicar modelos treinados em inglês. Em [Paiola et al. 2024], os autores apresentam a base de dados do RecognaSumm, um conjunto de dados contendo mais de 135 mil artigos de notícias para a tarefa de SAT. Os autores realizaram diferentes análises da base de dados proposta e avaliaram o desempenho do modelo base do P T T 5 para estabelecer um desempenho de referência para comparações futuras. \n",
            "\n",
            "Este trabalho busca expandir os anteriores ao realizar uma análise mais ampla considerando três bases de dados (Temário, RecognaSumm e CSTNews) para sumarização monodocumento e multidocumento, além de envolver desde técnicas de sumarização extrativas tradicionais de ponderação das frases assim como os usados nos trabalhos em [Oliveira et al. 2016a, Sodré and de Oliveira 2019], adaptação do sistema de PLI proposto em [Gomes and de Oliveira 2019], ajuste fino e avaliação de diferentes tamanhos de arquiteturas (small, base e large) dos modelos P T T 5 e F LAN -T 5 e o uso de LLMs de código aberto (Llama3 e Gemma2) e proprietários (GPT-3.5 e GPT-4o).\n",
            "\n",
            "3. Materiais e Métodos\n",
            "3.1. Bases de Dados\n",
            "Neste trabalho, foram utilizadas três bases de dados comumente usadas na literatura para a tarefa de SAT no domínio de artigos de notícias escritas em português.\n",
            "\n",
            "TeMário. Esse conjunto de dados é formado por 100 textos jornalísticos, provenientes da Folha de S.Paulo e do Jornal do Brasil. Os artigos, que abordam uma variedade de temas, foram selecionados por sua linguagem clara e objetiva. Todos os textos possuem resumos elaborados por um especialista, o que garante a qualidade dos resumos de referência [Pardo and Rino 2003].\n",
            "\n",
            "CSTNews. Essa base de dados é formada por 50 conjuntos de notícias, cada um com aproximadamente quatro artigos sobre o mesmo tema, coletados manualmente em sites de notícias como Folha de São Paulo e Estadão. Essa abordagem permitiu a seleção de notícias com linguagem clara e acessível, provenientes de diferentes fontes sobre um mesmo assunto [Cardoso et al. 2011]. \n",
            "\n",
            "RecognaSumm. Com o objetivo de construir um conjunto de dados robusto para estudos de sumarização de textos, Paiola et al. [Paiola et al. 2024] coletaram 135.272 artigos de notícias usando sistemas de web crawlers personalizados. A diversidade temática dos artigos foi garantida pela coleta de dados em diferentes portais de notícias e categorias. A base de dados é dividida em três subconjuntos: treinamento, validação e teste. Por conta de limitações de hardware, foi feita uma filtragem no conjunto de treinamento, sendo removidos os artigos com resumos contendo menos do que 25 palavras. \n",
            "\n",
            "Na Tabela 1 são apresentadas algumas estatísticas das bases de dados usadas nos experimentos. Para cada base, foi computado o total de documentos ou grupos, média e Desvio Padrão (DP) de frases e palavras no texto dos artigos.\n",
            "\n",
            "Tabela 1. Estatísticas das bases de dados usadas nos experimentos. \n",
            "Base de Dados     Conjunto            Docs / Grupos     Média (DP) Frases   Média (DP) Palavras  \n",
            "TeMário             CSTNews    RecognaSumm     Único       Único            Treino      Validação          Teste  \n",
            "100                     50               64.347           21.538         21.493        \n",
            "32,4 (10,38)           47,06 (19,47)   27,07 (24,82)     26,73 (24,15)   27,05 (24,88)     \n",
            "618,67 (163,93)      939,56 (331,42)  527,33 (468,38)  519,91 (458,68)  526,41 (470,02)   \n",
            "\n",
            "3.2. Modelos de Sumarização\n",
            "Os seguintes modelos de sumarização foram investigados:\n",
            "\n",
            "Baselines. Foram utilizadas como baselines oito técnicas de ponderação de frases [Sodré and de Oliveira 2019]. As técnicas utilizadas foram: Bushy Path, Centralidade das Frases, Frequência de Palavras, Frequência de Entidades Nomeadas, Frequência do Termo - Frequência Inversa das Sentenças (TF-ISF), Posição das Frases, Similaridade Agregada, TextRank. Essas técnicas foram usadas em conjunto com uma abordagem clássica de sumarização extrativa composta por três etapas [Nenkova and McKeown 2012]:\n",
            "\n",
            "• Pré-processamento: O documento ou grupo de documentos de entrada é pré-processado usando várias técnicas tradicionais de PLN, como divisão do texto em frases, palavras, lematização, identificação das classes gramaticais e reconhecimento de entidades nomeadas.\n",
            "\n",
            "• Ponderação das frases: Nesta etapa, cada uma das oito técnicas de ponderação de frases é aplicada para analisar cada frase do(s) documento(s) de entrada e gerar um valor que deve refletir sua relevância para ser incluído no resumo. Todos os valores gerados são normalizados no intervalo de 0 a 1.\n",
            "\n",
            "• Geração de resumo: As frases com os maiores valores de relevância geradas na etapa anterior são inseridas iterativamente no resumo até que o tamanho máximo desejado seja atingido. Uma nova frase é inserida no resumo somente se sua similaridade de cosseno com as frases já inseridas for menor que 0,5 [Nenkova and McKeown 2012]. \n",
            "\n",
            "Sistema de PLI. Foi utilizado o sistema de PLI proposto por Gomes e Oliveira [Gomes and de Oliveira 2019] para sumarização multidocumento e uma adaptação de um sistema similar apresentado em [Oliveira et al. 2016b] para a tarefa de sumarização monodocumento.\n",
            "\n",
            "Modelos Pré-treinados. Foram utilizados os modelos P T T 5 [Carmo et al. 2020] e F LAN -T 5 em suas arquiteturas small, base e large, que se diferenciam pelo tamanho da arquitetura. O P T T 5 é uma versão em português do modelo de linguagem T 5, pré-treinado no BrWac, um grande corpus de páginas da web em português brasileiro. O F LAN -T 5 é um modelo multilíngue desenvolvido pela Google que foi treinado para múltiplas tarefas de PLN [Chung et al. 2024].\n",
            "\n",
            "LLMs. Os recentes avanços no progresso de LLMs têm impulsionado o desenvolvimento de diversas aplicações. Neste trabalho, foram utilizados os modelos: Gemma 2 9B [Team et al. 2024], o Llama 3.1 8B [Touvron et al. 2023] e os modelos Text-davinci-003, GPT-3.5 Turbo, GPT-4o e GPT-4o mini desenvolvidos pela empresa OpenAI [OpenAI 2024].\n",
            "\n",
            "3.3. Desenho Experimental\n",
            "A análise de desempenho dos modelos de sumarização foi dividida em dois experimentos. No primeiro experimento, foi utilizada somente a base de dados do RecognaSumm, sendo considerados os sistemas extrativos (baselines e o sistema de PLI) e foram treinados seis modelos de sumarização abstrativos baseados no P T T 5 e F LAN -T 5. O segundo experimento foi realizado usando as bases de dados do Temário e CSTNews. Para esse experimento, foram usados os sistemas extrativos (baselines e o sistema de PLI), os modelos abstrativos baseados no P T T 5 e F LAN -T 5 treinados no primeiro experimento e os modelos de LLMs. Em todas as abordagens avaliadas, foi configurado o tamanho máximo de resumo para 150 palavras. \n",
            "\n",
            "Para os métodos de baselines e o sistema de PLI foram usadas implementações próprias. Os modelos da OpenAI foram acessados usando a API oficial disponibilizada pela empresa. A implementação dos modelos P T T 5, F LAN -T 5 e dos LLMs do Gemma 2 9B e Llama 3.1 8B foi baseada na biblioteca Transformers e foram usados os modelos pré-treinados disponibilizados publicamente pelos autores e empresas na plataforma do Hugging Face. Para o ajuste fino das três arquiteturas dos modelos P T T 5 e F LAN -T 5, o tamanho máximo de entrada foi definido para 512 tokens e o tamanho máximo do resumo a ser gerado foi configurado para 150 tokens. Os modelos foram ajustados por no máximo 20 épocas, sendo utilizada a estratégia de parada antecipada com uma paciência de 5 épocas. Para evitar sobreajuste dos modelos, foi feito um monitoramento do treinamento, no qual, ao final de cada época, o modelo resultante é aplicado no conjunto de validação e é computada a medida do ROUGE-L, sendo armazenado somente o modelo com maior valor. Para a geração dos resumos, foi usado o algoritmo de decodificação do Beam Search com tamanho 5 de largura. \n",
            "\n",
            "Baseado no trabalho de [Zhang et al. 2024], o seguinte prompt foi usado nos LLMs para geração dos resumos: “Escreva um resumo em PORTUGUÊS DO BRASIL para o artigo de notícias a seguir com no MÁXIMO 150 palavras. ARTIGO: {TEXTO}.”, onde {TEXTO} foi substituído pelo conteúdo completo do(s) artigo(s) de notícias. \n",
            "\n",
            "O desempenho dos modelos foi avaliado utilizando as medidas de avaliação do Recall-Oriented Understudy for Gisting Evaluation Longest Common Subsequence (LCS) (ROUGE-L) [Lin 2004] e a do BERTScore [Zhang et al. 2019]. O ROUGE-L computa a maior cadeia em comum entre um resumo candidato e o resumo de referência, enquanto o BERTScore calcula a similaridade do cosseno entre dois textos usando representações de embeddings extraídas do modelo Bidirecional Encoder Representations from Transformers (BERT) [Devlin et al. 2019]. Por questões de espaço, são reportadas somente a métrica do f1-score, que combina as métricas de precisão e revocação. Apesar de terem diversas limitações, essas medidas são alternativas válidas à realização de avaliações manuais e, conforme análise feita em Zhang et al. [Zhang et al. 2024], elas apresentaram correlação moderada com avaliações humanas na tarefa de sumarização.\n",
            "\n",
            "4. Resultados\n",
            "4.1. Experimento na base de dados do RecognaSumm\n",
            "Na Tabela 2 são apresentados os resultados dos experimentos na base de dados do RecognaSumm. Analisando o desempenho dos baselines, pode-se observar que a técnica da Posição das Frases foi a que obteve os melhores resultados. Essa técnica consiste em selecionar as n primeiras frases do documento para compor o resumo até que o tamanho máximo do resumo desejado seja alcançado. Essa técnica tem sido um dos baselines mais competitivos para sumarização de artigos de notícias [Oliveira et al. 2016a]. O sistema baseado em PLI obteve melhor desempenho do que quase todos os baselines, com exceção da Posição das Frases. Os modelos P T T 5 e F LAN -T 5 demonstraram melhor desempenho geral do que as demais abordagens analisadas. Em especial, os melhores desempenhos neste experimento foram obtidos pelos modelos F LAN -T 5Large e P T T 5Large em ambas as medidas de avaliação. Os resultados obtidos usando a arquitetura base foram muito próximos às arquiteturas da large, sendo que eles são menores e consomem menos recursos computacionais. \n",
            "\n",
            "Com base nos resultados, fica evidente que os modelos ajustados para sumarização abstrativa geraram resumos melhores do que as técnicas de baselines e que o sistema extrativo de PLI nas medidas do ROUGE-L e BERTScore. Essa superioridade demonstra a eficácia dos modelos P T T 5 e F LAN -T 5 para a tarefa de geração de resumos abstrativos. Entretanto, ao considerar o uso desses modelos, é importante levar em conta o custo computacional associado a cada um, tanto para o treinamento quanto para a geração dos resumos. Portanto, a relação custo-benefício deve ser ponderada na escolha da abordagem, especialmente em cenários com recursos computacionais limitados.\n",
            "\n",
            "4.2. Experimento nas bases de dados do Temário e CSTNews\n",
            "A Tabela 3 apresenta os resultados do experimento nas bases de dados do TeMário e CSTNews. As abordagens avaliadas incluem os métodos de baselines, o sistema extrativo usando PLI, os modelos do P T T 5 e F LAN -T 5 treinados no RecognaSumm e os LLMs analisados. Os resultados obtidos neste experimento foram bastante diversificados.\n",
            "\n",
            "Tabela 2. Resultados do experimento usando o corpus RecognaSumm.\n",
            "\n",
            "Abordagem                          ROUGE-L                 BERTScore\n",
            "Bushy Path                        0,249 (0,086)          0,691 (0,037)\n",
            "Centralidade das Frases    0,249 (0,087)          0,690 (0,038)\n",
            "Frequência de Palavras        0,240 (0,085)          0,686 (0,038)\n",
            "Frequência de Entidades Nomeadas  0,242 (0,088)  0,681 (0,038)\n",
            "Posição das Frases               0,279 (0,099)          0,701 (0,040)\n",
            "Similaridade Agregada        0,249 (0,085)          0,689 (0,039)\n",
            "TextRank                              0,206 (0,072)          0,674 (0,034)\n",
            "TF-ISF                                 0,235 (0,084)          0,684 (0,037)\n",
            "Sistema PLI                        0,270 (0,095)          0,694 (0,038)\n",
            "P T T 5Small                     0,315 (0,125)          0,713 (0,045)\n",
            "P T T 5Base                      0,337 (0,132)          0,722 (0,045)\n",
            "P T T 5Large                    0,346 (0,134)          0,726 (0,046)\n",
            "F LAN -T 5Small              0,314 (0,130)          0,714 (0,045)\n",
            "F LAN -T 5Base               0,338 (0,140)          0,724 (0,048)\n",
            "F LAN -T 5Large             0,349 (0,143)          0,729 (0,048)\n",
            "\n",
            "Na base de dados do Temário, os modelos Text-davinci-003 e GPT-4o obtiveram o melhor desempenho nas medidas do ROUGE-L e BERTScore, respectivamente. Na base do CSTNews, o baseline de Posição das Frases apresentou o melhor resultado no ROUGE-L e o GPT-3.5 Turbo no BERTScore.\n",
            "\n",
            "Tabela 3. Resultados do experimento usando o Temário e o CSTNews. \n",
            "CSTNews                          Temário\n",
            "Abordagens                   ROUGE-L                BERTScore\n",
            "Sistema                              Baselines\n",
            "Extrativo                          Abstrativos                      LLMs\n",
            "Bushy Path                      0,396 (0,069)          0,694 (0,024)\n",
            "Centralidade das Frases  0,384 (0,063)          0,690 (0,025)\n",
            "Frequência de Palavras       0,375 (0,069)          0,686 (0,023)\n",
            "Frequência de Ent. Nomeadas      0,389 (0,076)      0,683 (0,024)\n",
            "Posição das Frases                0,402 (0,070)          0,686 (0,022)\n",
            "Similaridade Agregada      0,390 (0,070)          0,696 (0,025)\n",
            "TextRank                             0,350 (0,059)          0,685 (0,021)\n",
            "TF-ISF                               0,379 (0,072)          0,685 (0,024)\n",
            "Sistema PLI                     0,396 (0,065)          0,687 (0,023)\n",
            "P T T 5Small                   0,348 (0,064)          0,679 (0,024)\n",
            "P T T 5Base                    0,346 (0,062)          0,681 (0,023)\n",
            "P T T 5Large                  0,339 (0,062)          0,678 (0,025)\n",
            "F LAN -T 5Small               0,241 (0,053)          0,654 (0,021)\n",
            "F LAN -T 5Base                0,242 (0,048)          0,658 (0,022)\n",
            "F LAN -T 5Large              0,225 (0,049)          0,654 (0,021)\n",
            "Gemma 2 9B                    0,354 (0,046)          0,690 (0,020)\n",
            "Llama 3.1 8B                0,320 (0,037)          0,671 (0,019)\n",
            "Text-davinci-003           0,424 (0,075)          0,705 (0,027)\n",
            "GPT-3.5 Turbo               0,402 (0,074)          0,705 (0,025)\n",
            "GPT-4o                            0,417 (0,062)          0,713 (0,025)\n",
            "GPT-4o Mini                   0,402 (0,059)          0,705 (0,021)\n",
            "\n",
            "Os métodos de baselines, o sistema extrativo baseado em PLI e os LLMs apresentaram resultados próximos com base nas medidas de avaliação. Por outro lado, os modelos ajustados do P T T 5 e F LAN -T 5 demonstraram desempenho inferior aos demais, especialmente os modelos do F LAN -T 5. Esse baixo desempenho pode ser atribuído ao fato desses modelos consistentemente gerarem resumos com tamanhos bem inferiores aos demais, mesmo sendo definido um tamanho máximo de 150 palavras. Essa característica aconteceu por conta do treinamento desses modelos no RecognaSumm, que possui resumos de referência bem menores do que os do Temário e do CSTNews.\n",
            "\n",
            "Apesar dos resultados quantitativos serem próximos, ao analisar os resumos gerados pelas abordagens extrativas e abstrativas, fica evidente que os resumos extrativos, em geral, possuem muitas informações contidas nos resumos de referências, mas os resumos possuem diversos problemas de coerência e coesão textual. Por outro lado, os resumos abstrativos são mais sucintos e, em sua maioria, apresentam uma boa qualidade textual em termos de coerência, coesão e estrutura ortográfica e gramatical. Os LLMs do Gemma 2 9B e do Llama 3.1 8B apresentaram uma tendência de terminar de forma brusca os resumos, por exemplo, no meio de uma frase. Cabe ressaltar que nenhum LLM foi ajustado para a tarefa de sumarização. \n",
            "\n",
            "Por fim, é importante enfatizar que os LLMs, como Gemma, Llama e especialmente os modelos da OpenAI, possuem um custo consideravelmente maior do que os demais modelos avaliados neste trabalho. Essa característica deve ser considerada em aplicações práticas, na qual a relação custo-benefício é determinante. Nesse contexto, abordagens extrativas, como o sistema baseado PLI ou mesmo os baselines, podem oferecer uma alternativa que equilibra desempenho com menor custo computacional. Em cenários com recursos computacionais moderados, os modelos ajustados do P T T 5 e F LAN -T 5 podem ser as melhores opções.\n",
            "\n",
            "5. Conclusões\n",
            "Este trabalho apresentou uma análise comparativa de várias abordagens para sumarização automática de texto, considerando desde tradicionais métodos de ponderação de frases até modelos de linguagem de grande escala, para sumarização abstrativa e extrativa de artigos de notícias escritas em português do Brasil. Essa avaliação fez uso de três bases de dados e de duas medidas de avaliação automática comumente usadas na literatura. Os resultados obtidos demonstram que os modelos de LLMs são promissores para a tarefa de criação automática de resumos, mas são sistemas com uma alta complexidade que requerem muitos recursos computacionais. Portanto, modelos especializados para a tarefa de sumarização ou sistemas extrativos ainda podem ser opções viáveis, especialmente em cenários de poucos recursos.\n",
            "\n",
            "Em trabalhos futuros, pretendemos expandir este trabalho visando: (i) analisar o desempenho de modelos de LLM de código aberto, considerando diferentes cenários de utilização, como zero shot-learning, few-shot learning e fazendo o ajuste fino desses modelos para a tarefa de sumarização; e (ii) realizar uma avaliação manual de um subconjunto dos resumos gerados para complementar as análises automáticas.\n",
            "18\n",
            "Resumo. Este artigo apresenta uma análise comparativa dos modelos neurais pré-treinados do P T T 5 e F LAN -T 5 para a geração automática de perguntas em português do Brasil. Para isso, foram utilizados dois conjuntos de dados, PIR A e FairyTaleQA, para avaliar a capacidade desses modelos de gerar perguntas a partir de dois cenários: (i) considerando apenas o contexto e (ii) usando o contexto e a resposta esperada. As medidas do ROUGE-L e do BERTScore foram usadas para avaliar as perguntas geradas, além de uma análise baseada no GP T -4. Os resultados demonstram que o modelo P T T 5Large apresentou consistentemente desempenho superior aos demais modelos, gerando 93,06% de perguntas válidas no PIR A e 82,32% no FairyTaleQA na avaliação baseada no GP T -4. \n",
            "\n",
            "1. Introdução  \n",
            "A Geração de Perguntas (QG, do inglês Question Generation) é uma tarefa da área de Processamento de Linguagem Natural (PLN), que envolve a criação automática de perguntas a partir de um dado texto ou conjuntos de dados textuais [Zhang et al. 2021]. Usando técnicas de PLN e algoritmos de Aprendizado de Máquina (AM), os sistemas de QG visam gerar perguntas gramaticalmente corretas e contextualizadas relevantes [da Rocha Junqueira et al. 2024]. Diante da grande abundância de informações digitais, sistemas de QG possuem diversas potenciais áreas de aplicação [Mulla and Gharpure 2023]. Na área da educação, a aplicação de abordagens de QG pode contribuir para o desenvolvimento de materiais de avaliação, questionários práticos, no desenvolvimento de sistemas de tutoria, aprimorando processos de aprendizagem e avaliação [Kurdi et al. 2020]. No âmbito dos sistemas de perguntas e respostas (QA, do inglês Question Answering), abordagens de QG têm sido usadas para o treinamento de modelos com pouca supervisão ou para fins de aumento de dados [Puri et al. 2020]. \n",
            "\n",
            "Apesar do crescente interesse em pesquisas envolvendo a tarefa de QG, a maioria desses estudos concentra-se predominantemente na língua inglesa, onde há diversos recursos e bases de dados disponíveis para experimentação e desenvolvimento [Zhang et al. 2021, Mulla and Gharpure 2023]. Em contrapartida, as pesquisas focadas na língua portuguesa, especialmente para o português do Brasil, ainda são limitadas, resultando em uma escassez tanto de estudos quanto de bases de dados [da Rocha Junqueira et al. 2024, Leite et al. 2024]. Essa lacuna impõe desafios adicionais para o avanço no desenvolvimento e na aplicação prática de sistemas de QG, uma vez que a adaptação de modelos e técnicas desenvolvidas para o inglês nem sempre se traduzem diretamente em resultados eficazes em outros idiomas, dada a complexidade e as particularidades linguísticas inerentes de linguagem natural. \n",
            "\n",
            "Este artigo tem como objetivo investigar a aplicação de modelos neurais de linguagem pré-treinados baseados na arquitetura Transformers [Vaswani 2017], mais especificamente os modelos P T T 5 [Carmo et al. 2020] e F LAN -T 5 [Chung et al. 2024] para a tarefa de QG em português do Brasil. Para isso, foi realizado o ajuste fino desses modelos em suas arquiteturas small, base e large, utilizando as bases de dados do PIR A [Paschoal et al. 2021], nativa em português, e uma versão traduzida da base de dados FairytaleQA [Leite et al. 2024] para o português do Brasil. Os experimentos foram realizados em dois cenários: no primeiro, as perguntas foram geradas a partir somente de um dado contexto; no segundo, as perguntas foram geradas considerando tanto o contexto quanto uma resposta prévia. A avaliação dos resultados foi realizada por meio das medidas automáticas do ROUGE-L e BERTScore, que são comumente utilizadas para avaliar abordagens de QG em termos de similaridade lexical e semântica das perguntas geradas com as perguntas de referência. Além disso, foi realizado um experimento adicional utilizando o modelo GP T -4 para avaliar as perguntas geradas. Esse experimento teve como objetivo complementar as avaliações quantitativas anteriores, proporcionando uma análise adicional da qualidade das perguntas geradas. \n",
            "\n",
            "As principais contribuições deste artigo incluem: (i) o ajuste fino e a avaliação de diferentes arquiteturas dos modelos P T T 5 e F LAN -T 5 para a tarefa de QG em português do Brasil; e (ii) uma extensa investigação considerando duas bases de dados, PIRA e FairytaleQA, e duas variações da tarefa. O código-fonte desenvolvido neste trabalho está público em um repositório do GitHub1. \n",
            "\n",
            "2. Trabalhos Relacionados  \n",
            "As abordagens de geração de perguntas podem ser classificadas em métodos convencionais e baseados em modelos neurais [Zhang et al. 2021]. Os métodos convencionais de QG baseiam-se principalmente na aplicação de regras heurísticas para transformar os textos em perguntas relacionadas. Recentemente, com a evolução das arquiteturas de redes neurais profundas, houve uma mudança de paradigma na tarefa para a adoção de modelos neurais, permitindo assim, o desenvolvimento de abordagens orientadas a dados e completamente treináveis, na qual a seleção de conteúdo e a construção de perguntas podem ser otimizadas de forma combinada. Embora exista uma vasta literatura sobre QG em diversos idiomas [Kurdi et al. 2020, Zhang et al. 2021, Mulla and Gharpure 2023], por limitação de espaço, esta seção foca em trabalhos envolvendo o português do Brasil. \n",
            "\n",
            "Em [Leite and Lopes Cardoso 2022], os autores apresentam um estudo que envolveu o treinamento do modelo P T T 5 para a geração de perguntas utilizando uma versão em português do conjunto de dados SQuAD 1.1. Os resultados obtidos foram encorajadores, com desempenho equiparável com a implementação em inglês do modelo T 5, evidenciando a eficácia dos modelos baseados na arquitetura Transformers e estabelecendo baselines para futuras comparações para a tarefa de QG em português. Oliveira et al. [Oliveira et al. 2023] abordam o desafio de gerar e classificar distratores (opções incorretas) para questões de múltipla escolha em português. Os autores desenvolveram e combinam vários métodos de geração de distratores, incluindo extração baseada em contexto, manipulação numérica e similaridade semântica a partir de recursos como WordNet. \n",
            "\n",
            "Junqueira et al. [da Rocha Junqueira et al. 2024] apresentaram uma investigação do desempenho dos modelos T 5, F LAN -T 5 e BART -P T para a geração de perguntas factuais em português do Brasil. Para mitigar o problema da escassez de dados, foi utilizada uma versão em português brasileiro do SQuAD v1.1, obtida por meio de tradução automática. Leite et al. [Leite et al. 2024] realizaram a construção de versões traduzidas automaticamente da base de dados FairytaleQA, que é um conjunto de dados comumente usado para o desenvolvimento de sistemas de perguntas e respostas em inglês. Foram desenvolvidas versões do FairytaleQA para o português de Portugal, português do Brasil, espanhol e francês, que podem ser usadas em pesquisas da área de QG e QA. Além disso, foram realizados experimentos usando modelos neurais baseados na arquitetura T 5. \n",
            "\n",
            "Este trabalho difere dos anteriores ao: (i) treinar e avaliar diferentes tamanhos de arquitetura dos modelos P T T 5 e F LAN -T 5, (ii) considerar dois cenários da tarefa de QG, (iii) adotar uma base de dados escrita nativamente em português (PIR A) e outra obtida por meio de tradução automática (FairytaleQA), e (iv) analisar o desempenho dos modelos usando uma abordagem com o modelo GP T -4, além de tradicionais medidas de avaliação consideradas em trabalhos anteriores. \n",
            "\n",
            "3. Materiais e Métodos  \n",
            "3.1. Bases de Dados  \n",
            "Neste trabalho foram utilizados dois conjuntos de dados, o PIR A [Paschoal et al. 2021] e o FairyTaleQA [Xu et al. 2022]. Essas bases de dados foram selecionadas por serem usadas em trabalhos da literatura na tarefa de geração de perguntas ou de sistemas de perguntas e respostas em português do Brasil. Além disso, elas possuem três componentes essenciais para a tarefa de QG: (i) contexto textual, (ii) pergunta associada e (iii) resposta correspondente. \n",
            "\n",
            "O PIR A é uma base de dados bilíngue (português-inglês) focada em questões oceânicas e da costa brasileira. A base contém 2.261 textos extraídos de trechos de relatórios das Nações Unidas sobre o oceano e de resumos relacionados ao litoral brasileiro [Paschoal et al. 2021]. As perguntas e respostas foram criadas manualmente em um processo de revisão em pares por avaliadores humanos. Após uma análise da base de dados, foi observado que alguns exemplos não apresentam as respostas para as perguntas. Por isso, esses exemplos foram removidos, já que a resposta é um elemento importante para os experimentos realizados neste trabalho. \n",
            "\n",
            "O FairyTaleQA é uma base de dados comumente usada para avaliar sistemas de perguntas e respostas em inglês. Essa base foi criada por especialistas em educação e é composta por textos narrativos infantis. Leite et al. [Leite et al. 2024] realizaram um processo de tradução do FairyTaleQA para diversos idiomas, incluindo o português de Portugal e do Brasil. Neste trabalho, foi utilizada a versão traduzida para o português do Brasil, que compreende 10.580 perguntas e respostas derivadas de 278 histórias infantis. \n",
            "\n",
            "Na Tabela 1 são apresentadas para cada base de dados as estatísticas do total de exemplos em cada conjunto (treinamento, validação e teste) e o tamanho médio e desvio padrão do total de palavras para cada componente (contexto, pergunta e resposta). Para gerar essas estatísticas, foi utilizada a ferramenta spaCy2 para o processamento dos textos. \n",
            "\n",
            "Tabela 1. Estatística das bases de dados do PIR A e FairyTaleQA  \n",
            "Base de Dados | Conjunto | Exemplos | Componente | Média de Palavras (Desvio Padrão)  \n",
            "Treino | 1.756 | PIR A | Validação | 215 | Teste | 216  \n",
            "Treino | 8.548 | FairyTaleQA | Validação | 1.025 | Teste | 1.007  \n",
            "Contexto | Pergunta | Resposta | Contexto | Pergunta | Resposta | Contexto | Pergunta | Resposta | Contexto | Pergunta | Resposta  \n",
            "274,73 (141,41) | 13,83 (5,62) | 14,32 (11,76) | 273,98 (157,08) | 13,65 (5,38) | 15,04 (12,06) | 250,58 (128,98) | 13,36 (5,68) | 14,92 (14,50) | 182,51 (94,53) | 10,23 (3,38) | 6,98 (5,73)  \n",
            "170,08 (74,18) | 10,93 (3,40) | 7,52 (5,96) | 168,92 (73,77) | 10,48 (3,30) | 6,80 (5,31)  \n",
            "\n",
            "3.2. Modelos Avaliados  \n",
            "Neste trabalho, foram avaliados os modelos P T T 5 e o F LAN -T 5, baseados na arquitetura Text-to-Text Transfer Transformer (T5) [Raffel et al. 2020]. Apesar de existirem diferentes tamanhos de arquitetura, as três comumente usadas são “small”, “base” e “large”. Elas possuem um número crescente de parâmetros, o que geralmente resulta em maior capacidade de aprendizado, mas também em um maior custo computacional. Esses modelos foram escolhidos devido ao seu desempenho promissor em tarefas de PLN e por terem sido explorados em trabalhos anteriores. \n",
            "\n",
            "O P T T 5 é uma adaptação do modelo T 5, especificamente pré-treinada para o português do Brasil [Carmo et al. 2020]. O modelo foi pré-treinado no corpus BrWac [Wagner Filho et al. 2018], uma extensa coleção de páginas web em português, contendo aproximadamente 2,7 bilhões de tokens. Foram utilizados os modelos P T T 5Small, que possui aproximadamente 60 milhões de parâmetros, o P T T 5Base, com cerca de 220 milhões de parâmetros, e o P T T 5Large, que apresenta aproximadamente 740 milhões de parâmetros.  \n",
            "\n",
            "O F LAN -T 5 [Chung et al. 2024] é uma versão aprimorada do T 5 pré-treinado em múltiplas tarefas de PLN. Esse modelo foi pré-treinado majoritariamente em documentos em inglês, mas possui suporte a outros idiomas, como o português. Foram avaliadas três variantes deste modelo: o F LAN -T 5Small com cerca de 80 milhões de parâmetros, o F LAN -T 5Base contendo aproximadamente 250 milhões de parâmetros, e o F LAN -T 5Large apresentando cerca de 780 milhões de parâmetros. Sua inclusão tem o objetivo de avaliar como um modelo com treinamento diversificado se comporta em comparação a modelos especializados em um único idioma e tarefa. \n",
            "\n",
            "3.3. Metodologia Experimental  \n",
            "A metodologia experimental utilizada neste trabalho envolveu o desenvolvimento, ajuste fino e a avaliação dos modelos investigados, especificamente ajustados para dois cenários da tarefa de geração de perguntas, conforme ilustrado na Figura 1. Para cada cenário, seis modelos foram treinados, considerando os três tamanhos de arquitetura e os dois modelos P T T 5 e F LAN -T 5. No primeiro cenário, foi analisada a variação da tarefa que gera perguntas a partir somente do contexto, como entrada. Já no segundo cenário, os modelos recebem tanto o contexto quanto uma resposta como entrada e devem gerar uma pergunta como saída. \n",
            "\n",
            "Figura 1. Cenários da tarefa de QG analisados.  \n",
            "\n",
            "Os modelos P T T 5 e F LAN -T 5 foram implementados usando a biblioteca Transformers3. O tamanho de entrada máximo foi definido para 512 tokens, enquanto a saída foi configurada para no máximo 40 tokens. Durante o treinamento, os modelos foram ajustados por no máximo 20 épocas, sendo utilizada a estratégia de parada antecipada com uma paciência de 5 épocas. Para mitigar o sobreajuste dos modelos, ao final de cada época, o modelo treinado é aplicado no conjunto de validação e é computada a medida do ROUGE-L, sendo salvo somente o modelo com maior valor. Durante a geração das perguntas, foi utilizado o algoritmo de Beam Search com uma largura de tamanho 5. Esses valores foram definidos a partir da análise de trabalhos anteriores. \n",
            "\n",
            "A avaliação do desempenho dos modelos foi realizada por meio de duas abordagens: a aplicação de métricas automáticas de similaridade e uma avaliação com base no modelo de linguagem GP T -4. Para a avaliação automática, foi utilizada a métrica Recall-Oriented Understudy for Gisting Evaluation Longest Common Subsequence (ROUGE-L) [Lin 2004], que mensura a similaridade com base na maior sequência de palavras em comum entre as perguntas geradas e as perguntas de referência. Adicionalmente, foi utilizada a métrica BERTScore [Zhang et al. 2019], que calcula a similaridade de cosseno a partir das representações em embeddings extraídas do modelo Bidirectional Encoder Representations for Transformers (BERT). \n",
            "\n",
            "Para uma avaliação mais holística e contextualmente relevante, foi realizada uma análise usando o modelo GP T -4. Essa avaliação foi pensada porque, dado um contexto específico, é possível gerar múltiplas perguntas válidas que não necessariamente precisam ser idênticas à pergunta de referência presente nas bases de dados usadas nos experimentos. Esta situação é particularmente relevante no Cenário 1, onde apenas o contexto é fornecido como entrada para o modelo. Neste caso, diversas perguntas podem ser consideradas válidas, desde que sejam respondíveis com base no contexto fornecido. Em contraste, no Cenário 2, onde o contexto e a resposta esperada são fornecidos como entrada, a pergunta gerada deve ser semanticamente equivalente à pergunta de referência. \n",
            "\n",
            "O processo de avaliação utilizando o GP T -4 foi inspirado na técnica de Retrieval Augmented Generation (RAG). Esta técnica consiste em fornecer um contexto e uma pergunta para um modelo de linguagem de grande escala (LLM, do inglês Large Language Model), solicitando que ele responda à pergunta usando apenas o contexto fornecido ou sinalize caso não seja possível [Chen et al. 2024]. Seguindo esta abordagem, foi criado um prompt4 contendo o contexto original e a pergunta gerada pelos modelos avaliados. Este prompt foi então submetido ao GP T -4, com a instrução de responder à pergunta utilizando somente o contexto fornecido ou indicar a impossibilidade de resposta. Com base nas respostas do GP T -4, foi calculado o percentual de perguntas válidas (aquelas que o LLM conseguiu responder com base no contexto) e inválidas (as que não puderam ser respondidas) para cada modelo avaliado. Deste modo, foi possível analisar se as perguntas geradas foram relevantes ao contexto, ainda que diferentes da pergunta de referência. Embora esta análise seja automatizada, foi realizada uma inspeção manual em amostras das saídas do GP T -4 para verificar sua confiabilidade. Foi observado que, em geral, o LLM identificava corretamente as perguntas válidas e inválidas. \n",
            "\n",
            "4. Resultados  \n",
            "Na Tabela 2 são apresentados os resultados dos experimentos, considerando os cenários 1 e 2, com base nas medidas de avaliação do ROUGE-L e BERTScore. No Cenário 1 (apenas contexto), o P T T 5Base e o P T T 5Large apresentaram os melhores desempenhos para as bases de dados do FairyTaleQA e PIR A, respectivamente. No Cenário 2 (contexto e resposta esperada), o P T T 5Large superou os demais modelos em ambas as bases. Fica evidente que os modelos P T T 5 consistentemente obtiveram melhores resultados do que os modelos F LAN -T 5 em ambos os conjuntos de dados e cenários, sugerindo que o pré-treinamento específico em português confere vantagens na tarefa de geração de perguntas. \n",
            "\n",
            "Comparando os resultados obtidos em ambos os cenários de avaliação, observa-se que os modelos apresentaram melhores desempenhos no Cenário 2 em comparação com o Cenário 1. Isso acontece porque no Cenário 2, como a resposta é dada como entrada, ela guia os modelos a gerarem perguntas para aquele contexto e resposta. Assim, a pergunta gerada precisa ser semanticamente equivalente à pergunta de referência. Tal situação não ocorre no Cenário 1, já que é somente dado o contexto como entrada e, para um mesmo contexto, é possível gerar diversas perguntas válidas. Por isso, para melhor avaliar o Cenário 1, foram realizadas as análises usando o modelo GP T -4. \n",
            "\n",
            "Na Tabela 3 são apresentados os resultados da avaliação dos modelos no Cenário 1 usando o GP T -4. Os resultados obtidos apresentam um padrão similar ao primeiro experimento, mas com algumas diferenças importantes. O P T T 5Large obteve o melhor desempenho em ambas as bases de dados, com 82,32% das perguntas geradas sendo consideradas válidas no FairyTaleQA e 93,06% no PIR A. É possível observar uma divergência entre as medidas automáticas e a avaliação GP T -4, particularmente no PIR A. Enquanto as medidas do ROUGE-L e BERTScore indicaram valores menores para o PIR A em comparação com o FairyTaleQA, a avaliação GP T -4 mostrou uma tendência oposta, com percentuais mais altos de perguntas válidas no PIR A. \n",
            "\n",
            "Tabela 3. Resultados da avaliação do Cenário 1 usando o GP T -4.  \n",
            "Base de Dados | FairyTaleQA | PIR A  \n",
            "Modelo | P T T 5Small | P T T 5Base | P T T 5Large | F LAN -T 5Small | F LAN -T 5Base | F LAN -T 5Large | P T T 5Small | P T T 5Base | P T T 5Large | F LAN -T 5Small | F LAN -T 5Base | F LAN -T 5Large  \n",
            "Válida | 680 | 726 | 829 | 451 | 525 | 719 | 135 | 199 | 201 | 122 | 138 | 197  \n",
            "Inválida | 327 | 281 | 178 | 556 | 482 | 288 | 81 | 17 | 15 | 94 | 78 | 19  \n",
            "% Válida | 67,53 | 72,10 | 82,32 | 44,79 | 52,14 | 71,40 | 62,50 | 92,13 | 93,06 | 56,48 | 63,89 | 91,20  \n",
            "\n",
            "Na Figura 2 é apresentado um exemplo extraído da base de dados do PIR A, contendo o contexto, as perguntas geradas pelos modelos P T T 5 e a saída da análise usando o GP T -4. Nesse exemplo, é possível ver que os modelos P T T 5Large e P T T 5Base foram capazes de gerar perguntas que podem ser respondidas pelo contexto, sendo assim consideradas válidas. Por outro lado, o modelo P T T 5Small gerou uma pergunta confusa sobre a Petróleo Brasileiro SA não ter comentado sobre a estimativa de produção revisada. Apesar de ser mencionado no contexto, não está explícito nele o porquê disso. Sendo assim, considerada inválida pela avaliação do GP T -4. \n",
            "\n",
            "Contexto: O BG GROUP produziu recentemente uma nova estimativa na descoberta de óleo de Tupi na Bacia de Santos, afirmando que o campo contém 12-30 bilhões boe ou mais. Por um lado, a Petroleo Brasileiro SA de Petróleo (Petrobras) não comentou sobre a estimativa de produção revisada. Pode-se lembrar que a Petrobras relatou a descoberta para ser de 8 bilhões bbl de luz em bruto em 2007. Enquanto isso, alegações de BG foram produzidas em uma declaração sobre a estratégia de crescimento a longo prazo da empresa, lançada à frente da apresentação de seus resultados do quarto trimestre.  \n",
            "P T T 5Small: Por que a Petroleo Brasileiro SA de Petróleo (Petrobras) não comentou sobre a estimativa de produção revisada? Avaliação: INVALIDA.  \n",
            "P T T 5Base: Qual foi a estimativa do BG Group para a descoberta de óleo de Tupi na Bacia de Santos? Avaliação: VÁLIDA.  \n",
            "P T T 5Large: Qual a estimativa do BG Group para a quantidade de óleo de Tupi na Bacia de Santos? Avaliação: VÁLIDA.  \n",
            "\n",
            "Figura 2. Exemplo de contexto extraído do PIR A e perguntas consideradas válidas e inválidas pelo GP T -4. \n",
            "\n",
            "5. Considerações Finais e Trabalhos Futuros  \n",
            "Neste trabalho, foi realizada uma análise comparativa dos modelos P T T 5 e F LAN -T 5 para a tarefa de geração automática de perguntas. Para isso, foram utilizadas as bases de dados do PIR A e uma versão traduzida do FairyTaleQA para o português do Brasil. O desempenho dos modelos foi avaliado usando uma abordagem tradicional, considerando as medidas de avaliação do ROUGE-L e do BERTScore. Além dessa abordagem, foi realizada uma análise das perguntas geradas pelos modelos usando o GP T -4, avaliando se as perguntas geradas poderiam ser respondidas somente a partir do contexto fornecido. Os resultados experimentais demonstraram que o modelo P T T 5Large obteve os melhores resultados em quase todos os cenários avaliados. Os resultados obtidos indicam a eficácia do pré-treinamento específico em português, evidenciada pelo desempenho superior consistente dos modelos P T T 5 em comparação com os modelos F LAN -T 5. \n",
            "\n",
            "Apesar dos resultados encorajadores obtidos, o trabalho apresenta diversas limitações, que serão melhor exploradas. Dentre elas, pode-se destacar duas linhas de pesquisa futuras: (i) investigar o desempenho de LLMs, como o Llama 3 [Touvron et al. 2023], Gemma [Team et al. 2024] e Sabiá [Almeida et al. 2024]; e (ii) realizar uma avaliação humana para complementar as avaliações automáticas realizadas.\n",
            "19\n",
            "Resumo. Este artigo apresenta um grafo de conhecimento unificado para as línguas indígenas brasileiras (BIL) a partir da perspectiva de aplicações potenciais, com foco particular no domínio educacional. Apresentamos o BIL-Graph, um protótipo construído para o Bororo e línguas tupis, como Guajajara, Munduruku e Akuntsu. Em seguida, descrevemos o processo de extração de conhecimento e ligação de entidades para construir o grafo a partir de um banco de árvores de dependências e de um banco de dados lexical para línguas Tupi e Bororo. Discutimos as limitações do BILGraph, destacando questões éticas e práticas de implementação.\n",
            "\n",
            "1. Introdução\n",
            "O desenvolvimento de aplicações para as línguas indígenas brasileiras (BIL) é severamente limitado pela falta de recursos e ferramentas. Como é comum com línguas em perigo, os recursos disponíveis são tanto escassos quanto dispersos [Pinhanez et al. 2023]. Para algumas línguas, como Guajajara, Asurini e Bororo, dicionários estão agora disponíveis [Harrison e Harrison 2013, Cabral e Rodrigues 2003, Ferraz Gerardi]. Para outras línguas, bancos de árvores estão disponíveis através do Projeto Universal Dependencies (UD) [Nivre et al. 2020a], embora variem em comprimento e qualidade. Algumas línguas, no entanto, possuem apenas um punhado de recursos diversos [Monserrat 2000]. Essa falta de padronização e dados vinculados adequados representa uma barreira significativa para desenvolver ferramentas e métodos que poderiam apoiar iniciativas de revitalização linguística e acelerar a produção de material pedagógico. \n",
            "\n",
            "Esforços recentes para unificar recursos de línguas indígenas brasileiras, como TuLeD [Gerardi et al. 2022a] e os bancos de árvores TuDeT no UD — um banco de dados lexical e um banco de árvores de dependência para várias línguas tupis (ainda em sua fase inicial), respectivamente — foram fundamentais no desenvolvimento de aplicações de aprendizado de línguas direcionadas a comunidades indígenas [Polleti 2024]. Além disso, a recente publicação do Corpus Bororo [Ferraz Gerardi et al. 2024], que está ligado ao Banco de Árvores UD [Ferraz Gerardi 2024], possibilitou o uso de várias ferramentas computacionais para desenvolver materiais educacionais e outros recursos online; notavelmente, um aplicativo de aprendizado de línguas para a língua Bororo. Bancos de árvores UD [Nivre et al. 2020b] são um recurso importante, pois o tipo de anotação padronizada para todas as línguas facilita o desenvolvimento de novas aplicações. Por outro lado, estruturas de rede heterogêneas e complexas, como grafos de conhecimento, são conhecidas por sua flexibilidade em incorporar características linguísticas [Cong e Liu 2014, Miller 1994] e podem ser efetivamente utilizadas para alimentar aplicações sofisticadas, incluindo sistemas de recomendação, recuperação de informações e assistentes educacionais.\n",
            "\n",
            "Neste trabalho, introduzimos uma versão preliminar de um grafo de conhecimento unificado para línguas indígenas brasileiras, que nos referiremos como \"BILGraph\", e descrevemos seu pipeline de extração de conhecimento. Desenvolvemos um protótipo para línguas tupis disponíveis em Tuled e Tudet [Gerardi et al. 2022b], e a língua Bororo [Ferraz Gerardi et al. 2024]. Discutimos o protótipo do grafo de conhecimento com foco nas potenciais aplicações. Conseguimos desenvolver um pipeline de processamento de linguagem natural para construir o BILGraph que pode lidar com dados semiestruturados de várias fontes, como frases anotadas de bancos de árvores e dicionários. Discutimos os desafios e limitações do pipeline. A principal contribuição deste trabalho é apresentar uma versão protótipo do BILGraph como um caso de estudo sobre a construção de um grafo de conhecimento unificado para BIL. Esperamos que o grafo de conhecimento e os métodos apresentados neste trabalho possam apoiar o desenvolvimento de aplicações sofisticadas.\n",
            "\n",
            "O artigo está organizado da seguinte forma. A Seção 2 descreve o design do BILGraph e seu desenvolvimento, incluindo suas fontes de dados e pipeline de extração de conhecimento. A Seção 3 discute os desafios e limitações de nosso protótipo, analisa nossos processos e recursos tanto de uma perspectiva de implementação prática quanto de possíveis aplicações, e oferece considerações finais.\n",
            "\n",
            "2. BilGraph: Grafo de Conhecimento Linguístico\n",
            "Desenvolvemos um pipeline de extração de conhecimento para estruturar e vincular recursos linguísticos para as línguas indígenas brasileiras (BIL) disponíveis em bancos de árvores Universal Dependencies (UD) e bancos de dados lexicais, como TuLeD e o dicionário Bororo. O resultado desse esforço é o “BILGraph”, um grafo de conhecimento para BIL que contém quatro tipos principais de nós: (1) frase, (2) token, (3) lema e (4) conceito. Considere o exemplo retratado na Figura 1. O nó da frase representa a frase do banco de árvores Bororo ure karo kowyje ‘Ele comeu o peixe’. Este nó de frase conecta-se aos seus nós de token, que representam as palavras individuais que compõem a frase e suas dependências sintáticas. Neste exemplo, “kowyje” é a raiz, com o objeto “karo” e o sujeito nominal “ure” ligando-se a ela. Cada nó de token está conectado a um único nó de frase. Cada token é ainda vinculado a um nó de lema, que representa a forma base da palavra e suas relações com classes linguísticas, incluindo quaisquer sinônimos aplicáveis. Até este ponto, as entidades e relações descritas são aquelas geralmente encontradas em bancos de árvores de dependência. No entanto, a base de dados lexical ou dicionário adiciona outra camada ao ligar os nós de lema aos nós de conceito. Os nós de conceito representam abstrações de alto nível que transmitem significado através de diferentes línguas e domínios. Em nosso exemplo, os lemmas “karo” e “kowyje” estão ligados aos conceitos “peixe” e “comer”, respectivamente. O objetivo é estabelecer os nós de conceito como uma camada semântica que permite a interoperabilidade entre as frases do banco de árvores e outras bases de conhecimento, como ontologias, recursos multimídia (por exemplo, bancos de dados fonéticos ou de imagens) e outras línguas. Com o BILGraph, poderia-se facilmente pesquisar frases em outras línguas com estruturas ou temas semelhantes, buscando todos os nós de frase conectados a um dado nó de conceito. Por exemplo, um mecanismo de busca poderia recuperar a frase Guajajara uPu ipirateteaPu ‘Ele come muitos peixes’ porque está conectado ao nó de conceito “PEIXE” como a frase semelhante em Bororo ure karo kowyje. Note que a estrutura do grafo é flexível o suficiente para codificar relações N-N entre lemmas e conceitos.\n",
            "\n",
            "As relações entre frases, tokens e lemmas podem ser extraídas diretamente de bancos de árvores UD, uma vez que as frases do banco de árvores são anotadas com atributos que permitem uma representação gráfica direta. Para construir o BILGraph, o verdadeiro desafio reside em ligar os nós de lema aos nós de conceito. Em nossa versão preliminar, aplicamos um simples processo de vinculação de entidades da seguinte forma. Para cada lema, geramos um conjunto de vizinhança de palavras semelhantes mudando e aparando caracteres com base em regras. Por exemplo, na língua Bororo, temos grafias diferentes onde algumas palavras trocam “u” por “y”, e palavras como “boe” são frequentemente aplicadas, então algumas de nossas regras de geração de vizinhança envolveram adicionar ou remover prefixos e mudar letras intercambiáveis. O tamanho da vizinhança foi definido considerando um limiar de similaridade com base na distância de Levenshtein. Em seguida, selecionamos de todo o vocabulário em nosso banco de dados as palavras que apresentam alta similaridade, considerando novamente um limiar baseado na distância de Levenshtein, com pelo menos uma instância em nossa vizinhança. Finalmente, testamos se a entrada ou descrição do dicionário para cada candidato possui pelo menos uma palavra na frase. Então, por exemplo, considere que estamos tentando ligar o lema “karo”, da frase “Ele comeu o peixe”, aos seus conceitos apropriados. Além disso, considere que a descrição do dicionário para um candidato à palavra “kabo” é “um tipo de peixe de rio”. Neste caso, estabeleceremos a ligação devido à similaridade lexical entre “karo” e “kabo”, e devido à palavra “peixe” que está presente tanto na entrada do dicionário quanto na frase. Note que confiar na similaridade lexical pode levar a imprecisões. Por exemplo, as palavras Bororo “apido” (coração de palmeira) e “apodo” (tucano) têm alta similaridade lexical, enquanto seus significados não estão relacionados. Se uma entrada do dicionário contém ambas as palavras, como “coração de palmeira, comestível para muitos animais como tucanos”, isso levaria à adição de ligações incorretas ao grafo. O código do pipeline de extração de conhecimento do BILGraph, com os limiares de distância de Levenshtein utilizados para cada língua, e o próprio grafo de conhecimento estão disponíveis no Github.2 Adotamos o formato RDF, onde cada aresta no grafo é representada como uma tripla.\n",
            "\n",
            "2https://github.com/gpadpoll/bilgraph\n",
            "\n",
            "3. Discussão e Considerações Finais\n",
            "A versão preliminar do BILGraph apresentada neste trabalho representa um passo significativo para avançar os recursos para línguas indígenas brasileiras. Envisionamos que o BILGraph poderia impulsionar aplicações típicas, como a recuperação de informações de textos escritos nessas línguas, com ênfase particular em seu potencial educacional. O processo de criação de recursos educacionais muitas vezes envolve a organização de textos com base em suas características linguísticas, temas e níveis de complexidade. Por exemplo, poderia-se pesquisar frases específicas para ensinar alguém a pedir comida. O BILGraph simplifica essa tarefa permitindo consultas para frases vinculadas a nós de conceito específicos. Para encontrar frases que incluam vocabulário relacionado a alimentos, pode-se anexar uma ontologia genérica aos nós de conceito do BILGraph e pesquisar frases associadas a conceitos relacionados a alimentos. Além disso, o BILGraph facilita a consulta de frases com base em características linguísticas, como aquelas que usam pronomes possessivos, formas verbais, plurais, advérbios, e mais. Acreditamos que a capacidade do BILGraph de consultar e organizar frases pode melhorar o uso de bancos de árvores e outros recursos disponíveis de BIL no desenvolvimento de materiais educacionais. Ao organizar recursos em um formato padronizado e unificado, podemos desenvolver aplicações que escalem entre várias línguas. Por exemplo, uma consulta que busca conceitos relacionados a alimentos em frases para uma língua pode ser reutilizada para outras línguas incluídas no BILGraph. Já estamos aproveitando o BILGraph para desenvolver um currículo para um curso de língua Bororo, que será lançado como um aplicativo de aprendizado de línguas. Pretendemos estender essa abordagem para outras línguas à medida que forem incorporadas ao grafo de conhecimento.\n",
            "\n",
            "Neste ponto, nosso protótipo do BILGraph deixa a desejar em vários aspectos e permanece em desenvolvimento, desde as dificuldades de trabalhar com fontes de dados limitadas até imprecisões e preocupações éticas. O BILGraph foi construído a partir de TuLeD, TuDet e do banco de árvores e dicionário Bororo. Todas essas fontes de dados foram desenvolvidas compilando várias fontes da literatura, sem um processo adequado de coleta de dados estruturados. Como resultado, sofre de incompletude, especialmente quando consideramos a cobertura de árvores de dependência com tradução para o português. Temos apenas traduções em português para “Bororo”, “Guajajara”, “Munduruku” e “Akuntsu” entre as 9 línguas disponíveis. A falta de traduções em português limita a aplicação desses recursos, como para fins educacionais, por exemplo. Além disso, é razoável esperar que algumas imprecisões possam ter sido introduzidas como parte do processo de vinculação de entidades e extração de conhecimento. Não avaliamos a correção de maneira abrangente ainda, exceto por uma verificação manual limitada pelos pesquisadores. Finalmente, vale mencionar preocupações éticas. O BILGraph foi desenvolvido sem a participação da comunidade indígena [Pinhanez et al. 2023], exceto no caso do Bororo, portanto, é difícil impor diretrizes éticas [Lewis et al. 2020], como proposto pela Declaração de Los Pinos,3 antes que o BILGraph possa ser devidamente inspecionado e validado por falantes indígenas reais.\n",
            "\n",
            "Reconhecemos uma limitação em distinguir formas semelhantes que mapeiam para diferentes lemmas. Embora várias soluções existam, a abordagem mais eficaz tende a ser probabilística, melhorando em precisão com conjuntos de dados maiores. Também focamos em pesquisas adicionais para desenvolver um pipeline que utilize apenas a língua-alvo, sem depender do uso de um dicionário. No geral, esperamos que o BILGraph represente um passo positivo em direção a uma fonte unificada de recursos para BIL, para que mais ferramentas e aplicações possam ser desenvolvidas para elas.\n",
            "\n",
            "3https://unesdoc.unesco.org/ark:/48223/pf0000374030\n",
            "\n",
            "Agradecimentos\n",
            "O segundo autor foi parcialmente apoiado pela bolsa CNPq 305753/2022-3. Também agradecemos o apoio da CAPES - Código de Financiamento 001. Os autores deste trabalho gostariam de agradecer ao Centro de Inteligência Artificial (C4AI-USP) e ao apoio da Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP, bolsa 2019/07665-4) e da IBM Corporation.\n",
            "20\n",
            "Resumo. Garantir a viabilidade de grandes modelos de linguagem (LLMs) em situações que exigem privacidade de dados com recursos limitados no local é um desafio atual significativo. Este trabalho investiga como enfrentar esse desafio usando grafos de conhecimento (KGs) e aprendizado por reforço (RL) para aprimorar LLMs menores, reduzindo respostas não factuais e lacunas nas respostas. Avaliamos variações do GPT (4o, 4 e 3.5), Llama2 (7b, 13b e 70b) e Llama3 (8b e 70b) para classificação multirrótulo e extração de informações, com ou sem KG e RL, e também ajustamos um modelo BERT. O Llama3 8b combinado com KG e RL superou todos os outros modelos de LLM, assim como o modelo BERT ajustado.\n",
            "\n",
            "1. Introdução  \n",
            "Modelos de linguagem grandes (LLMs) como GPT [Liu et al. 2023], Llama [Gao et al. 2023] e Gemini [Team et al. 2023] estão aumentando sua contagem de parâmetros a cada novo lançamento, visando ganhos de desempenho [Xue et al. 2024]. No entanto, essa tecnologia, geralmente disponível nas nuvens de grandes corporações privadas, continua fora do alcance de muitas empresas e projetos que precisam operar em servidores locais [Yao et al. 2024], devido aos altos custos e regulamentações como a Lei Geral de Proteção de Dados (LGPD) [Erickson 2018]. Essas empresas poderiam contar com modelos de código aberto com muitos parâmetros, mas seus requisitos computacionais são muito altos para serem executados localmente [Alizadeh et al. 2023]. \n",
            "\n",
            "Atualmente, há um sutil movimento de pesquisa em direção a LLMs menores de código aberto [Shridhar et al. 2023, Shen et al. 2024] e uma intensa busca por estratégias de otimização. Uma direção promissora é a Geração Aumentada Recuperável (RAG), que utiliza um Grafo de Conhecimento (KG) para adicionar conhecimento formal relevante aos LLMs [Pan et al. 2023]. Essa abordagem foi testada em várias tarefas, incluindo detecção de notícias falsas [Liu et al. 2024], classificação de texto [Shi et al. 2023] e classificação refinada de nós em gráficos de citação e redes [Bruno et al. 2023, He et al. 2023]. No domínio biomédico, essas soluções foram aplicadas em sistemas de recomendação e estudos de interação entre drogas e genes [Xu et al. 2024, Wang et al. 2023], bem como em recrutamento para estudos clínicos [Guan et al. 2023]. No entanto, ainda há poucos exemplos concretos que demonstrem ganhos de desempenho consistentes ao usar abordagens como Graph-RAG [Pan et al. 2023] em tarefas típicas de aprendizado de máquina, como classificação multirrótulo ou extração de informações, especialmente quando se utilizam LLMs de código aberto.\n",
            "\n",
            "Este artigo contribui para preencher essa lacuna de pesquisa, avaliando o sinergismo entre KGs, aprendizado por reforço (RL) e LLMs. Comparamos o desempenho de LLMs relativamente pequenos, como Llama2 (7b e 13b) e Llama3 (8b), com o de LLMs maiores, como GPT (4, 4o e 3.5), Llama2 (70b) e Llama3 (70b), each one alone and combined with the use of KGs and RL, em duas tarefas: (i) classificação multirrótulo de avaliações postadas por usuários de um aplicativo de entrega de alimentos em vários idiomas e suas traduções para o inglês e (ii) extração de informações de faturas de diferentes tipos de retroescavadeiras. Propondo e avaliando abordagens alternativas para explorar KGs específicas do domínio, enriquecemos prompts de LLMs com contexto relevante. Um agente de RL valida as respostas, restringindo-as a rótulos predefinidos, quando disponíveis, e fornecendo feedback aos modelos. Ele valida aleatoriamente algumas respostas de LLM com seus respectivos rótulos ao longo do processo de RL. Também ajustamos e avaliamos um modelo BERT para realizar a mesma classificação multirrótulo, nos mesmos conjuntos de dados.\n",
            "\n",
            "As principais contribuições deste artigo são: (i) uma avaliação sistemática de modelos de linguagem, considerando cada LLM isoladamente e assistido por um KG e/ou um agente de RL; (ii) demonstrar a superioridade de modelos menores de código aberto, como Llama3 8b, quando combinados com KGs; (iii) mostrar a viabilidade de sistemas de feedback para modelos de linguagem; e (iv) aplicar LLMs combinados com KGs e RL em campos inexplorados.\n",
            "\n",
            "2. Abordagem Proposta  \n",
            "A Figura 1 mostra a arquitetura do nosso sistema integrado Graph-RAG e RL para LLMs, projetado para otimizar respostas em tarefas de classificação e extração de informações. O processo começa com uma instrução enviada ao módulo de Aumento de Prompt, complementada por dados de corpora anotados (por exemplo, uma fatura de retroescavadeira). Este módulo interage com o componente de gestão de KG/vetor para buscar na Base de Conhecimento o contexto relevante, acessando o grafo de conhecimento vinculado à instrução. O contexto recuperado é então integrado ao prompt, que o LLM usa para gerar uma resposta. O Agente de RL verifica a saída do LLM em relação aos rótulos disponíveis (dados de treino). Se surgirem imprecisões, feedback é dado ao LLM e os resultados da interação são armazenados no banco de dados de Resultados.\n",
            "\n",
            "2.1. Pré-produção  \n",
            "Na fase de pré-produção, focamos na construção de KGs usando fontes de dados estruturados específicas do domínio. Por exemplo, em testes de extração de informações relacionadas a retroescavadeiras, utilizamos tabelas com descrições de produtos segmentadas em produtos, marcas e modelos. Estes são organizados em uma hierarquia de classes e subclasses, com conexões como “Produto” vinculados por “oferecido por” a “Marca”, que por sua vez se conecta via “tem” a “Modelo”. Assim que a estrutura é definida e validada, geramos embeddings para as classes, subclasses e conexões usando o modelo BGE [Chen et al. 2024]. Os gráficos são então implementados no banco de dados de gráficos Neo4j, incorporando os embeddings gerados. Esses KGs armazenados no Neo4j servem como nossa Base de Conhecimento.\n",
            "\n",
            "2.2. Aumento de Prompt  \n",
            "Na fase de \"Aumento de Prompt\", o sistema processa três entradas: a instrução, descrições textuais sobre gestão de dados e informações relevantes da base de conhecimento. A terceira entrada é obtida por meio de dois métodos: Graph-RAG Direcionado, recuperando informações altamente semelhantes, e Graph-RAG Abrangente, reunindo classes relacionadas e suas inter-relações sem filtros. \n",
            "\n",
            "Os embeddings são gerados a partir das entradas usando o modelo BGE, o mesmo utilizado para grafos de conhecimento (KGs). Em seguida, uma busca de similaridade compara esses embeddings com a base de conhecimento usando similaridade cosseno. O método Graph-RAG Direcionado identifica registros com uma similaridade cosseno acima de 85%, enquanto o método Graph-RAG Abrangente recupera todas as classes, subclasses e conexões relevantes. Por exemplo, ao processar \"RETROESCAVADEIRA JCB CABINE ABERTA 3C\" em uma instrução relacionada à marca, o Graph-RAG Direcionado pode indicar \"97% de probabilidade para JCB e 3% para New Holland\", enquanto o Graph-RAG Abrangente forneceria insights mais amplos, como \"A marca JCB inclui modelos 3CX e 5CX\" e \"New Holland cobre modelos B95C e B115C\". Assim, como mostrado na Figura 2, a saída do Aumento de Prompt consiste em Instrução e Indicador de Saída. Os Dados de Entrada representam uma descrição textual da Gestão de Dados, e o Contexto, neste exemplo, é Graph-RAG Direcionado, que recuperou os dados com as maiores similaridades cosseno de nossa Base de Conhecimento.\n",
            "\n",
            "2.3. LLM  \n",
            "Configuramos o LLM e invocamos sua API usando um prompt enriquecido derivado da fase de Aumento de Prompt. Parâmetros-chave, como temperatura e contagem de tokens de saída, são ajustados. A temperatura controla a aleatoriedade das previsões, com valores mais baixos resultando em resultados mais determinísticos e valores mais altos aumentando a criatividade. Para tarefas de classificação e extração, limitamos a saída a menos de 10 tokens. Utilizamos modelos como Llama2 (7b, 13b, 70b) e Llama3 (8b, 70b) por meio da API Deepinfra, bem como modelos GPT (3.5, 4.0) pela API OpenAI. Com o prompt enriquecido e as configurações de modelo otimizadas, a API é chamada para realizar extração ou classificação. Por exemplo, para a descrição do produto “RETROESCAVADEIRA JCB CABINE ABERTA 3C”, a resposta esperada seria “JCB”.\n",
            "\n",
            "2.4. Agente RL  \n",
            "O Agente de RL processa a saída do modelo LLM verificando se há um rótulo correspondente no banco de dados, conforme detalhado no prompt enriquecido. Os corpora anotados incluem uma porcentagem de dados pré-rotulados distribuídos aleatoriamente, e cada nova saída de LLM é comparada com esses corpora. Por exemplo, se o LLM classifica uma descrição de produto como \"New Holland\" para \"RETROESCAVADEIRA JCB CABINE ABERTA 3C\", o Agente de RL busca nos corpora anotados para verificar se há um rótulo. Se \"New Holland\" estiver correto ou se não houver um rótulo existente, a resposta é validada e armazenada; se estiver incorreta, o agente fornece feedback sugerindo o rótulo correto. Esse processo é repetido até cinco vezes para corrigir e reforçar o aprendizado do modelo. Para tarefas de classificação com rótulos predefinidos, o Agente de RL adota um processo de validação em duas etapas. Primeiro, verifica se a classificação do LLM corresponde aos rótulos predefinidos. Se não corresponder, o agente fornece feedback para alinhar a resposta com as categorias estabelecidas. Na segunda etapa, se a classificação estiver dentro das categorias, o Agente valida-a em relação ao rótulo associado (se houver).\n",
            "\n",
            "3. Cenários de Aplicação  \n",
            "3.1. Classificação Multirrótulo de Avaliações de Entrega de Alimentos  \n",
            "Em nosso primeiro cenário, analisamos um conjunto de dados de cerca de 4.000 avaliações de clientes de um aplicativo de entrega de alimentos europeu, variando de 0 a 889 caracteres, disponível em [Beckhauser e Fileto 2024]. Após remover duplicatas e outliers, permaneceram 3.451 avaliações. Aproximadamente 80% estão em português europeu, com o restante em inglês, espanhol, italiano e catalão. Dada a importância do inglês no treinamento de LLM, criamos um conjunto de dados paralelo traduzindo todas as avaliações para o inglês usando Googletrans, com correções manuais para cerca de 30 avaliações. Em seguida, identificamos termos-chave para cada rótulo removendo stop-words em vários idiomas usando nltk e spaCy e extraindo palavras frequentes com a biblioteca Counter. Para análise de sentimento, utilizamos o modelo SiEBERT [Hartmann et al. 2023], que mostrou desempenho consistente, mesmo quando comparado ao GPT-4 [Krugmann e Hartmann 2024]. Os resultados da análise de sentimento e detalhes do conjunto de dados estão resumidos na Tabela 1.\n",
            "\n",
            "Construímos manualmente um KG em formato de árvore para categorizar as avaliações, distinguindo entre “Produto” (relacionado a itens) e “Pedido” (relacionado a entrega/serviço). Subcategorias como “Problema de quantidade” e “Problema de qualidade” sob “Produto”, e “Problema de entrega” e “Comentário de elogio” sob “Pedido” são ainda mais refinadas com palavras-chave específicas.\n",
            "\n",
            "Tabela 1. Distribuição do conjunto de dados de avaliações por classe, subclasse e sentimento.   \n",
            "Classe   Produto   Produto   Pedido   Pedido   \n",
            "Subclass   Descrição   Problema de qualidade   Problema de quantidade   Problema de entrega   Comentário de elogio   \n",
            "Problemas com preparação de alimentos, gosto ou higiene. Insatisfação com a quantidade ou tamanho das porções servidas. Problemas relacionados a atrasos, entregas erradas ou itens faltando. Comentários positivos sobre a qualidade do serviço ou produto.   \n",
            "#Avaliações   %   Pos.   Neg.   \n",
            "671   605   19.44   26%   74%   \n",
            "17.53   28%   72%   \n",
            "1196   34.66   26%   74%   \n",
            "979   28.37   98%   2%   \n",
            "\n",
            "3.2. Extração de Informações de Faturas  \n",
            "Nosso segundo cenário de aplicação envolve um conjunto de dados de aproximadamente 17.000 descrições de faturas de compra de máquinas de trabalho, incluindo a Nomenclatura Comum do Mercosul (NCM) e valores unitários dos itens, fornecidos por (revisão cega). As descrições das faturas variam de 16 a 120 caracteres. Inicialmente, filtramos o conjunto de dados usando o código NCM, focando nos primeiros quatro caracteres, especificamente \"8429\", que cobre escavadeiras, motoniveladoras, retroescavadeiras e maquinário semelhante. Em seguida, aplicamos um dicionário de palavras-chave para identificar termos relevantes. Retroescavadeiras apareceram com maior frequência, com cerca de 1.100 descrições, tornando-se o foco principal de nossos experimentos. Esse conjunto de dados não tinha classificações iniciais, contendo apenas descrições brutas de faturas. Para facilitar a validação futura do modelo, categorizamos manualmente os dados em classes predefinidas, como marca, modelo e especificações. Um dicionário composto de marcas, modelos, palavras-chave, variações ortográficas, siglas e abreviações foi utilizado, considerando possíveis erros tipográficos. Os campos não cobertos pelos dicionários foram completados manualmente, garantindo uma validação completa das saídas do LLM.\n",
            "\n",
            "KGs para Faturas de Retroescavadeiras. A Figura 3 mostra um extrato de uma ontologia em formato de KG, centrado em maquinaria pesada. Ele retrata o conceito de 'Produto', com ‘Retroescavadeira’ como uma subclasse, vinculada a 16 marcas via a relação \"oferecido por\". Marcas como 'New Holland' e 'JCB' são destacadas, cada uma conectada a modelos específicos através da relação \"tem\". Por exemplo, 'New Holland' inclui modelos como 'B110B' e 'LB90', enquanto 'JCB' oferece '4CX ECO' e '3CX'. No total, 68 modelos estão representados.\n",
            "\n",
            "4. Experimentos  \n",
            "Nesta seção, descrevemos os experimentos conduzidos para classificação multirrótulo com avaliações de clientes, subseção 4.1, e experimentos de extração de informações usando dados de faturas de retroescavadeiras, subseção 4.2. Todos os conjuntos de dados e modelos foram testados em vários cenários distintos: (1) classificação ou extração usando apenas a instrução e o corpus, sem fornecer contexto enriquecido ao LLM; (2) usando apenas o Agente de RL; (3) adicionando uma busca abrangente nos KGs, que retorna todas as classes, subclasses, relações e folhas como contexto; (4) usando contexto direcionado com busca de similaridade acima de 85%, utilizando Graph-RAG; (5) usando Graph-RAG Abrangente com RL; (6) usando Graph-RAG Direcionado com RL. Além disso, para os experimentos de classificação multirrótulo, realizaremos um teste com embeddings e ajuste fino usando BERT. Uma descrição mais abrangente dos experimentos desenvolvidos está disponível no GitHub.\n",
            "\n",
            "4.1. Classificação Multirrótulo de Avaliações de Clientes  \n",
            "Nesta subseção, apresentamos os experimentos realizados para classificação multirrótulo. Os experimentos são realizados em dois subconjuntos de dados de avaliações de clientes: o primeiro contém avaliações de clientes em vários idiomas, e o segundo compreende as mesmas avaliações traduzidas para o inglês. Cada conjunto de dados inclui 3.451 avaliações. Selecionamos aleatoriamente 300 avaliações de cada rótulo para o agente usar como validador durante o processo de classificação, resultando em 1.200 avaliações utilizadas exclusivamente para treinamento de reforço no modelo.\n",
            "\n",
            "No experimento BERT, tokenizamos as avaliações e dividimos em conjuntos de treinamento e teste na proporção 80/20. Usamos BERT para produzir embeddings e uma função de treinamento com um otimizador AdamW e um planejador linear. Para otimizar os hiperparâmetros, configuramos uma função objetiva no Optuna, ajustando a taxa de aprendizado, a decadência de peso e as épocas.\n",
            "\n",
            "Conjunto de dados em inglês: Nesses experimentos, o modelo Llama3 8b, quando combinado com Graph-RAG Abrangente e um agente de RL, alcançou um aumento de 64,7% na cobertura em comparação com o experimento \"Base\", o maior entre todos os modelos e cenários. Sem Graph-RAG Abrangente e o agente, a cobertura caiu drasticamente para 27,2%. O Llama3 8b também se destacou em precisão (93,3%) e F1-Score (92%) nas mesmas condições.\n",
            "\n",
            "O modelo GPT-4o, quando usado exclusivamente com Graph-RAG Abrangente, registrou 90,4% de cobertura, 89,1% de precisão e um F1-Score de 89,1%, superando sua configuração com agentes, onde essas métricas foram 72%, 71,1% e 71%, respectivamente. O GPT-3.5 mostrou estabilidade moderada, com 81,8% de cobertura, 84,5% de precisão e 80,6% de F1-Score usando Graph-RAG Abrangente e agentes. Sem agentes, esses valores diminuíram apenas ligeiramente para 81,3%, 84,1% e 80%. As variantes do Llama2 tiveram um desempenho inferior, particularmente a versão 13b sem agentes. O modelo Llama3 70b melhorou em precisão (92%) e F1-Score (91%) com agentes de RL, mas apresentou desempenho reduzido sem eles. O modelo BERT alcançou 79% de precisão e 76% de F1-Score com condições de contexto geral e agente de RL, mas ainda fica atrás dos modelos Llama3.\n",
            "\n",
            "Conjunto de dados multilíngue: As melhorias na cobertura foram mais modestas para os modelos GPT-4 e GPT-4o, com um aumento de apenas 0,2%, mas mantiveram alta precisão (cerca de 88-89%). Notavelmente, o Llama3 70b apresentou bons resultados em ambos os contextos, com 90,1% de cobertura no cenário multilíngue e alta precisão em todos os conjuntos de dados. No entanto, em nenhum dos cenários, o conjunto de dados multilíngue superou os resultados alcançados com o conjunto de dados em inglês, destacando uma clara lacuna de desempenho. Modelos menores como o Llama2 13b especialmente lutaram em ambos os conjuntos de dados, especialmente em testes multilíngues onde a cobertura permaneceu baixa mesmo com técnicas avançadas. As descobertas enfatizam a superior adaptabilidade de modelos maiores como Llama3 e GPT-4 entre idiomas, enquanto modelos menores enfrentam dificuldades para manter eficácia sem melhorias adicionais.\n",
            "\n",
            "4.2. Extração de Informações de Faturas de Retroescavadeiras  \n",
            "A Figura 5 mostra que o modelo Llama3 8b, quando operado com KGs e agentes de RL, apresenta uma melhoria notável na precisão. Especificamente, a precisão aumentou de uma linha de base de 52,18% para 95,7% quando usado Graph-RAG Direcionado e Agente de RL, demonstrando uma melhoria de 43,52%.\n",
            "\n",
            "O modelo Llama3 8b alcançou a maior precisão de 95,7% e a maior melhoria de precisão entre as configurações, ilustrando sua forte sinergia com KGs e agentes de RL. Por outro lado, sem essas ferramentas, sua precisão diminui substancialmente para a linha de base de 52,18%. Para o modelo Llama3 70b, a maior precisão alcançada foi de 97,21% com Graph-RAG Direcionado e Agente de RL, mostrando um leve aumento de precisão em relação à sua linha de base de 79,93%. Este modelo também exibiu a maior consistência em diferentes configurações. O modelo GPT-4o também apresentou melhorias, alcançando uma precisão de 86,48% com Graph-RAG Abrangente e Agente de RL, o que representa um aumento de aproximadamente 7,25% em relação à sua linha de base de 79,23%. Esses resultados destacam o impacto significativo da utilização de KGs e agentes de RL na melhoria do desempenho de modelos de aprendizado de máquina, especialmente em tarefas que envolvem análise complexa de documentos, como extração de informações de faturas de retroescavadeiras.\n",
            "\n",
            "4.3. Discussão  \n",
            "Este estudo está alinhado com o crescente corpo de pesquisa explorando o potencial dos LLMs para abordar desafios de PNL. Embora esses modelos sejam capazes de lidar com uma ampla gama de tarefas sem a necessidade de dados especializados, em casos mais específicos, eles mostram limitações significativas devido à falta de ajuste fino, especialmente nas versões menores. Os LLMs enfrentam limitações substanciais em suas habilidades de raciocínio, particularmente ao lidar com tarefas que envolvem vários idiomas. Nesses cenários, os LLMs atuais ainda não superam abordagens que utilizam RL, seja por meio de técnicas como Otimização de Política Proximal (PPO), Otimização de Política de Região de Confiança (TRPO) ou Gradiente de Política Determinístico Profundo (DDPG), que requerem ajustes profundos do modelo, tornando sua aplicação consideravelmente cara, ou por meio de técnicas de RL que fornecem feedback textual, como explorado neste trabalho. Consequentemente, abordagens como Graph-RAG ou RL com feedback textual são mais viáveis em termos de custo e complexidade.\n",
            "\n",
            "A combinação de Graph-RAG e RL, ou até mesmo apenas uma dessas técnicas, é mais relevante para modelos menores, que se beneficiam de instruções com contexto e orientação mais detalhada, enquanto modelos maiores tendem a ter um desempenho melhor com dados mais concisos ou, em alguns casos, sem dados adicionais. Mesmo com a aplicação de técnicas como Graph-RAG, modelos maiores mantêm alta eficácia em inglês, mas apresentam quedas de desempenho quando aplicados a conjuntos de dados multilíngues.\n",
            "\n",
            "5. Trabalhos Relacionados  \n",
            "Estudos recentes combinando LLMs com KGs se concentraram em modelos como o GPT-3.5 da OpenAI e o Llama da Meta. O GPT-3.5 foi aplicado em áreas como educação em engenharia [Yang et al. 2023], classificação de texto [Shi et al. 2023] e classificação de nós em estruturas de grafo [Li et al. 2024]. O GPT-4 tem sido usado em sistemas de recomendação e estudos biomédicos [Xu et al. 2024, Guan et al. 2023]. Os modelos Llama2 da Meta mostraram eficácia no processamento de gráficos complexos, com aplicações em sistemas de visão, bancos de dados acadêmicos e domínios de notícias digitais [Gouidis et al. 2024, Hu et al. 2024, Wu et al. 2024]. A provocação de Cadeia de Pensamento (CoT) e técnicas de GNN também foram integradas aos LLMs para melhorar a interpretabilidade do modelo e processar conhecimento estruturado de KGs [Guan et al. 2023, Xu et al. 2023]. Técnicas como PCA, UMAP e métodos de prompt integram ainda mais os LLMs nos domínios visual e estrutural dos KGs, melhorando o aprendizado zero-shot [Gouidis et al. 2024, Alfasi et al. 2024]. Em RL, abordagens como RLHF e RLAIF demonstraram melhorias em resumo, diálogos de negociação e aplicações de conhecimento de domínio [Roit et al. 2023, Kwon et al. 2024, Mandi et al. 2023]. Embora eficazes, RLHF e ajuste fino são caros e praticamente inviáveis para a maioria dos experimentos devido aos significativos recursos computacionais e financeiros exigidos [Ouyang et al. 2022, Nguyen et al. 2023]. Questões persistentes como viés, toxicidade e alucinações permanecem críticas em contextos tanto de KGs quanto de RL [Gouidis et al. 2024, Xu et al. 2024, McKenna et al. 2023]. Diferentemente de trabalhos anteriores, nosso estudo aborda os altos custos de escalabilidade associados ao uso de modelos muito grandes e técnicas tradicionais de ajuste fino, combinando RAG com RL. Demonstramos a eficácia dessa abordagem para classificação multirrótulo e extração de informações usando KGs e conjuntos de dados específicos de domínio.\n",
            "\n",
            "6. Conclusões e Trabalhos Futuros  \n",
            "Este estudo demonstra a viabilidade e eficácia de integrar LLMs com Graph-RAG para aprimorar a classificação multirrótulo e extração de informações. Experimentos realizados com variações dos modelos GPT e Llama, combinados com o uso de KGs e um agente de RL, revelaram melhorias significativas no desempenho de modelos menores, como o Llama3 8b, especialmente quando combinado com Graph-RAG. A combinação de LLMs menores e Graph-RAG reduz a ocorrência de “alucinações”, contribuindo para aprimorar a precisão e eficácia, mesmo em contextos multilíngues. Esses resultados sugerem um futuro promissor para LLMs não tão grandes, especialmente em organizações que enfrentam restrições de privacidade de dados e limitações de recursos computacionais. Como direções de pesquisa futuras, vislumbramos a exploração de KGs mais diversas e a investigação de técnicas de RL para melhorar ainda mais os resultados de tarefas complexas. Além disso, estudos adicionais poderiam aplicar nossa proposta a linguagens de baixo recurso, expandindo sua acessibilidade e aplicabilidade.\n",
            "\n",
            "Agradecimentos: Este trabalho foi apoiado por uma bolsa universal do CNPq de 2022, bolsa FAPESC 2021TR1510, o Projeto de Automação 4.0 Print CAPES-UFSC, e indiretamente pelo projeto Céos, financiado pelo Ministério Público do Estado de Santa Catarina (MPSC).\n",
            "21\n",
            "Resumo. A identificação de aspectos é uma etapa fundamental da Análise de Sentimentos Baseada em Aspectos (ASBA) que consiste em detectar os aspectos alvos de opinião em avaliações de produtos ou serviços publicadas nas mídias sociais. Enquanto existem vários estudos focados na detecção de aspectos na língua inglesa, para o português há poucos trabalhos na área e os LLMs praticamente não têm sido explorados. Dado esse contexto, esta pesquisa investigou o potencial de uso de LLMs na identificação de aspectos em críticas gastronômicas em português. \n",
            "\n",
            "1. Introdução  \n",
            "A análise de sentimentos baseada em aspectos (ASBA) é uma subárea da Análise de Sentimentos (AS) que busca identificar e analisar opiniões e sentimentos relacionados a aspectos ou atributos específicos de uma entidade, produto ou serviço. Em uma avaliação de um restaurante, por exemplo, aspectos como “comida”, “serviço” e “preço” podem ser analisados individualmente, permitindo uma compreensão mais detalhada das opiniões dos clientes sobre cada um deles. A ASBA representa o nível mais complexo da análise automática, devido à dificuldade de se modelar as conexões semânticas entre um determinado aspecto (termo) e as palavras que fazem parte do seu contexto [Zhang et al. 2018]. Uma etapa fundamental da ASBA consiste na identificação de aspectos, os quais podem ser explícitos ou implícitos, de acordo com a literatura [Schouten and Frasincar 2016, Soni and Rambola 2022]. Enquanto o aspecto explícito ocorre diretamente no texto, o aspecto implícito não é mencionado explicitamente, mas pode ser inferido pelo contexto. Por exemplo, na avaliação de um restaurante “A comida estava deliciosa, mas demorou muito para chegar.”, temos um aspecto explícito “comida” com sentimento positivo e um aspecto implícito “serviço” com sentimento negativo. Enquanto para o inglês há uma vasta literatura relacionada à detecção de aspectos [Schouten and Frasincar 2016, Zhang et al. 2018, Soni and Rambola 2022], para o português as pesquisas ainda são emergentes [Pereira 2021]. Além disso, os trabalhos existentes se baseiam principalmente no uso de regras, léxicos e em algoritmos de aprendizado de máquina, sendo que o uso de modelos de linguagem em larga escala (Large Language Model – LLM, no inglês) tem sido pouco explorado. Dado o interesse público por modelos generativos pré-treinados, como os modelos da OpenAI, continua a crescer, espera-se que a utilidade desses modelos em resolver tarefas de PLN seja investigada. Nesse sentido, algumas iniciativas recentes têm surgido [Oliveira et al. 2023, Santos and Paraboni 2023]. Dado esse contexto, este estudo investigou a potencialidade de cinco LLMs na identificação de aspectos explícitos e implícitos em críticas gastronômicas em português. Críticas gastronômicas são textos escritos por críticos profissionais da gastronomia com experiência em avaliar restaurantes, pratos e experiências culinárias. A escolha desse domínio se justifica pelo fato de que as críticas gastronômicas, até onde se sabe, ainda não foram exploradas no contexto da ASBA em português. \n",
            "\n",
            "2. Trabalhos Relacionados  \n",
            "Os trabalhos de identificação de aspectos para o português se baseiam, principalmente, em regras de linguagem [Vargas and Pardo 2020, Machado et al. 2021], aprendizado de máquina tradicionais [Balage Filho 2017, Vargas and Pardo 2018] e no uso de deep learning [Lopes et al. 2021, Assi et al. 2022, Machado and Pardo 2022, Resplande et al. 2022]). Em [Resplande et al. 2022], por exemplo, os autores avaliaram o uso de modelos baseados em Transformers na extração de aspectos em avaliações de hotéis. Os aspectos extraídos foram classificados, posteriormente, como positivos, negativos ou neutros usando o LLM GPT-3. Em um trabalho anterior [Seno et al. 2024], o GPT-3.5 Turbo foi empregado na tarefa de detecção de aspectos e classificação de polaridade em comentários do domínio político. Em [Machado 2023], os autores compararam o uso de LLMs – GPT-3.5, Maritaca e Llama – com um modelo BERT e com vários classificadores tradicionais na identificação de aspectos em revisões de produtos eletrônicos, livros e hotéis. Nos experimentos, os melhores resultados para os aspectos explícitos foram obtidos pelo classificador CRF (o melhor F-score foi 81% para revisões de hotéis). Porém, para os aspectos implícitos o melhor resultado, em termos de porcentagem de acerto, foi obtido com o Llama 7B (52%). De forma similar, este estudo também explorou o uso do GPT-3.5 e dos modelos da família Maritaca na detecção de aspectos em críticas gastronômicas. Porém, os modelos investigados aqui são variações mais recentes das versões usadas por [Machado 2023]. \n",
            "\n",
            "3. Identificação de aspectos em Críticas Gastronômicas usando LLMs  \n",
            "Para a identificação de aspectos em críticas gastronômicas foram explorados alguns dos LLMs mais populares da atualidade como o GPT-3.5 Turbo, o GPT-4 e GPT-4 mini. Segundo a OpenAI, o GPT-4 é o seu modelo mais avançado e inteligente para tarefas. Tabela 1. Prompts usados na anotação de aspectos explícitos e implícitos. Aspectos explícitos: Dada a sentença EXEMPLO com os alvos de opiniões explícitos, identifique os alvos de opinião explícitos na sentença (se houver) no formato [e - alvo1], se não houver nenhum alvo, indique com um ‘-’. EXEMPLO: “A pizza estava gostosa. E a sobremesa também.”. Saída: [e - pizza] [e - sobremesa] Aspectos implícitos: Dada a sentença EXEMPLO com os alvos de opiniões implícitos, identifique os alvos de opinião implícitos (se houver) no formato [i - alvo], se não houver nenhum alvo, indique com um ‘-’. EXEMPLO: “A pizza estava gostosa, mas era muito cara. Além disso, estava fria”. Saída: [i - preço] [i - temperatura] mais complexas. O GPT-4 mini é o modelo mais avançado na categoria de modelos pequenos, que também inclui o GPT-3.5 Turbo. Além desses LLMs, também foram investigados dois modelos monolíngues treinados para o português, o Sabiá-2-medium e o Sabiá-3. Em experimentos reportados por [Almeida et al. 2024], o Sabiá-2-medium é comparado a vários outros LLMs, alcançando desempenho igual ou melhor que GPT-3.5 Turbo em várias análises. O Sabiá-3, por sua vez, lançado em julho de 2024, até o momento da escrita deste artigo não se tinha informações sobre o seu desempenho. Todos os LLMs são modelos generativos baseados em prompt, que recebem como entrada um texto (prompt) contendo a descrição da tarefa a ser realizada e geram as saídas conforme solicitado. O grande desafio em lidar com esses modelos consiste em definir um prompt que gere as saídas exatamente como se espera para a tarefa. Vários prompts diferentes foram testados para a identificação de aspectos explícitos e implícitos no corpus. Foram experimentados prompts específicos para cada tipo de aspecto usando exemplos de anotação humana (i.e. abordagem few-shot) e sem o uso de exemplos de anotação (i.e. abordagem zero-shot). Contudo, percebeu-se uma facilidade maior dos modelos ao usar a abordagem few-shot. Assim, na anotação do corpus foram adotados os prompts apresentados na Tabela 1. Em todos os LLMs investigados a temperatura foi ajustada em zero, a fim de obter modelos mais determinísticos, conforme apontado por outros trabalhos da literatura [Oliveira et al. 2023, Santos and Paraboni 2023]. \n",
            "\n",
            "4. Corpus  \n",
            "Para os experimentos foi usado um conjunto de 1005 sentenças extraídas do corpus de críticas gastronômicas de [Rebechi et al. 2021]. Cada sentença foi anotada por 5 anotadores humanos, todos pesquisadores da área de PLN, em duas etapas. Primeiramente os anotadores classificaram as sentenças em opinativa ou factual. Em seguida, aspectos explícitos e implícitos foram anotados, em dupla/trio, para as 374 (37,2%) sentenças consideradas opinativas pelos anotadores. Para estas, 432 aspectos foram identificados, sendo 88,6% explícitos e 11,4% implícitos. A Tabela 2 apresenta exemplos de sentenças com anotação de aspectos explícitos (em negrito) e implícitos. Dado o fato de que não é possível determinar todos os aspectos possíveis para o corpus, não foi possível calcular o coeficiente Kappa para estimar a concordância entre os anotadores. Embora não se tenha obtido uma estimativa da concordância na anotação do corpus, a busca pelo consenso, seguida da clara convergência dos anotadores, permite assegurar que os aspectos identificados reproduzem de forma bastante fiel os aspectos que geralmente são considerados na avaliação de uma experiência gastronômica. \n",
            "\n",
            "Tabela 2. Exemplos de anotação de aspectos explícitos (em negrito) e implícitos.  \n",
            "Sentença: Se estiver sozinho, desista de tentar o omakassê (sequência de iguarias decididas e enviadas aos poucos pelo chef) — ele é gigante (para uma pessoa) e caro (42 itens, R$ 390). Não é demais lembrar: a casa só aceita dinheiro ou cheque – costume fora de moda, também trazido de outros tempos. Carta de vinhos: Excelente, com muitas opções argentinas para todos os bolsos.  \n",
            "Implícito: tamanho; preço; forma de pagamento; variedade; preço (vinhos);  \n",
            "\n",
            "5. Experimentos e Resultados  \n",
            "A Tabela 3 apresenta os resultados obtidos por cada LLM na detecção de aspectos explícitos e implícitos. O Sabiá-medium-2 obteve o melhor F-score (48,37%) para os aspectos explícitos, alcançando também a maior cobertura (77,75%). Contudo, a maior precisão (40,90%) foi obtida pelo GPT-4 mini. Já no que se refere aos aspectos implícitos, os resultados mostram uma grande dificuldade dos LLMs em identificar esse tipo de aspecto. Vale mencionar que essa dificuldade também foi relatada pelos humanos na anotação do corpus. Como os aspectos implícitos são inferidos pelo contexto, nem sempre é trivial perceber qual é o alvo de opinião. Em alguns casos, essa inferência exige um conhecimento mais especializado como no exemplo “Na boca, é equilibrado, com taninos firmes e boa estrutura.”, que se refere ao aspecto “vinho”. Para esse caso específico, apenas o modelo GPT-3.5 Turbo conseguiu identificar o aspecto implícito.  \n",
            "\n",
            "Tabela 3. Resultados obtidos para aspectos explícitos e implícitos.  \n",
            "LLM                | Precisão Explícitos | Cobertura Explícitos | F-score Explícitos | Precisão Implícitos | Cobertura Implícitos | F-score Implícitos  \n",
            "-------------------|--------------------|----------------------|-------------------|---------------------|----------------------|--------------------  \n",
            "Sabiá-2-medium   | 35,11%           | 77,75%               | 48,37%            | 3,01%               | 26,00%               | 3,68%  \n",
            "GPT-3.5 turbo      | 31,53%           | 1,60%                | 1,95%             | 4,13%               | 32,00%               | 4,01%  \n",
            "Sabiá-3            | 33,21%           | 44,60%               | 2,21%             | 7,00%               | 34,00%               | 7,00%  \n",
            "GPT-4              | 21,51%           | 67,80%               | 3,90%             | 4,00%               | 20,00%               | 5,00%  \n",
            "GPT-4 mini         | 40,90%           | 76,18%               | 2,23%             | 15,82%              | 22,81%               | 5,00%  \n",
            "\n",
            "6. Conclusões  \n",
            "Este estudo investigou o uso de LLMs na detecção de aspectos em críticas gastronômicas. Nos experimentos, o LLM monolíngue Sabiá-2-medium mostrou um potencial maior na detecção de aspectos explícitos do que os modelos multilíngues analisados. Enquanto que o Sabiá-3, também monolíngue, mostrou-se equivalente ao GPT-3.5 Turbo, superando o GPT-4 e o GPT-4 mini. Além de apresentarem desempenho superior ou equivalente aos obtidos pelos modelos multilíngues, os modelos monolíngues são bem mais acessíveis. Com relação aos aspectos implícitos, todos os LLMs tiveram bastante dificuldade em identificar esse tipo de aspecto. O melhor desempenho foi obtido pelo GPT-4 (7% de F-score). Como trabalhos futuros, pretende-se investigar a combinação de LLMs para a tarefa de identificação de aspectos, bem como a utilização de conhecimento do domínio de críticas gastronômicas para enriquecer os prompts.  \n",
            "\n",
            "Referências  \n",
            "Almeida, T. S., Abonizio, H., Nogueira, R., and Pires, R. (2024). Sabiá-2: A new generation of portuguese large language models. ArXiv, abs/2403.09887.  \n",
            "Assi, F. M., Candido, G. B., dos Santos Silva, L. N., Silva, D. F., and Caseli, H. M. (2022). Ufscar’s team at ABSAPT 2022: using syntax, semantics and context for solving the tasks. In Montes-y-Gómez, M. and et al., editors, Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022), volume 3202 of CEUR Workshop Proceedings. CEUR-WS.org.  \n",
            "Balage Filho, P. P. (2017). Aspect extraction in sentiment analysis for portuguese language. PhD thesis, São Carlos - SP.  \n",
            "Costa, R. and Pardo, T. (2020). Métodos baseados em léxico para extração de aspectos de opiniões em português. In Anais do IX Brazilian Workshop on Social Network Analysis and Mining, pages 61–72, Porto Alegre, RS, Brasil. SBC.  \n",
            "Lopes, E., Correa, U., and Freitas, L. (2021). Exploring BERT for aspect extraction in portuguese language. The International FLAIRS Conference Proceedings, 34.  \n",
            "Machado, M., Pardo, T., Ruiz, E., and Felippo, A. (2021). Learning rules for automatic identification of implicit aspects in portuguese. In Anais do XIII Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 82–91, Porto Alegre, RS, Brasil. SBC.  \n",
            "Machado, M. and Pardo, T. A. S. (2022). Evaluating methods for extraction of aspect terms in opinion texts in Portuguese - the challenges of implicit aspects. In Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3819–3828, Marseille, France. European Language Resources Association.  \n",
            "Machado, M. T. (2023). Methods for identifying aspects in opinion texts in Portuguese: the case of implicit aspects and their typological analysis. PhD thesis, São Carlos - SP.  \n",
            "Oliveira, A., Cecote, T., Silva, P., Gertrudes, J., Freitas, V., and Luz, E. (2023). How good is ChatGPT for detecting hate speech in portuguese? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 94–103, Porto Alegre, RS, Brasil. SBC.  \n",
            "Pereira, D. A. (2021). A survey of sentiment analysis in the portuguese language. Artificial Intelligence Review, 54(2):1087–1115.  \n",
            "Rebechi, R. R., Nunes, R. R., Munhoz, L. R., and Marcon, N. O. (2021). Restaurant reviews in Brazil and the USA: A feast of cultural differences and their impact on translation. Mutatis Mutandis. Revista Latinoamericana de Tradução, 14:372–396.  \n",
            "Resplande, J., Garcia, E., Junior, A., Rodrigues, R., Silva, D., Maia, D., Da Silva, N., Filho, A., and Soares, A. (2022). Deep learning Brasil at ABSAPT 2022: Portuguese transformer ensemble approaches. In Montes-y-Gómez, M. and et al., editors, Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022), volume 3202 of CEUR Workshop Proceedings. CEUR-WS.org.  \n",
            "Santos, W. and Paraboni, I. (2023). Predição de transtorno depressivo em redes sociais: Bert supervisionado ou ChatGPT zero-shot? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 11–21, Porto Alegre, RS, Brasil. SBC.  \n",
            "Schouten, K. and Frasincar, F. (2016). Survey on aspect-level sentiment analysis. IEEE Transactions on Knowledge and Data Engineering, 28(3):813–830.  \n",
            "Seno, E., Silva, L., Anno, F., Rocha, F., and Caseli, H. (2024). Aspect-based sentiment analysis in comments on political debates in Portuguese: evaluating the potential of ChatGPT. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Computational Processing of the Portuguese Language: 16th Conference, PROPOR 2024, pages 312–320, Santiago de Compostela, Galicia/Spain. Association for Computational Linguistics.  \n",
            "Soni, P. K. and Rambola, R. (2022). A survey on implicit aspect detection for sentiment analysis: Terminology, issues, and scope. IEEE Access, 10:63932–63957.  \n",
            "Vargas, F. A. and Pardo, T. A. S. (2018). Aspect clustering methods for sentiment analysis. In Computational Processing of the Portuguese Language: 13th International Conference, PROPOR 2018, Canela, Brazil, September 24–26, 2018, Proceedings, page 365–374, Berlin, Heidelberg. Springer-Verlag.  \n",
            "Vargas, F. A. and Pardo, T. A. S. (2020). Linguistic rules for fine-grained opinion extraction. Proceedings of the 14th International AAAI Conference on Web and Social Media, 2020.  \n",
            "Zhang, L., Wang, S., and Liu, B. (2018). Deep learning for sentiment analysis: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8.\n",
            "22\n",
            "Resumo. Apresentamos o TableRAG, um novo pipeline projetado para integrar dados tabulares em sistemas tradicionais de Geração Aumentada por Recuperação (RAG). Nossa abordagem é composta por três partes principais: (i) geração de representações textuais de tabelas; (ii) indexação das representações das tabelas em bancos de dados vetoriais para recuperação e (iii) uso de grandes modelos de linguagem para gerar código SQL ou Python para manipulação de dados sobre uma tabela determinada. Avaliamos a eficácia do TableRAG comparando as precisões de recuperação e reclassificação no benchmark OTT-QA e utilizando tanto LLMs de código aberto quanto de código fechado para gerar código para responder perguntas do benchmark WikiTableQuestions. Nossos melhores resultados mostram 86,7% de HITS@5 para recuperação e 74% de precisão para Q&A, demonstrando a viabilidade de integrar dados tabulares em sistemas RAG com alta precisão. \n",
            "\n",
            "1. Introdução  \n",
            "Nos últimos anos, a aplicação de modelos generativos em sistemas de Pergunta e Resposta (Q&A) ganhou considerável atenção, particularmente em ambientes corporativos onde a recuperação de informações precisa e eficiente é crucial para a tomada de decisões, suporte ao cliente e eficiência operacional. Grandes Modelos de Linguagem (LLMs) mostraram capacidades notáveis em gerar respostas em linguagem natural com base em vastas quantidades de dados textuais, no entanto, esses modelos não estão isentos de desafios—um dos mais significativos sendo o problema da alucinação, onde o modelo gera respostas que parecem plausíveis, mas são incorretas ou sem sentido [Kandpal et al. 2023, Gao et al. 2023, Lin et al. 2023, Tonmoy et al. 2024]. Este problema se torna particularmente pronunciado quando a informação requerida não é puramente textual, mas está embutida em dados semi-estruturados, como tabelas, que podem ser armazenadas em grandes e diversificados bancos de dados, onde a capacidade de recuperá-las e interpretá-las com precisão é essencial. \n",
            "\n",
            "Para mitigar a alucinação e recuperar informações relevantes para um determinado domínio, um paradigma amplamente adotado conhecido como Geração Aumentada por Recuperação (RAG) é empregado. Esta técnica é baseada na premissa de que os LLMs são mais propensos a fornecer respostas precisas quando fornecidos com contexto relevante em tempo de execução, ou seja, dentro do prompt que define as instruções da tarefa, em uma estratégia chamada aprendizado em contexto [Dong et al. 2024]. Métodos tradicionais de Recuperação de Informação (IR) são utilizados para buscar documentos pertinentes, que são então alimentados no modelo de linguagem para geração de texto. \n",
            "\n",
            "Neste trabalho, propomos um pipeline RAG que integra dados tabulares dentro de bancos de dados textuais para recuperação de informações, utilizando tabelas recuperadas para gerar respostas confiáveis (Figura 1). Com base em pipelines tradicionais de RAG, implementamos um módulo de recuperação de tabelas baseado na mesma estratégia de similaridade vetorial comumente usada para textos, e outro módulo para Q&A baseado na geração de código para manipular tabelas. Avaliamos os resultados utilizando dois benchmarks bem conhecidos de Q&A com dados tabulares, Open Table-and-Text Question Answering (OTT-QA) [Chen et al. 2020a] para a parte de recuperação do nosso pipeline, e WikiTableQuestions [Pasupat e Liang 2015] para a parte de Q&A. \n",
            "\n",
            "Nossos resultados mostram que é possível incorporar dados tabulares em sistemas de Geração Aumentada por Recuperação de maneira eficiente, representando tabelas como textos, de forma semelhante a como os sistemas de recuperação de informações tradicionais funcionam. Além disso, mostramos que a geração de código produz bons resultados para a obtenção de respostas a partir de tabelas recuperadas, e destacamos a considerável promessa de melhorar a performance dos LLMs de código aberto, o que pode levar ao desenvolvimento de sistemas RAG mais robustos, adaptáveis e acessíveis, capazes de gerenciar fontes de dados multimodais. \n",
            "\n",
            "2. Trabalhos Relacionados  \n",
            "Diferentemente do Q&A sobre textos, que envolve a leitura de trechos de documentos para responder perguntas usando modelos extrativos ou generativos, o Q&A sobre tabelas envolve fatores adicionais. Para responder a perguntas complexas usando tabelas, é necessário interpretar o arranjo de linhas e colunas, realizar filtragem, junções, operações matemáticas e várias outras formas de manipulação de tabelas. \n",
            "\n",
            "Existem pelo menos três tarefas que foram testadas na literatura para Q&A com dados tabulares. Primeiro, analisar perguntas semanticamente composicionais a fim de determinar os passos de manipulação necessários para obter uma determinada informação de uma tabela. O benchmark WikiTableQuestions [Pasupat e Liang 2015] aborda essa tarefa. Ele compreende tabelas semi-estruturadas, que podem conter dados textuais, e cada pergunta pode exigir operações como consulta à tabela, agregação (contagem de registros, soma de valores numéricos, etc.), superlativos (encontrar o valor máximo ou mínimo), operações aritméticas, entre outros, que precisam ser identificados na pergunta para determinar os passos de manipulação necessários para respondê-la corretamente. \n",
            "\n",
            "Em segundo lugar, manipular tabelas de bancos de dados relacionais utilizando seus relacionamentos com outras tabelas para juntar informações de diferentes fontes. O benchmark Spider [Yu et al. 2018] introduziu essa tarefa. Sua especificidade envolve o uso de chaves estrangeiras, junção de múltiplas tabelas e construção de consultas SQL aninhadas. \n",
            "\n",
            "Terceiro, responder a perguntas de múltiplas etapas em domínios abertos que requerem raciocínio sobre tanto passagens textuais quanto informações tabulares. O benchmark Open Table-and-Text Question Answering (OTT-QA) [Chen et al. 2020a] introduziu essa tarefa. Sua peculiaridade é dupla: o modelo de recuperação deve encontrar a tabela que melhor responde à pergunta a partir de uma grande coleção de tabelas, e o modelo de leitura deve examinar simultaneamente dados de duas modalidades diferentes, textos e tabelas. \n",
            "\n",
            "Para a parte de Q&A, nossa abordagem busca abordar os desafios propostos pelo benchmark WikiTableQuestions, utilizando LLMs para gerar código que manipule tabelas corretamente a fim de encontrar respostas para perguntas semanticamente composicionais. Em 2015, quando o benchmark WikiTableQuestions foi proposto, a abordagem usada para resolver a tarefa envolvia converter tabelas em grafos de conhecimento e analisar as perguntas em forma lógica, seguida pela seleção dos candidatos a grafo mais prováveis para responder à pergunta [Pasupat e Liang 2015]. Os resultados obtidos foram 37,1% de precisão, considerando todas as respostas candidatas, e 76,6%, considerando que pelo menos uma das respostas candidatas era correta. \n",
            "\n",
            "Trabalhos mais recentes abordam a tarefa WikiTableQuestions usando grandes modelos de linguagem. [Yin et al. 2020] obteve um resultado de 52,3% de precisão ao pré-treinar um novo modelo, chamado TaBERT, com dados de 26 milhões de tabelas e seus respectivos contextos em linguagem natural [Liang et al. 2018]. O trabalho de [Liu et al. 2024] usou GPT-3.5 para transpor tabelas a fim de normalizá-las e então usou o mesmo modelo para realizar dois tipos de inferência: com prompting direto (DP), pedindo ao modelo que raciocinasse sobre a tabela em forma textual, e como um agente Python, pedindo ao modelo que interagisse com um shell Python em até cinco interações. Eles alcançam uma precisão de ponta de 73,6% no WikiTableQuestions. \n",
            "\n",
            "3. Componentes do Pipeline TableRAG  \n",
            "A instrução na Figura 2 é um exemplo do prompt direcionado aos modelos de linguagem para gerar código Python. Neste prompt, observamos, além do pedido de geração de código, como as tabelas são representadas textualmente e em consideração às suas metadados. Esses procedimentos serão explicados passo a passo nas próximas subseções. \n",
            "\n",
            "Obtendo Representação Textual das Tabelas  \n",
            "Esta parte da nossa abordagem é inspirada no trabalho de [Abraham et al. 2022]. Dado que nem sempre é viável carregar tabelas inteiras na memória, os autores empregam várias estratégias para representar a tabela por meio de seu esquema, garantindo que a consulta SQL a ser criada seja baseada nesta forma de representação indireta. A ideia funciona particularmente bem no contexto da Geração Aumentada por Recuperação, onde o primeiro passo é recuperar documentos específicos de um repositório diversificado de documentos. No nosso caso, buscamos recuperar as tabelas que melhor respondem à consulta do usuário, utilizando a mesma estratégia para indexação e recuperação de textos. \n",
            "\n",
            "Os metadados gerados para uma tabela consistem em uma descrição textual, gerada por um LLM, juntamente com informações adicionais como nomes de colunas, valores únicos e tipos de dados (numéricos, textuais, datas) para cada coluna da tabela. Outros dados, como o título da tabela, parágrafos que a explicam, significados de siglas e assim por diante podem ser inseridos aqui para enriquecer a descrição textual de cada tabela. Quando a visualização da tabela é passada ao modelo de linguagem, ela é colocada como o último elemento do prompt, para poder ser truncada se exceder o limite de tokens de entrada que o modelo aceita.\n",
            "\n",
            "Renomeação de Colunas: Um erro comum que observamos na construção de código Python e consultas SQL pelos LLMs é o uso de colunas inexistentes, um caso particular de alucinação. Para abordar essa questão, implementamos uma estratégia para renomear colunas para espaços reservados temporários, como Col1, Col2, Col3, … ColN. Isso força os modelos de linguagem a utilizarem colunas com base na descrição do tipo de dado que elas contêm, em vez de seus nomes.\n",
            "\n",
            "Indexação da Tabela: A tabela é então indexada em um banco de dados vetorial através de sua representação textual. A ideia é que o resumo gerado pelo LLM deve ser suficiente para que a tabela seja recuperada, aproveitando a mesma estratégia usada na indexação de texto, transformando descrições de tabelas em vetores contextuais densos.\n",
            "\n",
            "Recuperação, Reclassificação e Filtragem: A recuperação de tabelas é realizada por meio de medidas de similaridade, como similaridade cosseno, comparando a representação vetorial da consulta do usuário com as representações vetoriais das descrições das tabelas. Após a recuperação, empregamos uma etapa de reclassificação pedindo a um LLM para reorganizar as tabelas recuperadas por relevância, e uma etapa de filtragem, pedindo para eliminar as tabelas recuperadas que, apesar de sua semelhança com a consulta do usuário, não conseguem responder à pergunta diretamente. \n",
            "\n",
            "Geração de Código: Em seguida, solicitamos a um LLM que gere código para obter a resposta da tabela, conforme exemplificado na Figura 2. O prompt de geração de código foi meticulosamente ajustado para evitar armadilhas comuns que esses modelos tendem a encontrar, como tentar realizar operações em colunas de dataframe com valores ausentes sem primeiro tratar essas células. Esses ajustes foram baseados nas saídas do GPT-4, tanto para geração de SQL quanto para geração de código Python, e melhoraram nossa precisão em 16% para o mesmo modelo, sendo o procedimento de renomeação de colunas mencionado anteriormente uma das mudanças mais benéficas que fizemos. \n",
            "\n",
            "Geração de Respostas: O código gerado é então executado e a saída é passada para outro LLM, que é responsável por fornecer uma resposta em linguagem natural à pergunta do usuário, considerando a pergunta, os dados e a saída do código executado. \n",
            "\n",
            "4. Metodologia  \n",
            "Para avaliar a qualidade do nosso pipeline, utilizamos dois benchmarks tradicionais, chamados OTT-QA [Chen et al. 2020a], para a parte de recuperação, e WikiTableQuestions [Pasupat e Liang 2015], para a parte de Q&A. As tabelas de ambos os conjuntos de dados foram extraídas da Wikipedia. \n",
            "\n",
            "Embora o conjunto de dados WikiTableQuestions seja ideal para nosso propósito de testar a geração de código para manipulação de tabelas, não é ideal para avaliar a eficiência do nosso módulo de recuperação de tabelas. Isso porque as perguntas são construídas de maneira fechada, o que significa que só podem ser respondidas se já se souber qual tabela elas se referem. Portanto, ao usar o benchmark para avaliar nossas capacidades de Q&A, fornecemos ao modelo de linguagem a tabela correta. Para avaliar a qualidade dos nossos módulos de recuperação e reclassificação, estaremos testando-os contra o benchmark OTT-QA. \n",
            "\n",
            "O benchmark OTT-QA é composto por perguntas baseadas em questões de outro conjunto de dados, HybridQA [Chen et al. 2020b], mas foram adaptadas para se tornarem \"descontextualizadas\", tornando-as de domínio aberto e, portanto, adequadas para testar sistemas de recuperação. Ele contém mais de 400K tabelas para recuperar e 45K perguntas anotadas por humanos. Usamos todas as 2.214 perguntas na partição de desenvolvimento do benchmark para testar recuperação e reclassificação. Usamos HITS@K para medir o desempenho, onde K varia de 1 a 5, e significa se a tabela correta para cada pergunta está entre as tabelas recuperadas ou reclassificadas no topo-K. A reclassificação é feita após a etapa de recuperação, e comparamos resultados com e sem reclassificação. \n",
            "\n",
            "O benchmark WikiTableQuestions contém 2.108 tabelas semi-estruturadas e 22.033 pares de perguntas-respostas complexas. As perguntas e respostas foram construídas por humanos através de crowdsourcing, com instruções para criar perguntas que envolvem vários tipos de operações necessárias para respondê-las, como mostrado na distribuição na Figura 3. \n",
            "\n",
            "Devido a restrições operacionais envolvendo custos de LLMs, todos os testes no WikiTableQuestions foram realizados em uma amostra de 200 perguntas do conjunto de testes. Como mencionado, vários ajustes foram feitos nas instruções do LLM e no método de indexação de tabelas para facilitar a inferência de respostas corretas e a inferência de código que não gerou exceções. Todos os ajustes foram baseados unicamente nos resultados observados para as perguntas do conjunto de dados de treinamento, prevenindo qualquer vazamento de informações do teste para o treinamento. \n",
            "\n",
            "As respostas no WikiTableQuestions são fornecidas como listas com tamanho maior ou igual a 1, com tamanhos maiores que 1 quando mais de um termo é esperado na resposta prevista para que ela seja considerada correta. Assim, usamos a precisão para medir o desempenho do nosso módulo de Q&A, normalizando as strings no conjunto de dados e as strings inferidas pelos modelos, para verificar se todos os termos esperados estão presentes na resposta prevista. \n",
            "\n",
            "Além da precisão, também consideramos o número de códigos gerados que produziram exceções em tempo de execução e o tempo levado para responder a cada pergunta. Em outras palavras, também estamos testando a capacidade desses modelos de gerar código funcional e correto no menor tempo possível. \n",
            "\n",
            "Os cenários em que testamos nosso pipeline são os seguintes:  \n",
            "• Para gerar descrições de tabelas, reclassificar e a resposta final em linguagem natural, usamos consistentemente o modelo GPT 3.5 Turbo da OpenAI, devido ao seu bom desempenho e baixo custo.  \n",
            "• Para gerar representações vetoriais densas de cada descrição de tabela, utilizamos embeddings gerados por um modelo baseado no XLM-RoBERTa [Conneau et al. 2019] ajustado para recuperação de informações [Wang et al. 2024], que são então indexados na plataforma Elasticsearch e recuperados usando a estratégia de similaridade cosseno.  \n",
            "• Para gerar código de manipulação de tabelas, testamos:  \n",
            "– Modelos de código fechado, a saber, GPT 3.5 Turbo e GPT 4 [OpenAI et al. 2024], acessados pela API da OpenAI;  \n",
            "– Modelos de código aberto disponíveis no hub GPT4All [Anand et al. 2023], a saber, LLaMa 3 8B Instruct [Dubey et al. 2024], Nous Hermes 2 Mistral DPO 7B [Jiang et al. 2023], Falcon 7B [Almazrouei et al. 2023], e GPT4All Snoozy 13B [Anand et al. 2023]. Usamos uma única GPU V100 de 32 GB para inferir respostas com esses modelos.  \n",
            "• Todos os modelos de geração de código foram testados gerando consultas SQL (executadas usando a biblioteca Python sqldf) e código para manipulação de dataframe (executado usando a biblioteca Python pandas). \n",
            "\n",
            "5. Resultados  \n",
            "A Figura 4 mostra o desempenho dos módulos de recuperação e reclassificação no pipeline TableRAG para encontrar as tabelas mais adequadas para responder perguntas do OTT-QA. A reclassificação é aplicada sobre as tabelas recuperadas e aumenta os resultados da recuperação em 3,7 pontos em HITS@1, quando a tabela ranqueada primeiro é a correta. A reclassificação ainda é melhor do que apenas a recuperação ao observar HITS@2, mas depois disso, apenas a recuperação se torna melhor do que quando a reclassificação é aplicada. Ao observar HITS@5, os resultados da recuperação são 1,4 ponto melhores do que aqueles com reclassificação. Essa diminuição ocorre principalmente devido a tabelas de menor classificação que o LLM julgou que precisavam ser filtradas da lista durante a reclassificação, quando na verdade eram as corretas. \n",
            "\n",
            "A Figura 5 mostra o desempenho dos diferentes modelos testados para gerar código Python ou consultas SQL para responder às perguntas do WikiTableQuestions. As barras amarelas representam o valor de precisão, enquanto as laranjas representam o número de códigos que produziram exceção em tempo de execução, ambos em % de perguntas. A linha verde representa a média de tempo em segundos levado para responder a cada pergunta. \n",
            "\n",
            "Os melhores resultados vêm dos modelos de código fechado da OpenAI, tanto GPT-4 quanto GPT-3.5. A disparidade entre o melhor modelo de código fechado e o melhor modelo de código aberto pode ser facilmente explicada, entre outros fatores, pelo tamanho dos modelos: enquanto o Nous Hermes 2 tem 7 bilhões de parâmetros em sua arquitetura neural e alcançou 40% de precisão, especula-se que o GPT-4 tenha mais de 1 trilhão, alcançando 74%. \n",
            "\n",
            "A diferença nos resultados entre gerar consultas SQL e código Python para modelos de código fechado é relativamente pequena (74% em Python versus 72,5% em SQL para o GPT-4). No entanto, a diferença no tempo é impressionante: enquanto o GPT-4 levou uma média de 11,5 segundos para gerar código Python por pergunta, gerar código SQL foi muito mais rápido, em média 2,8 segundos. Isso se deve ao fato de que mais pré-processamento é necessário para construir um código Python funcional, e quanto maior o código, maior o tempo de inferência. Para modelos de código aberto, em geral, os melhores resultados são obtidos ao gerar código SQL. O Nous Hermes 2 alcançou 40% de precisão construindo consultas SQL e apenas 20% construindo códigos Python, enquanto o melhor gerador de código Python de código aberto, LLaMA, obteve apenas 26%. \n",
            "\n",
            "6. Conclusões  \n",
            "Apresentamos o TableRAG, um pipeline para integrar dados tabulares em sistemas tradicionais de Geração Aumentada por Recuperação. O pipeline consiste em obter representações textuais de tabelas, indexá-las e recuperá-las como vetores contextuais densos, gerar código SQL ou Python para manipulação de tabelas e gerar uma resposta candidata em linguagem natural. Com o pipeline descrito, alcançamos resultados de até 86,7% de HITS@5 na recuperação e 74% de precisão em Q&A usando o GPT-4. \n",
            "\n",
            "Algumas limitações deste trabalho devem ser consideradas. Devido ao custo computacional e ao tempo de processamento, os resultados para Q&A foram obtidos usando uma amostra de 200 perguntas do conjunto de testes WikiTableQuestions, dificultando a comparação de nossos resultados com aqueles de outros trabalhos utilizando o mesmo benchmark. Além disso, as instruções fornecidas aos modelos de linguagem para geração de código foram ajustadas com base nas saídas do modelo GPT-4, pois foi o modelo com melhor desempenho, mas poderiam também ter sido ajustadas considerando os erros mais frequentes de cada um dos outros modelos individualmente. Os modelos de código aberto não passaram por nenhum processo de ajuste fino, o que poderia melhorar significativamente seus resultados, e não testamos modelos de código aberto maiores do que 13B de parâmetros. Finalmente, não realizamos nenhum pré-processamento nas tabelas para torná-las mais facilmente interpretáveis, como o procedimento de transposição de tabelas realizado por [Liu et al. 2024], que rendeu resultados de ponta com 73,6% de precisão usando GPT-3.5. \n",
            "\n",
            "Agradecimentos  \n",
            "O trabalho foi realizado com a assistência concedida pela Agência Nacional do Petróleo, Gás Natural e Biocombustíveis (ANP), Brasil, associado ao investimento de recursos oriundos das Cláusulas de P&D, por meio do Acordo de Cooperação entre Petrobras e PUC-Rio.\n",
            "23\n",
            "Resumo. Amplia-se a análise de dependência do português brasileiro (pt-br) para lidar com “conteúdo-gerado por usuários” ao desenvolver e anotar o primeiro treebank de tweets (atuais posts do X) em pt-br segundo o modelo Universal Dependencies. O DANTEStocks possui 4,048 tweets do mercado financeiro e anotação-UD de tags PoS e traços morfológicos. Neste artigo, descreve-se a estratégia de anotação sintática adotada para lidar com as idiossincrasias do Twitter e do domínio desse corpus. A versão do DANTEStocks enriquecida com as relações de dependência-UD e as diretrizes de anotação já estão publicamente disponíveis.\n",
            "\n",
            "1. Introdução\n",
            "O projeto Universal Dependencies (UD) [Nivre et al. 2020] especifica uma representação morfológica e sintática completa com o objetivo de facilitar o desenvolvimento de analisadores e etiquetadores multilíngues [Nivre 2016]. A morfologia de uma palavra consiste em 3 níveis de informação: tag PoS, lema e características. A anotação sintática consiste em relações de dependência tipificadas (deprels) entre palavras. Atualmente, o modelo possui 17 tags PoS e 37 deprels, além de um conjunto de características morfológicas não fixo. A Figura 1 mostra um exemplo de um tweet anotado no DANTEStocks. Em uma árvore de dependência, uma palavra é a cabeça da locução (raiz) e todas as outras palavras dependem de outra palavra. Os arcos rotulados representam as deprels, apontando das cabeças para seus dependentes. A tag PoS e o lema de cada palavra são exibidos abaixo do texto. As características morfológicas não estão incluídas nesta figura. No entanto, o token “acordo” (“agreement”), por exemplo, possui as seguintes características e valores de acordo com o UD: Number=Sing, Gender=Masc.\n",
            "\n",
            "Figura 1. Anotação UD de “#BR #BOVESPA #GOLL4 Gol assina acordo de compartilhamento de voos com TAP - http://t.co/wHGukBg7qp”1.\n",
            "\n",
            "Motivados pelo UD, treebanks para novos domínios, gêneros e variedades de linguagem foram recentemente construídos. Entre os treebanks que apresentam conteúdo gerado por usuários (UGC) criados a partir de 2014, um número significativo é composto parcial ou inteiramente por dados do Twitter, cuja linguagem diverge de textos escritos padrão de várias maneiras, trazendo desafios significativos para a construção de treebanks baseados em UD. Esses desafios incluem grafia não padrão, capitalização, pontuação, sintaxe, convenções de plataforma e uso criativo da linguagem, que muitas vezes introduzem muitas palavras desconhecidas. Promovendo consistência entre línguas, diretrizes de UD para anotação de UGC foram fornecidas [e.g. Sanguinetti et al. 2022], no entanto, no que diz respeito a um domínio técnico, estratégias específicas são necessárias. Devido à variedade e complexidade da linguagem, o tratamento adequado dos fenômenos por meio de um modelo já existente, como o UD, é uma tarefa não trivial.\n",
            "\n",
            "Relatamos a anotação sintática do DANTEStocks dentro da estrutura do UD. Primeiro, descrevemos brevemente a segmentação, tokenização e a anotação PoS anterior do corpus (§2). Em seguida, apresentamos as diretrizes de anotação para os UD-deprels (§3). No (§4), detalhamos a abordagem semiautomática para anotar as relações de dependência, incluindo organização de dados, criação de um subcorpus de referência e treinamento de um modelo de análise de ponta para tweets. No (§5), relatamos uma avaliação em pequena escala da anotação sintática. Por fim, colocamos nosso trabalho em contexto e delineamos o trabalho futuro (§6).\n",
            "\n",
            "2. O Corpus DANTEStocks\n",
            "DANTEStocks é um corpus que compreende 4,048 tweets (com limite de 140 caracteres) do domínio do mercado de ações. Foi coletado automaticamente através da busca por posts contendo um ticker2 de uma das 73 ações que compõem o Ibovespa3. Considerando o tweet inteiro como uma unidade básica de análise sintática, os tweets do DANTEStocks não são segmentados em unidades menores (frases, orações ou locuções). Essa decisão economizou o esforço de conduzir uma segmentação manual ou de revisar um processo automático. Além disso, o corpus não foi normalizado para preservar sua diversidade, uma vez que o objetivo era desenvolver aplicações multigênero. Embora focando na sintaxe, apresentamos a segmentação e a anotação morfológica UD anteriores, pois elas contextualizam algumas decisões de anotação.\n",
            "\n",
            "1 “#BR #BOVESPA #GOLL4 Gol assina acordo de compartilhamento de voos com TAP - http://t.co/wHGukBg7qp”\n",
            "2 É uma string alfanumérica de cinco ou seis caracteres que representa um tipo específico de ação de uma empresa, como “PETR4” para as ações preferenciais da Petrobras.\n",
            "3 É o indicador de referência da B3 (“Brasil, Bolsa, Balcão”), que é a principal bolsa de valores do Brasil.\n",
            "\n",
            "Seguindo a visão lexicalista da sintaxe do UD, as palavras sintáticas4 (tokens) foram segmentadas automaticamente por uma versão do NLTK TweetTokenizer5, acrescida de regras específicas para UGC [Silva et al. 2021]. A ferramenta preserva a maioria dos tokens delimitados por espaços em branco, incluindo fonetização (ex.: “d+” > “demais”), hashtags, cashtags6, menções, emoticons e URLs, e separa tokens ortográficos únicos que correspondem a várias palavras (sintáticas), como clíticos, contrações (canônicas e não canônicas), marcas de pontuação (exceto por abreviações), e taxas de valorização e valores monetários com ortografia não convencional. Após a revisão manual da saída da ferramenta, o corpus acaba com um total de 81,037 tokens.\n",
            "\n",
            "A anotação morfológica também foi conduzida de forma semiautomática7 [Silva et al. 2021]. As tags PoS geradas pelo parser UDPipe 2 [Straka 2018], treinadas incrementalmente sobre o UD-Portuguese Bosque [Rademaker et al. 2017] e tweets, foram analisadas manualmente por três anotadores, e os casos de desacordo entre eles foram adjudicados por um linguista sênior com base em diretrizes elaboradas para textos padrão em BP [Duran 2021] e tweets [Di-Felippo et al. 2022]. Todas as 17 tags UD podem ser encontradas no DANTEStocks. PUNCT, NOUN e PROPN são as mais frequentes, com cerca de 16%, 15% e 14% de todas as tags, respectivamente. Lemas e características gramaticais foram obtidos de forma semiaautomática utilizando o léxico PortiLexicon-UD [Lopes et al. 2022]. Ajustes manuais significativos foram necessários para lematização devido à elevada taxa de palavras fora do vocabulário. No que diz respeito a características gramaticais, o cenário foi bastante diferente. A extração de características foi guiada pelas tags PoS e lemas já validados, o que diminuiu o esforço de revisão manual. A maioria das correções estava relacionada a erros decorrentes de ambiguidades sobre características da classe VERB (VerbForm, Mood, Tense, Gender, Number e Person). A revisão manual também concentrou-se em verificar Typo, Abbr e Foreign, que são características que podem ser associadas a palavras pertencentes a todas as classes PoS.\n",
            "\n",
            "Enquanto muitas estruturas sintáticas de tweets poderiam ser anotadas de forma bastante direta usando as diretrizes gerais adaptadas para o português [Duran 2022], muitas delas precisaram de escolhas específicas. Na seção seguinte, discutimos os principais problemas desafiadores para decisões de anotação relacionadas às relações de dependência (deprels).\n",
            "\n",
            "3. Questões de Anotação Sintática\n",
            "3.1. Fenômenos (lexicais) dependentes de meio e domínio\n",
            "Seguindo principalmente as recomendações de Sanguinetti et al., tokens classificados como variação ortográfica da norma padrão por [Scandarolli et al. 2023] foram anotados com seus papéis sintáticos reais, uma vez que são sempre integrados sintaticamente. Essas variações incluem fenômenos de conteúdo gerado por usuários, como substituição, omissão, inserção e transposição de caracteres (ex.: letras, espaços, hífens e diacríticos). Um bom exemplo é o token “nao” (em vez de “não”) (“no”) em (1) “VALE5 nao passa de 29,9”8, que apresenta um caso de omissão de diacríticos. No exemplo, “nao” foi relacionado à raiz “passa” por advmod, já que é um advérbio que modifica um predicado.\n",
            "\n",
            "4 É a unidade básica de anotação que desempenha uma função sintática em uma locução.\n",
            "5 https://www.nltk.org/api/nltk.tokenize.html\n",
            "6 Foi especificamente projetado para acompanhar instrumentos financeiros (ex.: $PETR4).\n",
            "7 A versão do corpus contendo anotação de PoS e características está disponível publicamente em: https://sites.google.com/icmc.usp.br/poetisa/resources-and-tools.\n",
            "8 “VALE5 não excede 29,9”\n",
            "\n",
            "A mesma estratégia foi adotada para tratar a maioria dos fenômenos classificados como “norma inovadora”9 por [Scandarolli et al. 2023] (ou seja, abreviação, neologismo, marca de expressividade e escrita homofona), uma vez que também estão sempre integrados sintaticamente. Pictograma (emoticon/emoji), que é uma marca de expressividade, é o único que ocorre de forma não integrada sintaticamente (isolado), sendo ligado à raiz por discurso. Os outros dois tipos de fenômenos da norma inovadora exigiram diretrizes de anotação quando isolados e integrados sintaticamente (Tabela 1). Para os dispositivos dependentes do meio, o tratamento dado às menções quando precedidas pela marca RT é apenas o que difere da recomendação de Sanguinetti et al. Em vez de considerar a menção como isolada e ligá-la ao predicado principal com vocativo, tratamos como um token integrado sintaticamente anexo à marca RT por nmod. Isso se deve à nossa interpretação de uma preposição elíptica “de” (“of”) (“RT de @user”), indicando uma relação atributiva entre o RT/SYM e o @user/PROPN. De maneira diferente, todos os casos de parataxe envolvendo um fenômeno de UGC no DANTEStocks são anotados com uma sub-relação correspondente, não apenas para URLs e hashtags.\n",
            "\n",
            "Tabela 1. Diretrizes de dependência UD para questões específicas do Twitter e do domínio.\n",
            "Questão UGC     Subtipo       URL      Hashtag       Menção        Integração sintática      Não      Sim      Não      Sim      Não      Sim      Não      Sim      Sim      Truncamento      Código alternativo (intra)      Sim      Sim      Ticker      Não      Sim\n",
            "Cashtag      RT      Papel sintático padrão      Outro      ✓      ✓      ✓      parataxis:hashtag      parataxis:mention   nmod (do RT)      parataxis:url      parataxis:rt      ✓      ✓ (:wtrunc ou :strunc)      ✓ (se conhecido)      ✓      flat:foreign (se desconhecido)      parataxis:cashtag      ✓      Token dependente do meio   Token específico do domínio\n",
            "\n",
            "3.2. Sintaxe não convencional\n",
            "Além de todas as questões linguísticas mencionadas anteriormente, a complexidade da anotação UD também aumenta devido à alta natureza contextual do Twitter, e ao alto nível de fragmentação que parece ser típico no UGC do domínio do mercado de ações. Isso fornece um rico contexto para ambiguidades e elipses, resultando em estruturas sintáticas não convencionais cuja análise UD mais apropriada depende da interpretação do conteúdo do tweet. Um exemplo é nsbuj:pass sem o aux:pass. Para recomendar anexar “#cyre3” à raiz “postado” por nsbuj:pass no tweet da Figura 2, presumimos que o verbo auxiliar está elidido. Na Figura 2, também presumimos uma preposição elíptica (“a”) precedendo “+1,78” para conectar “1,78” por obl. Como a função sintática de “(+)1,78” é ambígua (ou seja, obl de “postado” ou nmod de “abertura”), a escolha de “1,78” como dependente da raiz por obl ilustra decisões de anotação baseadas na interpretação de especialistas do domínio.\n",
            "\n",
            "9 Elas são alternativas lexicais a palavras padrão existentes e dispositivos linguísticos frequentes encontrados na linguagem do Twitter e/ou no domínio do mercado de ações [Scandarolli et al. 2023].\n",
            "\n",
            "Figura 2. Elipse sintática no fragmento “#cyre3 postado hj antes da abertura +1,78”10.\n",
            "\n",
            "3.3. Padrões estruturais\n",
            "Além dos fenômenos (lexicais) de UGC e questões de sintaxe não convencional, também identificamos 22 padrões estruturais recorrentes entre os tweets no DANTEStocks. Tais padrões correspondem a quase 1.000 instâncias do corpus, ou seja, tweets únicos. Para cada padrão, criamos um template para guiar a anotação das instâncias do padrão no corpus. Os 22 templates também compõem as diretrizes de anotação de dependência para o corpus DANTEStocks, bem como as recomendações para o tratamento dos fenômenos lexicais e estruturas não convencionais [Di-Felippo et al. 2024].\n",
            "\n",
            "Mais precisamente, um template contém 3 campos: (i) padrão, ou seja, uma descrição mnemônica, (ii) elementos, ou seja, lista de elementos do padrão e a diretriz de anotação correspondente dentro do UD, e (iii) exemplo, ou seja, pelo menos uma instância atestada do padrão do corpus com sua anotação de dependência UD. É importante mencionar que, uma vez que os padrões geralmente se referem a tweets fragmentados e/ou cheios de elipses sintáticas, a especificação do template é baseada em uma possível interpretação dos tweets, que foi feita com o suporte de especialistas do mercado de ações.\n",
            "\n",
            "Para ilustração, o Template 11 é mostrado na Tabela 2. Ele corresponde a 20 instâncias únicas no corpus. Como o padrão do template representa tweets muito fragmentados, os especialistas do domínio nos ajudaram a interpretar a locução do corpus, como a da Tabela 2, como composta por três blocos de informação, resultando na seguinte descrição de padrão: <hashtag-ticker><tema><url>. O <tema> fornece informações sobre uma ação específica, codificada pelo <hashtag-ticker>, e foi considerado a principal informação da locução. Uma vez que o <tema> é sempre introduzido pela expressão coordenada “suportes e resistências”11, o primeiro elemento da expressão (ou seja, “suportes”) é a raiz, como indicado no campo “elementos”. No campo “elemento”, também é indicado que o <hashtag-ticker> é dependente da raiz com a tag nmod, devido à interpretação de “#VALE” como um nominal que funcionalmente corresponde a um modificador de outro substantivo (“suportes”). Uma vez que a relação nmod geralmente é introduzida por uma preposição (tag ADP) em português, assumimos, para propor o template, que há uma preposição elíptica “de(+a)” (ou seja, “suportes e resistências da VALE4”) (“suporte e resistência” de #VALE5). Finalmente, o <url> é dependente da raiz com parataxis:url porque é um segmento contínuo.\n",
            "\n",
            "10 “#cyre3 postado hoje antes da abertura +1,78”.\n",
            "11 Termos que indicam níveis de preço onde uma ação específica tende a rejeitar a tendência atual e reverter, ou seja, indicam pontos de virada potenciais no preço de uma ação.\n",
            "\n",
            "Tabela 2 Template para anotação de dependência UD de tweets com padrão estrutural.\n",
            "Padrão     Elementos     a. <hashtag-ticker> é dependente da raiz com o rótulo nmod   <hashtag-ticker> <tema> <url>, onde:   b. <tema> contém a expressão “suportes e resistências”; “suportes” é a raiz   c. <url> é dependente da raiz com a tag parataxis:url   Exemplo     #VALE5 suportes e resistências http://t.co/c8OrWXrECN\n",
            "\n",
            "4. Abordagem de Anotação Sintática\n",
            "A anotação baseada em dependência do DANTEStocks foi realizada em duas etapas semiautomáticas [Barbosa 2024]. A primeira teve como objetivo criar um subcorpus de referência e a segunda etapa da anotação se concentrou no ajuste de um parser pré-treinado para tweets usando o subcorpus de referência como parte de seu conjunto inicial de treinamento. Para iniciar a anotação sintática, todos os 4,048 tweets foram agrupados em três conjuntos principais, capturando tweets com: (i) linguagem relativamente padrão, (ii) padrões estruturais recorrentes, e (iii) outros (tweets que não pertencem a nenhum dos primeiros dois conjuntos). Os tweets foram classificados através de agrupamento k-means [Macqueen 1967] com tf-idf (“frequência de termo–frequência inversa de documento”) [Luhn 1957].\n",
            "\n",
            "4.1. Criação de um Subcorpus de Referência\n",
            "A organização dos tweets em conjuntos, como mencionado acima, nos permitiu selecionar algumas instâncias de cada conjunto, cobrindo toda a diversidade lexical e estrutural do DANTEStocks para compor um subcorpus de referência de 1,000 tweets. Além disso, como uma tentativa de alcançar consistência na anotação, particularmente dado a linguagem não canônica do corpus, a anotação semiautomática do subcorpus também foi baseada nessa classificação. Isso significa que os dados de cada conjunto maior foram revisados manualmente de forma separada.\n",
            "\n",
            "Para criar um subcorpus padrão, também utilizamos o parser UDPipe 2 treinado sobre o UD-Portuguese Bosque para anotar os 1,000 tweets. O subcorpus anotado em UD foi posteriormente revisado manualmente por um único especialista. Aproveitando a experiência anterior do especialista na anotação UD de textos jornalísticos e o treinamento do UDPipe 2 sobre Bosque, a revisão manual começou com os tweets que apresentavam linguagem relativamente padrão. Os próximos tweets foram aqueles com padrões estruturais recorrentes, e finalmente os tweets com uma variedade de características lexicais e estruturais. Durante o processo de revisão, as questões desafiadoras descritas na Seção 3 foram discutidas, e as decisões de anotação deram origem às diretrizes para o tratamento de tweets do domínio do mercado de ações dentro da estrutura do UD [Di Felippo et al. 2024]. As diretrizes foram usadas para apoiar a revisão manual do resto do corpus, que foi feita ao treinar um parser de ponta nos tweets do DANTEStocks. Após a revisão do subcorpus, acabamos com um subconjunto padrão de 1,000 tweets anotados sintaticamente.\n",
            "\n",
            "4.2. Treinamento do Modelo de Análise\n",
            "O restante do corpus foi anotado personalizando o Stanza [Qi et al. 2020] para o DANTEStocks. O Stanza é um modelo pré-treinado bem conhecido para o português, tendo a vantagem de ser uma pipeline amigável ao usuário para análise de texto. O procedimento começou com a arquitetura base do Stanza, ajustada no Porttinari-base [Duran et al. 2023], que é um corpus jornalístico composto por 8,418 sentenças (168,080 tokens) anotadas manualmente com UD, e o subcorpus de referência. Para a primeira execução do Stanza, abrangendo o Porttinari-base e o subcorpus de referência como conjunto de treinamento inicial, foi aplicada a mesma distribuição de dados encontrada no Porttinari-base12, resultando em um conjunto de dados de 9,893 amostras, sendo 70% para treinamento, 10% para validação e 20% para teste. O parser resultante foi usado para anotar um novo pacote de dados (fora os 3,048 tweets restantes), que foi revisado manualmente e incorporado ao conjunto de dados anterior, sendo então usado para iniciar uma nova execução de treinamento do Stanza. Esse ciclo continuou incrementando até que o último pacote de tweets fosse anotado/revisado.\n",
            "\n",
            "Além da primeira iteração de treinamento, realizamos cinco execuções de treinamento, adicionando pacotes de 203, 300, 400, 400 e 1233 tweets por iteração, respectivamente (totalizando 2,536 tweets). O modelo resultante da 6ª (final) execução foi usado para anotar os 512 tweets restantes. Os pacotes de tweets foram adicionados na mesma ordem da revisão manual do subcorpus de referência: tweets em linguagem padrão, tweets de padrões estruturais e tweets com propriedades lexicais/estruturais variadas.\n",
            "\n",
            "Para cada uma das cinco execuções, mantivemos, sempre que possível, a mesma distribuição de dados para treinamento, validação e teste usada na primeira iteração, e computamos o desempenho do Stanza com base na Unlabeled Attachment Score13 (UAS) e na Labeled Attachment Score14 (LAS). A precisão UAS aumentou de 94.46% na primeira execução para 95,78% na última (6ª) iteração, tornando-se 1,32% melhor. Para LAS, a precisão final (6ª execução) alcançou 94,62%, aumentando 0,76% da precisão da primeira execução de 93,86%. O aumento das medidas de relação de dependência indica que a capacidade do modelo de capturar as estruturas sintáticas dos tweets melhorou à medida que incorporamos novos tweets aos conjuntos de treinamento. Para fins de comparação, a precisão do melhor modelo para textos jornalísticos em português também estava em torno de 96% (UAS) e 95% (LAS) [Lopes e Pardo 2024]. A Figura 3 ilustra a distribuição geral das relações de dependência (sem subrelações) no DANTEStocks.\n",
            "\n",
            "Figura 3. Distribuição de frequência do conjunto de rótulos de deprel do UD no DANTEStocks.\n",
            "\n",
            "12 As 8,418 sentenças foram divididas em conjuntos de treinamento, desenvolvimento e teste, com 70% (5,893 sentenças), 10% (842 sentenças) e 20% (1,683 sentenças) do corpus, respectivamente.\n",
            "13 UAS indica a precisão da cabeça ignorando o nome da relação (deprel) [Nivre e Fang, 2017].\n",
            "14 LAS avalia a saída de um parser considerando quantas palavras foram atribuídas tanto à cabeça sintática correta quanto ao rótulo correto, ignorando subrelações [Nivre e Fang 2017].\n",
            "\n",
            "5. Confiabilidade da Anotação\n",
            "Para fornecer uma medida de confiabilidade da anotação do DANTEStocks, um segundo especialista em PLN (também com experiência em anotação UD) revisou manualmente a anotação automática de 100 tweets aleatórios com base nas mesmas diretrizes [Duran 2022; Di Felippo et al. 2024]. As árvores de dependência analisadas pelo anotador adicional poderiam ser do subcorpus de referência ou geradas pelo Stanza em uma de suas interações. O índice de Acordo Inter-Anotador (IAA) foi calculado usando o coeficiente Kappa [Cohen, 1960; Carletta, 1996] em dois contextos diferentes [Barbosa 2024]. No primeiro, o foco foi avaliar a anotação da cabeça e da deprel separadamente. Os resultados do Kappa para cabeça e deprel foram 0.96 e 0.97, respectivamente. No segundo contexto, a avaliação tinha como alvo a combinação de cabeça e deprel, obtendo o score Kappa de 0.95. O IAA por deprel foi medido usando a pontuação de acordo total [Sobrevilha Cabezudo 2015], uma vez que o Kappa não é apropriado dada a distribuição não equilibrada das relações. Obtivemos o acordo total de 100% para mais da metade das 46 diferentes deprels (incluindo subrelações) que ocorrem na amostra de 100 tweets. Das 1,743 relações anotadas, existem 42 casos de desacordo. O conflito mais frequente foi entre obl e nmod. Alguns deles foram causados por diferentes interpretações potenciais sobre o papel funcional da frase preposicional (em negrito) em estruturas como “arrisque vd em #petr4” (“arriscar venda em #petr4”). Enquanto um anotador anexou “petr4” ao verbo via obl, funcionando como um argumento ou adjunto não núcleo (oblíquo), o outro assumiu que “petr4” é um modificador do substantivo “vd” (“venda”), sendo anexado a ele por nmod. Também é interessante que, entre as 22 deprels com acordo total diferente de 100%, 12 delas contenham subrelações, indicando que a anotação é mais complexa ao usar relações específicas de linguagem. Embora uma avaliação em pequena escala, os resultados indicam que o IAA geral foi bastante alto, especialmente para a tarefa desafiadora. Isso pode ser devido às extensas e detalhadas recomendações de nossas diretrizes para a anotação sintática dos tweets.\n",
            "\n",
            "6. Considerações Finais e Trabalho Futuro\n",
            "Descrevemos nosso esforço para construir o primeiro treebank BP para microtextos do Twitter, anotados dentro da estrutura do UD. As contribuições são o próprio treebank, a implementação das diretrizes do UD para tweets do mercado de ações em BP, e a personalização de um parser de ponta atual para tweets. Nossa principal dificuldade foi interpretar os tweets, devido aos fenômenos lexicais dependentes de meio e domínio e construções incomuns. Assim, apesar da constante ajuda de especialistas do domínio, podemos dizer que a anotação de dependência de muitos tweets no DANTEStocks (especialmente aqueles com fragmentação, ex.: texto abortado) representa uma análise sintática potencial dos tweets. Atualmente, os dois anotadores envolvidos neste trabalho estão analisando os desacordos para atribuir uma deprel consensual para cada caso e disponibilizar o treebank em breve. As diretrizes para a anotação sintática baseada em UD do DANTEStocks e o próprio treebank (versão beta) estão disponíveis na página do projeto POeTiSA (https://sites.google.com/icmc.usp.br/poetisa/).\n",
            "\n",
            "Agradecimentos. Este trabalho foi realizado no Centro de Inteligência Artificial da Universidade de São Paulo (C4AI - http://c4ai.inova.usp.br/), com apoio da Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP, grant #2019/07665-4) e da IBM Corporation. O projeto também contou com o apoio do Ministério da Ciência, Tecnologia e Inovações, com recursos da Lei N. 8.248, de 23 de outubro de 1991, no âmbito do PPI-SOFTEX, coordenado pela Softex e publicado como Residência em TIC 13, DOU 01245.010222/2022-44.\n",
            "24\n",
            "Resumo. Este artigo aborda o problema da detecção e censura de linguagem ofensiva em extensos textos em português brasileiro na web. Este trabalho propõe um fluxo de trabalho para classificar e censurar textos extensos, focando em comentários, publicações e artigos, utilizando técnicas de PLN. Os resultados incluem uma revisão aprofundada dos métodos atuais para classificação de conteúdo ofensivo em português e a implementação de um fluxo de trabalho baseado em BERTimbau para detecção de ofensas. Este trabalho representa um avanço significativo no estado-da-arte da PLN em português, promovendo ambientes online mais seguros e respeitosos para os usuários, especialmente crianças. \n",
            "\n",
            "1. Introdução \n",
            "\n",
            "Nos últimos anos, a Internet tem crescido a uma taxa impressionante em termos de usuários e dos dados gerados pela publicação de páginas web. O uso crescente da Internet muitas vezes introduz muitas crianças em ambientes virtuais desde muito cedo. Consequentemente, a linguagem ofensiva em textos online se torna uma preocupação por razões éticas [Economist 2019]. Questões como cyberbullying, discurso de ódio e várias formas de conteúdo ofensivo em postagens de redes sociais também são relevantes [Cook 2024]. \n",
            "\n",
            "Em relação à interpretação inteligente de dados textuais provenientes de redes sociais, o Processamento de Linguagem Natural (PLN) é comumente utilizado. Para abordar o problema da linguagem ofensiva em postagens web, foram desenvolvidos artigos no campo da PLN, combinados com técnicas de Aprendizado de Máquina (AM) e Aprendizado Profundo (AP) [Hajibabaee et al. 2022]. Esses esforços incluem a criação de fluxos de pesquisa e desenvolvimento de conjuntos de dados anotados de alta qualidade por profissionais [Leite et al. 2020]. \n",
            "\n",
            "Dadas as diferenças nas línguas e culturas, é possível formalizar um modelo apenas para algumas línguas. No entanto, esforços estão sendo feitos para aprender várias línguas [Husain e Uzuner 2021]. O corpus HateBR [Vargas et al. 2022] é um exemplo para o português brasileiro. Tal artigo evidencia a falta de produção acadêmica relacionada à linguagem ofensiva na língua nacional, juntamente com o conjunto de dados e seus resultados. Apesar das contribuições acadêmicas existentes para lidar com a linguagem ofensiva em português brasileiro, aplicar técnicas de PLN e AM/AP para classificar textos mais longos (como artigos de notícias ou postagens de blog) continua sendo desafiador. Essas técnicas geralmente são limitadas a postagens em redes sociais ou comentários em notícias. \n",
            "\n",
            "Com base nessas informações, o presente trabalho foca em detectar e, posteriormente, censurar ou filtrar a linguagem ofensiva online. O objetivo é garantir que páginas web que não são explicitamente focadas em conteúdo adulto possam ser ambientes adequados para crianças. Além disso, esforços são feitos para reduzir preconceitos e ofensas em postagens online, beneficiando o bem-estar emocional dos usuários e promovendo o respeito mútuo. Especificamente, propomos identificar palavras ofensivas usando técnicas de AP, que podem então ser filtradas ou censuradas. Definimos pequenos textos como comentários, postagens e frases, enquanto parágrafos e páginas inteiras constituem textos mais longos, sendo estes últimos o foco deste estudo. \n",
            "\n",
            "Como as principais contribuições deste trabalho, temos: 1) Uma revisão aprofundada dos métodos de ponta aplicados à classificação de conteúdo ofensivo para a língua portuguesa; 2) Uma atualização sobre os resultados de estado-da-arte no conjunto de dados HateBR [Vargas et al. 2022]; e 3) Um fluxo de trabalho baseado em Aprendizado Profundo que classifica e censura efetivamente textos extensos em português brasileiro com base em sua ofensividade. \n",
            "\n",
            "2. Trabalhos Relacionados\n",
            "\n",
            "Ao considerar trabalhos que abordam o processamento de texto com linguagem ofensiva no contexto brasileiro, incluindo ToLD-BR, a gama de trabalhos existentes é muito pequena [Leite et al. 2020], OffComBr [Pelle e Moreira 2017], HateBR [Vargas et al. 2022] e OLID-BR [Trajano et al. 2023]. Esses métodos são explicados nas seções seguintes. \n",
            "\n",
            "2.1. ToLD-BR \n",
            "\n",
            "Este conjunto de dados [Leite et al. 2020] apresenta um número não especificado de postagens extraídas da plataforma Twitter. Um total de 42 indivíduos, escolhidos entre 129 voluntários, foram encarregados de anotar cada postagem, classificando-as em várias categorias de preconceito: homofobia, linguagem obscena, misoginia e xenofobia. \n",
            "\n",
            "As postagens foram classificadas usando algoritmos ao estilo BERT [Devlin 2018], que alcançaram resultados ótimos para uma tarefa tão complexa e subjetiva. Os pesquisadores também exploraram a possibilidade de construir modelos para essa tarefa em múltiplas línguas, mas seus resultados indicaram que dados monolíngues ainda são preferíveis para classificações mais precisas. \n",
            "\n",
            "2.2. OffComBr \n",
            "\n",
            "Este artigo apresenta o desenvolvimento de um conjunto de dados que compreende comentários em artigos de notícias provenientes do site g1.globo.com, denominado “OffComBr” [Pelle e Moreira 2017]. Os pesquisadores obtiveram cerca de 10.000 comentários, mas dado o processo de anotação manual realizado por três especialistas em detecção de linguagem ofensiva, incluíram apenas 1.250 comentários no conjunto de dados final. \n",
            "\n",
            "Os autores realizaram dois algoritmos de classificação (SMO e Naive Bayes) para avaliar o conjunto de dados, com diferentes avaliações dependendo de diferentes métodos de pré-processamento de dados. Duas versões do conjunto de dados foram desenvolvidas, OffComBR-2 e OffComBR-3, com a diferença sendo o tamanho, pois a última reteve da primeira apenas as anotações acordadas por todos os três especialistas. \n",
            "\n",
            "2.3. HateBR \n",
            "\n",
            "O trabalho de [Vargas et al. 2022] apresentou o primeiro grande corpus anotado de linguagem ofensiva em comentários do Instagram em português brasileiro. Motivados pela presença de discurso de ódio nas redes sociais e pela falta de estudos sobre o assunto em português, o projeto coletou 7.000 comentários do Instagram, anotados por especialistas quanto à presença, grau e categoria de ofensividade. O processo envolveu coleta de dados, seleção de contas de figuras políticas brasileiras (três de esquerda e três de direita) e a seleção de 30 postagens das quais foram extraídos 15.000 comentários, sendo 7.000 balanceados entre ofensivos e não ofensivos. \n",
            "\n",
            "Os comentários foram rotulados em três níveis de ofensividade: se eram ofensivos ou não, o grau de ofensividade (leve, moderado ou alto) e se continham discurso de ódio, categorizado em nove tipos como xenofobia, racismo e homofobia. Dos 7.000 comentários, 3.500 eram ofensivos, dos quais 778 eram altamente ofensivos, 1.044 moderadamente ofensivos e 1.678 levemente ofensivos. Entre os comentários ofensivos, 727 continham algum tipo de discurso de ódio. A Tabela 1 apresenta amostras de dados do HateBR. \n",
            "\n",
            "Tabela 1. Exemplos de comentários extraídos do conjunto de dados HateBR.\n",
            "\n",
            "Classe Ofensivo Não Ofensivo Com discurso de ódio Sem discurso de ódio Comentários Essa besta humana é o câncer do País, tem que voltar para a jaula, urgentemente! E viva o Presidente Bolsonaro. Quem falou isso para você deputada? O Sergio Moro está aprovado pela maioria dos brasileiros. Vagabunda. Comunista. Mentirosa. O povo chileno não merece uma desgraça dessa. Pois é, deveria devolver o dinheiro aos cofres públicos do Brasil. Canalha. \n",
            "\n",
            "Finalmente, após uma explicação detalhada de todo o seu sistema de anotação, bem como avaliações para julgar as anotações de cada um dos três especialistas e decidir as anotações mais apropriadas para cada comentário, o estudo apresenta os resultados do teste com alguns modelos de AM treinados no corpus HateBR, comparando o melhor resultado obtido com os melhores resultados de dois outros trabalhos de referência. Neste trabalho, buscamos replicar os resultados obtidos pelo HateBR, seguindo o mesmo procedimento de treinamento e usando os mesmos modelos para comparação com nosso modelo treinado. \n",
            "\n",
            "2.4. OLID-BR \n",
            "\n",
            "O trabalho de [Trajano et al. 2023] também desenvolveu um conjunto de dados anotados de comentários ofensivos em português, semelhante ao HateBR. No entanto, a principal vantagem deste conjunto de dados reside em sua aplicação a várias tarefas de PLN, incluindo classificação binária de ofensividade, previsão de múltiplas categorias do tipo de toxicidade, identificação de comentários tóxicos direcionados, previsão do alvo da toxicidade e identificação de intervalos de toxicidade em comentários. \n",
            "\n",
            "O foco principal do trabalho foi na tarefa de identificação de intervalos de toxicidade, que envolve detectar sequências de caracteres contendo linguagem ofensiva. Para coletar dados, o OLID-BR utilizou várias fontes, como Twitter, YouTube e outros conjuntos de dados com diferentes esquemas de anotação. \n",
            "\n",
            "A anotação foi realizada em três etapas: detecção de linguagem ofensiva, categorização de linguagem ofensiva e identificação do alvo da ofensa. Em comparação com o HateBR, o OLID-BR distingue entre ofensividade contra um indivíduo, um grupo ou outro tipo de alvo, enquanto o HateBR foca na categorização do discurso de ódio. \n",
            "\n",
            "A anotação de dados no OLID-BR não foi feita exclusivamente por humanos, mas também com a assistência da API Perspective1, permitindo que anotadores humanos corrigissem as classificações. Todo o corpus foi dividido em três conjuntos de dados para treinamento e teste, com uma distribuição semelhante das classificações em cada um. Este trabalho replicou a parte do OLID-BR relacionada à identificação de intervalos de toxicidade, usando o código disponível no GitHub para treinar o modelo e aplicá-lo ao conjunto de dados HateBR para detectar frases ofensivas e ao OLID-BR para identificar intervalos ofensivos. \n",
            "\n",
            "3. Principais Tecnologias \n",
            "\n",
            "Para o desenvolvimento, treinamento e teste de técnicas para detecção e censura de linguagem ofensiva, utilizamos principalmente duas bibliotecas disponíveis para a linguagem Python para desenvolver algoritmos de AM: Transformers e spaCy. \n",
            "\n",
            "3.1. Transformers \n",
            "\n",
            "A biblioteca Transformers é uma ferramenta Python que oferece arquiteturas de ponta para tarefas de PLN, com mais de 32 modelos pré-treinados em mais de 100 idiomas. Ela proporciona profunda interoperabilidade entre TensorFlow 2.0 e PyTorch. A biblioteca é nomeada após a arquitetura Transformer introduzida pelo Google Brain em 2017, que se baseia no \"mecanismo de atenção\". Este mecanismo permite que o modelo se concentre em partes importantes dos dados de entrada, levando a um desempenho superior em tarefas de PLN, como classificação de sentenças, reconhecimento de entidades nomeadas (NER) e geração de linguagem natural em comparação com modelos anteriores como redes neurais recorrentes (RNNs) [Vaswani et al. 2017]. \n",
            "\n",
            "3.2. BERT e BERTimbau \n",
            "\n",
            "O algoritmo usado na primeira etapa de nosso fluxo de trabalho foi uma versão ajustada do BERTimbau [Souza et al. 2020], um modelo brasileiro baseado em BERT (Representações de Codificador Bidirecional de Transformers) [Devlin 2018]. O BERT, apresentado pelo Google em 2018, é um modelo de linguagem que gera representações numéricas para palavras com base em seu contexto circundante e é utilizado para várias tarefas de PLN. O BERTimbau adapta o BERT para o português brasileiro usando aprendizado por transferência, onde um modelo BERT foi treinado em um corpus em português (brWaC) [Wagner Filho et al. 2018] e avaliado em tarefas como similaridade entre sentenças, implicação textual e reconhecimento de entidades nomeadas. Neste trabalho, o BERTimbau foi utilizado especificamente para desenvolver um modelo para detectar linguagem ofensiva em sentenças. \n",
            "\n",
            "3.3. SpaCy \n",
            "\n",
            "O SpaCy é uma biblioteca de PLN de código aberto para Python, escrita em Cython, que facilita tarefas como etiquetagem de partes do discurso, reconhecimento de entidades nomeadas (NER) e análise de dependência [Honnibal et al. 2020]. Ela proporciona modelos pré-treinados e permite que os usuários treinem seus próprios modelos para NER, onde sentenças são segmentadas em palavras, cada uma categorizada (por exemplo, substantivos, advérbios ou categorias relacionadas a problemas específicos como ofensivas e não ofensivas). Neste trabalho, o SpaCy foi utilizado na segunda etapa para detectar quais palavras em uma sentença ofensiva são ofensivas, seguindo a metodologia do OLID-BR, que também usou o SpaCy para tarefas como detecção de intervalos ofensivos em texto. \n",
            "\n",
            "4. Metodologia \n",
            "\n",
            "Este estudo focou no desenvolvimento de modelos de PLN para detectar linguagem ofensiva em textos extensos. Tanto a aquisição de dados de treinamento quanto a censura real de palavras detectadas como ofensivas foram realizadas de forma simplificada, pois não eram o foco principal. O processo de desenvolvimento consistiu em quatro etapas: coleta de dados, limpeza/tokenização, treinamento de modelo e avaliação de resultados, explicadas na subseção seguinte. \n",
            "\n",
            "4.1. Coleta de Dados \n",
            "\n",
            "Para a coleta de dados, utilizamos os conjuntos de dados HateBR [Vargas et al. 2022] e OLID-BR [Trajano et al. 2023] para treinar os modelos de detecção de conteúdo ofensivo, o primeiro para classificar um segmento de texto como potencialmente ofensivo ou não e o último para identificar palavras ou expressões que contêm ofensividade. Além disso, para avaliar qualitativamente nossa solução, selecionamos artigos de notícias do portal G1 [Monteiro 2023], Catraca Livre [Leray 2023], e duas postagens de blog do Senso Incomum [Trielli 2021, Martins 2022]. \n",
            "\n",
            "4.2. Limpeza de Dados e Extração de Recursos \n",
            "\n",
            "Utilizamos ferramentas das bibliotecas Transformers e spaCy para limpeza de dados e extração de recursos. Especificamente, para o modelo treinado com a biblioteca Transformers, empregamos um tokenizador pronto para transformar sentenças em representações numéricas usadas pelo modelo para cálculos. Para o modelo treinado com o spaCy, uma funcionalidade de tokenização incorporada estava disponível. Em resumo, nesta etapa, os modelos treinados foram capazes de limpar os dados e processá-los sem a necessidade de código externo. Para o treinamento e avaliação do primeiro modelo (desenvolvido com a biblioteca Transformers), o comparamos aos modelos de AM apresentados pelo HateBR, que haviam obtido os melhores resultados, usando o mesmo método de extração de recursos que eles: TF-IDF. O TF-IDF (Term Frequency–Inverse Document Frequency) calcula quão relevante uma palavra em um corpus é para um texto, obtido pela razão entre o número de vezes que o termo em questão aparece em um dos textos do corpus (Term Frequency) e a frequência de aparições deste mesmo termo em todo o corpus (Inverse Document Frequency). A frequência da palavra em um texto refere-se à razão entre o número de vezes que ela aparece no texto e o número de palavras no texto, enquanto a frequência da palavra no corpus é a contagem de quantas vezes aparece em todos os textos do conjunto de dados. A razão para usar essa extração de recursos é devido aos resultados empíricos demonstrados pelo HateBR [Vargas et al. 2022], que mostram que, em geral, modelos treinados com recursos extraídos usando TF-IDF superaram outros métodos. \n",
            "\n",
            "4.3. Treinamento do Modelo \n",
            "\n",
            "O processo de treinamento do modelo foi dividido em duas partes: a primeira foi responsável por detectar ofensividade em um texto (neste caso, em um parágrafo selecionado) e a segunda foi responsável por identificar palavras ou expressões que contêm linguagem ofensiva em segmentos classificados como ofensivos pelo primeiro modelo. \n",
            "\n",
            "Empregamos o modelo BERTimbau da plataforma HuggingFace para o primeiro modelo. O BERTimbau é um modelo de linguagem brasileira desenvolvido pela NeuralMind [Souza et al. 2020] por meio de uma técnica chamada fine-tuning. No caso deste estudo, este processo envolveu adicionar uma camada extra de neurônios no final do modelo, responsável por classificar um texto de entrada como ofensivo ou não ofensivo. \n",
            "\n",
            "Para o segundo modelo, o processo de treinamento utilizado pelo estudo OLID-BR [Trajano et al. 2023] foi empregado para detectar intervalos ofensivos (ou seja, sequências de caracteres contendo linguagem ofensiva, não se limitando a palavras isoladas, mas também incluindo expressões e pontuações). \n",
            "\n",
            "4.4. Demonstração do Modelo \n",
            "\n",
            "Avalíamos os modelos de detecção de linguagem ofensiva utilizando as métricas de Precisão, Recall e F1-Score. A Precisão mede o quanto podemos confiar em um modelo quando ele prevê que um exemplo pertence a uma determinada classe, calculando o número de exemplos que o modelo previu corretamente como pertencentes a essa classe dividido pelo número total de exemplos que ele previu como pertencentes a essa classe. Recall é o número de amostras que o modelo identificou corretamente como pertencentes a uma classe dividido pelo número total de amostras que pertencem a essa classe nos dados. O F1-Score é a média harmônica entre a precisão e o recall, ou seja, é a média dos valores de precisão e recall, dando mais importância a valores baixos, uma vez que um valor de precisão ou recall muito baixo indica que o modelo não está equilibrando bem essas duas métricas quando queremos dar igual importância a ambas. \n",
            "\n",
            "Para demonstrar a eficácia dos modelos, realizamos testes qualitativos em textos selecionados, desde que fossem de pelo menos uma página de comprimento ou contivessem mais de um parágrafo. Para o processo de aplicação dos modelos treinados, a Figura 1 apresenta graficamente os seguintes passos: (1) um texto extenso é coletado; (2) o texto é dividido em fragmentos com base nos caracteres de pontos, vírgulas, ponto e vírgula, exclamações, perguntas e novas linhas; (3) para cada fragmento, o modelo baseado em BERTimbau é usado para verificar se é ofensivo; (4) se não for ofensivo, o fragmento é retornado normalmente, mas se for, prossegue para a próxima etapa; (5) o modelo treinado com spaCy é usado para identificar os intervalos ofensivos; (6) retorno dos intervalos censurados ofensivos e a versão censurada do fragmento; (7) finalmente, todos os fragmentos, censurados ou não, são remontados utilizando os mesmos separadores de antes, retornando assim o texto completo agora com a censura apropriada. \n",
            "\n",
            "Figura 1. Processo de censura da nossa proposta. \n",
            "\n",
            "5. Resultados \n",
            "\n",
            "Os resultados obtidos nas sessões de treinamento, em termos de Precisão, Recall e F1-score avaliados em quatro modelos diferentes testados para detecção de toxicidade em sentenças, são apresentados na Tabela 2. Os parâmetros para conduzir o treinamento e teste seguiram a proposta no estudo HateBR [Vargas et al. 2022]. Como o HateBR não fornece os modelos pré-treinados originais e também a configuração de divisão de treinamento/validação/teste, foi necessário retreinar cada um dos quatro modelos nesse conjunto de dados para garantir consistência entre os resultados: modelos SVM, Naive Bayes, Regressão Logística e MLP. Também avaliamos nossa versão ajustada do modelo BERTimbau [Souza et al. 2020], seguindo o método TF-IDF para extração de recursos. Para treinar o modelo baseado em BERTimbau, utilizamos o tokenizador que vem pré-embarcado com o modelo para realizar a extração de recursos, pois é assim que os modelos baseados em BERT operam. \n",
            "\n",
            "Modelo SVM Naive Bayes Regressão Logística MLP BERTimbau + fine-tuning Precisão Recall 0.84 0.87 0.85 0.82 0.87 0.87 0.85 0.86 0.86 0.92 F1-score 0.86 0.86 0.85 0.84 0.90 \n",
            "\n",
            "Tabela 2. Comparação dos modelos treinados no conjunto de dados HateBR [Vargas et al. 2022]. \n",
            "\n",
            "A análise dos dados mostra que o modelo BERTimbau + fine-tuning empregado neste estudo supera os resultados anteriores em todos os parâmetros de comparação, estabelecendo este trabalho como o estado-da-arte para o conjunto de dados HateBR. Este resultado ressalta a importância do uso de modelos de AP no contexto de PLN, pois podem trazer benefícios significativos nos processos de reconhecimento e classificação de texto. \n",
            "\n",
            "Após concluir o teste do modelo BERTimbau e treinar o modelo spaCy seguindo a mesma metodologia que o OLID-BR [Trajano et al. 2023], realizamos a avaliação final do pipeline completo. Para esse propósito, executamos o fluxo de trabalho proposto na Figura 1, ou seja, se uma sentença é tóxica, as partes ofensivas são detectadas e censuradas. Para apresentação e avaliação qualitativa, a censura é realizada simplesmente substituindo os caracteres que constituem a palavra ou expressão ofensiva por asteriscos (*). \n",
            "\n",
            "Selecionamos alguns artigos de notícias de portais especializados, como G1, Estadão e Catraca Livre, para realizar um teste preliminar, mas nenhuma linguagem ofensiva foi detectada. Posteriormente, um artigo de opinião sobre Chico Buarque do blog Senso Incomum [Martins 2022] foi utilizado, onde a presença de palavrões, que seriam apropriadamente detectados, era evidente. \n",
            "\n",
            "A Tabela 3 apresenta exemplos para análise qualitativa dos resultados da aplicação do pipeline proposto neste trabalho. A tabela mostra a comparação entre o texto original e o texto censurado. O texto demonstrado é a única postagem de um blog coletado, uma vez que os três artigos de notícias obtidos não continham linguagem ofensiva. Esses resultados são limitados a demonstrar as partes do texto original que nosso algoritmo censurou. \n",
            "\n",
            "Uma consideração é que o modelo censurou a palavra “censura”, que geralmente não é considerada ofensiva, assim como a expressão “Rock das Aranhas”, ao invés da palavra anterior “lesbofóbico”, que julgamos ser muito mais ofensiva do que uma expressão sobre rocha e aranhas. As demais censuras consideramos apropriadas. \n",
            "\n",
            "Tabela 3. Exemplos de trechos do texto original (à esquerda) e trechos censurados (à direita). \n",
            "\n",
            "Parte Censurada parte observador de ******* Texto Original parte observador de bonobos A autocensura é o pior tipo de censura que existe A auto******* é o pior tipo de ******* que existe Joga pedra na Geni/Joga bosta na Geni/Ela é feita pra apanhar/Ela é boa de cuspir/Ela dá pra qualquer um/Maldita Geni quer um/Maldita Geni! seu lesbofóbico “Rock das Aranhas”! seu lesbofóbico ****************** Paga-pau dos porcos estadunidenses, com toda certeza! Espumando de ódio Espumando de **** \n",
            "\n",
            "5.1. Código Fonte dos Experimentos \n",
            "\n",
            "O código fonte desenvolvido foi disponibilizado publicamente na forma de Notebooks, disponíveis no repositório do GitHub: https://github.com/ICDI/censorship-offensive-language: \n",
            "- O notebook para treinamento e teste do modelo baseado no BERTimbau; \n",
            "- O notebook para treinamento do modelo de detecção de spam com spaCy, reutilizando o código do OLID-BR; \n",
            "- O notebook com os testes utilizando ambos os modelos para detecção e censura de texto. \n",
            "\n",
            "6. Conclusão \n",
            "\n",
            "Este trabalho desenvolveu um programa baseado em AM/AP para detectar e censurar linguagem ofensiva em textos extensos em português extraídos da web. Para isso, apresentamos um fluxo de trabalho que compreende duas partes: uma que detecta a ofensividade em uma parte do texto (precisamente uma sentença) e outra que detecta as partes ofensivas (spam) dentro da sentença. Isso permite censurar uma grande parte do texto fragmento por fragmento e retornar o texto censurado. \n",
            "\n",
            "Um aspecto positivo deste trabalho é fornecer um programa especificamente focado em detectar e censurar textos extensos, apresentando resultados satisfatórios nas análises quantitativas e qualitativas. Como uma evolução deste trabalho, pode-se vislumbrar o desenvolvimento de um conjunto de dados específico para censura de grandes textos, que, ao contrário dos conjuntos de dados utilizados [Vargas et al. 2022] e [Trajano et al. 2023], representaria um avanço significativo. \n",
            "\n",
            "Os resultados para detectar ofensividade em textos foram superiores à nossa referência original (o artigo HateBR), representando um novo padrão em relação ao estado-da-arte, superando as técnicas utilizadas como referência no conjunto de dados HateBR [Vargas et al. 2022]. A parte de censura foi realizada simplesmente substituindo os caracteres nas partes ofensivas por asteriscos, o que pode, sem dúvida, ser aprimorado por trabalhos futuros. A análise poderia ter sido mais robusta devido à falta de um conjunto de dados específico para grandes textos, mesmo ao discutir dados qualitativos. Esse fato demonstra a necessidade de criar um conjunto de dados específico construído a partir de julgamentos de especialistas sobre linguagem ofensiva ou por métricas calculáveis, que podem evoluir em trabalhos futuros.\n",
            "25\n",
            "Resumo. Fake news se espalha mais rápido em algumas redes sociais do que notícias regulares, o que pode ter diferentes consequências, desde influências nos resultados eleitorais até mortes devido a tratamentos incorretos de doenças. Este trabalho tem como objetivo empregar métodos baseados em aprendizado por transferência e modelos de aprendizado de máquina baseados em Transformers para classificar a veracidade de tweets na língua portuguesa (Brasil pt-BR). Para isso, foi criada uma base de dados confiável e rotulada, aberta para acesso gratuito. O conjunto de dados relaciona postagens extraídas do X (anteriormente conhecido como Twitter) e sua proximidade com fatos ou informações falsas. Subsequentemente, cinco modelos Transformer foram treinados em português. O modelo BERT ajustado, inicializado com pré-treinamento em textos em português, alcançou um desempenho superior, obtendo uma acurácia de 95.1%. \n",
            "\n",
            "1. Introdução  \n",
            "Os veículos de comunicação de grande circulação por muito tempo foram jornais, revistas, rádio e televisão. Hoje, notícias circulam através de vídeos no YouTube, portais de notícia e em redes sociais como Facebook, X (Antigo Twitter) e WhatsApp, tornando a Internet um dos principais meios de comunicação e consumo de notícias. No Brasil, 65% usam a Internet e suas aplicações como principais fontes de informação, nos Estados Unidos 53% e, no mundo, o número estimado é de 62% [NegociosSC 2024, Gente 2024, DataReportal 2024, Data 2024]. O que, por um lado, demonstra que a Internet ampliou o acesso à informação, mas, por outro lado, também transformou a forma como as notícias são consumidas e compartilhadas. A ascensão da Internet e, consequentemente, das redes sociais democratizaram a produção de notícias, permitindo que qualquer pessoa assuma o papel de produtor de conteúdo sem a supervisão tradicional de jornalistas. Este fenômeno pode ter impactado negativamente na qualidade das informações disseminadas, resultando em um aumento de notícias que propagam desinformação ou divulgam informações falsas [Reis et al. 2019, Vargas et al. 2021]. \n",
            "\n",
            "2. Proposta de Modelo  \n",
            "Neste trabalho, propomos um modelo baseado em transferência de aprendizagem, transformers e aprendizagem supervisionada para classificar textos em português nas redes sociais, com foco na plataforma X. Também criamos uma base de dados em português (162 amostras e balanceada), que relaciona textos da plataforma X com sua veracidade, visando melhorar a detecção de fake news e promover a qualidade da informação nas redes sociais. \n",
            "\n",
            "3. Metodologia  \n",
            "O primeiro passo foi a construção de uma base de dados contendo postagens de usuários da rede social X (Antigo Twitter). Foi utilizada a ferramenta Get Old Tweets (GOT) [Henrique 2018] para coletar tweets históricos, incluindo notícias falsas. Dessa forma, cada tweet foi analisado e classificado manualmente para determinar sua proximidade com o fato, assegurando que o conjunto de dados fosse rigoroso e preciso. Esse processo permitiu a criação de um conjunto de dados robusto e rotulado, essencial para o treinamento e validação eficazes do modelo de classificação de textos em português proposto. Em seguida, os textos coletados passam por uma fase de preparação, onde são inicialmente pré-processados e, posteriormente, rotulados. Após esse processo, os dados são ajustados para servirem como entradas adequadas para os modelos de aprendizagem de máquina. \n",
            "\n",
            "3.1. Conjunto de dados: Coleta e Processamento  \n",
            "Este trabalho utiliza dados textuais em português extraídos da plataforma de rede social X. A plataforma permite a extração de informações através de sua Application Programming Interface (API). Com a rede social selecionada, iniciou-se a coleta de dados para compor a base de dados. O processo de obtenção dos tweets consistiu em buscar no site de checagem de notícias verdadeiras ou falsas (LUPA) e pesquisar por elas utilizando a ferramenta GOT [Henrique 2018]. Para isso, foram realizadas filtragens de notícias e alinhamento temporal aproximado para obtenção das postagens realizadas sobre a notícia verificada.  \n",
            "\n",
            "3.2. Seleção dos Modelos de Aprendizagem  \n",
            "Neste trabalho, foram adotados modelos de aprendizagem baseados em Transformers. Originalmente desenvolvido para tradução automática, o Transformer se destacou por sua capacidade de capturar relações de dependência de longo alcance de forma eficaz. Buscou-se por modelos Transformers que receberam pré-treinamento em português, visando aproveitar a transferência de aprendizagem. Como resultado, optou-se por ajustar os seguintes modelos: BERT base pré-treinado em português brasileiro por [Souza et al. 2020]; BERT base pré-treinado em 104 idiomas, incluindo português, por [Devlin et al. 2019]; RoBERTa pré-treinado por [Liu et al. 2019] com um corpus de 6,9 milhões de frases em português; XLM-R base, pré-treinado por [Conneau et al. 2020] incluindo português; e, por fim, o modelo ELECTRA uncased em 100 idiomas, [Clark et al. 2020], pré-treinado especificamente em português. \n",
            "\n",
            "3.3. Treinamento  \n",
            "Para avaliar a capacidade de generalização do modelo, foi utilizado o método de validação cruzada com 10 partições (10-fold cross-validation). Este método divide o conjunto de dados em 10 sub-conjuntos. Cada subconjunto é usado uma vez como conjunto de teste, enquanto os restantes são usados como conjunto de treinamento. Esse processo é repetido 10 vezes, garantindo que cada amostra do conjunto de dados seja utilizada para testes ao menos uma vez. Esse procedimento não apenas melhora a capacidade de generalização do modelo, mas também fornece uma estimativa mais robusta do desempenho do modelo em dados não vistos. \n",
            "\n",
            "4. Resultados  \n",
            "As métricas de avaliação incluem Acurácia, F1-Score, Precisão, Sensibilidade (Recall) e MCC. \n",
            "\n",
            "Tabela 1. Resultados obtidos em cada modelo \n",
            "Épocas          | Acurácia  \n",
            "Modelo          \n",
            "ELECTRA (uncased)  | 0.864  \n",
            "RoBERTa pré-treinado em Português | 0.901  \n",
            "XLM-R pré-treinado em multi-idiomas    | 0.903  \n",
            "BERT com Pré-treinamento em Português | 0.944  \n",
            "BERT com Pré-treinamento Multi-idioma    | 0.914  \n",
            "\n",
            "F1          | \n",
            "0.848       | \n",
            "0.897       | \n",
            "0.898       | \n",
            "0.955       | \n",
            "0.918       | \n",
            "\n",
            "Precisão    | Sensibilidade    | MCC  \n",
            "0.720      | 0.883 | 0.824  \n",
            "0.812      | 0.852 | 0.962  \n",
            "0.804      | 0.883 | 0.922  \n",
            "0.887      | 0.944 | 0.971  \n",
            "0.825      | 0.900 | 0.944  \n",
            "\n",
            "Os resultados deste trabalho, apresentados na Tabela 1, mostram que cada modelo de aprendizado de máquina treinado para a classificação de notícias em português teve um desempenho variado, dependendo do número de épocas e das características do próprio modelo. O ELECTRA (uncased), treinado com 10 épocas, apresentou o desempenho mais baixo, o que pode ser atribuído à falta de diferenciação entre letras maiúsculas e minúsculas, bem como à qualidade dos pesos de pré-treinamento disponíveis. O modelo RoBERTa pré-treinado em Português, configurado com 7 épocas, superou o ELECTRA, beneficiando-se de uma arquitetura que captura melhor as nuances linguísticas do português e diferencia entre maiúsculas e minúsculas. O modelo XLM-R pré-treinado em multi-idiomas, com 9 épocas, demonstrou uma leve superioridade em relação ao RoBERTa em termos de acurácia e F1, aproveitando o conhecimento adquirido em múltiplos idiomas. Já o BERT com Pré-treinamento em Multi-idioma, utilizando 10 épocas, mostrou robustez com acurácia e F1 acima de 0.9, destacando-se pela capacidade de transferir conhecimento linguístico de um corpus multilíngue para o português. Por fim, o BERT pré-treinado em Português foi o modelo com melhor desempenho geral, utilizando apenas 6 épocas de treinamento. Este modelo se destacou na classificação correta das notícias, com acurácia, F1 e MCC superiores, evidenciando a eficácia do pré-treinamento específico em português e a importância do ajuste fino dos hiperparâmetros para maximizar a eficácia do modelo em tarefas específicas de classificação de texto. \n",
            "\n",
            "5. Discussão  \n",
            "Os modelos ELECTRA uncased pré-treinado em Português e RoBERTa pré-treinado em Português apresentaram resultados abaixo do esperado, pode-se levantar a questão de que se tais modelos passaram pelo mesmo processo de pré-treinamento dos outros métodos. O modelo RoBERTa exige mais recursos computacionais comparado com o BERT, além de ser um aprimoramento do mesmo, portanto, melhores resultados eram esperados desse modelo. O modelo ELECTRA sendo um modelo uncased, esperava-se um desempenho abaixo dos outros classificadores pré-treinados exclusivamente em português. Ainda assim, acredita-se que o modelo não conseguiu generalizar bem o problema. O XLM-R foi um modelo originalmente proposto para a tradução de idiomas, por isso ele está disponível em versão multi-idiomas pré-treinado em vários idiomas, inclusive português. Apesar do XLM-R não ter sido originalmente proposto para classificação de texto, ele obteve resultados melhores que o ELECTRA. O Modelo BERT com Pré-treinamento em Português obteve acurácia e F1 superiores a todos os outros modelos, mostrando que o pré-treinamento em português feito por [Souza et al. 2020] foi muito eficiente e contribuiu positivamente para o bom desempenho do modelo. Os resultados preliminares mostraram que o modelo foi capaz de classificar notícias de uma base de dados relativamente pequena, bases de dados com poucas amostras é um desafio em algumas áreas, como na saúde. \n",
            "\n",
            "6. Conclusões  \n",
            "Este trabalho apresentou uma abordagem para detecção de tweets falsos em português através de NLP. Além disso, foi criada e disponibilizada uma base de dados balanceada com tweets classificados de forma confiável. A base possibilitou o treinamento de modelos para detecção de notícias falsas. Sendo que o modelo BERT com 6 épocas foi o melhor comparado aos outros modelos testados. \n",
            "\n",
            "7. Trabalhos Futuros  \n",
            "Na continuação do trabalho, pretendemos estender a avaliação comparativa com outros modelos estado da arte da literatura de classificação de texto baseados em aprendizado profundo. Pretende-se aumentar a base de dados com mais dados rotulados, mantendo a confiabilidade, e também buscar dados de outras fontes. Além de mostrar os resultados da classificação de notícias verdadeiras, planeja-se apresentar também os resultados de classificação das notícias falsas, assim como utilizar outras estratégias para o treinamento, como a validação cruzada com 5 partições.  \n",
            "\n",
            "Referências  \n",
            "[Clark et al. 2020] Clark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. (2020). ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR.  \n",
            "[Conneau et al. 2020] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451, Online. Association for Computational Linguistics.  \n",
            "[Data 2024] Data, P. (2024). Global social media users in 2024. Accessed: 2024-06-28.  \n",
            "[DataReportal 2024] DataReportal (2024). Social media users 2024 (global data & statistics). Accessed: 2024-06-28.  \n",
            "[Devlin et al. 2019] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.  \n",
            "[Gente 2024] Gente, https://gente.globo.com/notícias-nas-pandemia-e-o-consumo-de-noticias-nas-redes-sociais/. Acesso em 28 de junho de 2024.  \n",
            "[Henrique 2018] Henrique, J. (2018). Get old tweets programatically. Repository on GitHub.  \n",
            "[Liu et al. 2019] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.  \n",
            "[NegociosSC 2024] NegociosSC (2024). O uso da internet, redes sociais e mídia no Brasil em 2024. Acesso em 28 de junho de 2024. https://www.negociossc.com.br/blog/o-uso-da-internet-redes-sociais-e-midia-no-brasil-em-2024/.  \n",
            "[Reis et al. 2019] Reis, J. C. S., Correia, A., Murai, F., Veloso, A., and Benevenuto, F. (2019). Supervised learning for fake news detection. IEEE Intelligent Systems, 34(2):76–81.  \n",
            "[Souza et al. 2020] Souza, F., Nogueira, R., and Lotufo, R. (2020). BERTimbau: pretrained BERT models for Brazilian Portuguese. In 9th Brazilian Conference on Intelligent Systems, BRACIS, Rio Grande do Sul, Brazil, October 20-23 (to appear).  \n",
            "[Vargas et al. 2021] Vargas, F., Benevenuto, F., and Pardo, T. (2021). Toward discourse-aware models for multilingual fake news detection. In Proceedings of the Student Research Workshop Associated with RANLP 2021, pages 210–218.  \n",
            "26\n",
            "**Resumo.** Este artigo apresenta a primeira tentativa de anotar automaticamente as Dependências Universais Aprimoradas para o português brasileiro. Utilizamos um sistema simbólico de anotação, baseado em regras de reescrita de grafo, e modificamos suas regras originais para melhor atender às características linguísticas do português, usando uma amostra anotada manualmente da parte jornalística do treebank Porttinari como verdade básica. Nosso objetivo é avaliar o desempenho da anotação automática para uma nova língua e determinar a extensão das possíveis melhorias por meio de modificações nas regras. Os resultados demonstram melhorias significativas no desempenho, onde ajustes nas regras impulsionados por linguística melhoraram a precisão da anotação em 11,38 pontos, atingindo 96,05% de F1-score.\n",
            "\n",
            "**1. Introdução** Anotações morfológicas e sintáticas mostraram-se relevantes para várias iniciativas de Processamento de Linguagem Natural (PLN). Por exemplo, tarefas de extração de informações abertas (Oliveira et al. 2023) e simplificação de texto (Candido et al. 2009) podem basear diretamente suas decisões na sintaxe. Considerando as tendências mais recentes dos Modelos de Linguagem de Grande Escala, vários trabalhos demonstraram melhorias nos resultados quando o conhecimento linguístico é fornecido (Zhou et al. 2020; Bai et al. 2021; Lin et al. 2021; Bölüc et al. 2023). Do ponto de vista linguístico, a anotação linguística pode ajudar a descrever fenômenos variados da linguagem, possivelmente apoiando a validação e/ou proposta de novas teorias.\n",
            "\n",
            "As Dependências Universais (UD) são uma estrutura para anotações morfológicas, morfo-sintáticas e sintáticas de línguas humanas. A UD fornece diretrizes padronizadas e tem sido usada para anotar mais de 283 treebanks para 161 línguas, sendo amplamente adotada, pois propõe decisões de anotação consensuais e permite esforços comparativos e multilíngues. Em relação à anotação sintática, a estrutura da UD suporta dois níveis de profundidade: árvores de dependência básicas e gráficos aprimorados. As árvores de dependência básicas fornecem informações sobre dependências sintáticas, onde cada token está conectado a um token governante (cabeça) por meio de uma relação (por exemplo, na frase \"O menino chorou\", \"menino\" está conectado como sujeito à cabeça \"chorou\" por uma relação nsubj). As Dependências Universais Aprimoradas (EUD) geralmente se baseiam nas dependências básicas, adicionando relações e nós (ou tokens) para tornar explícitas as relações implícitas entre os tokens (Nivre et al. 2020) (por exemplo, na Figura 1, \"menino\" também está conectado a \"saiu\" por uma relação nsubj aprimorada, como é compartilhada pelos verbos \"chorou\" e \"saiu\"). Essa melhoria pode facilitar tarefas de PLN, fornecendo informações adicionais.\n",
            "\n",
            "**Figura 1.** Anotação EUD - a dependência nsubj em vermelho é uma nova dependência EUD.\n",
            "\n",
            "**Figura 2.** Anotação EUD - relação estendida com o item lexical \"com\".\n",
            "\n",
            "Este artigo investiga a questão da anotação EUD para o português brasileiro. Até onde sabemos, esta é a primeira avaliação da anotação EUD para esta língua. Seguindo duas tarefas compartilhadas anteriores sobre anotação EUD (Bouma et al. 2020; Bouma et al. 2021), que não incluíram o português, baseamo-nos em um dos sistemas que participaram da tarefa de 2021, a saber, Grew (Guillaume e Perrier 2021), baseado em regras de reescrita de grafo para árvores sintáticas anotadas. Este sistema simbólico vem com um conjunto de regras originais (e universais), e fizemos uma série de modificações com base na investigação do corpus, gerando um conjunto de regras aprimorado. Os dois conjuntos de regras foram aplicados a um conjunto de dados amostrais da parte jornalística do Porttinari (Duran et al. 2023), um treebank português disponível no catálogo do projeto Dependências Universais, que enriquecemos manualmente com anotação EUD para avaliar a qualidade da anotação automática. Portanto, nosso objetivo é verificar o desempenho das regras originais do programa para o português e o quanto podemos melhorá-lo com regras modificadas.\n",
            "\n",
            "No final, discutimos erros de anotação persistentes e perspectivas futuras sobre a anotação automática EUD. Como contribuição adicional, as regras e os dados anotados também estão disponíveis para o leitor interessado.\n",
            "\n",
            "**2. Trabalho Relacionado** As EUD apresentam desafios significativos quando comparadas à anotação UD tradicional. Além do site da UD, onde as diretrizes são atualizadas conforme necessário, existem vários trabalhos discutindo a relevância e explicando a aplicação desse tipo de anotação em treebanks (De Marneffe et al. 2014; Nivre et al. 2016; Schuster e Manning 2016; Nivre et al. 2020). A instância dessas relações para o português foi introduzida e detalhada em (Pagano et al. 2023). No geral, as EUD podem incluir 6 situações de anotação:\n",
            "\n",
            "1. Inclusão dos lemmas de preposições, conjunções coordenativas e conjunções subordinativas no rótulo das relações que introduzem (como na Figura 2);\n",
            "2. Identificação do sujeito controlador do sujeito nulo em cláusulas xcomp (como na Figura 3);\n",
            "\n",
            "**Figura 3.** Anotação EUD - relação nsubj para um verbo dependente de xcomp.\n",
            "\n",
            "**Figura 4.** Anotação EUD - relação obj propagada para o dependente de conj.\n",
            "\n",
            "**Figura 5.** Anotação EUD - \"livro\" é o objeto de \"ler\" e \"que\" é ref de \"livro\".\n",
            "\n",
            "3. Propagação, para o dependente de conj, da relação que atinge a cabeça de conj (como na Figura 4);\n",
            "4. Propagação, para o dependente de conj, de algumas relações que partem da cabeça de conj (como na Figura 1);\n",
            "5. Substituição do pronome relativo em cláusulas relativas por seu antecedente, marcando a relação do pronome relativo com seu antecedente com um rótulo exclusivo da EUD: ref (como na Figura 5);\n",
            "6. Inserção de um token vazio para ocupar o lugar de um predicado elíptico e estabelecimento de relações desse token vazio com os participantes da relação órfã (como na Figura 6).\n",
            "\n",
            "Enquanto as árvores UD são estruturas hierárquicas simples com uma raiz, os gráficos EUD são conectados e podem conter ciclos. Por exemplo, na Figura 5, o nó \"livro\" é dependente de \"ler\" em uma relação obj, no entanto, também é cabeça de \"ler\" em uma relação de cláusula relativa (acl:relcl), uma anotação sintática básica que é mantida no gráfico aprimorado, estabelecendo um ciclo entre dois nós. Outro desafio é que algumas relações são lexicalizadas (como na Figura 2), aumentando consideravelmente o conjunto de rótulos a serem previstos e tornando-os dependentes da língua. Além disso, um token pode ter mais de uma relação aprimorada, tendo múltiplas cabeças, e pode haver tokens vazios adicionais para representar predicados elípticos (Bouma et al. 2020). Na Figura 1, o nó \"menino\" tem duas cabeças: os verbos \"chorou\" e \"saiu\", que são coordenados, enquanto na anotação básica apenas o primeiro verbo seria sua cabeça. Na Figura 6, um token vazio, [tem], foi adicionado ao gráfico EUD para resolver a questão do predicado elíptico, e várias dependências foram alteradas para se adequar a esse novo token. \n",
            "\n",
            "As tarefas compartilhadas realizadas no IWPT em 2020 (Bouma et al. 2020) e em 2021 (Bouma et al. 2021) forneceram uma plataforma para comparar resultados entre diferentes sistemas. Até o momento, não existe um treebank anotado com EUD para a língua portuguesa, o que significa que a língua nunca foi submetida a nenhuma tentativa de anotação automática. Para participar da competição, um treebank não precisava ter todos os seis tipos de EUD; aqui, estamos testando uma abordagem baseada em regras em um conjunto de dados totalmente anotado em português com todos os seis tipos de EUD, produzidos para o propósito deste trabalho. \n",
            "\n",
            "O sistema que escolhemos usar, Grew, ficou em sétimo lugar na competição de 2021, com 81,58% de ELAS (uma F1-score sobre as relações EUD), sendo o sistema simbólico com melhor classificação. Nosso objetivo é testar as possibilidades e limitações de uma abordagem baseada em regras orientada linguisticamente, que pode ser construída com supervisão linguística, sendo facilmente aplicada a outras línguas também, sem treinamento, e com alta interpretabilidade.\n",
            "\n",
            "**3. Metodologia** Utilizamos dois pequenos conjuntos padrões de EUD: um para teste (gold-test) e um para desenvolvimento (gold-dev). O conjunto gold-dev foi retirado do Porttinari-base, a principal parte do Porttinari, enquanto o gold-test foi fonte do Porttinari-test, projetado para avaliar sistemas de anotação automática (Duran et al. 2023). O gold-dev compreende 100 sentenças selecionadas manualmente, escolhidas por um linguista para representar fenômenos desafiadores de EUD. Em contraste, as 100 sentenças de teste foram selecionadas aleatoriamente para refletir a frequência natural dos fenômenos no Porttinari. Devido a métodos de seleção (intencionais) diferentes, os conjuntos dev e test apresentam disparidades, por exemplo, o conjunto dev contém 23 sentenças com o rótulo órfão (elipse de predicado), enquanto o conjunto de teste inclui apenas duas. \n",
            "\n",
            "Começamos nosso trabalho analisando as regras originais para EUD do Grew (Guillaume e Perrier 2021), referidas como \"regras originais\", que são universais e idealmente aplicáveis a qualquer língua. Observamos os resultados de anotação no conjunto de desenvolvimento e, conforme os erros foram identificados, criamos novas regras e modificamos as existentes para abordar essas deficiências. Notavelmente, nenhuma das sentenças do conjunto de teste influenciou as modificações nas regras. Como resultado do processo, temos o conjunto de regras nomeado \"regras modificadas\".\n",
            "\n",
            "Nossa avaliação concentra-se no F1-score geral do programa (também chamado de ELAS, ou seja, pontuação de anexação rotulada sobre dependências aprimoradas), bem como no F1-score para cada um dos 6 tipos de EUD. Para alcançar isso, classificamos automaticamente cada relação aprimorada em uma das 6 categorias usando regras linguísticas. Por exemplo, sabemos, a partir da Figura 3, que as relações nsubj de verbos que são dependentes de xcomp em relação a nominais, quando o nominal também tem uma relação nsubj proveniente do verbo que é o governador de xcomp, são relações do tipo \"atribuição de sujeitos de xcomp\".\n",
            "\n",
            "As regras do Grew consistem em padrões (que podem envolver qualquer informação de anotação UD) a serem identificados nas sentenças e um conjunto de comandos a serem executados quando esses padrões são encontrados. Essas regras são incorporadas em um mecanismo conhecido como \"estratégia\", que permite o controle de quais regras são aplicadas para cada língua e em que ordem. Por exemplo, a resolução de elipses de predicado deve ser feita primeiro, pois outras regras relacionadas à propagação de dependentes de elementos coordenados podem ser aplicadas considerando o token vazio inserido na sentença.\n",
            "\n",
            "Na Tabela 1, encontramos o número de regras para cada tipo de relação EUD (1-6) no conjunto de regras do Grew, de acordo com nossa identificação automática dos tipos de EUD, além de nossas novas regras (7). Existem também regras \"não classificadas\", pois não produzem nenhuma mudança visível em uma sentença, mas sim mudanças implícitas que serão utilizadas para outras regras dentro de uma estratégia do Grew. Além das novas regras, algumas das regras originais foram modificadas, e elas serão vistas na seção de Resultados, onde consideramos quantas vezes as regras para cada tipo de EUD foram aplicadas antes e depois de nossas modificações.\n",
            "\n",
            "| Tipos de EUD | Número de Regras |\n",
            "|--------------|------------------|\n",
            "| 1 - Adição de preposições e conjunções | 3 |\n",
            "| 2 - Atribuição de sujeitos xcomp | 11 |\n",
            "| 3 - Propagação da cabeça da conj | 18 |\n",
            "| 4 - Propagação de dependentes da conj | 8 |\n",
            "| 5 - Anotação do referente do pronome relativo | 13 |\n",
            "| 6 - Inclusão de predicado elíptico | 23 |\n",
            "| 7 - Novas regras | 15 |\n",
            "| **Total** | **64** |\n",
            "\n",
            "**Tabela 1.** Número de regras relacionadas a cada tipo de EUD.\n",
            "\n",
            "**4. Resultados** Tanto as amostras de teste quanto as de desenvolvimento foram anotadas manualmente para EUD. A Tabela 2 apresenta uma descrição desses corpora, bem como a distribuição de cada um dos tipos de EUD. O número de relações EUD nesta seção ignora relações que são simples réplicas de relações básicas sem nenhuma modificação, bem como relações de pontuação. \"Mais de uma classificação\" refere-se a relações que foram classificadas como resultado de mais de um tipo de EUD em ação; \"Não classificadas\" refere-se às poucas relações que não puderam ser corretamente classificadas como um dos seis tipos de EUD usando nossas regras automáticas de identificação de tipos.\n",
            "\n",
            "| Sentenças | Tokens | Relações EUD | Sentenças com predicados elípticos |\n",
            "|-----------|--------|--------------|-------------------------------------|\n",
            "|                  |     |                 |                                     |\n",
            "| **gold-dev** % dev | **gold-test** % test | | |\n",
            "| 100 | 2.213 | 776 | 23 |\n",
            "| 397 | 44 | 67 | 45 |\n",
            "| 72 | 113 | 37 | 1 |\n",
            "| - | - | - | - |\n",
            "| 23.0% | 51.16% | 5.67% | 8.63% |\n",
            "| 5.8% | 9.28% | 14.56% | 4.77% |\n",
            "| 0.13% | | |  |\n",
            "| 100 | 2.012 | 587 | 2 |\n",
            "| - | - | - | - |\n",
            "| 2.0% | 362 | 34 | 56 |\n",
            "| 30 | 55 | 11 |  |\n",
            "| 31 | 8 | 61.67% | 5.79% |\n",
            "| 9.54% | 5.11% | 9.37% | 1.87% |\n",
            "| 5.28% | 1.36% | | |\n",
            "\n",
            "**Tabela 2.** Distribuição dos fenômenos nas amostras padrão de EUD.\n",
            "\n",
            "Em relação à distribuição dos tipos de EUD por amostra, vemos uma diferença razoavelmente grande entre as duas, com a frequência dos fenômenos sempre sendo maior na amostra de desenvolvimento. Particularmente na classe 6, a diferença (14.56% das relações no gold-dev versus 1.87% das relações no gold-test) se deve ao fato de que a relação órfã, indicativa de elipse de predicado, é infrequente no corpus, como comentado anteriormente.\n",
            "\n",
            "A Tabela 3 mostra quantas vezes as regras para cada tipo de EUD foram aplicadas para anotar as amostras padrão. A diferença nas aplicações de \"Original\" para \"Modif.\" são resultado das mudanças que fizemos nessas regras para melhor atender aos nossos dados. O aumento de 0 para 60 e 31 em \"4 - Propagação de dependentes da conj\" se deve à remoção de restrições nas regras originais para melhor atender aos dados em português. Novas regras, como a da Figura 7, poderiam ser classificadas em um de cada tipo de EUD, mas foram deixadas como um novo tipo para destacar que são completamente novas.\n",
            "\n",
            "| Tipos de EUD | gold-dev | gold-test |\n",
            "|--------------|----------|-----------|\n",
            "|              | Original | Modif.    | Original | Modif. |\n",
            "| 1 - Adição de preposições e conjunções | 415 | 448 | 355 | 374 |\n",
            "| 2 - Atribuição de sujeitos xcomp | 36 | 40 | 27 | 29 |\n",
            "| 3 - Propagação da cabeça da conj | 84 | 87 | 57 | 57 |\n",
            "| 4 - Propagação de dependentes da conj | 0 | 60 | 0 | 31 |\n",
            "| 5 - Anotação do referente do pronome relativo | 108 | 117 | 95 | 95 |\n",
            "| 6 - Inclusão de predicado elíptico | 71 | 75 | 6 | 7 |\n",
            "| 7 - Novas regras | 0 | 75 | 0 | 8 |\n",
            "\n",
            "**Tabela 3.** Número de aplicações de regras para cada tipo de EUD.\n",
            "\n",
            "**Figura 7.** Uma nova regra, criada para anotar sentenças como \"Essa lei permitiu-lhes ganhar um aumento de salário\", onde \"lhes\" é um objeto indireto pronominal (IOBJ) de um governador da relação xcomp (HEADXCOMP), \"permitiu\", portanto, deve ganhar uma nova relação aprimorada como nsubj do dependente xcomp (DEPXCOMP), \"ganhar\".\n",
            "\n",
            "A Tabela 4 mostra o desempenho do programa considerando ambas as amostras (teste e desenvolvimento) e ambos os conjuntos de regras (originais e modificadas). O ELAS indica o desempenho geral do programa. Os itens 1 a 6 representam o desempenho, de acordo com a métrica F1-score, para cada um dos seis tipos de EUD. A última linha mostra o número de sentenças em que uma inserção de token vazio foi feita para resolver uma elipse, mas a inserção foi feita incorretamente. Considerando que sentenças com elipses são mais desafiadoras para anotar, pois requerem que o token vazio inserido na sentença seja colocado na posição correta, e considerando que várias relações na sentença podem sofrer impacto negativo devido à colocação incorreta desse token vazio, calculamos dois tipos de ELAS: um considerando toda a amostra e outro excluindo as sentenças com elipses de predicado.\n",
            "\n",
            "| gold-dev | gold-test |\n",
            "|----------|-----------|\n",
            "|          | Original  | Modif.   | Original | Modif.  |\n",
            "| **ELAS** | 61,36%    | 78,97%   | 84,67%   | 96,05%  |\n",
            "| **ELAS (excluindo sentenças c/ elipses)** | 88,50% | 99,07% | 88,97% | 96,05% |\n",
            "| 1 - Adição de preposições e conjunções | 93,35% | 98,99% | 85,39% | 89,89% |\n",
            "| 2 - Atribuição de sujeitos xcomp | 72,00% | 87,94% | 84,13% | 96,43% |\n",
            "| 3 - Propagação da cabeça da conj | 84,11% | 92,47% | 96,67% | 96,67% |\n",
            "| 4 - Propagação de dependentes da conj | 88,28% | 100,0% | 94,23% | 94,23% |\n",
            "| 5 - Anotação do referente do pronome relativo | 100,0% | 9,05% | | |\n",
            "| 6 - Inclusão de predicado elíptico | 40,71% | 0% | | |\n",
            "| Sentenças com token vazio deslocado | 21 | 8 | 2 | 0 |\n",
            "\n",
            "**Tabela 4.** ELAS geral e por tipo de EUD.\n",
            "\n",
            "De modo geral, observamos que os números são mais baixos na amostra de desenvolvimento, refletindo o fato de que ela contém muitas mais sentenças com elipses do que a amostra de teste e que os fenômenos foram selecionados por sua complexidade. Os resultados são superiores utilizando o conjunto de regras modificadas, chegando a até 99,07% de ELAS para a amostra de desenvolvimento, excluindo sentenças com elipses. Para sentenças com elipses de predicado, reduzimos o número de erros na inserção do token vazio. Na amostra de teste, os erros caíram de 2 para 0, e na amostra de desenvolvimento, de 21 para 8. Consequentemente, na amostra de teste, os resultados para relações relacionadas à inclusão do predicado elíptico atingem 100%, mas na amostra de desenvolvimento, onde as sentenças são mais complexas, só conseguimos alcançar 40,71% de ELAS, indicando que ainda há espaço para melhorias em sentenças particularmente difíceis.\n",
            "\n",
            "Comparando os números de regras modificadas e as originais, a melhora no desempenho obtido é evidente. Ao usar a distribuição de dados regular do treebank como referência (dados de teste), onde a elipse de predicado não é muito frequente, obtemos 11,38 ELAS absoluta melhor usando o conjunto de regras modificadas em comparação ao conjunto original. Conforme observado pelo Grew em 2021 (Guillaume e Perrier 2021), o desempenho do parser depende fortemente da precisão do parser sintático básico. Trabalhando com anotação UD padrão, a anotação EUD está acima de 92% de ELAS para todas as línguas, sendo o inglês a que tem o melhor desempenho (99% ELAS) e o lituano a com o menor desempenho (92,1%). Nosso resultado para o português, em comparação, seria de 96,05% de ELAS utilizando o conjunto de regras modificadas.\n",
            "\n",
            "Observamos que rotular as relações de dependência entre o token vazio e os antigos participantes da relação órfã permanece particularmente desafiador. Dicas para isso podem ser encontradas na cláusula cabeça de conj: as relações de dependência disponíveis são aquelas que existem na cláusula cabeça e não existem na cláusula dependente. No entanto, argumentos semanticamente equivalentes frequentemente têm formas sintáticas diferentes (por exemplo, um modificador temporal pode ocorrer como advmod, obl ou advcl), o que torna o rotulamento das relações de dependência difícil. A tarefa é computacionalmente complexa, e, uma vez que a ocorrência desse fenômeno é infrequente, recomendamos revisar manualmente todas as relações após a inserção do token vazio até que avancemos nas soluções para melhorar a precisão. \n",
            "\n",
            "Notamos que as dependências aprimoradas da inserção do token elíptico e da anotação co-referencial (ref), porque apresentam uma anotação alternativa à daquela das dependências básicas, constituem uma nova base para as outras dependências aprimoradas. Isso tem duas implicações: (1) como constituem uma nova base, esses dois tipos aprimorados devem ser anotados antes dos outros, e (2) erros nesses dois tipos aprimorados podem gerar erros em cascata nas outras anotações aprimoradas. Por exemplo, na frase da Figura 8, quando o programa não identifica que \"bandidos\" é o sujeito do token vazio, o slot do sujeito fica vazio e as regras de propagação do sujeito conjuntivo anotam \"gente\" como o sujeito do token vazio, o que é incorreto.\n",
            "\n",
            "**5. Considerações Finais** Abordamos a questão da anotação automática de dependências aprimoradas para o português, que, até onde sabemos, consiste na primeira tentativa para esta língua. O sistema apresentado juntamente com nossas regras modificadas mostrou sua eficácia em gerar automaticamente anotações completas, servindo como um recurso valioso para análises linguísticas posteriores e treinamento de modelos, alcançando um ELAS geral de 96,05% sobre a anotação sintática básica padrão.\n",
            "\n",
            "O próximo passo é usar este sistema e regras para anotar completamente o Porttinari, criando o primeiro treebank UD com anotações EUD para o português brasileiro. Aproveitando as capacidades do Grew, pretendemos fornecer anotações abrangentes e precisas que incluam todos os 6 tipos de dependências aprimoradas, que serão feitas em lotes com supervisão humana para garantir a qualidade do conjunto de dados.\n",
            "\n",
            "Mais informações sobre este trabalho podem ser encontradas no portal do projeto POeTiSA: https://sites.google.com/icmc.usp.br/poetisa\n",
            "\n",
            "**Agradecimentos** Este trabalho foi realizado no Centro de Inteligência Artificial da Universidade de São Paulo (C4AI - http://c4ai.inova.usp.br/), com apoio da Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP, contrato #2019/07665-4) e da IBM Corporation. O projeto também foi apoiado pelo Ministério da Ciência, Tecnologia e Inovações, com recursos da Lei N. 8.248, de 23 de outubro de 1991, no âmbito do PPI-SOFTEX, coordenado pela Softex e publicado como Residência em TIC 13, DOU 01245.010222/2022-44.\n",
            "27\n",
            "**Resumo.** O campo do Reconhecimento Automático de Fala (ASR) se expandiu significativamente dentro do cenário tecnológico devido ao seu extenso uso em setores como educação, saúde e atendimento ao cliente. Muitas aplicações modernas dependem da análise de conteúdo falado por meio de modelos de conversão de Fala para Texto (STT). No entanto, as transcrições produzidas por esses sistemas muitas vezes contêm elementos indesejáveis, como repetições de palavras e a prolongação de certos sons, conhecidos como disfluências ou muletas linguísticas. Esses elementos podem afetar negativamente a qualidade da análise automática de conteúdo por modelos de Processamento de Linguagem Natural (NLP), incluindo aqueles para reconhecimento de entidades nomeadas, detecção de emoções ou análise de sentimentos. Portanto, este estudo tem como objetivo avaliar a viabilidade de identificar e eliminar disfluências linguísticas usando Modelos de Linguagem de Grande Escala (LLMs), como GPT-4, LLaMA, Claude e Gemini, através de técnicas de Engenharia de Prompt. A abordagem foi testada usando um corpus de transcrições de debates com ocorrências de disfluência anotadas manualmente, gerando resultados promissores. \n",
            "\n",
            "**1. Introdução**  \n",
            "O Reconhecimento Automático de Fala (ASR) tornou-se essencial na sociedade moderna, permitindo a conversão da fala humana em texto escrito. Essa tecnologia facilita uma variedade de aplicações através de modelos de Fala para Texto (STT), incluindo assistentes virtuais, transcrição de reuniões, legendagem automática e atendimento ao cliente. Apesar de avanços significativos na precisão do reconhecimento de fala, uma característica constante nas transcrições geradas por esses sistemas é a presença de disfluências linguísticas. Durante a produção da fala humana, é comum gerar vários sons dentro da fala, conhecidos como disfluências. \n",
            "\n",
            "As disfluências foram amplamente estudadas e são classificadas principalmente em três tipos: hesitações, repetições e correções [Corley e Stewart 2008]. Quando um modelo de fala transcreve a voz em texto, muitas vezes não considera o contexto das palavras faladas, focando em alcançar uma transcrição precisa. Como resultado, essas disfluências são comuns e aparecem em transcrições de legendas, notas de reuniões e em qualquer texto derivado da fala humana espontânea. Vários estudos exploram diferentes técnicas para a detecção de disfluências, variando de abordagens unimodais a multimodais, algumas até utilizam métodos baseados em Transformers, mas nenhuma investiga minuciosamente a utilidade dos modernos e amplamente acessíveis Modelos de Linguagem de Grande Escala (LLMs) para a detecção e remoção de muletas linguísticas. \n",
            "\n",
            "Os LLMs baseados em Transformers apresentam uma alternativa promissora. Devido à sua capacidade de capturar contextos complexos e entender nuances linguísticas, como diferenciar entre texto disfluente e fluente, eles representam uma alternativa promissora. Os LLMs podem ser facilmente manipulados usando técnicas de Engenharia de Prompt, que envolvem a criação de instruções para orientar seu comportamento em direção a um objetivo específico. Este trabalho visa preencher uma lacuna no estudo da detecção e remoção de disfluências em transcrições de texto, avaliando as capacidades dos LLMs mais avançados disponíveis hoje, como o GPT-4 da OpenAI [OpenAI et al. 2024], Gemini 1.5 Pro Experimental 0827 [Team et al. 2024], Claude 3.5 [Anthropic 2024] e LLaMa 3 (70B parâmetros) [Meta 2024], para avaliar a extensão de sua aplicabilidade a essa tarefa. \n",
            "\n",
            "As principais contribuições deste artigo incluem:  \n",
            "• Uma análise da capacidade dos LLMs de remover trechos textuais específicos enquanto preservam outras informações relevantes.  \n",
            "• Uma análise comparativa dos modelos disponíveis e sua eficácia em lidar com a fala humana espontânea transcrita.  \n",
            "• Uma avaliação da viabilidade, em termos de custo computacional, de limpeza de transcrições da fala humana natural.  \n",
            "• Um conjunto de dados com disfluências anotadas em português brasileiro.  \n",
            "\n",
            "As seções seguintes deste artigo estão organizadas da seguinte forma: A Seção 2 apresenta uma revisão da literatura, cobrindo pesquisas fundamentais e relevantes sobre detecção e remoção de disfluências, levando ao estado atual da arte. A Seção 3 detalha a metodologia de pesquisa, explicando os processos de coleta e tratamento de dados, bem como a construção de prompts e uma análise exploratória de dados, seguida pela Seção 4, que apresenta os resultados. Por fim, a Seção 5 oferece a conclusão. \n",
            "\n",
            "**2. Trabalhos Relacionados**  \n",
            "A pesquisa sobre a detecção e remoção de disfluências na fala abrange uma variedade de técnicas, cada uma contribuindo para o avanço do estado da arte nesse campo. Os estudos nesse domínio normalmente utilizam um dos três tipos de entrada: transcrição textual, sinal de áudio ou uma combinação dos dois. Soluções unimodais dependem de uma única fonte de informação, enquanto soluções multimodais integram múltiplas fontes, como áudio e texto, para realizar a tarefa de detecção/removal de disfluências. As próximas subseções apresentam pesquisas realizadas usando a abordagem textual unimodal, seguidas pela abordagem de áudio unimodal, uma comparação entre as duas abordagens e, finalmente, a conclusão desta seção. \n",
            "\n",
            "**2.1. Abordagens Baseadas em Texto**  \n",
            "Neste contexto, [Snover et al. 2004] propuseram um algoritmo de Aprendizado Baseado em Transformação (TBL) para a detecção de disfluências em transcrições de fala, empregando características lexicais (uso de palavras e estrutura de frases). O sistema, denominado Sistema A, obteve resultados comparáveis aos que usam características prosódicas (variações de entonação, ritmo, duração e intensidade/volume da fala), demonstrando que um desempenho satisfatório pode ser alcançado sem depender fortemente de pistas prosódicas. O estudo enfatiza a importância de características como o lexema em si, etiquetas de Parte do Discurso (POS) e frequência de palavras para o falante na identificação de disfluências. O Sistema A apresentou resultados promissores na detecção de vários tipos de disfluências e pavimentou o caminho para futuras pesquisas focadas em técnicas de processamento de linguagem natural. \n",
            "\n",
            "[Ferguson et al. 2015] propuseram um método semi-Markoviano condicional (semi-CRF) para a detecção de disfluências em transcrições de fala, focando em reparos como repetições e arranques falsos. Essa técnica utiliza características lexicais, estruturais e prosódicas, como pausas e duração de palavras, extraídas do alinhamento com o sinal de fala. Essa abordagem alcançou um F-score de 85,4% no corpus Switchboard (um conjunto de dados composto por conversas telefônicas em inglês coletadas nos Estados Unidos durante a década de 1990), superando o desempenho de estudos anteriores. Paralelamente, [Zayats et al. 2016] introduziram um novo método para a detecção de disfluências em transcrições de fala usando uma rede neural LSTM Bidirecional (BLSTM). A solução deles utiliza embeddings de palavras (representações numéricas de palavras), etiquetas POS e características de padrão lexical como entrada. Além disso, o modelo incorpora um mecanismo de reparo explícito e usa Programação Linear Inteira (ILP) para impor restrições estruturais na sequência de disfluência. Essa abordagem obteve um F-score de 85,9% no corpus Switchboard. A análise dos resultados indica que essa abordagem apresenta melhor desempenho na detecção de disfluências complexas que não envolvem meras repetições de palavras. Apesar de sua eficácia, a dependência do modelo em recursos predefinidos limita sua adaptabilidade a diferentes tipos de disfluências, contextos e estilos de fala. \n",
            "\n",
            "[Bach e Huang 2019] também exploraram a técnica BiLSTM com autoatenção para a detecção de disfluências em transcrições de fala. Os autores demonstraram resultados competitivos com o BERT no corpus Switchboard, superando-o em termos de robustez e eficiência em conjuntos de dados de domínio diferente. A adição artificial de palavras extras e incorretas durante o treinamento do modelo mostrou-se altamente eficaz para melhorar sua robustez a vários tipos de dados e erros de transcrição, tornando-se uma alternativa atraente para a detecção de disfluências em cenários do mundo real. Além disso, os modelos propostos são menores que o BERT, o que resulta na redução das necessidades de recursos computacionais em geral. \n",
            "\n",
            "**2.2. Abordagens Baseadas em Áudio**  \n",
            "[Bassi et al. 2023] propõem uma abordagem de ponta a ponta para a transcrição de fala com remoção de disfluências usando um modelo acústico HuBERT pré-treinado em grande escala. O método tradicional em duas etapas, que primeiro transcreve o áudio em texto e depois remove disfluências, negligencia as pistas prosódicas presentes no áudio original. A abordagem proposta processa o áudio diretamente e utiliza representações acústicas aprendidas durante o pré-treinamento para identificar e remover disfluências durante a transcrição. Os autores demonstram que a solução de ponta a ponta supera a abordagem em duas etapas em termos de Taxa de Erro de Palavra (WER) e Taxa de Erro de Caracteres (CER) no conjunto de testes Switchboard, alcançando 12,2% de WER e 7,3% de CER. O estudo também destaca a importância do objetivo de pré-treinamento: o HuBERT, pré-treinado com um objetivo de agrupamento que agrupa representações de áudio com base em semelhanças, superou significativamente o Wav2Vec2, que foi pré-treinado com um objetivo contrastivo que maximiza a similaridade entre amostras semelhantes e minimiza a similaridade entre amostras diferentes. Esses resultados sugerem que modelos de ponta a ponta com pré-treinamento acústico em larga escala com objetivos de agrupamento são uma abordagem promissora para a transcrição precisa da fala disfluente. \n",
            "\n",
            "**2.3. Comparação Entre Modelos Unimodais e Multimodais**  \n",
            "[Romana et al. 2023] investigaram a detecção automática de disfluências na fala comparando métodos baseados em linguagem, acústica e multimodais. Seus resultados demonstraram que, enquanto modelos de linguagem como o BERT apresentaram alta precisão com transcrições manuais, a performance caiu significativamente com o uso de transcrições geradas por Reconhecimento Automático de Fala (ASR). Abordagens acústicas utilizando modelos como Wav2Vec 2.0, HuBERT e WavLM mostraram-se promissoras ao evitarem a dependência de transcrições. No entanto, os autores descobriram que soluções multimodais que combinam informações acústicas e linguísticas por meio de uma rede de fusão BLSTM alcançaram os melhores resultados, superando técnicas unimodais na detecção e categorização de disfluências. Este estudo destaca o potencial dos métodos multimodais para criar sistemas de detecção de disfluências mais robustos. \n",
            "\n",
            "Os trabalhos acadêmicos apresentados nesta seção ilustram o progresso feito no campo, com técnicas avançadas de inteligência artificial, transformers e robustos métodos multimodais aplicáveis a vários tipos de dados e erros de transcrição. Essas soluções provaram ser eficazes na detecção e remoção de disfluências em diversos contextos. No entanto, o uso de Modelos de Linguagem de Grande Escala (LLMs) amplamente disponíveis para a limpeza de transcrições automáticas foi insuficientemente estudado. Portanto, existe a necessidade de investigar como os LLMs podem ser aproveitados para esse fim, complementando os avanços alcançados nos trabalhos acadêmicos revisados e democratizando o acesso a essas tecnologias. \n",
            "\n",
            "**3. Metodologia**  \n",
            "Esta seção contém informações sobre a metodologia utilizada na pesquisa, incluindo como os dados foram obtidos e organizados, e a construção dos prompts. \n",
            "\n",
            "**3.1. O Conjunto de Dados**  \n",
            "O conjunto de dados para este estudo consiste em texto extraído de quatro sessões de debate realizadas na Universidade Federal de Campina Grande para analisar o desempenho dos debatedores. Inclui 114 minutos de áudio transcrito em português, fornecendo insights sobre a dinâmica e a eficácia de várias técnicas de debate. Os debates foram moderados, com cada sessão envolvendo de 4 a 5 debatedores discutindo tópicos relacionados à Inteligência Artificial. Cada debatedor teve a oportunidade de falar após perguntas feitas pelo moderador, e interrupções não foram permitidas, resultando em um discurso espontâneo e fluido. Após os debates, as gravações de áudio foram transcritas usando o modelo da Azure da Microsoft, com as transcrições armazenadas em um arquivo JSON. Este arquivo foi então convertido em tabelas do Excel contendo todos os dados transcritos. Os dados passaram por uma revisão humana para corrigir erros de transcrição significativos, como palavras inexistentes ou sem sentido. Além disso, cada tabela foi anotada para disfluências. As disfluências foram categorizadas em três tipos: hesitações, repetições e correções. Quatro tags de estilo HTML foram criadas para marcar essas disfluências no texto:  \n",
            "• <hes {conteúdo}/>, que marca hesitações  \n",
            "• <rep {conteúdo}/>, que marca repetições   \n",
            "• <erro {conteúdo}/>, que marca erros  \n",
            "• <corr {conteúdo}/>, que marca correções  \n",
            "\n",
            "Esse processo de marcação e correção resultou em quatro arquivos do Excel com as transcrições dos respectivos debates. Esses arquivos foram então submetidos a uma análise exploratória de dados. \n",
            "\n",
            "**3.2. Os Prompts**  \n",
            "Para realizar a tarefa de detecção e remoção de disfluências, quatro prompts diferentes foram desenvolvidos. Para determinar qual técnica de prompt é mais eficaz, três tipos de métodos de engenharia de prompt foram testados:  \n",
            "• Zero-Shot Prompting  \n",
            "• Few-Shot Prompting  \n",
            "• Chain-of-Thought Prompting  \n",
            "\n",
            "Esses três tipos de prompts diferem significativamente na forma como apresentam informações ao modelo de linguagem (LLM), e o estudo visa entender a extensão do conhecimento dos LLMs sobre disfluências. No caso Zero-Shot, o prompt fornece pouco ou nenhum contexto sobre a tarefa, sendo dividido em dois prompts. O primeiro prompt é um comando direto ao LLM para remover repetições, hesitações e correções do texto, mantendo-o inalterado. O segundo prompt adiciona uma descrição do que são disfluências e como os três tipos almejados são caracterizados. O prompt Few-Shot inclui todas as informações dos dois primeiros prompts, além de um exemplo de texto disfluente em três estágios: o texto original disfluente, o texto com tags de disfluência e o texto limpo. Por fim, o prompt Chain-of-Thought é projetado para ajudar o LLM a adotar uma abordagem passo a passo para detectar e remover disfluências do texto. Esses quatro prompts foram executados com cada um dos LLMs. O número médio de tokens processados pelos LLMs no Grupo 14, o menor grupo, variou de 4.273 tokens (com o menor prompt) a 5.041 tokens (com o maior prompt). Em contraste, o Grupo 1, o maior grupo, processou entre 5.250 tokens (menor prompt) e 6.004 tokens (maior prompt). Esse cálculo foi estimado usando o Tokenizer da Plataforma OpenAI. \n",
            "\n",
            "**Tabela 1.** Prompts Criados Para a Tarefa  \n",
            "| Técnica de Prompting | Contexto |  \n",
            "|----------------------|----------|  \n",
            "| Zero-Shot Prompting | Nenhum |  \n",
            "| Zero-Shot Prompting | Definição de disfluências |  \n",
            "| Few-Shot Prompting | Definição de disfluências e uma visão em três etapas do texto durante o processo de limpeza de disfluências |  \n",
            "| Chain-of-Thought Prompting | Definição de disfluências mais um guia sobre como reconhecer e remover cada tipo de disfluência |  \n",
            "\n",
            "**3.3. Análise Exploratória de Dados**  \n",
            "Os dados das transcrições marcadas nos arquivos do Excel foram analisados para obter uma visão geral de como cada texto disfluente é caracterizado. A análise inicial focou na quantidade de disfluências por grupo. Para isso, as disfluências foram contadas em cada arquivo utilizando os marcadores descritos na seção do Conjunto de Dados. Essas contagens foram agregadas para cada grupo, e os totais foram visualizados utilizando gráficos para auxiliar na interpretação. A Figura 1 exibe a comparação das taxas de disfluências entre quatro grupos, rotulados 1, 14, 8 e 7 no eixo X. Esta figura apresenta a proporção de disfluências calculadas por 100 palavras para cada grupo, facilitando a comparação da frequência relativa de disfluências entre os grupos. A Figura 2, utilizando os mesmos rótulos de grupo (1, 14, 8 e 7) no eixo X, ilustra o número de disfluências segmentado por tipo (hesitação, erro, repetição). Esta figura ilustra a distribuição de diferentes tipos de disfluências dentro de cada grupo. \n",
            "\n",
            "**3.4. Configuração e Execução dos LLMs**  \n",
            "A execução de dados em Modelos de Linguagem de Grande Escala (LLMs) foi realizada através de APIs específicas. O Google Gemini 1.5 Pro Experimental 0827, Claude 3.5 Sonnet da Anthropic e o ChatGPT-4o da OpenAI foram acessados através das APIs oficiais fornecidas por suas respectivas empresas. O LLaMa 3 72B foi utilizado através da plataforma Groq. A implementação foi estruturada em 16 notebooks Python no ambiente Google Colaboratory, com quatro notebooks designados para cada LLM, correspondendo aos grupos de debate. Cada notebook foi inicializado com a configuração do LLM correspondente, seguido pelos códigos de extração de resultados detalhados nesta seção metodológica, e então executado usando os prompts pré-estabelecidos. Os resultados obtidos foram registrados ao final de cada notebook, sendo compilados em tabelas para este trabalho e analisados para os objetivos da pesquisa. Em um notebook separado, foi realizada uma análise exploratória de dados utilizando os arquivos do Excel dos grupos, com os procedimentos e resultados descritos em detalhe neste trabalho. \n",
            "\n",
            "**4. Resultados**  \n",
            "| Modelo  | Gemini | GPT-4o | LLaMa | Claude |  \n",
            "|---------|--------|--------|-------|--------|  \n",
            "| Tabela 2. Zero-Shot (Sem Contexto) - Grupo 14 | | | | |  \n",
            "| Taxa Total de Remoção | 14,06% | 62,50% | 53,12% | 57,81% |  \n",
            "| Semelhança de Levenshtein | 97,95% | 96,83% | 95,24% | 32,98% |  \n",
            "| Tempo (segundos) | 85,69 | 49,77 | 21,86 | 18,47 |  \n",
            "\n",
            "Os dados apresentados nas Tabelas 2 e 3 mostram claramente que, ao usar prompts Zero-Shot no Grupo 14 (o grupo mais disfluente), o GPT-4o, Gemini e LLaMa mantiveram uma estrutura textual relativamente boa, como indicado pelo valor de semelhança de Levenshtein. Nenhum dos LLMs conseguiu equilibrar com sucesso a remoção de disfluências enquanto preservava a qualidade do texto original com esses dois prompts, mas os resultados de manutenção de texto para Claude e LLaMa ficaram significativamente abaixo das expectativas para a maioria dos prompts testados, tornando-os atualmente não confiáveis para esse tipo de tarefa. Portanto, a análise a seguir foca exclusivamente no GPT-4o e no Gemini 1.5.  \n",
            "\n",
            "| Modelo  | Gemini | GPT-4o |  \n",
            "|---------|--------|--------|  \n",
            "| Tabela 4. Resultados do Teste para GPT-4o e Gemini - Few Shot - Grupo 14 | | |  \n",
            "| Taxa Total de Remoção | 51,56% | 67,19% |  \n",
            "| Semelhança de Levenshtein | 98,18% | 96,83% |  \n",
            "| Tempo (segundos) | 82,57 | 74,73 |  \n",
            "\n",
            "Com o prompting Few-Shot (Tabela 4), o GPT-4o alcançou uma taxa de remoção de disfluências de 67,19% enquanto mantinha 96,83% do texto original. Ele também superou o Gemini em tempo de resposta. Embora o Gemini tenha apresentado uma taxa de manutenção de texto um pouco mais alta, teve um desempenho fraco na remoção de disfluências. \n",
            "\n",
            "| Modelo  | Gemini | GPT-4o |  \n",
            "|---------|--------|--------|  \n",
            "| Tabela 5. Resultados do Teste para GPT-4o e Gemini - Chain of Thought - Grupo 14 | | |  \n",
            "| Taxa Total de Remoção | 26,56% | 68,75% |  \n",
            "| Semelhança de Levenshtein | 98,09% | 97,85% |  \n",
            "| Tempo (segundos) | 84,61 | 45,76 |  \n",
            "\n",
            "Usando o prompting Chain-of-Thought (Tabela 5), o GPT-4o foi o único entre os quatro LLMs a produzir um resultado minimamente favorável. Quando comparado ao Gemini, o GPT-4o alcançou uma taxa total de remoção de disfluências de 68,75%, apesar de uma taxa de manutenção de texto semelhante, enquanto o Gemini, embora mantivesse a qualidade do texto, falhou na remoção de disfluências. \n",
            "\n",
            "| Modelo  | Gemini | GPT-4o |  \n",
            "|---------|--------|--------|  \n",
            "| Tabela 6. Resultados do Teste para GPT-4o e Gemini - Few Shot - Grupo 8 | | |  \n",
            "| Taxa Total de Remoção | 62,96% | 48,15% |  \n",
            "| Semelhança de Levenshtein | 95,80% | 88,22% |  \n",
            "| Tempo (segundos) | 102,58 | 47,05 |  \n",
            "\n",
            "| Modelo  | Gemini | GPT-4o |  \n",
            "|---------|--------|--------|  \n",
            "| Tabela 7. Resultados do Teste para GPT-4o e Gemini - Chain of Thought - Grupo 8 | | |  \n",
            "| Taxa Total de Remoção % | 33,33% | 40,74% |  \n",
            "| Semelhança de Levenshtein % | 98,19% | 88,51% |  \n",
            "| Tempo (segundos) | 104,26 | 45,25 |  \n",
            "\n",
            "No Grupo 8, um dos grupos menos disfluentes, a eficácia do GPT-4o diminuiu tanto na remoção de disfluências quanto na manutenção da fluência do texto, conforme mostrado nas Tabelas 6 e 7.  \n",
            "\n",
            "| Modelo  | Gemini | GPT-4o |  \n",
            "|---------|--------|--------|  \n",
            "| Tabela 8. Resultados do Teste para GPT-4o e Gemini - Few Shot - Grupo 1 | | |  \n",
            "| Taxa Total de Remoção | 63,89% | 68,06% |  \n",
            "| Semelhança de Levenshtein | 96,68% | 78,18% |  \n",
            "| Tempo (segundos) | 126,40 | 52,48 |  \n",
            "\n",
            "| Modelo  | Gemini | GPT-4o |  \n",
            "|---------|--------|--------|  \n",
            "| Tabela 9. GPT-4o e Gemini - Chain of Thought - Grupo 1 | | |  \n",
            "| Taxa Total de Remoção | 25,00% | 68,06% |  \n",
            "| Semelhança de Levenshtein | 97,85% | 77,55% |  \n",
            "| Tempo (segundos) | 127,22 | 53,69 |  \n",
            "\n",
            "O Gemini alcançou uma taxa de remoção de 62,96% com boa manutenção do texto, embora tenha levado mais do que o dobro do tempo. Essa tendência, na qual o GPT-4o não igualou o Gemini na manutenção do texto, também foi observada no Grupo 1, conforme mostrado nas Tabelas 8 e 9, que é o maior grupo, mas não tão disfluente quanto o Grupo 14. Os modelos demonstraram alta eficácia na remoção de repetições, alcançando 91,30% de remoção no Grupo 14 para o GPT-4o, em comparação com 56,52% do Gemini no prompt Few-Shot. No prompt Chain-of-Thought, o GPT-4o manteve uma taxa de remoção consistente de 91,30% enquanto também superava o Gemini em tempo de processamento. Embora o GPT-4o tenha mostrado um forte desempenho no Grupo 14, teve dificuldades no Grupo 1, onde o Gemini alcançou 87,18% de remoção com preservação de texto superior (96,68%). Esses resultados sugerem que, enquanto o GPT-4o se destaca em contextos específicos, o Gemini pode ser mais robusto ao lidar com textos maiores e mais complexos.\n",
            "\n",
            "**5. Conclusão e Trabalhos Futuros**  \n",
            "Este estudo explorou a eficácia de Modelos de Linguagem de Grande Escala (LLMs) na detecção e eliminação de disfluências linguísticas de transcrições de debates acadêmicos. Ao alavancar técnicas avançadas de engenharia de prompts, como Zero-Shot, Few-Shot e Chain-of-Thought prompting, avaliamos o desempenho dos principais LLMs — GPT-4, Gemini 1.5, Claude 3.5 e LLaMa 3 — nessa tarefa. Os resultados revelaram vários insights chave sobre as capacidades e limitações desses modelos. O GPT-4o demonstrou o melhor desempenho geral na remoção de disfluências, alcançando um equilíbrio ideal entre remover disfluências e manter a coerência do texto, particularmente sob condições de prompting Few-Shot e Chain-of-Thought. O Gemini 1.5 também teve um bom desempenho, mas apresentou variabilidade dependendo do tipo de prompt e do grupo de debate específico analisado. Ele se destacou na manutenção do texto, mas teve taxas de remoção mais baixas em comparação com o GPT-4o em alguns casos. O Claude 3.5 e o LLaMa 3 produziram resultados mais fracos, lutando para manter a coerência do texto enquanto removiam disfluências. O GPT-4o demonstrou tempos de processamento mais eficientes em comparação com os outros modelos, o que é crucial para aplicações práticas no mundo real. Em conclusão, embora LLMs como GPT-4o e Gemini 1.5 mostrem promessas para melhorar a qualidade das transcrições ao remover disfluências, são necessários avanços adicionais—como ajuste fino, uso de técnicas de engenharia de prompt mais avançadas, integração de LLMs amplamente utilizados com sistemas multimodais ou desenvolvimento de futuros modelos—para aprimorar totalmente suas capacidades. \n",
            "28\n",
            "Resumo. Apresentamos o Quati,1 um conjunto de dados especificamente projetado para avaliar sistemas de Recuperação de Informação (IR) para a língua portuguesa brasileira. Ele inclui uma coleção de consultas formuladas por falantes nativos e um conjunto curado de documentos extraídos de uma seleção de sites em português brasileiro frequentemente acessados, o que garante um corpus representativo e relevante. Para rotular os pares de consulta-documento, utilizamos um LLM de última geração, que apresenta níveis de concordância entre anotadores comparáveis ao desempenho humano em nossas avaliações. Nossa metodologia de anotação é descrita, permitindo a criação econômica de conjuntos de dados semelhantes para outras línguas, com um número arbitrário de documentos rotulados por consulta. Como referência, avaliamos uma ampla gama de recuperadores de código aberto e comerciais. O Quati está disponível publicamente em https://huggingface.co/datasets/unicamp-dl/quati, e todos os scripts podem ser encontrados em https://github.com/unicamp-dl/quati.\n",
            "\n",
            "1. Introdução\n",
            "O desenvolvimento de sistemas de Recuperação de Informação (IR) depende de conjuntos de dados de avaliação de alta qualidade, que devem conter consultas e documentos idealmente na mesma língua-alvo desses sistemas, a fim de capturar necessidades específicas de informação e aspectos socioculturais. Isso contrasta com conjuntos de dados traduzidos, que potencialmente representam as necessidades de informação e conhecimentos de uma cultura ou sociedade diferentes. Assim, conjuntos de dados traduzidos podem não medir efetivamente a capacidade de um sistema de recuperação em cenários do mundo real que envolvem usuários nativos. \n",
            "\n",
            "*Contribuição igual.\n",
            "1 Nomeamos nosso conjunto de dados em homenagem a este mamífero sul-americano, cujo comportamento de forrageamento representa a busca resoluta por recursos. Apesar de ser uma das línguas mais faladas do mundo, existe uma escassez de conjuntos de dados de IR em português. Conjuntos de dados existentes, como REGIS [Lima de Oliveira et al. 2021] e RCV1 [Lewis et al. 2004]2, embora valiosos, ficam aquém devido ao seu tamanho limitado e domínios especializados (geociências e notícias). Enquanto conjuntos de dados traduzidos, como mMARCO [Bonifacio et al. 2021] e mRobust04 [Jeronymo et al. 2022], ajudaram a aliviar esse problema, o uso de traduções automáticas geralmente representa a perda de características socioculturais das línguas-alvo, e as avaliações podem acabar sendo enviesadas pela língua de origem.\n",
            "\n",
            "Para abordar essas questões, criamos o Quati, um conjunto de dados de avaliação em português brasileiro, composto por consultas escritas por humanos e um corpus nativo de alta qualidade. O Quati é criado utilizando um pipeline semi-automatizado, visando reduzir a barreira de custo de rotulagem. Usamos um Modelo de Linguagem de Grande Escala (LLM) para julgar a relevância de um trecho para uma determinada consulta, publicando um pipeline econômico para criar um conjunto de dados de avaliação de IR com um número arbitrário de trechos anotados por consulta.3 Neste contexto, nosso trabalho visa responder à seguinte questão de pesquisa: Pode-se usar LLMs para compor um pipeline semi-automatizado para anotar a relevância de consultas e trechos para sistemas de IR em português brasileiro?\n",
            "\n",
            "Para avaliar a qualidade das anotações do LLM, comparamos-as com anotações humanas em uma amostra de pares de consulta e trecho e confirmamos um coeficiente Kappa de Cohen de 0.31. Embora esse número esteja abaixo do 0.41 observado na concordância de anotações humanas, é consistente com as descobertas relatadas na literatura [Faggioli et al. 2023, Thomas et al. 2023, Farzi e Dietz 2024] e provavelmente aumentará à medida que os LLMs melhorarem em qualidade. O uso de um pipeline modular semi-automatizado permite que o método de construção do conjunto de dados seja replicado para criar conjuntos de dados de IR de alta qualidade para outras línguas.\n",
            "\n",
            "2. Trabalho Relacionado\n",
            "[Jeronymo et al. 2022], Mr.Tydi\n",
            "Conjuntos de dados de avaliação são uma variável importante no contexto de IR, pois expõem as limitações dos sistemas de busca e orientam seu desenvolvimento. No entanto, a maioria dos conjuntos de dados disponíveis está em inglês, como é o caso do MS MARCO [Bajaj et al. 2016]. Existem esforços em desenvolvimento de CIR [Sakai et al. 2021] e conjuntos de dados HC4 para outras línguas, mas a maioria se baseia na tradução de língua para adaptar o inglês à língua-alvo. Esforços em andamento [Lima de Oliveira et al. 2021, Vitório et al. 2024] estão começando a mudar esse cenário criando conjuntos de dados de IR para português brasileiro, mas até agora focando em domínios específicos. [Lawrie et al. 2022] são esforços que não incluem português ou não abordam línguas.\n",
            "\n",
            "A criação de conjuntos de dados para IR é uma tarefa que exige muitos recursos, particularmente no processo de julgar a relevância de documentos. Empreendimentos recentes testemunharam uma mudança em direção ao uso de LLMs para avaliar a relevância de consultas e trechos [Zendel et al. 2024]. Faggioli et al. [Faggioli et al. 2023] ressaltaram ainda mais o potencial de empregar LLMs para automatizar o julgamento da relevância de documentos, abrindo assim promissoras avenidas para exploração nesse domínio. Avaliações complementares realizadas por Thomas et al. [Thomas et al. 2023] demonstraram uma correlação significativa entre julgamentos humanos e aqueles feitos pelo modelo GPT-3.5-turbo.\n",
            "\n",
            "3. Metodologia\n",
            "Usamos um método semi-automático para criar o Quati, conforme ilustrado na Figura 1. Os insumos necessários são: 1) um grande corpus, originalmente escrito na língua-alvo, do qual extraímos os trechos para compor nosso conjunto de dados de IR; 2) um conjunto de consultas de teste, criadas manualmente para representar as necessidades de informação de falantes nativos. Nas seções seguintes, detalhamos as etapas do pipeline.\n",
            "\n",
            "Figura 1. Metodologia proposta para a criação de conjuntos de dados de IR.\n",
            "\n",
            "3.1. Preparação de trechos\n",
            "A etapa de preparação de trechos é composta pelos seguintes subpassos:\n",
            "Coleta de dados: Usamos o subconjunto em português do ClueWeb22 [Overwijk et al. 2022] categoria B, que inclui 4,1 milhões de páginas da web mais propensas a serem visitadas de acordo com os algoritmos de busca do Bing durante a primeira metade de 2022 [Overwijk et al. 2022].\n",
            "Filtragem de URLs: Excluímos quaisquer documentos de nosso conjunto de dados cujos domínios de URL terminassem em “.pt”, que se referem ao português de Portugal, já que o estilo linguístico nesses documentos pode diferir significativamente do utilizado nas páginas da web em português brasileiro. Além disso, utilizamos o FastText [Joulin et al. 2016b, Joulin et al. 2016a] como um método adicional de verificação linguística para garantir que apenas documentos em português fossem incluídos em nosso corpus.\n",
            "Segmentação de documentos em trechos: Após a verificação linguística, segmentamos os documentos em segmentos de aproximadamente 1.000 caracteres e avaliamos a porcentagem de ocorrências de quebras de linha (%) dentro de cada segmento, removendo aqueles com mais de 20%. Esse critério foi usado para aumentar a probabilidade de reter segmentos predominantemente compostos de texto em linguagem natural.\n",
            "Separação em versões grandes e pequenas: Com o processo descrito nas etapas anteriores, coletamos um total de 20 milhões de segmentos. A partir desse conjunto, selecionamos aleatoriamente 10 milhões de segmentos (daqui em diante referidos como corpus 10M) para serem os trechos de nosso corpus, criando um conjunto de dados grande, mas ainda gerenciável, com mais de 11 GB de tamanho. Um segundo conjunto de dados foi construído a partir do primeiro aplicando regras de filtragem adicionais retiradas do MassiveWeb Corpus [Rae et al. 2021] — por exemplo, removendo trechos com mais de 10% de símbolos, ou com comprimento médio de palavras fora do intervalo de 3 a 10 — e amostrando apenas 1 milhão de segmentos (daqui em diante referidos como corpus 1M) dos 7M documentos filtrados resultantes — o objetivo era criar um conjunto de dados menor e de maior qualidade que facilitasse a experimentação com modelos de embutimento, já que a codificação dos 10 milhões de segmentos originais pode ser computacionalmente cara.\n",
            "\n",
            "3.2. Criação de consultas manuais\n",
            "Utilizamos consultas criadas por humanos para o conjunto de dados de avaliação, visando obter questões de alta qualidade para capturar necessidades comuns de informação de um corpus diversificado, criado por falantes nativos da língua-alvo. Criamos um total de 200 consultas de teste.\n",
            "\n",
            "3.3. Recuperação de trechos\n",
            "A próxima etapa é a recuperação de trechos para construir uma lista de consultas-trechos a serem anotados. Como seria proibitivo ter as pontuações de relevância para cada consulta em todo o corpus, optamos por anotar os k melhores trechos retornados por múltiplos sistemas de IR. Assume-se que a diversidade de seus resultados permitirá a coleta de uma variedade de trechos, criando um conjunto de dados de avaliação robusto. Selecionamos uma mistura de sistemas de IR fortes e fracos, para incluir uma variedade de trechos: BM25: uma linha de base forte para recuperação; BM25 + mT5-XL: pipeline em duas etapas com BM25 seguido por mT5-XL (3,7 bilhões de parâmetros) [Xue et al. 2020]; BM25 + E5-large: pipeline em duas etapas com BM25 e E5-large [Wang et al. 2022]4; E5-large e E5-base: variantes E5 como recuperadores densos, usando FAISS [Johnson et al. 2019] com produto interno para busca; ColBERT-X [Nair et al. 2022]: um ColBERT-v1 multilíngue afinado no subconjunto de português brasileiro do mMARCO; SPLADE v2: um recuperador esparso aprendido [Formal et al. 2021] afinado no subconjunto de português brasileiro do mMARCO; SPLADE v2 + mT5-XL: pipeline em duas etapas usando SPLADE v2 seguido por mT5. Também usamos a Fusão de Classificação Recíproca (RRF) [Cormack et al. 2009] para aumentar a diversidade dos documentos recuperados, usando as seguintes combinações: E5-large + ColBERT-X; E5-large + SPLADE v2; e E5-large + BM25 + mT5-XL. Incluímos também modelos comerciais de embutimento: text-embedding-ada-0025, text-embedding-3-small6 e, como emprega a técnica de Aprendizado de Representação Matryoshka [Kusupati et al. 2022], realizamos a recuperação usando apenas a primeira metade das dimensões (identificadas como text-embedding-3-small half). O FAISS [Johnson et al. 2019] usando produto interno foi aplicado para a busca de vetores densos para todos eles.\n",
            "\n",
            "4https://huggingface.co/intfloat/multilingual-e5-large\n",
            "5https://openai.com/blog/new-and-improved-embedding-model\n",
            "6https://openai.com/blog/new-embedding-models-and-api-updates\n",
            "\n",
            "Para avaliar a diversidade dos trechos recuperados, contávamos as combinações consulta-trecho retornadas exclusivamente por cada sistema de IR, que deve ser um número de 0 a 500, sendo que 0 significa que os trechos da consulta retornados por um sistema de IR particular também foram retornados por outro sistema de IR. Embora busquemos diversidade, deve haver um equilíbrio: poderíamos ter alcançado 5.000 combinações diferentes de consulta-trecho (10 sistemas de IR, 50 consultas, 10 trechos/consulta) se todos os sistemas retornassem trechos exclusivos, mas isso indicaria nenhuma concordância sobre os trechos mais relevantes por consulta.\n",
            "\n",
            "3.4. Anotação de consultas-trechos\n",
            "A etapa final da anotação de consultas é usar um LLM para rotular a relevância dos trechos recuperados para cada consulta. Selecionamos os k melhores trechos=10 para uma amostra de 50 consultas usando todos os sistemas de recuperação considerados nos corpora de 10M e 1M e os enviamos para avaliação do LLM. Aplicamos um prompt deChain-of-Thought (CoT) de few-shot [Wei et al. 2022] e adotamos a escala de anotação de relevância de 4 pontos da trilha de Aprendizado Profundo TREC 2021 [Craswell et al. 2021]: (1) Irrelevante: o trecho está fora do escopo da pergunta; (2) Relevante: o trecho diz respeito ao tema da pergunta, mas não fornece uma resposta direta; (3) Altamente relevante: o trecho responde à pergunta, mas falta clareza ou contém informações não relacionadas; (4) Perfeitamente relevante: o trecho responde à pergunta com clareza e precisão.\n",
            "\n",
            "Selecionamos o modelo OpenAI GPT-4 como o anotador. Devido a limitações de custo, utilizamos uma amostra de 50 das 200 consultas. Pedimos ao LLM que rotulasse apenas os 10 trechos recuperados mais relevantes de cada sistema de IR para cada consulta. Usamos um prompt CoT com dois exemplos em contexto selecionados do conjunto de dados mMARCO pt-BR [Bonifacio et al. 2021]. O prompt escrito em português brasileiro inclui a explicação da tarefa e os exemplos de CoT para produzir o valor de relevância do trecho de 4 pontos para uma dada consulta. A avaliação final foi solicitada em formato JSON para simplificar o processo de análise da resposta do LLM. O prompt foi construído e refinado usando um conjunto limitado de questões amostradas do mesmo conjunto de dados mMARCO pt-BR. A versão final do prompt pode ser encontrada online.8\n",
            "\n",
            "4. Experimentos\n",
            "4.1. Avaliação da qualidade da anotação do LLM\n",
            "Avalíamos a qualidade do nosso anotador baseado em LLM comparando suas pontuações de relevância consulta-trecho com aquelas fornecidas por anotadores humanos. Esse processo foi conduzido em uma amostra de 24 das 50 consultas anotadas. Usando o sistema Doccano [Nakayama et al. 2018], três pesquisadores anotaram os 10 trechos mais relevantes retornados pelo sistema IR BM25 + mT5 aplicando o mesmo sistema de classificação de 4 pontos TREC-DL 2021. A concordância entre as anotações de relevância consulta-trecho geradas pelo LLM e as humanas foi medida usando os coeficientes de correlação Kappa de Cohen, Pearson e Spearman.\n",
            "\n",
            "4.2. Avaliação dos sistemas de recuperação\n",
            "Usamos as consulta-trechos anotados pelo LLM para avaliar a eficácia dos sistemas de IR nos conjuntos de dados Quati de 10M e 1M. Como já temos as execuções de IR para a recuperação de trechos por todos os sistemas (veja Seção 3.3), simplesmente calculamos a métrica nDCG@10 sobre esses resultados. Além de estabelecer uma linha de base para uma variedade de sistemas de IR, estes experimentos também avaliam indiretamente a qualidade geral do conjunto de dados de validação do Quati: ao verificar diferentes eficiências para sistemas de IR já publicados, validamos o potencial do Quati em realmente avaliar tais sistemas.\n",
            "\n",
            "5. Resultados e Discussão\n",
            "5.1. Variabilidade dos trechos anotados\n",
            "A Tabela 1 indica uma faixa de 29 a 262 combinações de consulta-trecho retornadas exclusivamente por um único sistema de IR. Em média, cada sistema retornou 28,85% de novos trechos, e, dos 4.889 pares consulta-trecho avaliados, 61,96% (3029) foram retornados por um único sistema, sugerindo que nosso conjunto de sistemas é diversificado. Como mostrado na Tabela 2, os sistemas de IR foram capazes de recuperar um conjunto diversificado de consulta-trechos, incluindo aqueles “perfeitamente relevantes” (pontuação=3); além disso, a diversidade aumentou para trechos menos relevantes, indicando que os sistemas concordaram mais à medida que a relevância dos trechos aumentou.\n",
            "\n",
            "5.2. A qualidade das anotações do LLM está alinhada com os trabalhadores da multidão\n",
            "A Tabela 3 apresenta os coeficientes de correlação Kappa de Cohen e Rho de Spearman para as anotações humanas e as do LLM, calculados para as 240 combinações consulta-trecho. O Kappa de Cohen médio de 0,31 está alinhado com a literatura. Por exemplo, [Faggioli et al. 2023] reportaram 0,26 para o GPT-3.5, e [Thomas et al. 2023] reportaram Kappa de Cohen variando de 0,20 a 0,64, dependendo do prompt usado no GPT-4. O Kappa médio dos nossos anotadores humanos de 0,4256 está dentro do intervalo dos trabalhadores da multidão de 0,24 a 0,52, de acordo com [Damessie et al. 2017].\n",
            "\n",
            "Como a anotação da relevância consulta-trecho é uma tarefa subjetiva, argumentamos que uma métrica não categórica, como o Rho de Spearman, seria mais apropriada para medir a correlação dos anotadores, uma vez que erros em um único nível de pontuação devem ser considerados “menos críticos”, ou dentro da subjetividade intrínseca à tarefa. Embora a correlação dos anotadores humanos ainda esteja acima de suas correlações com o LLM, as métricas de Spearman estão dentro de um valor mais alto, capturando melhor a eficácia atual do LLM na avaliação da relevância consulta-trecho.\n",
            "\n",
            "5.3. Resultados da avaliação dos sistemas de recuperação\n",
            "Avalíamos a eficácia dos recuperadores usando os consulta-trechos anotados pelo LLM (qrels); a Tabela 4 apresenta os resultados para os conjuntos de dados de 10M e 1M. A classificação dos recuperadores com relação à eficácia correspondeu às nossas expectativas, seguindo a literatura. Consideramos isso uma indicação adicional da qualidade geral dos conjuntos de dados, pois, apesar de ter sido criada de maneira semi-automatizada e econômica, elas conseguem avaliar uma diversidade de recuperadores.\n",
            "\n",
            "6. Conclusão\n",
            "Este artigo apresentou o Quati, um conjunto de dados para apoiar o desenvolvimento de sistemas de IR para tarefas de recuperação em português brasileiro. O Quati está disponível publicamente em dois tamanhos, 10M e 1M trechos, com qrels de 50 consultas, tendo respectivamente uma média de 97,78 e 38,66 trechos anotados por consulta. Por meio de comparações com anotadores humanos, respondemos à nossa questão de pesquisa, mostrando que LLMs de última geração podem ser utilizados de maneira semi-automatizada e econômica para criar conjuntos de dados de IR para uma língua-alvo específica, na função de anotação de consulta-trecho, com desempenho equivalente ao dos humanos: as anotações do LLM correlacionam-se com as dos humanos de maneira semelhante às anotações de trabalhadores da multidão, por uma fração do custo.\n",
            "\n",
            "Agradecimentos\n",
            "Agradecemos a Leodécio Braz da Silva Segundo pelo valioso apoio durante a tarefa de anotação humana. Também agradecemos a Leonardo Benardi de Ávila e Monique Monteiro pelas recuperações do SPLADE v2, usando o modelo que treinaram para o português brasileiro. Esta pesquisa foi parcialmente financiada pela bolsa 2022/01640-2 da Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP).\n",
            "29\n",
            "Resumo. Por serem recursos que permitem a observação de comportamentos e usos linguísticos e sociais, os corpora anotados passaram a ser de interesse de diferentes áreas do conhecimento. No contexto da Rhetorical Structure Theory (RST) apresentamos neste trabalho os processos metodológicos e práticos de anotação de sinalizadores discursivos em um corpus jornalístico do português do Brasil. Ainda, apresentamos as primeiras avaliações (quanti e qualitativa) sobre as decisões tomadas pelo grupo de anotadores. 1. Introdução A Linguística de corpus (LC), enquanto área, instiga a utilização de técnicas e metodologias que nos levam a reunir grandes conjuntos de dados textuais (escritos, orais ou multimodais), a fim de descrever fenômenos linguísticos. Em interface ao Processamento de Linguagem Natural (PLN), uma das tarefas que a LC se propõe a realizar é a anotação desse conjunto de dados, tida como “o processo de enriquecer um corpus, adicionando informações linguísticas inseridas por humanos ou máquinas com um objetivo teórico ou prático” [Pedro e Vale 2018]. Por serem recursos que permitem a observação de comportamentos e usos linguísticos e sociais, os corpora anotados passaram a ser de interesse de diversas áreas do conhecimento, como Humanidades digitais, Linguística e Computação. [Pustejovsky e Stubbs 2012] apontam que a análise dos corpora permite desvendar a natureza da linguagem e, consequentemente, capturar possíveis propriedades que possam ser modeladas computacionalmente. Porém, esse processo de anotação tende a ser custoso, já que grande parte é realizada de forma semiautomática e requer intervenção humana. [Hovy e Lavid 2010] apresentam uma metodologia genérica sobre esse processo, que engloba etapas como preparação do conjunto de dados, instância da base teórica, anotação de fragmentos do corpus, medição das decisões de anotação e escalabilidade do processo de maneira automática. No entanto, essa tarefa pode ser ajustada conforme o tipo de anotação a ser realizada, o que pode levar à omissão de algumas das etapas sugeridas pelos autores. [Taboada e Das 2013] e [Liu e Zeldes 2019], a partir de corpora pré-anotados com o modelo Rhetorical Structure Theory (RST) [Mann e Thompson 1987] identificaram uma série de pistas linguísticas e estruturais que serviam de sinalizadores para as relações discursivas previamente identificadas. Ambos os trabalhos organizaram os Sinalizadores Discursivos (SD) em função de suas características (semânticas ou sintáticas, por exemplo), pondo em xeque a ideia de que as relações RST deveriam ser identificadas majoritariamente por meio de Marcadores Discursivos (MD), tidos como preposições e conjunções. Com base nessa metodologia, [Rodrigues et al. 2023] descreveram SDs para além dos MDs a partir do corpus CSTNews [Cardoso et al. 2011]. Tal recurso linguístico-computacional consiste em um conjunto de textos jornalísticos em Português que já havia sido anotado segundo o modelo RST. A RST propõe que um texto coerente é formado por unidades mínimas de discurso (Elementary Discourse Units - EDU ou proposições) que desempenham funções retóricas para que o objetivo comunicacional do autor seja atingido. Partindo dessa anotação prévia, os anotadores deste trabalho, por sua vez, identificaram apenas os sinalizadores que consideraram relevantes para caracterizar e/ou indicar determinadas relações, como em (1) - extraído do corpus CSTNews. (1) [A seleção brasileira masculina de vôlei,]A [que é treinada por Bernardinho,]B [venceu a Finlândia por 3 sets a 0.]C As porções (1a) e (1c) foram conectadas por meio da relação RST Same-Unit, já que estão separadas por haver detalhamento informacional em 1B em relação à (1a) por meio da relação Elaboration. [Rodrigues et al. 2023] indicaram que a pontuação (no caso, vírgula), a concordância verbal e o encaixamento de outra relação RST poderiam ser utilizadas como pistas para a identificação da relação Same-unit. Esse estudo preliminar resultou em um manual de anotação de SDs em textos jornalísticos [Dantas et al. 2024], em que há, além de instruções, a proposta da primeira taxonomia de sinalizadores de relações RST para o PB. Destaca-se que esse tipo de recurso com explicações, exemplos e instruções objetivas subsidia a decisão dos anotadores diante de fatos novos e/ou já conhecidos [Duran et al. 2022]. Assim, objetivamos neste trabalho relatar as etapas metodológicas e práticas de anotação de SDs no corpus CSTNews, além de apontar as primeiras avaliações sobre as decisões tomadas pelo grupo de anotadores. Para tanto, este trabalho está organizado em 5 seções, além desta Introdução. Na Seção 2, destacamos trabalhos relacionados ao processo de anotação e análise em RST, sobretudo para o PB. Na Seção 3, detalhamos a metodologia de anotação empreendida neste estudo. Na Seção 4 apresentamos os resultados e as discussões correspondentes. Por fim, na Seção 5, tecemos algumas considerações finais. 2. Trabalhos Relacionados Identificar relações RST por meio de marcas explícitas no texto não é uma tarefa nova, especialmente em PLN para análise de discurso. Os MDs são tidos como conectivos entre porções textuais, sinalizando determinadas relações discursivas, como o “mas” para oposição, por exemplo. A análise das relações discursivas (ou de coerência) está intimamente ligada a descobrir a intenção do autor ao apresentar partes do texto em uma ordem e combinação específicas. Portanto, trata-se de uma tarefa que vai além de identificar os MDs. A literatura [Marcu 2000, Pardo 2005, Taboada e Das 2013] indica que a identificação de MD, em função da relação RST a que ocorrem, facilita o processamento do texto. Estudos recentes [Das e Taboada 2018, Liu e Zeldes 2019] afirmam que os MDs sinalizam apenas um número restrito de relações dentro de um texto, e sugerem que as relações RST podem ser identificadas por sinais que vão além deles. Como os MDs não marcam explicitamente as relações e não são exclusivos, a noção de SD parece ser mais apropriada do que a de MD nesse contexto. [Das e Taboada 2018] argumentam que para uma comunicação ser eficaz, é fundamental que as relações sejam interpretadas de maneira relativamente clara, o que requer sinalizadores precisos. Os autores acreditam que as relações de coerência são entendidas cognitivas, e, portanto, é possível descobrir como ouvintes e leitores as identificam usando indicadores que auxiliem o processo interpretativo. Utilizando o RST Discourse Treebank, os autores realizaram uma anotação detalhada dos SD, resultando no RST Signalling Corpus (RST-SC). Eles observaram que pode haver relações sinalizadas por um único sinalizador (como MD, referências pessoais, orações relativas ou dois pontos) ou por combinações de SD (como vírgula + oração no particípio passado, ou construção sintática paralela + cadeia lexical). Quando surgia uma nova instância de um tipo específico de relação, os anotadores consultavam a taxonomia para encontrar o(s) sinalizador(es) mais adequado para aquela instância. Durante o processo de anotação, os autores observaram casos em que não foi possível determinar com precisão o SD que representava uma determinada relação. Em [Liu e Zeldes 2019] descreve-se um esforço de anotação para ancorar SD a partir de diversas categorias tais como sintática, semântica, gráfico e morfológica. Seus resultados mostraram que, com 11 documentos e 4.732 tokens, 923 foram instâncias de SD, o que representou mais de 92% dos sinais discursivos. O tipo semântico representou a maioria dos casos, enquanto as relações discursivas ancoradas por DM corresponderam a apenas cerca de 8,5% dos tokens ancorados. Quanto à língua portuguesa, [Pardo 2005] foi o precursor em investigar a construção de analisadores discursivos. A partir de um corpus de textos científicos e anotado com RST, o autor identificou diversos padrões de análise que especificam os relacionamentos entre as relações retóricas e seus marcadores textuais. Apesar de muitos padrões serem baseados em MD, o autor ressalta que não existe uma relação sine qua non entre MD e as relações que sinalizam, pois uma mesma relação pode ser sinalizada por vários marcadores (por exemplo, a relação Concession pode ser sinalizada pelos marcadores “entretanto”, “no entanto”, entre outros) e um mesmo marcador pode sinalizar várias outras relações (por exemplo, o marcador “porque” pode sinalizar as relações Cause e Result (volitivas ou não), Justify, Explanation, entre outras). [Maziero 2016] Ainda com relação ao português, investigou atributos de organização textual, da morfossintaxe, da sintaxe, da semântica e discurso para construir um analisador discursivo baseado na RST. A partir da análise de corpora anotados com RST, o autor aponta que: a) existem relações que apresentam grande subjetividade, tais como as relações Evidence, Justify e Explanation; b) a relação Same-unit ocorre apenas no nível intrassentencial, o que é esperado, pois é responsável por ligar proposições quebradas por uma relação de Parenthetical ou Elaboration, por exemplo; c) algumas relações são mais frequentes no nível intrassentencial do que no intersentencial. Após vários experimentos com aprendizado de máquina para identificação das relações discursivas no nível intrassentencial, o autor concluiu que atributos morfossintáticos proporcionaram melhores resultados do que os atributos semânticos e discursivos. 3. Metodologia A anotação de SDs foi feita a partir do corpus CSTNews¹. Os textos do corpus estão organizados em 50 conjuntos, com dois ou três documentos que noticiam o mesmo evento. Por essa característica multidocumento e de redundância, a anotação foi feita apenas no maior texto do conjunto, pois acreditamos que quanto maior for o texto, maior é a chance de encontrarmos mais relações RST e, possivelmente, essas relações ocorram nos outros textos da mesma coleção. Nesse caso, foram separados 50 textos para esta tarefa de anotação. A anotação de SDs foi realizada por meio da ferramenta rstWeb [Zeldes 2016], que é uma plataforma desenvolvida para facilitar a análise e a anotação de textos com base na RST. Essa ferramenta permite aos usuários realizar análises estruturais detalhadas dos textos, identificando proposições e suas relações de coerência conforme proposto pela teoria. Neste trabalho, a taxonomia de SDs, na Figura 1, foi implementada. Os textos escolhidos foram pré-processados e distribuídos a um grupo de oito pesquisadores. A anotação aconteceu de maneira assíncrona, em que cada anotador recebia semanalmente 3 ou 4 textos. Cada texto foi anotado por três anotadores para que pudéssemos ter uma versão do corpus com a decisão sobre a indicação dos SDs por maioria simples. Para promover discussão e resolução de dúvidas, especialmente sobre casos não previstos pelo manual, foram conduzidas reuniões semanais com o grupo. Além disso, dois dos anotadores, por terem mais experiência com tarefas nesse sentido, nunca ficavam juntos no trio, para que pudessem auxiliar na resolução de dúvidas de maneira assíncrona. Ressalta-se que os anotadores possuíam diferentes formações acadêmicas (linguistas ou cientistas da computação) e com experiências distintas em tarefas de anotação de corpus. Por conta disso, foi necessária uma etapa de treinamento para que o grupo se familiarizasse com a taxonomia de SDs e com o modelo RST, além de ter acesso ao manual de anotação para auxiliar em suas decisões. Na fase de treinamento, foi realizada a anotação de três textos do corpus. Em reuniões síncronas, os anotadores puderam corrigir possíveis equívocos e identificar quais unidades do discurso deveriam ser devidamente anotadas, ou seja EDUs que estivessem presentes em um mesmo período sintático. Na Figura 2, tem-se um exemplo de anotação para a relação Circumstance. Essa relação RST deve apresentar uma situação realizável, em que o satélite (EDU 1) provê a situação que é apresentada no núcleo - EDU 2. No exemplo, tem-se que essa relação está sendo sinalizada por meio de Advérbio (vermelho), Oração circunstancial (azul) e Pontuação (lilás). Três textos de diferentes tamanhos foram anotados por todos os anotadores em distintas fases do processo, com o objetivo de medir periodicamente a concordância do grupo. Esse processo se repetiu a cada 10 conjuntos de textos anotados. Ao final de 2 meses, 47 textos foram anotados. Trecho anotado com a relação RST Concession “Tal capacidade de mutação fez escola, mas dificilmente as criaturas saberão superar o criador.” Anotadores Sinalizadores indicados A B C “mas” + “,” “mas” + “,” “mas” Tabela 1. Comparação de anotações. A concordância foi medida automaticamente a partir de duas abordagens. Na abordagem gold observa-se estritamente o que o grupo de anotadores apontaram como sinalizador, sendo, portanto, mais restrita. Já na abordagem silver definiu-se um intervalo de cinco janelas (à esquerda e à direita) em relação ao sinalizador-alvo, como demonstrado na Tabela 1. Na Tabela 1, tem-se um exemplo de como as duas abordagens da concordância foram aplicadas. Os anotadores A e B indicaram os mesmos sinalizadores, ao passo que o sinalizador C indicou apenas um em comum com o grupo. Caso fosse considerada apenas uma análise mais restritiva sobre a concordância, a decisão do anotador C prejudicaria o cálculo, ao passo que numa abordagem mais ampla, sua decisão não traria tantos prejuízos. Apesar de todos os esforços metodológicos de distribuição de textos, e de todos os anotadores estarem alinhados junto ao modelo teórico e à ferramenta utilizados, é possível que fatores externos à tarefa influenciam na disposição dos anotadores, fazendo-os eventualmente não apenas discordarem sobre um sinalizador, mas também não se atentarem a realizar a indicação adequada. Por conta disso, escolheu-se neste trabalho não apenas realizar uma análise mais restrita sobre a concordância, mas também mais ampla, admitindo-se nesta, sobretudo, a dimensão mais subjetiva da tarefa. Em ambas utiliza-se abordagens Em ambas a medida Krippendorff Alpha [Krippendorff 2011]. Trata-se de uma medida que avalia a concordância entre dois ou mais anotadores, o que se encaixa melhor no contexto deste trabalho, já que cada texto foi anotado por três pessoas. O resultado da concordância é medido num intervalo que varia entre -1 e 1, em que valores mais próximos a 1 indicam alta concordância; valores próximos a 0 indicam baixa concordância; e valores próximos a -1 indicam discordância total. 4. Resultados e Discussão Na Tabela 2, tem-se a média dos resultados das concordâncias gold e silver da anotação em diferentes etapas do processo. Como dito, na fase de treinamento (clusters 1, 2 e 3), os anotadores realizaram uma primeira anotação e, após reunião de alinhamento, fizeram correções. O cálculo da concordância geral (clusters 16, 31 e 39) foi feito sobre o mesmo texto anotado por todo o grupo. Dado que o processo de anotação pode ser longo e complexo, fatores externos aos aspectos linguísticos (como cansaço e diminuição da atenção, por exemplo) pode ter influenciado os anotadores. É possível perceber isso ao comparar as fases de treinamento com as demais, em que as demais sofreram decréscimos discretos. Além disso, outro possível aspecto que pode ter influenciado nesse resultado é a distribuição das relações Fase do trabalho Treinamento Concordância geral Rodadas de anotação Concordância Gold Silver 0,693 0,581 0,596 0,460 0,691 0,496 Tabela 2. Resultado da concordância. RST no corpus CSTNews. [Cardoso et al. 2011] apontam que há relações RST que ocorrem apenas uma vez, como Otherwise, por exemplo, e outras que aconteceram de maneira predominante, como Elaboration, que ocorreu 1,514 vezes. Nesse caso, é possível que, ao se deparar com uma relação RST não prevista na fase de treinamento e, portanto, ausente no manual de anotação, os anotadores enfrentaram dificuldades em indicar possíveis sinalizadores das relações em questão. Além de uma análise quantitativa, foram feitas observações qualitativas preliminares. Para tanto, durante a anotação, os anotadores realizaram indicações de dúvidas, inconsistências e/ou outras questões em um formulário eletrônico. Ao final de cada semana, todos os apontamentos eram compilados e discutidos entre o grupo para aprimorar o processo. A partir disso, é possível destacar alguns pontos: a) Considerações sobre o processo de anotação Em caso de não encontrar uma etiqueta para representar o fenômeno observado, o anotador poderia registrar os tokens envolvidos e marcar como CPD (Casos Para Discutir depois). Em discussões e análises preliminares, os anotadores destacaram a intenção de marcar as entidades mencionadas no segmento textual. [Das e Taboada 2018], por sua vez, descrevem que os anotadores discordavam bastante entre entidade e tipos semânticos, ou seja, enquanto um anotador seleciona entidade como o sinal relevante para uma certa relação, o outro anotador a anota como sendo semântica. Os autores observaram que muitos dos atributos de entidade e características semânticas na verdade se sobrepõem. Dessa forma, essa dificuldade acontece também para a língua inglesa. Assim como [Liu 2019, Das e Taboada 2018] relatam, também observamos no corpus de estudo várias relações que não tinham um token explícito para servir de sinalizador. Esses casos foram registrados como CPD. Por outro lado, as primeiras análises revelaram que alguns SD são altamente indicativos, enquanto outros são genéricos ou ambíguos. Assim, para obter uma compreensão mais precisa, é necessário considerar os contextos ao redor dos SD para desambiguá-los. b) Considerações sobre dificuldades e limitações encontradas A anotação das relações RST é um processo que se baseia na interpretação do analista. Assim, a depender dessa interpretação serão indicadas determinadas relações RST em detrimento de outras, resultando, então, em diferentes sinalizadores para essas relações. Neste estudo, a identificação de SDs foi feita por um grupo majoritariamente diferente de quem fez a anotação RST, com uma distância temporal considerável entre as duas tarefas. Esse fato, portanto, pode ter sido um dificultador para o grupo que fez a indicação dos sinalizadores. Além disso, os anotadores destacaram que algumas relações RST utilizadas no CSTNews são mais difíceis de interpretar, e consequentemente, torna-se um desafio apontar SD específicos, como exemplificado em (2). (2) (...) [com o Programa Fome Zero, conseguiu atingir o primeiro ponto das Metas do Milênio - erradicar a fome -, com dez anos de antecedência,]A [reduzindo em mais da metade a pobreza extrema.]B O trecho (2b) em relação ao trecho (2a) apresenta a relação Volitional result, ou seja, o resultado ocasionado foi não intencional. Nesse caso em específico, os anotadores indicaram que o sentido do verbo “reduzindo” seria o indicativo do resultado, porém sem menção ao aspecto volitivo. Destaca-se que a maioria dos rols de relações para outras línguas não preveem diferença nesse aspecto. Outro aspecto que parece ter apresentado dificuldade aos anotadores foi o fato de o manual de anotação ter sido desenvolvido com base no estudo de [Rodrigues et al. 2023] e os resultados da fase de treinamento. Como citado, relações e sinalizadores que não estavam previstos e que ocorreram ao longo do corpus podem ter ocasionado certos equívocos entre os anotadores. Ademais, o fato de o manual indicar certa correlação entre SDs e relações pode ter condicionado o olhar dos anotadores, como demonstrado em (3). (3) [nesta terça deve se encontrar com o relator do caso na Câmara, deputado José Carlos Araújo (PR-BA)]A [para tratar do assunto.]B De acordo com [Cardoso et al. 2011], a sentença entre (3a) e (3b) é de Purpose. O manual de anotação de SDs utilizou esse exemplo e indicou que a preposição “para” pode ser utilizada para identificar essa relação. Entretanto, o objetivo entre os segmentos pode também ser evidenciado por meio de “oração final” presente em (3b). Nesse caso, é possível que os anotadores tenham sido condicionados a partir de determinados pressupostos sobre as relações, ainda que tenham sido estimulados a indicarem em formulário eletrônico outros possíveis SDs e definições não previstos no manual. Por fim, cabe pontuar que no repositório online do projeto de pesquisa “RST além dos marcadores discursivos”² disponibilizamos para consulta o corpus com a versão unificada entre os anotadores, a anotação de SDs e a planilha completa da concordância dos anotadores. 5. Considerações Finais Neste trabalho buscamos detalhar a metodologia empregada na identificação de SDs em textos jornalísticos a partir da taxonomia proposta por [Dantas et al. 2024]. Destacamos que um estudo com essa abordagem em PB ainda não havia sido realizado, ao contrário do que já ocorre em outros idiomas, especialmente o inglês. Os resultados relatados podem subsidiar outras análises em estudos futuros. Um desses estudos se concentra na investigação quali e quantitativa da correlação entre SDs e as relações RST, algo já iniciado por [Rodrigues et al. 2023] e tal como outros trabalhos fizeram [Liu 2019, Das e Taboada 2018, Pardo 2005]. Outro estudo será em relação à concordância de aspectos da anotação, como tipos (sintático e semântico, por exemplo) e subtipos (pronome relativo e conhecimento de mundo, por exemplo) dos sinalizadores. Ao final desses estudos será possível fazer o levantamento da distribuição dos SDs no corpus, bem como observar quais são mais ou menos consensuais entre os anotadores. Dados os apontamentos críticos realizados sobre as limitações identificadas, destaca-se que este trabalho apresenta potencial de servir de diretriz de investigações de análises sobre as relações RST e seus SDs e aprimoramento de ferramentas e recursos para anotação de corpus. Tais aspectos são de extrema importância ao alargar a anotação a escalas maiores buscando não apenas ampliar a quantidade de textos, mas também diversificar os gêneros textuais a serem considerados.\n",
            "30\n",
            "Resumo. Neste artigo, apresenta-se um esforço pioneiro para o desenvolvimento de um modelo de parsing multigênero para o português brasileiro. Seguindo o projeto Universal Dependencies, treinou-se um dos modelos do estado-da-arte em três corpora gold-standard de diferentes gêneros textuais (jornalístico, acadêmico e conteúdo gerado por usuário – postagens do X). Os experimentos revelam que nosso modelo multigênero de parsing produz resultados melhores ou competitivos em relação aos modelos de gênero único.\n",
            "\n",
            "1. Introdução\n",
            "\n",
            "O parsing sintático é a tarefa de descobrir automaticamente as relações sintáticas entre as palavras de uma frase, resultando em árvores sintáticas, que correspondem a um dos primeiros níveis de análise em Processamento de Linguagem Natural (NLP) [Jurafsky e Martin 2024]. Essa tarefa provou ser útil para várias aplicações diferentes, como simplificação de texto, extração de informações, sumarização automática e análise de sentimentos, entre muitas outras.\n",
            "\n",
            "No início, era comum ter o parsing como uma etapa em aplicações de NLP (por exemplo, verificação gramatical [Martins et al. 1998] e simplificação de texto [Candido et al. 2009]). Avanços recentes em aprendizado profundo, modelos distribuicionais e modelagem de linguagem permitiram que muitas aplicações dispensassem uma análise linguística mais profunda, mas os esforços de pesquisa atuais indicaram que a inclusão de conhecimento linguístico durante o treinamento do modelo ou em etapas de pós-processamento (por exemplo, em abordagens neuro-simbólicas) pode ser relevante para melhorar os resultados [Zhou et al. 2020, Bai et al. 2021, Lin et al. 2021, Bölücü et al. 2023]. Além disso, considerando os caros requisitos computacionais para treinar os modelos acima e a busca por explicabilidade e interpretabilidade, os sistemas de análise linguística ressurgiram como alternativas relevantes em várias situações de pesquisa.\n",
            "\n",
            "Existem alguns parsers bem conhecidos para o português, incluindo aqueles considerados clássicos, como PALAVRAS [Bick 2000] e PassPort [Zilio et al. 2018], e modelos mais recentes alinhados ao projeto Universal Dependencies (UD) [de Marneffe et al. 2021], como o UDPipe, do estado-da-arte Porparser [Lopes e Pardo 2024] (com precisão próxima de 95% para textos jornalísticos).\n",
            "\n",
            "Propomos aqui dar um passo adiante no parsing para o português brasileiro (BP). Usando os diferentes corpora anotados disponíveis na iniciativa UD e adotando um framework de parsing amplamente conhecido (o pipeline Stanza [Qi et al. 2020]), investigamos a questão do parsing multigênero, visando produzir um parser que funcione bem para diferentes estilos de escrita em língua, incluindo posts curtos e geralmente sintaticamente fragmentados do X (anteriormente conhecidos como tweets), a “linguagem diária” de textos de notícias e a escrita (supostamente) mais refinada de textos acadêmicos. O sistema resultante, chamado Genipapo (um acrônimo para “multiGENre PArser for POrtuguese”), alcança resultados melhores ou competitivos em relação aos parsers treinados em um único gênero, representando um passo para liberar o potencial das ferramentas de análise de texto em português para trabalhar em uma ampla variedade de textos.\n",
            "\n",
            "O restante deste artigo está organizado da seguinte forma. A Seção 2 introduz o framework UD. A Seção 3 apresenta brevemente os principais trabalhos relacionados na área. Os recursos adotados e a metodologia são relatados na Seção 4, enquanto a Seção 5 apresenta os resultados de nossos experimentos. Concluímos este artigo na Seção 6.\n",
            "\n",
            "2. O framework Universal Dependencies\n",
            "\n",
            "UD [Nivre et al. 2020] é atualmente o framework baseado em dependência mais utilizado para análise morfológica e sintática em NLP [Sanguinetti et al. 2023]. É uma tentativa de padronizar a anotação de morfologia e sintaxe, propondo uma estratégia de anotação “universal” para todas as línguas, facilitando o desenvolvimento de etiquetadores e parsers multilíngues. No momento da redação deste texto, já existem mais de 240 bancos de dados de árvores disponíveis para mais de 150 idiomas, abordando uma variedade de gêneros textuais.\n",
            "\n",
            "Na UD, as seguintes informações morfológicas são consideradas: (i) etiquetas de Parte do Discurso (PoS), (ii) lemas e (iii) características. A anotação sintática consiste em relações de dependência tipificadas (deprels) entre as palavras. Atualmente, o modelo possui 17 etiquetas de PoS e 37 deprels, além de um conjunto de características morfológicas não fixo. A Figura 1 mostra um exemplo de um post anotado do corpus DANTEStocks [Di-Felippo et al. 2021]. A representação básica de dependência é uma árvore, onde exatamente uma palavra é a cabeça da enunciação (raiz) (por exemplo, “assina” – “sign”), e todas as palavras restantes dependem de alguma outra palavra. Os arcos rotulados representam as relações de dependência, apontando das cabeças para seus dependentes. As etiquetas de PoS, lemas e características morfológicas são exibidas abaixo das palavras na Figura 1.\n",
            "\n",
            "A fruta correspondente, “Jenipapo” (com ‘J’ em vez de ‘G’), é uma fruta tropical, apreciada em vários estados do Brasil e utilizada para diferentes propósitos, desde pintura até alimentação e preparação de bebidas. Ao adotar essa inspiração para o nome do nosso parser, buscamos essa conexão simbólica com algo enraizado na cultura e linguagem brasileiras.\n",
            "\n",
            "3. Trabalhos relacionados\n",
            "\n",
            "Sobre os recursos linguísticos para treinar parsers UD, existem alguns conjuntos de dados disponíveis em BP. Um dos primeiros corpora com anotação UD para textos em português padrão (ou canônico) é o banco de árvores UD-Portuguese-Bosque [Rademaker et al. 2017], que contém 210.958 tokens em 9.357 frases. A parte brasileira desse corpus consiste em 4.213 frases bem escritas extraídas de textos jornalísticos. Existe também o PetroGold [Souza et al. 2021], que é um banco de árvores totalmente revisado que consiste em textos acadêmicos do domínio de petróleo e gás, totalizando 8.946 frases (e 232.333 tokens). Diferentemente do UD-Portuguese-Bosque, o PetroGold é um corpus especializado ou de domínio específico. Além disso, o projeto UD disponibiliza o corpus UD-Portuguese-GSD [Zeman 2017]. Totalizando 12.020 frases (296.169 tokens) de textos de notícias e blogs, apresenta dois gêneros textuais diferentes, com diferentes graus de canonicalidade.\n",
            "\n",
            "Especificamente visando aumentar os recursos baseados em sintaxe para BP, foi criado outro banco de árvores (com gêneros além de textos jornalísticos). O Porttinari [Pardo et al. 2021] atualmente inclui dois gêneros principais (com outros em construção): (i) textos jornalísticos, representando a linguagem padrão escrita, e (ii) conteúdo gerado por usuário (UGC), representando a linguagem da web informal não canônica (em particular, tweets/posts do X).\n",
            "\n",
            "Em relação aos modelos de parsing, alguns parsers de dependência UD estão disponíveis para BP, especialmente para textos de notícias. O UDPipe [Straka 2018] é provavelmente o modelo mais utilizado. Usando uma arquitetura de atenção biaffine baseada em grafo, ele alcança uma Pontuação de Atribuição Rotulada (LAS) de 87,04% para textos de notícias. O Stanza [Qi et al. 2020] é outro sistema bem conhecido, que utiliza um método profundo biaffine baseado em Bi-LSTM enriquecido com características. De acordo com os resultados para a versão 2.1 da UD, o Stanza alcança 87,75% de LAS para textos de notícias. O UDify [Kondratyuk e Straka 2019] é outro sistema importante. É um modelo semissupervisionado de multitarefa baseado em autoatenção. Há também o recém-lançado Port-Parser [Lopes e Pardo 2024], que foi construído ao treinar o UDPipe com BERTimbau [Souza et al. 2020] no corpus Porttinari-base [Duran et al. 2023a], que faz parte da porção jornalística do maior banco de árvores Porttinari [Pardo et al. 2021]. O modelo alcançou uma LAS em torno de 95%. Esse valor de LAS traz uma melhoria de cerca de 7% sobre algumas linhas de base existentes bem conhecidas para a língua portuguesa padrão escrita.\n",
            "\n",
            "Como exemplo final, é importante citar o trabalho de [Zilio et al. 2018]. Embora tenha apresentado resultados inferiores aos de trabalhos mais recentes, os autores compararam alguns métodos anteriores e clássicos de parsing para BP. Os autores relataram que o melhor modelo (chamado PassPort) alcançou uma LAS de 85,21% no corpus UD. Em uma avaliação adicional em pequena escala, o PassPort foi comparado manualmente ao PALAVRAS, utilizando um único corpus de 90 frases (1.295 tokens), selecionadas aleatoriamente de três gêneros diferentes, a saber, literatura, textos de notícias e legendas. Os sistemas apresentaram resultados semelhantes para parsing de dependência, com uma LAS de 85,02% para o PassPort contra 84,36% para o PALAVRAS.\n",
            "\n",
            "4. Materiais e métodos\n",
            "\n",
            "Dado o objetivo de construir um parser UD multigênero para BP, três corpora, pertencentes a três gêneros diferentes, compõem nossos materiais.\n",
            "\n",
            "Nosso primeiro corpus, DANTEStocks [Di-Felippo et al. 2021], compreende 4.048 tweets (com 81.048 tokens) do domínio do mercado de ações coletados automaticamente durante 2014 (o que limita cada post a 140 caracteres). O corpus foi construído coletando mensagens contendo um ticker de uma das 73 ações que compunham o Ibovespa na época [da Silva et al. 2020]. DANTEStocks apresenta uma combinação de linguagem escrita padrão e não padrão, além de marcas de discurso, vocabulário específico do domínio e características do meio (Twitter). As relações de dependência do corpus foram anotadas em duas etapas semi-automáticas [Barbosa 2024]. Primeiro, um subconjunto de referência de 1.000 tweets foi anotado usando o UDPipe, que havia sido treinado no UD-Portuguese-Bosque e foi escolhido porque está facilmente disponível para uso online e oferece desempenho confiável. Este subconjunto foi, então, revisado manualmente antes de ser designado como um padrão ouro. O restante do corpus foi então anotado personalizando o Stanza para o DANTEStocks. Usamos a combinação do Porttinari-base e do subconjunto de referência como o conjunto inicial de treinamento para o Stanza. O modelo de parsing resultante foi usado para anotar automaticamente um novo (primeiro) pacote de dados (dos 3.048 tweets restantes). O primeiro pacote foi revisado manualmente e incorporado ao conjunto de dados anterior, sendo utilizado para começar uma nova rodada de treinamento do Stanza. Este ciclo de iteração de treinamento continuou incrementalmente até que o último (num total de 6) pacote fosse anotado/revisado. Em relação à LAS, a pontuação final (6ª rodada) alcançou 94,62%, aumentando 0,76% em relação à pontuação da primeira rodada de 93,86%.\n",
            "\n",
            "O segundo corpus, PetroGold [de Souza e Freitas 2023], é um corpo padrão ouro do corpus Petrolês, que é um banco de árvores para o domínio de petróleo e gás (O&G). PetroGold é composto por 19 textos acadêmicos (teses e dissertações), com um total de 9.127 frases e 253.640 tokens. A anotação sintática do PetroGold também seguiu uma abordagem semi-automática. Especificamente, quatro especialistas foram responsáveis por revisar a saída de uma versão customizada do Stanza, treinada na combinação do UD-Portuguese-Bosque (v.2.6) e uma pequena coleção de dados do domínio de O&G. Através de uma avaliação intrínseca utilizando um modelo criado pela ferramenta UDPipe, o corpus alcançou 88,53% de LAS. Para fins de NLP, o corpus é subdividido em três subconjuntos. Os subconjuntos têm 7.170, 737 e 1.039 frases para treinamento (80%), validação (8%) e teste (12%).\n",
            "\n",
            "Nosso último corpus, Porttinari-base [Duran et al. 2023a], é o subconjunto jornalístico padrão ouro (ou seja, totalmente anotado e revisado manualmente) do Porttinari, que é composto por 8.418 frases (168.080 tokens) selecionadas do jornal Folha de São Paulo. O processo de anotação do Porttinari-base começou com uma anotação automática pelo UDPipe usando o corpus UD-Portuguese-Bosque, que alcançou 87% de precisão (em termos de LAS). Em seguida, as relações de dependência foram revisadas manualmente em detalhe seguindo um manual de anotação contendo diretrizes específicas para BP [Duran 2022]. O Porttinari-base também é subdividido em subconjuntos de treinamento, validação e teste. Os subconjuntos têm 5.893, 842 e 1.683 frases nos arquivos de treino (70%), dev (10%) e teste (20%), respectivamente.\n",
            "\n",
            "Para o desenvolvimento de nosso parser, empregamos o pipeline Stanza, que foi treinado e avaliado em diferentes corpora. Como os corpora PetroGold e Porttinari-base já vêm subdivididos em conjuntos de treinamento, validação e teste, primeiro separamos seus conjuntos de teste para garantir que seriam utilizados apenas para fins de avaliação final. Após isso, unificamos os conjuntos de treinamento e validação de cada corpus para construir um conjunto de treinamento maior para cada um, que foi então utilizado em nossos experimentos. A seguir, dividimos aleatoriamente (de uma distribuição uniforme) o DANTEStocks em conjuntos de treinamento e teste, seguindo o mesmo princípio de manter o conjunto de teste estritamente para os testes finais. A Tabela 1 detalha o tamanho de cada conjunto nos corpora.\n",
            "\n",
            "Tabela 1. Tamanho e proporção dos conjuntos de treino e teste nos corpora.\n",
            "\n",
            "| Corpus          | Treinamento       | Unidades | Proporção | Teste         | Unidades | Proporção |\n",
            "|------------------|----------------|---------|-----------|---------------|---------|-----------|\n",
            "| DANTEStocks      | tweet          | 3.643   | 90%       | 405           | 1.039   | 10%       |\n",
            "| Porttinari-base  | frase          | 7.907   | 88%       | 1.683         | 20%     |\n",
            "| PetroGold        | frase          | 6.735   | 80%       | 1.039         | 12%     |\n",
            "\n",
            "Para avaliar o desempenho do modelo em diferentes gêneros, combinamos os conjuntos de treinamento de todos os três corpora para criar um quarto conjunto de treinamento unificado, junto com um conjunto de teste correspondente. Uma busca em grade foi conduzida para otimização de hiperparâmetros, focando no tamanho do lote (2000, 3000, 4000 e 5000) e taxa de dropout (0,2, 0,3 e 0,4), uma vez que o Stanza não suporta nativamente ajustes na taxa de aprendizado. Em seguida, realizamos validação cruzada em 5 dobras com busca em grade (usando a grade mencionada acima) em cada um dos quatro conjuntos de treinamento, onde cada conjunto foi dividido em cinco subconjuntos, com quatro sendo usados para treinar o modelo, e o quinto mantido para fins de validação. Esse procedimento de subdivisão foi repetido cinco vezes. Então selecionamos, para cada conjunto de treinamento, os hiperparâmetros que produziram o maior valor de LAS nos conjuntos de validação durante a validação cruzada.\n",
            "\n",
            "Tendo o melhor conjunto de hiperparâmetros para cada um dos quatro corpora (DANTEStocks, PetroGold, Porttinari-base e sua união), re-treinamos o modelo em cada conjunto de treinamento do corpus, variando sua semente aleatória (42, 123, 456, 789 e 101.112), alterando assim as configurações iniciais do modelo. Para isso, os conjuntos de treinamento do PetroGold e do Porttinari-base foram divididos de volta em seus conjuntos originais de treinamento e validação, enquanto o conjunto de treinamento do DANTEStocks foi dividido aleatoriamente em conjuntos de treinamento e validação, de modo que todo o corpus DANTEStocks contivesse 10% dos dados para teste, 10% para validação e 80% para fins de treinamento. O modelo de melhor desempenho, em todas as sementes, foi então selecionado para cada corpus. Como etapa final, todos os quatro modelos foram testados e comparados usando os conjuntos de teste previamente separados, que haviam sido reservados exclusivamente para esta avaliação final.\n",
            "\n",
            "5. Resultados e discussão\n",
            "\n",
            "As Tabelas 2 e 3 apresentam os resultados do nosso modelo, quando treinado em cada conjunto de treinamento do corpus (linhas nas tabelas), e testado nos diferentes conjuntos de teste do experimento. A Tabela 2 refere-se aos resultados do modelo em termos de LAS, enquanto a Tabela 3 apresenta os resultados em termos de Pontuação de Atribuição Não Rotulada (UAS). Nas tabelas, as linhas “Genipapo” referem-se ao modelo treinado em todos os conjuntos de treinamento disponíveis, ou seja, nosso modelo multigênero, enquanto as colunas “Todos juntos” referem-se à união de todos os conjuntos de teste.\n",
            "\n",
            "Tabela 2. LAS (%) do modelo em cada conjunto de teste do corpus.\n",
            "\n",
            "| Conjunto de treinamento                   | Conjunto de teste | LAS (%)  |\n",
            "|-------------------------------------------|-------------------|----------|\n",
            "| Porttinari-base                           | Porttinari-base   | 94,82    |\n",
            "| DANTEStocks                               | DANTEStocks       | 87,61    |\n",
            "| PetroGold                                 | PetroGold         | 86,74    |\n",
            "| DANTEStocks + Porttinari-base             | Todos juntos      | 94,91    |\n",
            "| DANTEStocks + PetroGold                   | Todos juntos      | 87,66    |\n",
            "| Porttinari-base + PetroGold               | Todos juntos      | 94,92    |\n",
            "| Genipapo                                  | Todos juntos      | 94,94    |\n",
            "\n",
            "Tabela 3. UAS (%) do modelo em cada conjunto de teste do corpus.\n",
            "\n",
            "| Conjunto de treinamento                   | Conjunto de teste | UAS (%)  |\n",
            "|-------------------------------------------|-------------------|----------|\n",
            "| Porttinari-base                           | Porttinari-base   | 95,88    |\n",
            "| DANTEStocks                               | DANTEStocks       | 90,36    |\n",
            "| PetroGold                                 | PetroGold         | 89,69    |\n",
            "| DANTEStocks + Porttinari-base             | Todos juntos      | 95,95    |\n",
            "| DANTEStocks + PetroGold                   | Todos juntos      | 90,26    |\n",
            "| Porttinari-base + PetroGold               | Todos juntos      | 95,91    |\n",
            "| Genipapo                                  | Todos juntos      | 95,97    |\n",
            "\n",
            "Observamos que cada modelo treinado isoladamente produz os melhores resultados para seu gênero correspondente. Por exemplo, considerando LAS, o treinamento com Porttinari-base produziu os melhores resultados para o conjunto de teste de Porttinari-base (94,82%) e os piores resultados para DANTEStocks (66,10%) e PetroGold (87,47%). Esse padrão se mantém entre os gêneros, onde os modelos isolados consistentemente desempenham melhor quando testados no mesmo gênero em que foram treinados. Mais interessante, o Genipapo, nosso parser multigênero, supera os parsers treinados em um único gênero para 2 dos gêneros (textos de notícias e posts do X), mas não para o gênero acadêmico. As diferenças, no entanto, são mínimas (menos de 1%), sugerindo que poderiam se dever a flutuações aleatórias ao invés de diferenças estatisticamente significativas.\n",
            "\n",
            "Ao combinar todos os conjuntos de teste (colunas “Todos juntos” nas tabelas), o Genipapo entrega os melhores resultados, alcançando uma melhoria de 7% em LAS e quase 5% em UAS em comparação aos segundos melhores resultados dos parsers de gênero único, e uma melhoria de 3% em LAS e 2% em UAS sobre os parsers treinados em pares. Isso sugere que o Genipapo pode ser a escolha mais adequada para processar textos de fontes variadas, como conteúdo diverso da web.\n",
            "\n",
            "Ao observar os resultados produzidos pelo Genipapo, quando testado em cada corpus separadamente, vemos alguns erros comuns entre pares de deprels. Um dos erros mais comuns entre os três corpora foi a confusão entre obl e nmod. Esse resultado não é uma total surpresa, uma vez que a classificação de um nominal como um adjunto adverbial (obl) ou como um modificador nominal (nmod) já foi relatada na literatura como um desafio para o parsing do português padrão (e também para humanos em algumas situações), como em textos jornalísticos e acadêmicos [Duran et al. 2023b, Souza et al. 2021]. Uma vez que esse fenômeno também é observado no UGC do DANTEStocks, essa dificuldade parece estar desconectada do grau de “canonicidade” do corpus. Os pares acl (cláusula adnominal) e advcl (cláusula adverbial) e obj (o segundo argumento de um verbo) e nsubj (um sujeito nominal) mostram uma confusão relevante apenas nos corpora da língua padrão. A confusão entre acl e advcl parece ser um caso de ambiguidade que requer conhecimento semântico para ser resolvido, e a confusão entre obj e nsubj ocorre quando o candidato a sujeito está à direita do verbo, pois frases nominais à direita podem ser tanto objeto quanto sujeito em português [Duran et al. 2023b].\n",
            "\n",
            "Ao comparar os erros do Genipapo em cada deprel, observamos que o modelo faz um número maior de previsões erradas de raiz no DANTEStocks, dado sua taxa de erro de 7,7% contra 2,0% no Porttinari-base e 0,9% no PetroGold. Isso pode ser devido aos fenômenos linguísticos dos tweets que trazem algumas dificuldades para a anotação sintática da raiz. Outra observação interessante relaciona-se à parátaxe, que é uma das tags mais frequentes em nosso corpus UGC (708 casos), mas não nos demais corpora. A taxa de erro relativamente baixa no DANTEStocks (9,3%) indica que esse deprel foi bem aprendido pelo Genipapo no UGC. Além disso, pudemos observar que as tags de deprel mais erradamente previstas devido à sub-representação no Porttinari-base e no DANTEStocks são as mesmas: reparandum, deslocado e órfão. As duas primeiras tags não ocorrem no PetroGold, e as únicas duas ocorrências de órfão nesse corpus foram previstas incorretamente.\n",
            "\n",
            "Como forma de comparar o desempenho do Genipapo com o de um modelo de estado-da-arte, também executamos o Portparser nos mesmos conjuntos de teste que o Genipapo (Tabela 4). Observamos que as divisões de treinamento, validação e teste do Porttinari-base usadas pelo Portparser diferem daquelas publicamente disponíveis e usadas em nossos experimentos com o Genipapo. Essa discrepância significa que algumas frases presentes nos conjuntos de teste do Porttinari-base e no conjunto unificado (Todos juntos) podem ter sido incluídas nos conjuntos de treinamento ou validação do Portparser, aumentando artificialmente suas pontuações de LAS e UAS. Apesar disso, o Genipapo superou o Portparser em todos os conjuntos de teste, exceto no conjunto de teste do Porttinari-base. Em termos de LAS, o Genipapo mostrou melhorias significativas sobre o Portparser no conjunto de teste do DANTEStocks (92,69% vs. 64,45%), no conjunto de teste do PetroGold (95,11% vs. 86,74%) e no conjunto de teste combinado (94,75% vs. 89,51%). No entanto, o Portparser teve um desempenho melhor no conjunto de teste do Porttinari-base (98,06% vs. 94,94%). O mesmo padrão se observa nas pontuações de UAS, onde o Genipapo superou o Portparser no DANTEStocks (94,42% vs. 75,81%), no PetroGold (95,81% vs. 90,50%) e no conjunto de teste combinado (95,73% vs. 92,62%). No entanto, o Portparser obteve UAS mais alta no Porttinari-base (98,58% vs. 95,97%).\n",
            "\n",
            "Tabela 4. LAS e UAS do Portparser em cada conjunto de teste do corpus.\n",
            "\n",
            "| Conjunto de teste | LAS (%) | UAS (%) |\n",
            "|-------------------|---------|---------|\n",
            "| Porttinari-base   | 98,06   | 98,58   |\n",
            "| DANTEStocks       | 64,45   | 75,81   |\n",
            "| PetroGold         | 86,74   | 90,50   |\n",
            "| Todos juntos      | 89,51   | 92,62   |\n",
            "\n",
            "6. Considerações finais\n",
            "\n",
            "Neste trabalho, introduzimos o Genipapo, um parser UD multigênero para o português brasileiro, e mostramos que ele teve desempenho melhor ou competitivo em relação a parsers treinados especificamente para um gênero. Trabalhos futuros incluem (i) estender o treinamento do Genipapo a outros gêneros e domínios, como transcrições de áudio, textos literários e tweets relacionados à pandemia de COVID-19, cujos corpora ainda estão em construção, e (ii) explorar diferentes estratégias e pipelines de parsing.\n",
            "\n",
            "Mais detalhes sobre este trabalho podem ser encontrados no portal web do projeto POeTiSA: https://sites.google.com/icmc.usp.br/poetisa/.\n",
            "\n",
            "Agradecimentos\n",
            "\n",
            "Este trabalho foi realizado no Centro de Inteligência Artificial da Universidade de São Paulo (C4AI-http://c4ai.inova.usp.br/), com apoio da Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP 2019/07665-4) e da IBM Corporation. O projeto também foi apoiado pelo Ministério da Ciência, Tecnologia e Inovação, com recursos da Lei N.8.248, de 23 de outubro de 1991, no âmbito do PPI-SOFTEX, coordenado pela Softex e publicado como Residência em TIC13, DOU01245.010222/2022-44.\n",
            "31\n",
            "Resumo. O surgimento dos Grandes Modelos de Linguagem (LLMs) representou um avanço significativo nas aplicações de geração de texto. No entanto, os LLMs enfrentam desafios em domínios fora do escopo de seu treinamento original. Este estudo investiga duas abordagens para adaptar LLMs a novos domínios no contexto de questionamento e resposta generativa (QA) com dados em português: ajuste fino (fine-tuning) e Geração Aumentada por Recuperação (RAG). Os experimentos realizados neste estudo demonstram a eficácia de incorporar fontes de dados externas, mesmo em modelos que não foram ajustados para o domínio específico. Além disso, a combinação de ajuste fino supervisionado com RAG provou ser a abordagem mais eficaz.\n",
            "\n",
            "1. Introdução\n",
            "O surgimento dos Grandes Modelos de Linguagem (LLMs) marcou um avanço significativo no Processamento de Linguagem Natural (NLP), especialmente em tarefas de geração de texto [Brown et al., 2020; Achiam et al., 2023]. Esses modelos, que são treinados com grandes volumes de dados, podem reter vastas quantidades de conhecimento implicitamente em seus parâmetros. No entanto, os LLMs enfrentam desafios em domínios fora do escopo de seus dados de treinamento originais, como áreas de conhecimento especializado ou questões atuais [Kandpal et al., 2023, Kasai et al., 2024]. Esse problema acentua a necessidade de adaptar os LLMs a contextos específicos, especialmente para modelos menores com capacidade de memória limitada.\n",
            "\n",
            "Este estudo explora a adaptação de LLMs a novos domínios na tarefa de questionamento e resposta generativa (QA), um cenário em que o modelo gera respostas com base nas perguntas fornecidas. O ajuste fino é uma abordagem comum para ajustar LLMs a novos domínios, modificando os parâmetros do modelo com treinamento em dados específicos de aplicação. Na tarefa de QA, o ajuste fino pode ser feito com pares de perguntas-respostas em um cenário de 'livro fechado' [Zhang et al., 2024], onde o modelo não tem acesso a informações externas. No entanto, essa abordagem pode exigir considerável poder computacional e extensa anotação de dados [Guo et al., 2023].\n",
            "\n",
            "Uma alternativa amplamente adotada é a integração de fontes de conhecimento externas, como documentos e livros, em um ambiente conhecido como 'livro aberto' [Zhang et al., 2024]. Uma estratégia típica dessa abordagem é a Geração Aumentada por Recuperação (RAG) [Lewis et al., 2020], que combina um modelo de recuperação de informações (IR), voltado para buscar dados relevantes em uma fonte externa, com um modelo de linguagem para gerar respostas com base nas informações recuperadas. Essa estratégia permite que os LLMs se adaptem a novos domínios sem a necessidade de ajuste fino.\n",
            "\n",
            "Este trabalho analisou essas duas abordagens para adaptar LLMs a domínios específicos na tarefa de QA com dados em português. Analisamos tanto configurações de ajuste fino quanto de RAG, além da integração das duas. Os experimentos demonstraram a eficácia de incorporar fontes de dados externas para melhorar os resultados. Além disso, diferentes estratégias de ajuste fino mostraram ser particularmente eficazes quando combinadas com a inclusão de dados externos, mesmo com um volume reduzido de dados de treinamento. Para toda análise, consideramos um cenário com recursos computacionais limitados, onde usamos apenas uma GPU de propósito geral (Nvidia GeForce RTX 4090). O ajuste fino dos modelos foi realizado usando a técnica QLoRA [Dettmers et al., 2023] com modelos quantizados.\n",
            "\n",
            "2. Contexto e Trabalhos Relacionados\n",
            "Nesta seção, discutimos brevemente RAG e ajuste fino de LLMs.\n",
            "\n",
            "2.1. Modelos de Linguagem Aumentada por Recuperação\n",
            "Condicionar as respostas dos LLMs a informações de fontes externas provou ser eficaz na adaptação desses modelos a domínios específicos em várias tarefas de NLP. A abordagem RAG foi aplicada com sucesso em áreas como agricultura [Balaguer et al., 2024], literatura científica [Lála et al., 2023] e dados médicos [Zakka et al., 2024]; atestando sua utilidade em melhorar a precisão e a relevância das respostas. Além disso, pode reduzir a ocorrência de alucinações [Borgeaud et al., 2022; Shuster et al., 2021], melhorar a capacidade do modelo de gerenciar a gradual diminuição de seu conhecimento ao longo do tempo [Vu et al., 2023] e aumentar a interpretabilidade das respostas [Lewis et al., 2020; Izacard et al., 2023].\n",
            "\n",
            "A eficácia do RAG depende da qualidade do mecanismo de recuperação utilizado, o que impacta a relevância das informações contextuais obtidas. Entre os mecanismos de recuperação tradicionais, os baseados em frequência de termos se destacam, empregando representações esparsas de trechos de texto, como TF-IDF [Sparck Jones, 1972] e BM25 [Robertson e Zaragoza, 2009]. Alternativamente, abordagens mais recentes empregam representações densas de textos, como o Recuperador de Passagens Densas (DPR) [Karpukhin et al., 2020] e ColBERT [Khattab e Zaharia, 2020].\n",
            "\n",
            "2.2. Ajuste Fino\n",
            "Para adaptar modelos de QA a um novo domínio, o ajuste fino busca ajustar o modelo para responder de acordo com o padrão observado nos dados de treinamento. Além disso, espera-se que com o ajuste fino o modelo adquira conhecimento específico do domínio, aumentando sua capacidade de fornecer respostas mais precisas.\n",
            "\n",
            "O ajuste fino de grandes modelos de linguagem (LLMs) normalmente requer recursos computacionais significativos. O Ajuste Fino Eficiente em Parâmetros (PEFT) [Xu et al., 2023] aborda isso congelando os parâmetros do modelo e ajustando apenas aqueles que foram recém-adicionados. Entre os métodos PEFT, a Adaptação de Baixa Classificação (LoRA) [Hu et al., 2021] reduz o número de parâmetros treináveis usando matrizes de baixa classificação. A Adaptação de Baixa Classificação Quantizada (QLoRA) [Dettmers et al., 2023] leva essa abordagem um passo adiante aplicando LoRA a modelos quantizados. Estudos indicam que modelos ajustados por PEFT muitas vezes apresentam desempenho comparável àqueles totalmente ajustados [Li et al., 2023].\n",
            "\n",
            "2.3. RAG e Ajuste Fino\n",
            "A eficácia das estratégias RAG e de ajuste fino foi extensivamente estudada. [Balaguer et al., 2024] comparam esses métodos em um modelo de QA para agricultura, enquanto [Ovadia et al., 2023] expandem a comparação para vários tópicos com um modelo de QA de múltipla escolha. Muitos modelos RAG passam por ajuste fino, como [Lewis et al., 2020], onde tanto o modelo quanto o recuperador são ajustados juntos. [Zhang et al., 2024] introduzem RAFT (Ajuste Fino Aumentado por Recuperação), que ajuda o modelo a ignorar documentos irrelevantes. No geral, modelos pré-treinados requerem ajuste fino adicional para aprender tarefas específicas de compreensão de leitura, o que é essencial para a eficácia do RAG. Este ajuste fino instrucional não precisa sempre ser feito com dados específicos do domínio.\n",
            "\n",
            "3. Metodologia\n",
            "\n",
            "3.1. Modelos de Linguagem\n",
            "Para os experimentos deste estudo, utilizamos uma versão adaptada em português do modelo T5 [Raffel et al., 2020], chamada PTT5 [Carmo et al., 2020], e duas versões do modelo Llama-3 8B [AI at Meta, 2024]. O PTT5 foi pré-treinado no BrWac [Wagner et al., 2018], um conjunto de dados composto por milhões de páginas da Internet em português brasileiro. Este estudo utilizou a versão base do modelo, que contém 220M de parâmetros. Essa escolha se deu por seu tamanho relativamente pequeno, pré-treinamento em português e arquitetura de codificador-decodificador, distinguindo-o do Llama 3. O Llama 3, que foi desenvolvido pela Meta, utiliza uma arquitetura apenas de decodificador e está disponível tanto em versões pré-treinadas quanto ajustadas para instrução. Apesar de ter sido treinado principalmente com dados em inglês, o Llama 3 é multilíngue e foi avaliado exclusivamente com dados em português neste estudo. A versão de 8 bilhões de parâmetros do Llama foi escolhida por sua adequação a recursos computacionais limitados e sua popularidade como um modelo de código aberto amplamente utilizado.\n",
            "\n",
            "3.2. Abordagem de Ajuste Fino\n",
            "Para o ajuste fino dos modelos, utilizamos duas técnicas diferentes: ajuste fino total e a técnica QLoRa [Dettmers et al., 2023]. O ajuste fino total foi aplicado apenas ao modelo PTT5, devido ao seu tamanho reduzido e à disponibilidade limitada de recursos computacionais. Para os modelos Llama 3-8B, escolhemos a técnica QLoRa devido ao grande número de parâmetros nesses modelos.\n",
            "\n",
            "3.3. Configuração do RAG\n",
            "Adotamos uma configuração básica de RAG, na qual usamos os modelos de recuperação e de linguagem sem alterações em suas arquiteturas originais. Três modelos de recuperação foram avaliados: BM25 [Robertson e Zaragoza, 2009], Recuperador de Passagens Densas (DPR) [Karpukhin et al., 2020] e ColBERT [Khattab e Zaharia, 2020]. Com o DPR, as incorporações foram geradas por um modelo baseado em BERT [Devlin et al., 2019] conhecido como Sentence-BERT [Reimers e Iryna, 2019]. No caso do ColBERT, utilizamos especificamente sua segunda versão - ColBERTv2 [Santhanam et al., 2021]. Todos os modelos IR foram usados sem qualquer tipo de treinamento nos dados em estudo. Os textos recuperados para cada consulta foram concatenados e inseridos no prompt de entrada dos LLMs como textos de apoio.\n",
            "\n",
            "3.4. Configuração de Avaliação\n",
            "Consideramos quatro métricas para avaliar os resultados dos experimentos: (i) Rouge-1 e (ii) Rouge-L [Lin, 2004], que se baseiam na sobreposição de palavras; (iii) BERTScore [Zhang et al., 2019], que emprega incorporações geradas por um modelo BERT; e (iv) uma métrica específica desenvolvida com o uso do GPT (GPT 4o mini). O GPT foi utilizado para verificar a precisão das respostas geradas pelos modelos. Para atingir isso, desenvolvemos a métrica GPTScore, que avalia se as respostas dos modelos estão alinhadas com o conteúdo das respostas de referência, mesmo quando diferem em redação, comprimento ou estilo. Essa avaliação utilizou o prompt mostrado na Figura 1. A métrica foi computada contando o número de respostas \"sim\" ou \"não\" fornecidas pelo GPT.\n",
            "\n",
            "Figura 1. Prompt usado para obter o GPTScore.\n",
            "\n",
            "4. Experimentos e Resultados\n",
            "Esta seção descreve a configuração experimental e apresenta os resultados obtidos usando as estratégias de RAG e ajuste fino.\n",
            "\n",
            "4.1. Configuração Experimental\n",
            "Utilizamos dois conjuntos de dados em português. O primeiro, Pirá 2.0 [Paschoal et al., 2021; Pirozelli et al., 2024], foca em tópicos relacionados à costa brasileira, oceanos e mudanças climáticas. Este conjunto de dados contém perguntas, respostas e textos de apoio derivados dos resumos de artigos científicos e relatórios especializados sobre os tópicos mencionados, todos os quais possuem uma versão em português. Composto por 2258 amostras, este conjunto de dados foi dividido da seguinte forma: 80% para treinamento, 10% para validação e 10% para teste. Selecionamos este conjunto de dados porque inclui textos em português e foca em um domínio específico.\n",
            "\n",
            "O segundo conjunto de dados é uma tradução em português do conjunto de dados Databricks-Dolly [Conover et al., 2023], que consiste em pares de instruções e respostas em várias categorias de tarefas geradas por funcionários da Databricks. Para este estudo, mantivemos apenas os registros classificados como tarefa de 'QA fechada', que envolve perguntas e respostas baseadas em trechos da Wikipedia. Este conjunto de dados contém 1766 registros, distribuídos da seguinte forma: 70% para treinamento, 15% para validação e 15% para teste. Ele foi incluído neste estudo porque contém perguntas de um domínio mais amplo que ainda requerem textos de apoio para a formulação de respostas.\n",
            "\n",
            "Os experimentos realizados neste estudo visaram explorar diferentes estratégias para adaptar LLMs a novos domínios, com foco particular em ajustes finos e abordagens baseadas em RAG. Investigamos várias configurações em relação ao uso de textos de apoio, tanto no processo de ajuste fino quanto durante a fase de validação. Para os experimentos de ajuste fino, avaliamos duas abordagens: uma que inclui textos de apoio e pares de pergunta-resposta no prompt de entrada, denominada 'RAG FT'; e outra que utiliza apenas pares de pergunta-resposta, referida como 'QA FT'. Para validação, os cenários em que textos de apoio foram incluídos no prompt de entrada foram chamados de 'RAG'. Para configurações em que apenas a pergunta foi usada como entrada, o seguinte prompt foi utilizado: “Responda à pergunta de forma sucinta. Pergunta: {question}”. Nos casos em que também foram incluídos textos de apoio, o prompt foi modificado para: “Responda à pergunta de forma sucinta e com base no contexto dado. Contexto: {context} Pergunta: {question}”.\n",
            "\n",
            "Em nossos experimentos, empregamos uma estratégia de decodificação 'gananciosa', selecionando o token com a maior probabilidade durante a geração. Estabelecemos um limite máximo de saída de 100 tokens, enquanto o limite de entrada foi fixado em 1024 tokens para acomodar a maioria dos textos de apoio sem truncamento. O ajuste fino dos modelos Llama utilizou o método QLoRa com quantização de 4 bits ao longo de 10 épocas, utilizando um tamanho de lote e acumulação de gradiente de 4 para otimizar a capacidade do hardware. O modelo gerou respostas com precisão de 16 bits. Enquanto isso, o modelo PTT5 passou por ajuste fino total por 60 épocas, usando um tamanho de lote de 8 e acumulação de gradiente de 4. Todo o treinamento foi conduzido usando bibliotecas Hugging Face em uma GPU NVIDIA GeForce RTX 4090.\n",
            "\n",
            "Para selecionar o melhor recuperador para os experimentos de RAG, usamos o GPT 4o mini para responder às perguntas em cada conjunto de dados com base no contexto fornecido por cada recuperador. Para cada pergunta, os textos recuperados por cada modelo foram concatenados e adicionados ao prompt usado pelo GPT para gerar a resposta. Para o conjunto de dados Pirá, usamos os quatro trechos mais relevantes identificados por cada recuperador, enquanto para o conjunto de dados Dolly, usamos os três trechos mais relevantes. O ColBERT superou todos os outros modelos em todas as métricas avaliadas e, portanto, foi escolhido para este estudo. A Tabela 1 resume esses resultados e inclui um recuperador hipoteticamente ideal, simulado usando textos de contexto que são sempre corretos para cada pergunta.\n",
            "\n",
            "Tabela 1. Avaliação dos métodos de recuperação (ver texto).\n",
            "\n",
            "4.2. Modelos sem ajuste fino\n",
            "Os experimentos com modelos sem ajuste fino, considerando apenas a pergunta no prompt de entrada, podem revelar seu nível de conhecimento prévio sobre o domínio dos conjuntos de dados. Os resultados para essa configuração, referida como \"No FT, No RAG\" na Tabela 2, indicam que esses modelos têm baixo conhecimento prévio do domínio do conjunto de dados Pirá e conhecimento moderado do conjunto de dados Dolly. Nesta análise, os resultados dos modelos PTT5 não são relatados, pois não conseguimos obter respostas satisfatórias desse modelo sem ajuste fino.\n",
            "\n",
            "No caso de experimentos de RAG com modelos sem ajuste fino, referidos como \"No FT, RAG\", observamos um aumento no GPTScore e um aumento mais modesto nas métricas Rouge. Esse comportamento é esperado, uma vez que as métricas Rouge, que avaliam a correspondência de termos, são mais influenciadas pelo estilo das respostas – particularmente seu comprimento e vocabulário. Como os modelos não foram ajustados para os conjuntos de dados de interesse, as respostas geradas podem não se alinhar aos padrões de resposta do conjunto de dados. Observou-se que o modelo pré-treinado muitas vezes respondia às perguntas e, em seguida, continuava gerando pares de perguntas-respostas indefinidamente até atingir o número máximo de tokens de saída. O modelo Llama Instruct teve um desempenho significativamente melhor tanto no GPTScore quanto no BERTScore devido ao seu pré-ajuste fino, que aprimorou suas habilidades de compreensão de leitura. Isso sugere que modelos com habilidades de compreensão avançadas, mesmo que treinados em domínios diferentes dos que estão sendo testados, podem se beneficiar substancialmente do uso de contextos de apoio para alavancar seu desempenho. Podemos também observar que modelos sem conhecimento prévio do domínio e que não exploraram RAG foram os de pior desempenho.\n",
            "\n",
            "4.3. Modelos ajustados apenas com pares pergunta-resposta\n",
            "Esta configuração visa avaliar se os modelos podem internalizar conhecimentos sobre os domínios por meio do processo de ajuste fino, especificamente com base em perguntas e respostas dos conjuntos de dados, referida como \"QA FT, No RAG\" na Tabela 2. Os resultados mostram que, para ambos os conjuntos de dados, o ajuste fino não proporciona uma melhoria significativa quando o modelo é testado sem RAG. No entanto, quando o modelo inclui RAG, referida como \"QA FT, RAG\" na Tabela 2, observamos um ganho significativo em alguns cenários de avaliação. Isso sugere que o processo de ajuste fino ajuda o modelo a aprender o estilo das respostas – o comprimento das respostas se torna mais semelhante ao observado no conjunto de dados – mas não necessariamente permite que ele retenha conhecimento do domínio. Vale ressaltar que a quantidade limitada de dados de treinamento pode prejudicar a capacidade do modelo de aprender de maneira eficaz por meio do ajuste fino.\n",
            "\n",
            "4.4. Modelos ajustados com perguntas, respostas e contexto\n",
            "Neste cenário, os modelos foram ajustados com a adição de contextos nos prompts de treinamento. Na configuração de validação sem RAG, referida como \"RAG FT, No RAG\", todos os modelos apresentaram desempenho fraco. Esse resultado é esperado, uma vez que o objetivo principal do ajuste fino com contextos adicionados é treinar o modelo para gerar respostas com base no próprio contexto. Como essa configuração não inclui os textos de apoio nos prompts de entrada, o ajuste fino não parece ter alcançado o resultado desejado.\n",
            "\n",
            "Na configuração que inclui RAG, referida como \"RAG FT, RAG\", os modelos alcançaram os melhores resultados em todas as métricas para os dois conjuntos de dados analisados. É importante notar que, para o modelo Llama Instruct, que já foi ajustado para a tarefa de compreensão de leitura, todas as configurações que utilizaram RAG apresentaram bom desempenho de acordo com o GPTScore. No entanto, para as métricas Rouge, os modelos com ajuste fino em dados específicos do domínio mostraram desempenho superior. Este experimento sugere que, mesmo se o modelo for capaz de extrair respostas do contexto, ajustar-se a dados específicos do problema pode ser benéfico para gerar respostas em um formato mais alinhado ao que foi encontrado no conjunto de dados. Também observamos que o ajuste fino do modelo Llama pré-treinado permitiu que ele alcançasse resultados comparáveis aos do modelo Llama Instruct, apesar de este último ter sido ajustado anteriormente com uma quantidade significativamente maior de dados. Esse resultado indica que o ajuste fino com textos de contexto, mesmo quando realizado com um conjunto de dados reduzido, pode aumentar a capacidade do modelo de extrair informações relevantes do contexto. Nesta configuração, também observamos uma melhora significativa nos resultados do PTT5, que foram claramente superados pelos obtidos com os modelos Llama, provavelmente devido ao seu muito maior número de parâmetros.\n",
            "\n",
            "Tabela 2. Resultados experimentais (ver texto).\n",
            "\n",
            "5. Conclusão\n",
            "Este trabalho analisou vários métodos para adaptar LLMs a domínios específicos em tarefas de QA, incluindo o ajuste fino do modelo e a integração de dados externos por meio de RAG. Os experimentos demonstraram que a incorporação de dados externos geralmente melhora o desempenho dos modelos, independentemente de um ajuste fino ser aplicado. Os resultados também mostraram que o ajuste fino, mesmo quando realizado com um conjunto de dados reduzido, pode aumentar o desempenho dos modelos. Além disso, observamos que, embora os melhores resultados tenham sido alcançados por modelos especificamente ajustados a dados do domínio, um modelo com instruções previamente ajustadas produziu resultados semelhantes, com a clara vantagem de não exigir nenhum ajuste fino adicional.\n",
            "\n",
            "Os experimentos apresentados aqui foram conduzidos usando uma arquitetura básica de RAG, sem qualquer treinamento adicional dos recuperadores nos conjuntos de dados de interesse. Trabalhos futuros poderiam explorar as mesmas configurações com ajustes nos recuperadores também. \n",
            "\n",
            "6. Agradecimentos\n",
            "Agradecimentos ao CNPq, FAPERJ e CAPES. Este estudo foi financiado em parte pela Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) – Código de Financiamento 001.\n",
            "32\n",
            "Resumo. Este estudo avalia o desempenho de Grandes Modelos de Língua (LLMs) recentes, como GPT-4o, GPT-3.5, Claude Opus e LLaMA 2, na análise automática de coerência textual. A pesquisa foca em três aspectos: coerência local, onde GPT-4o e Claude Opus se destacam; coerência global, na qual Claude Opus é o mais eficaz; e detecção de incoerências, onde GPT-4o apresenta melhor desempenho. Esses resultados revelam as capacidades e limitações dos modelos atuais, contribuindo para o entendimento de suas aplicações no âmbito do Processamento de Línguas Naturais e trazendo avanços contínuos à área.\n",
            "\n",
            "1. Introdução\n",
            "\n",
            "O conceito de coerência está no cerne da comunicação eficaz, servindo como um elemento chave que determina a clareza, compreensão e qualidade geral do conteúdo textual [Koch e Travaglia 2003]. A coerência transcende as fronteiras da sintaxe ou gramática; ela incorpora o fluxo lógico das ideias, garantindo que um texto não seja apenas uma coleção de frases, mas um todo unificado que transmite significado com precisão e sutileza [Freitas 2013]. À medida que avançamos mais para a era digital, onde as interações escritas são cada vez mais prevalentes [Hoey 2013], a capacidade de analisar automaticamente a coerência textual se tornou uma tarefa crucial no campo do Processamento de Linguagem Natural (NLP).\n",
            "\n",
            "O advento de Grandes Modelos de Linguagem (LLMs), como GPT-3, Llama e Gemini, revolucionou nossa abordagem para gerar textos que refletem a nuance e a profundidade do conteúdo escrito por humanos. Esses modelos, treinados em extensos corpora, demonstraram uma impressionante capacidade de produzir textos coerentes e contextualmente relevantes em uma ampla gama de tópicos. Essa proficiência na geração de texto se estende naturalmente ao potencial desses modelos para se destacarem em tarefas relacionadas à análise textual. A hipótese subjacente é simples, mas profunda: se um LLM pode gerar texto coerente, deve, por extensão, possuir uma habilidade refinada para discernir coerência – ou a falta dela – em textos existentes.\n",
            "\n",
            "No campo da linguística computacional, a coerência textual é definida pela sequência lógica e ordenada em que as ideias são apresentadas dentro de um texto, garantindo que informações e argumentos sejam transmitidos de maneira compreensível e fluida [Seno e Rino 2005]. Isso envolve não apenas a conexão superficial entre frases por meio de marcadores de discurso ou palavras de transição, mas também uma harmonia mais profunda em termos de tema, propósito e conhecimento compartilhado entre o autor e o leitor [Charolles 1978]. Para sistemas de NLP, avaliar a coerência de um texto implica entender como suas partes constituintes – seja no nível de frase, parágrafo ou documento – se juntam para formar um todo unificado que é logicamente consistente e esteticamente agradável [Jurafsky e Martin 2024]. Essa definição destaca a complexidade da tarefa de análise de coerência textual, sublinhando-a como um desafio significativo dentro do campo.\n",
            "\n",
            "Historicamente, a coerência foi conceptualizada através de várias estruturas teóricas. A Teoria da Estrutura Retórica (RST) [Mann e Thompson 1987] postula que a coerência do texto deriva da organização hierárquica de unidades textuais, enquanto a Teoria de Centro [Grosz et al. 1995] enfatiza o papel das entidades do discurso e sua continuidade através das frases. Com o tempo, as abordagens computacionais para a coerência evoluíram de sistemas baseados em regras, dependendo de marcadores explícitos de coerência e padrões estruturais, para algoritmos sofisticados de aprendizado de máquina que inferem coerência implicitamente a partir de grandes conjuntos de dados [Jurafsky e Martin 2024]. O desenvolvimento de modelos baseados em redes neurais, particularmente aqueles que empregam mecanismos de atenção como BERT [Devlin et al. 2018], marcou um avanço significativo, permitindo uma compreensão mais profunda das relações contextuais dentro dos textos.\n",
            "\n",
            "Dado esse contexto, o objetivo principal deste estudo é avaliar as capacidades de vários LLMs na análise da coerência textual. Especificamente, o estudo avalia como os modelos GPT 3.5, GPT 4, GPT 4o, Claude 3 Opus, Claude 3.5 Sonnet, Claude 3 Haiku, Gemini, LLaMA 2 13b, LLaMA 2 7b e Bard se saem em três tarefas-chave: classificar textos como (i) localmente ou (ii) globalmente coerentes ou incoerentes, e (iii) identificar segmentos incoerentes específicos dentro dos textos. Ao examinar esses aspectos, o estudo visa contribuir para o diálogo contínuo sobre a melhoria das tecnologias de NLP e avançar nossa compreensão de como as máquinas processam e entendem as sutilezas da linguagem humana.\n",
            "\n",
            "O restante deste artigo está estruturado da seguinte forma: a Seção 2 revisa teorias e modelos-chave na análise da coerência textual. A Seção 3 apresenta a literatura relevante, enquanto a Seção 4 detalha a metodologia, incluindo os modelos avaliados e as métricas utilizadas. A Seção 5 discute os resultados e suas implicações, e a Seção 6 conclui com um resumo das descobertas e sugestões para pesquisas futuras.\n",
            "\n",
            "2. Fundamentação Teórica\n",
            "\n",
            "A coesão e a coerência textual são fundamentais para a análise do discurso e NLP, pois explicam como os textos são estruturados e interpretados. Coesão refere-se às conexões dentro de um texto criadas por meio de várias relações linguísticas, como pronomes, conjunções e laços lexicais, assegurando que o texto seja percebido como um todo unificado em vez de uma coleção aleatória de frases [Halliday e Hasan 1976]. Coerência, por outro lado, é um conceito mais abstrato, referindo-se à organização lógica e significativa das ideias dentro de um texto, permitindo que os leitores sigam o fluxo de informações e entendam a mensagem pretendida [Van Dijk 1977].\n",
            "\n",
            "A coesão pode ser alcançada por meio de meios gramaticais e lexicais. A coesão gramatical inclui o uso de pronomes, elipses e conjunções para vincular frases, enquanto a coesão lexical envolve a repetição de palavras ou o uso de sinônimos para manter a continuidade das ideias. No entanto, um texto pode ser coeso sem ser coerente se as frases não contribuírem para um todo significativo [Koch e Travaglia 2003].\n",
            "\n",
            "A coerência pode ser examinada em dois níveis: local e global. A coerência local refere-se às conexões lógicas entre frases e parágrafos adjacentes, garantindo que cada ideia flua suavemente para a próxima. Isso é frequentemente alcançado por meio de dispositivos coesivos, como pronomes e conjunções, que ajudam a manter a continuidade no significado. A coerência global, por outro lado, diz respeito à estrutura geral e à unidade do texto, onde todas as partes contribuem para um todo consistente e significativo [Charolles 1978]. Ambos os níveis de coerência são essenciais para que um texto seja entendido como uma entidade coesa e logicamente organizada.\n",
            "\n",
            "Estruturas teóricas como a RST e a Teoria de Centro têm sido fundamentais no estudo da coerência. Por exemplo, a RST [Mann e Thompson 1987] analisa a organização hierárquica do texto ao examinar as relações entre diferentes segmentos, que ajudam a estruturar o texto de maneira coerente. A Teoria de Centro [Grosz et al. 1995] foca em como as entidades do discurso são gerenciadas através das frases, garantindo que o leitor possa acompanhar o progresso das ideias de forma fluida. Esses modelos influenciaram significativamente a pesquisa em NLP, particularmente na análise e geração de textos coerentes, oferecendo insights sobre os mecanismos que tornam um texto compreensível e logicamente conectado [Jurafsky e Martin 2024].\n",
            "\n",
            "3. Trabalhos Relacionados\n",
            "\n",
            "Como uma área central de investigação em NLP, a coerência textual, particularmente no contexto da coerência local, que foca no fluxo lógico e sequencial entre frases ou parágrafos adjacentes, tem sido amplamente estudada através de modelos como a grade de entidades. Introduzido por [Lapata e Barzilay 2005] e desenvolvido posteriormente por [Barzilay e Lapata 2008], o modelo de grade de entidades abstrai um texto em uma grade que captura a distribuição e as transições de entidades do discurso entre frases. Ao analisar esses padrões, o modelo pode inferir efetivamente o nível de coerência local dentro de um texto. Essa abordagem foi amplamente adotada e inspirou numerosos estudos subsequentes. Por exemplo, [Elsner et al. 2007] aprimoraram a avaliação de coerência ao integrar a grade de entidades com um modelo de conteúdo, enquanto [Lin et al. 2011] refinou o método ao incorporar relações de discurso, avançando assim o entendimento do campo sobre como as frases se conectam e mantêm a coerência.\n",
            "\n",
            "O teste de embaralhamento, introduzido por [Barzilay e Lapata 2008], tornou-se um método padrão para avaliar modelos de coerência local. Esse teste envolve a comparação da coerência de um texto em sua ordem original versus uma versão embaralhada, desafiando os modelos a reconhecer a sequência coerente. Estudos como os de [Lin et al. 2011] e [Dias 2016] utilizaram esse teste para validar a eficácia de seus modelos, destacando sua importância como um padrão na avaliação de coerência.\n",
            "\n",
            "Em contraste, a coerência global, que diz respeito à unidade geral e à consistência temática de um texto, recebeu menos atenção, mas permanece um aspecto chave para entender como os textos funcionam como um todo. Trabalhos iniciais de [Thompson 1986] enfatizaram o papel da coerência global em melhorar a legibilidade e a compreensão, argumentando que um texto coerente permite que os leitores sigam o tema ou argumento central sem esforço. Contribuições mais recentes de [Sagi 2010] exploraram a estrutura hierárquica dos textos, demonstrando como um discurso bem organizado contribui para a coerência global.\n",
            "\n",
            "Avanços recentes em NLP introduziram o BERT e LLMs como GPT-3, que expandiram significativamente as possibilidades para análise de coerência. Esses modelos, treinados em conjuntos de dados extensos, exibem uma notável capacidade de capturar tanto a coerência local quanto a global, levando a avaliações mais refinadas e semelhantes às humanas da estrutura textual. Por exemplo, [Braz Junior e Fileto 2021] aplicaram o BERT, especificamente o BERTimbau, em fóruns educacionais para medir a coerência. Ao analisar embeddings de frases, o modelo avaliou efetivamente a ordem das frases, distinguindo com precisão entre textos coerentes e permutados, demonstrando assim sua capacidade de capturar relacionamentos textuais sutis. Da mesma forma, [Naismith et al. 2023] utilizaram o GPT-4 para avaliação de coerência em contextos educacionais, onde o modelo não só avaliou a coerência, mas também forneceu racionalizações explicativas que se alinhavam de perto com as avaliações humanas. Este estudo demonstrou a eficácia do GPT-4 em replicar julgamentos humanos e até mesmo superar métricas tradicionais de NLP ao oferecer avaliações apoiadas por justificativas, destacando assim seu potencial para melhorar a avaliação automatizada da coerência do discurso e suas aplicações em ambientes educacionais.\n",
            "\n",
            "4. Metodologia\n",
            "\n",
            "O objetivo principal deste estudo é analisar e comparar o desempenho de vários LLMs na avaliação da coerência textual em diferentes aspectos usando duas abordagens distintas: (i) através de APIs de LLMs e (ii) através de interfaces de chat de LLMs. Para alcançar isso, selecionamos um conjunto diversificado de corpora pela sua relevância e variedade em tipos de texto, o que permite uma avaliação completa das capacidades dos modelos em diferentes contextos linguísticos. Esses conjuntos de dados foram pré-processados e anotados conforme necessário para garantir consistência nas tarefas.\n",
            "\n",
            "Um dos quatro corpora utilizados neste estudo é o Corpus de Inglês Americano Contemporâneo (COCA) [Davies 2008], que oferece uma compilação equilibrada de mais de um bilhão de palavras em gêneros como linguagem falada, ficção, textos acadêmicos e páginas da web. Para nossa análise, focamos nas partes livres das seções de blog e acadêmicas do COCA, que juntas compreendem uma variedade de níveis de coerência. A seção de blog, com 991 textos, é caracterizada por sua natureza informal e subjetiva, frequentemente apresentando menor coerência, enquanto a seção acadêmica, composta por 256 textos, é conhecida por sua linguagem estruturada e precisa, tipicamente demonstrando maior coerência. Essas seções foram empregadas tanto em tarefas de coerência local quanto global, com subconjuntos específicos anotados para análise detalhada da coerência global e identificação de incoerências.\n",
            "\n",
            "O estudo também incorpora o CST News Corpus [Aleixo e Pardo 2008], que consiste em 50 coleções de artigos de notícias em português brasileiro, cada uma centrada em um evento ou tópico específico. Originalmente desenvolvido para apoiar pesquisas sobre sumarização de múltiplos documentos, o corpus inclui aproximadamente 150 artigos de notícias e 300 resumos gerados por humanos de vários jornais, como Folha de São Paulo, Estadão e O Globo. Essa diversidade de fontes torna o corpus especialmente adequado para estudos de coerência, pois permite avaliar tanto a coerência local quanto a global em um contexto multilíngue. O CST News Corpus foi especialmente valioso para avaliar o desempenho dos modelos em português brasileiro, acrescentando uma dimensão multilíngue às nossas avaliações.\n",
            "\n",
            "Outro corpus chave nesta pesquisa é o Grammarly Corpus de Coerência do Discurso (GCDC) [Lai e Tetreault 2018], que contém 4.800 textos de quatro fontes do mundo real: Yahoo Answers, Clinton Emails, Enron Emails e Yelp Reviews. Devido à sua estrutura dependente de contexto, a porção do Yahoo Answers (1.200 textos) foi excluída do nosso estudo. Os Clinton Emails fornecem uma mistura de correspondência profissional e pessoal, os Enron Emails focam em comunicação empresarial formal, e os Yelp Reviews apresentam feedback gerado por usuários sobre negócios. Cada texto é anotado para coerência global em uma escala de 3 pontos (baixa, média, alta), com 8.000 classificações de anotadores especialistas e não especialistas via Amazon Mechanical Turk. Essas anotações pré-existentes foram utilizadas para comparar o desempenho dos modelos em relação aos julgamentos humanos, aprimorando nossa avaliação das tarefas de coerência global.\n",
            "\n",
            "Por último, o corpus DDisCo [Mikkelsen et al. 2022] foi desenvolvido para preencher uma lacuna em recursos para estudar a coerência do discurso em dinamarquês. Ele compreende 1.002 textos de duas fontes principais: Reddit e Wikipedia dinamarquesa. Os textos do Reddit, totalizando 501, consistem em conteúdo informal gerado por usuários, enquanto os 501 textos da Wikipedia dinamarquesa oferecem informações mais formais e estruturadas. Cada texto é anotado para a coerência global em uma escala de 3 pontos (baixa, média, alta) por especialistas em linguística. Esse corpus introduz diversidade linguística em nossa pesquisa, permitindo avaliar o desempenho dos modelos em outro contexto não inglês. Foi particularmente útil para avaliar como os modelos se generalizam em diferentes idiomas e estruturas de discurso.\n",
            "\n",
            "4.1. Análise de Coerência Local\n",
            "\n",
            "A análise de coerência local neste estudo utilizou o teste de embaralhamento, que avalia a coerência do texto comparando a ordem original das frases dentro de cada texto a uma versão aleatoriamente embaralhada. Este teste foi aplicado a textos de quatro corpora: COCA, CST News, GCDC e DDisCo. Um total de 2.318 textos foi selecionado, compreendendo 991 textos de blog e 256 textos acadêmicos do COCA, 251 artigos de notícias do CST News, 842 textos do GCDC e 991 textos do DDisCo. Cada texto foi segmentado em frases, e aqueles com menos de quatro frases foram excluídos, pois não permitiriam as 20 permutações exigidas. Os textos restantes foram embaralhados 20 vezes, gerando 46.360 versões incoerentes, resultando em um conjunto de dados de 48.678 textos para análise.\n",
            "\n",
            "O desempenho dos modelos foi avaliado usando dois métodos distintos: (i) via APIs de LLMs e (ii) através de interfaces de chat de LLMs. Na avaliação baseada em API, os textos foram processados diretamente através de chamadas automatizadas de API, agilizando o processo de avaliação. Em contraste, a avaliação da interface de chat simulou o uso do mundo real, submetendo os textos por meio de prompts interativos.\n",
            "\n",
            "Para ambas as abordagens, os modelos receberam um prompt padronizado para Análise de Coerência Local1 para guiá-los a distinguir entre textos coerentes e incoerentes. O desempenho foi medido usando precisão, revocação e F1-score, comparando as classificações dos modelos com os rótulos originais do texto.\n",
            "\n",
            "1https://github.com/bryankhelven/coherence-findings\n",
            "\n",
            "4.2. Análise de Coerência Global\n",
            "\n",
            "A análise de coerência global neste estudo teve como objetivo avaliar a capacidade de vários LLMs de avaliar a consistência lógica geral e a organização temática dos textos em um total de 2.142 textos. Essa análise incluiu 1.200 textos do Corpus DDisCo e 842 textos do Corpus GCDC, ambos já contendo anotações humanas. Adicionalmente, uma nova fase de anotação foi conduzida para um subconjunto de 100 textos dos corpora COCA e CST News, já que estes careciam de rótulos de coerência pré-existentes. Três especialistas em linguística avaliaram este subconjunto, que consistia em 10 textos acadêmicos e 60 textos de blog do COCA, além de 30 artigos de notícias do CST News. Para consistência, cada texto neste estudo recebeu uma pontuação de coerência em uma escala de Likert variando de baixa a alta coerência (1 a 3), usando a mesma escala previamente adotada pelos trabalhos de [Lai e Tetreault 2018] e [Mikkelsen et al. 2022]. Isso garantiu que a avaliação da coerência global fosse padronizada entre os vários corpora utilizados nesta análise.\n",
            "\n",
            "Após o processo de anotação, o estudo avaliou o desempenho dos modelos nas tarefas de coerência global usando dois métodos: (i) APIs de LLMs para um processo automatizado, e (ii) interfaces de chat de LLMs para simular interações guiadas por usuários no mundo real. Um total de 2.142 textos foi usado para essa análise, compreendendo os 100 textos anotados manualmente, 1.200 textos do corpus DDisCo e 842 textos do corpus GCDC. Ambos os métodos utilizaram um prompt padronizado para Análise de Coerência Global2 para guiar os modelos na avaliação da coerência global. As métricas de avaliação foram consistentes com aquelas usadas na análise de coerência local, mas neste caso, as classificações dos modelos foram comparadas diretamente às anotações humanas originais (notas 1, 2 ou 3).\n",
            "\n",
            "4.3. Identificação de Incoerências\n",
            "\n",
            "A tarefa de identificação de incoerências avaliou a capacidade de vários LLMs de detectar segmentos dentro dos textos que interrompem o fluxo lógico. Utilizamos 130 textos para essa tarefa, incluindo 100 textos previamente anotados para coerência global (10 textos acadêmicos e 60 textos de blog do COCA, 30 artigos de notícias do CST News) e 30 textos adicionais do corpus GCDC (10 de cada um dos grupos Yelp, Clinton e Enron). Os mesmos três anotadores da tarefa de coerência global identificaram segmentos incoerentes, focando nas categorias de Uso Incorreto de Conectores Lógicos, Repetição Desnecessária, Informação Irrelevante, Contradições, Sequência de Eventos e Tempos Verbais Inconsistentes. O Kappa de Fleiss, que obteve 0.8326 e indicou excelente concordância, foi escolhido por sua capacidade de contabilizar a concordância casual entre múltiplos anotadores em vários tipos de incoerência. Cada segmento anotado foi tratado como uma unidade, garantindo robustez na confiabilidade.\n",
            "\n",
            "Os anotadores, que se conheciam bem, se comunicaram livremente para resolver dificuldades, seguindo um entendimento compartilhado de coerência a partir de [Koch e Travaglia 2003]. O desempenho dos modelos foi avaliado utilizando os mesmos dois métodos mencionados anteriormente – APIs de LLMs e interfaces de chat. No entanto, nesta tarefa, cada modelo foi tratado como um anotador adicional. O acordo entre as anotações geradas pelos modelos e as anotações humanas foi medido utilizando o Kappa de Fleiss para determinar o quão próximas as avaliações dos modelos estavam do julgamento humano. O prompt para identificação de incoerências3 também foi padronizado e utilizado entre todos os modelos nessa tarefa.\n",
            "\n",
            "2 Disponível no GitHub (ver primeira nota de rodapé).\n",
            "3 Ibid.\n",
            "\n",
            "5. Resultados e Discussão\n",
            "\n",
            "Os resultados obtidos durante a execução da análise são resumidos nas Tabelas 1, 2 e 3, destacando o desempenho de vários modelos tanto em interações via API quanto baseadas em chat.\n",
            "\n",
            "Tabela 1. Métricas de Desempenho para Classificação de Coerência Local\n",
            "\n",
            "| API | Chat |\n",
            "| --- | --- |\n",
            "| Modelo | Bard | Claude 3 Haiku | Claude 3 Opus | Claude 3.5 Sonnet | Gemini | GPT 3.5 | GPT 4 | GPT 4o | LLaMA 2 13b | LLaMA 2 7b |\n",
            "| Acc | 0.756 | 0.914 | 0.979 | 0.973 | 0.978 | 0.918 | 0.970 | 0.982 | 0.831 | 0.817 |\n",
            "| Pr | 0.755 | 0.906 | 0.991 | 0.986 | 0.989 | 0.908 | 0.982 | 0.990 | 0.825 | 0.804 |\n",
            "| Re | 0.740 | 0.898 | 0.983 | 0.981 | 0.980 | 0.901 | 0.980 | 0.988 | 0.816 | 0.797 |\n",
            "| F1 | 0.748 | 0.902 | 0.987 | 0.983 | 0.985 | 0.905 | 0.981 | 0.989 | 0.820 | 0.800 |\n",
            "\n",
            "| Acc | 0.739 | 0.949 | 0.974 | 0.972 | 0.971 | 0.962 | 0.969 | 0.977 | 0.888 | 0.805 |\n",
            "| Pr | 0.742 | 0.902 | 0.971 | 0.969 | 0.971 | 0.905 | 0.966 | 0.975 | 0.821 | 0.801 |\n",
            "| Re | 0.739 | 0.899 | 0.973 | 0.968 | 0.970 | 0.902 | 0.965 | 0.973 | 0.818 | 0.798 |\n",
            "| F1 | 0.740 | 0.900 | 0.972 | 0.968 | 0.970 | 0.903 | 0.965 | 0.974 | 0.819 | 0.799 |\n",
            "\n",
            "Tabela 2. Métricas de Desempenho para Classificação de Coerência Global\n",
            "\n",
            "| API | Chat |\n",
            "| --- | --- |\n",
            "| Modelo | Claude 3 Haiku | Claude 3 Opus | Claude 3.5 Sonnet | Gemini | GPT 3.5 | GPT 4 | GPT 4o | LLaMA 2 13b | LLaMA 2 7b |\n",
            "| Acc | 0.959 | 0.982 | 0.980 | 0.976 | 0.960 | 0.974 | 0.978 | 0.970 | 0.968 |\n",
            "| Pr | 0.918 | 0.986 | 0.984 | 0.963 | 0.920 | 0.961 | 0.965 | 0.930 | 0.928 |\n",
            "| Re | 0.921 | 0.987 | 0.982 | 0.966 | 0.923 | 0.964 | 0.968 | 0.933 | 0.931 |\n",
            "| F1 | 0.920 | 0.986 | 0.983 | 0.965 | 0.921 | 0.963 | 0.967 | 0.932 | 0.930 |\n",
            "\n",
            "| Acc | 0.911 | 0.933 | 0.930 | 0.928 | 0.912 | 0.926 | 0.930 | 0.922 | 0.920 |\n",
            "| Pr | 0.871 | 0.936 | 0.934 | 0.915 | 0.873 | 0.914 | 0.918 | 0.887 | 0.881 |\n",
            "| Re | 0.875 | 0.939 | 0.931 | 0.918 | 0.879 | 0.919 | 0.920 | 0.883 | 0.884 |\n",
            "| F1 | 0.875 | 0.937 | 0.932 | 0.916 | 0.877 | 0.917 | 0.919 | 0.888 | 0.883 |\n",
            "\n",
            "Tabela 3. Kappa de Fleiss para Identificação de Incoerências\n",
            "\n",
            "| Modelo | Apenas Anotadores (base) | Claude 3 Haiku | Claude 3 Opus | Claude 3.5 Sonnet | Gemini | GPT 3.5 | GPT 4 | GPT 4o | LLaMA 2 13b | LLaMA 2 7b |\n",
            "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
            "| API | 0.8326 | 0.7995 | 0.8166 | 0.8279 | 0.8119 | 0.8038 | 0.8152 | 0.8316 | 0.6787 | 0.5823 |\n",
            "| Chat | 0.8326 | 0.7653 | 0.7987 | 0.8082 | 0.7858 | 0.7716 | 0.8093 | 0.8234 | 0.6492 | 0.5418 |\n",
            "\n",
            "A Tabela 1 mostra as métricas de desempenho para a Classificação de Coerência Local, com o GPT 4o alcançando as pontuações mais altas tanto em interações via API quanto em chat. Claude 3 Opus e Claude 3.5 Sonnet também se saíram bem, especialmente na interação via API, o que demonstra sua eficácia em identificar corretamente textos coerentes. Em contraste, os modelos LLaMA 2 13b e LLaMA 2 7b tiveram desempenho semelhante e inferior em ambos os cenários, sugerindo limitações no processamento e classificação da coerência local. Da mesma forma, para a Classificação de Coerência Global, GPT 4o e Claude 3 Opus se destacaram com o melhor desempenho em ambos os modos de interação, enquanto Claude 3 Haiku teve o pior desempenho, como mostrado na Tabela 2.\n",
            "\n",
            "Os resultados para a tarefa de Identificação de Incoerências estão resumidos na Tabela 3, onde o GPT 4o novamente demonstrou o maior acordo com os anotadores humanos, com um Kappa de Fleiss de 0.8316 na interação via API e 0.8234 em chat. Claude 3.5 Sonnet seguiu de perto, com valores de Kappa de 0.8279 na API e 0.8082 em chat, enquanto os modelos LLaMA, particularmente o LLaMA 2 7b, mostraram valores de Kappa significativamente mais baixos, indicando que esses modelos têm mais dificuldades para identificar segmentos incoerentes.\n",
            "\n",
            "A diferença no desempenho entre interações via API e chat é notável, com todos os modelos geralmente apresentando melhor desempenho nos testes baseados em API em todos os cenários. Isso pode indicar que as interações via API permitem um processamento mais preciso e estruturado, levando a uma maior exatidão e consistência.\n",
            "\n",
            "6. Conclusões e Trabalhos Futuros\n",
            "\n",
            "Este estudo avaliou o desempenho de LLMs na avaliação da coerência textual em níveis locais e globais e na identificação de incoerências em vários corpora. Modelos como GPT 4o e Claude 3 consistentemente superaram outros, especialmente em avaliações baseadas em API, onde alcançaram alta precisão e confiabilidade. Nas tarefas de coerência local, o GPT 4o demonstrou uma pontuação F1 de 0.989 em testes baseados em API, enquanto nas tarefas de coerência global, Claude 3 Opus liderou com uma pontuação F1 de 0.986. No entanto, as interações baseadas em chat revelaram um declínio no desempenho, com a pontuação F1 do GPT 4o caindo para 0.974 na coerência local e Claude 3 Opus para 0.937 na coerência global. Isso sugere que o modo de interação impacta a eficácia do modelo, com métodos baseados em API sendo mais estáveis.\n",
            "\n",
            "Apesar do forte desempenho dos modelos principais, a tarefa de Identificação de Incoerências provou ser desafiadora em geral. O GPT 4o mostrou o maior acordo com os anotadores humanos (Kappa de Fleiss de 0.8316), mas todos os modelos exibiram desempenho inferior em configurações baseadas em chat. Essas descobertas ressaltam a necessidade de melhorias nesta área, especialmente considerando que modelos de categorias inferiores como o LLaMA 2 tiveram dificuldades significativas, com Kappa de Fleiss caindo até 0.5418 nas avaliações baseadas em chat.\n",
            "\n",
            "Essas descobertas têm implicações práticas para o NLP, visto que modelos como GPT 4o e Claude 3 podem ser integrados em ferramentas de revisão, geradores de conteúdo e softwares educacionais para melhorar a coerência textual. Sua capacidade de avaliar e aprimorar a coerência beneficia conteúdos gerados por máquinas e ajuda os usuários a criar textos coerentes. Reconhecer o impacto dos modos de interação no desempenho orienta os desenvolvedores na escolha de estratégias de implementação eficazes, favorecendo integrações via API para consistência e precisão.\n",
            "\n",
            "O estudo reconhece ameaças à validade, particularmente o risco de que alguns dos corpora de avaliação possam ter sido parte dos dados de treinamento para os LLMs, potencialmente inflacionando o desempenho. Essa sobreposição introduz viés que pode comprometer a objetividade, uma vez que os modelos podem recordar padrões de treinamento em vez de avaliar genuinamente a coerência. A suposição de coerência em textos originais e o tamanho limitado e a diversidade dos conjuntos de dados anotados também apresentam riscos à generalização dos achados.\n",
            "\n",
            "Trabalhos futuros devem abordar essas limitações expandindo a gama de tipos de texto avaliados e incorporando grupos de anotadores maiores e mais diversos, bem como utilizando novos corpora coletados manualmente para garantir que os modelos não tenham tido acesso prévio ao material. Além disso, explorar técnicas de ajuste fino e avaliar novas arquiteturas de modelos será essencial. O desenvolvimento de métricas de avaliação aprimoradas e a exploração da análise de coerência cross-linguística e multimodal também são recomendadas para aumentar a robustez e aplicabilidade dos LLMs em tarefas complexas de linguagem.\n",
            "33\n",
            "Resumo. A detecção de anomalias, impulsionada pelos avanços em aprendizado de máquina e aprendizado profundo, ganhou importância significativa em diversas áreas. No entanto, sua aplicação a dados textuais não estruturados, particularmente em português, ainda é pouco explorada. Na análise textual, essas técnicas são cruciais para detectar desvios dentro de coleções de texto. Este artigo investiga métodos de ponta para detecção de anomalias em corpora de texto em português e apresenta uma nova função de perda flexível projetada para aprimorar a detecção em diferentes níveis de contaminação. Ao avaliar esses métodos em conjuntos de dados de referência, especificamente nos contextos de detecção de discurso de ódio e análise de sentimento, abordamos desafios existentes e contribuímos para o desenvolvimento de técnicas de detecção de anomalias mais eficazes para dados textuais em português.\n",
            "\n",
            "1. Introdução\n",
            "A detecção de anomalias refere-se à identificação de padrões em dados que se desviam das normas esperadas [Chandola et al. 2009]. Anomalias, frequentemente chamadas de outliers ou exceções, são distintas da maioria das observações que definem o padrão \"normal\". Embora as técnicas de detecção de anomalias tenham sido amplamente aplicadas a dados estruturados, como variáveis contínuas e categóricas [Boutalbi et al. 2023], menos atenção foi dada a dados não estruturados, como texto — o foco deste trabalho. A detecção de anomalias começou com métodos estatísticos no final do século 19 [Edgeworth 1887] e desde então se expandiu, com o aprendizado profundo ampliando seu escopo para domínios não estruturados, como imagens e texto [Chandola et al. 2009, Pimentel et al. 2014]. No entanto, sua aplicação à detecção de anomalias textuais ainda é limitada [Pang et al. 2019]. Detectar anomalias em dados textuais é particularmente desafiador devido à variedade de níveis linguísticos envolvidos, como ortografia, sintaxe e semântica [Xu et al. 2023b]. Aproveitar a capacidade do aprendizado profundo de modelar padrões complexos levou a avanços significativos na área.\n",
            "\n",
            "Tradicionalmente tratada como uma tarefa não supervisionada devido à ausência de rótulos de verdade, a detecção de anomalias tem empregado técnicas como autoencoders e GANs [Pang et al. 2019]. Abordagens populares neste domínio incluem DeepSVDD [Ruff et al. 2018] e Deep Isolation Forest [Xu et al. 2023a], ambas focando na modelagem de dados normais e na identificação de desvios como anomalias. No entanto, abordagens neurais semissupervisionadas recentes, como DevNet [Pang et al. 2019] e DeepSAD [Ruff et al. 2020], mostraram precisão de detecção aprimorada ao integrar anomalias rotuladas limitadas no processo de treinamento [Xu et al. 2023b], fechando a lacuna entre aprendizado não supervisionado e supervisionado. Apesar dos avanços recentes, ainda há uma lacuna significativa em pesquisas abrangentes focadas na detecção de anomalias em corpora de texto em português.\n",
            "\n",
            "Neste artigo, estendemos técnicas de detecção de anomalias baseadas em redes neurais para lidar com essas complexidades em dados textuais em português. Além disso, propomos uma mudança na função de perda para estabelecer um compromisso entre as amostras que correspondem a anomalias em relação às outras. Experimentos mostram que essa abordagem é bastante promissora. Para abordar efetivamente os desafios exclusivos de representar dados textuais, utilizamos dois modelos pré-treinados baseados em BERT, verificando os pontos fortes e fracos de cada representação nas diferentes tarefas.\n",
            "\n",
            "2. Metodologia\n",
            "2.1. Definição do Problema\n",
            "Dado um conjunto de dados X = {x1, x2, . . . , xN +K}, onde U = {x1, x2, . . . , xN} é dados não rotulados e K = {xN +1, . . . , xN +K} representa anomalias rotuladas (K ≪ N), o objetivo é treinar um modelo para identificar essas raras anomalias. Essa tarefa é desafiadora devido ao desbalanceamento entre o grande conjunto não rotulado e o pequeno conjunto de anomalias rotuladas. O processo envolve duas etapas principais:\n",
            "1. Transformação de Embedding: Os dados X são transformados em embeddings Z = {z1, z2, . . . , zN +K}, com cada zi sendo um vetor em Rd.\n",
            "2. Função de Pontuação: Uma rede neural aprende uma função de pontuação ϕ: Z → R para garantir que ϕ(zi) > ϕ(zj) quando zi é uma anomalia e zj é normal, minimizando o uso de exemplos rotulados.\n",
            "\n",
            "Adotamos o modelo DevNet devido ao seu bom desempenho demonstrado em estudos considerando o domínio textual [Xu et al. 2023b], junto com sua capacidade de gerenciar espaços de alta dimensão, como embeddings. Além disso, a função de perda interpretável do modelo, baseada em uma estratégia de Z-score simples, oferece insights valiosos que podem ser utilizados posteriormente para avaliar o texto identificado como anômalo.\n",
            "\n",
            "2.2. DevNet para Texto Anômalo\n",
            "O algoritmo DevNet [Pang et al. 2019] introduz uma abordagem semissupervisionada que aprende uma função de pontuação interpretável para outliers, ϕ(z; Θ), usando uma perda de desvio Z-score. Embora a formulação original seja baseada em pontos de dados brutos x, denominamos os embeddings como z para refletir as representações de dados transformadas. Essa abordagem assume uma distribuição normal prévia sobre os escores de anomalia, modelada com l objetos aleatórios ri ∈ R amostrados de uma distribuição normal padrão N(µR, σR):\n",
            "dev(z) = ϕ(z; Θ) − µR σR,\n",
            "(1)\n",
            "onde µR e σR são a média e o desvio padrão dos escores de anomalia dentro da distribuição de referência. Esse desvio é então incorporado em uma função de perda contrastiva para melhorar a distinção entre amostras anômalas e normais, onde y indica o status de anomalia, e a garante uma separação mínima entre as classes [Pang et al. 2019].\n",
            "L(ϕ(z; Θ), µR, σR) = (1 − y)|dev(z)| + y · max(0, a − dev(z)),\n",
            "(2)\n",
            "\n",
            "2.3. Proposta\n",
            "Para proporcionar flexibilidade, o parâmetro η ∈ [0, 1] é introduzido na função de perda DevNet dada na Eq. 2, controlando o equilíbrio entre amostras regulares e anômalas,\n",
            "L(ϕ(x; Θ), µR, σR) = (1 − η)(1 − y)|dev(x)| + η · y · max(0, a − dev(x)),\n",
            "(3)\n",
            "O parâmetro η ajusta a ênfase do modelo em anomalias, permitindo a adaptação a diferentes níveis de contaminação (ou seja, porcentagem de anomalias rotuladas) e disponibilidade de dados. Investigamos diferentes proporções de anomalias rotuladas para avaliar a robustez da solução em vários cenários, visando determinar a quantidade mínima de dados rotulados necessária para um bom desempenho. Além disso, empregamos duas estratégias distintas de representação de texto: o modelo BERTimbau monolíngue [Souza et al. 2020], projetado especificamente para processar texto em português, e o Sentence-BERT multilíngue (SBERT) [Reimers e Gurevych 2019], que gera embeddings em nível de sentença em vários idiomas, incluindo português. Nossa implementação personalizada do DevNet, chamada η-DevNet, foi avaliada em relação à sua versão original utilizando ambas as estratégias de representação.\n",
            "\n",
            "3. Experimentos e Resultados\n",
            "3.1. Experimentos\n",
            "Para avaliar o desempenho de diferentes métodos de representação e funções de perda, primeiro testamos valores de η variando de 0.5 a 1 usando a estratégia de embedding BERTimbau, onde η = 0.5 corresponde à formulação original do DevNet. Os outros parâmetros foram adotados da referência do DevNet: a = 5, l = 5000, µR = 0 e σR = 1. Os níveis de contaminação foram ajustados introduzindo entre 5 e 1000 anomalias ao longo dos experimentos, com anomalias selecionadas aleatoriamente. Após identificar o valor ótimo de η, aplicamos este valor em experimentos subsequentes para comparar ambas as estratégias de embedding em diferentes conjuntos de dados. Os valores médios de ROC-AUC foram calculados ao longo de 10 execuções experimentais para cada cenário.\n",
            "\n",
            "3.2. Conjunto de Dados\n",
            "Avaliamo nossa abordagem utilizando dois conjuntos de dados brasileiros. O primeiro, Told-Br [Leite et al. 2020], contém 21.000 instâncias rotuladas de tweets marcados com discurso de ódio, categorizadas em temas como homofobia, racismo e misoginia, com o discurso de ódio servindo como a classe de anomalia. O segundo conjunto de dados, UTLC-Movies [Sousa et al. 2019], compreende mais de um milhão de avaliações de filmes. Desse conjunto de dados, amostramos 40.000 críticas para análise de sentimento, onde o sentimento negativo é tratado como a classe de anomalia.\n",
            "\n",
            "3.3. Resultados\n",
            "A Figura 1 mostra melhorias de desempenho e estabilidade conforme η é ajustado, com η = 0.7 resultando no desempenho ideal. Este valor foi utilizado posteriormente para análises adicionais. Os resultados apresentados na Tabela 1 delineiam esses resultados para ambas as tarefas, demonstrando que, na maioria dos casos, a função de perda adaptada levou a melhorias de desempenho. Atingir um nível razoável de precisão requer um limite mínimo de exemplos rotulados, que varia com a complexidade da tarefa. Em nossos experimentos, a análise de sentimento precisou apenas de 0.87% de anomalias rotuladas para alcançar um ROC-AUC de 0.85, enquanto a detecção de discurso de ódio exigiu 2.59% para alcançar um ROC-AUC de 0.73. Essa discrepância provavelmente surge da maior complexidade da detecção de discurso de ódio, que envolve sutilezas linguísticas e expressões diversas. Além disso, modelos pré-treinados podem não capturar totalmente gírias e contextos politicamente específicos, que são comuns no discurso de ódio, mas podem estar sub-representados durante o treinamento.\n",
            "\n",
            "Tabela 1. Comparação dos valores de ROC-AUC em diferentes cenários e níveis de contaminação para os conjuntos de dados UTLC-Movies e Told-BR quando η = 0.7, onde % refere-se ao nível de contaminação.\n",
            "\n",
            "| UTLC-Movies | Told-BR |\n",
            "|-------------|---------|\n",
            "| Nb. Outliers|         |\n",
            "| 5           |  10     |  25    |  50    |  100   |  250   |  500   |  1000  | %     |\n",
            "| 0.02        | 0.04    | 0.09   | 0.18   | 0.35   | 0.87   | 1.73   | 3.41   |\n",
            "| Bη          | 0.58    | 0.62   | 0.66  | 0.70   | 0.78   | 0.82   | 0.83   | 0.85  |\n",
            "| BD          | 0.57    | 0.58   | 0.65  | 0.69   | 0.71   | 0.76   | 0.83   | 0.84  |\n",
            "| Mη          | 0.52    | 0.54   | 0.60  | 0.64   | 0.71   | 0.75   | 0.81   | 0.81  |\n",
            "| MD          | 0.52    | 0.57   | 0.68  | 0.66   | 0.70   | 0.69   | 0.79   | 0.75  |\n",
            "| %           | 0.05    | 0.11   | 0.27  | 0.53   | 1.05   | 2.59   | 5.05   | 9.62  |\n",
            "| Bη          | 0.46    | 0.48   | 0.57  | 0.58   | 0.66   | 0.73   | 0.75   | 0.76  |\n",
            "| BD          | 0.48    | 0.50   | 0.57  | 0.63   | 0.63   | 0.68   | 0.73   | 0.76  |\n",
            "| Mη          | 0.50    | 0.53   | 0.54  | 0.56   | 0.61   | 0.56   | 0.50   | 0.50  |\n",
            "| MD          | 0.50    | 0.53   | 0.60  | 0.60   | 0.60   | 0.52   | 0.51   | 0.52  |\n",
            "\n",
            "Siglas: BERTimbau η-loss (Bη), BERTimbau Devnet loss (BD), multilíngue SBERT η-loss (Mη), multilíngue SBERT Devnet loss (MD).\n",
            "\n",
            "Nossos resultados mostram que a representação BERTimbau [Souza et al. 2020] consistentemente superou o modelo multilíngue em tarefas. Essa vantagem pode ser atribuída à especialização do BERTimbau em português, permitindo capturar nuances linguísticas mais intrincadas, como expressões idiomáticas e variações regionais.\n",
            "\n",
            "4. Conclusão e Trabalho Futuro\n",
            "Este estudo mostra que o BERTimbau, adaptado para o português, supera consistentemente modelos multilíngues na detecção de anomalias, com a função de perda personalizada proporcionando melhorias notáveis. Esses resultados destacam o potencial de métodos semissupervisionados para tarefas como detecção de conteúdo prejudicial e análise de sentimento em contextos portugueses com dados rotulados limitados.\n",
            "\n",
            "Trabalhos futuros podem expandir essa abordagem para tarefas relacionadas, como modelagem de tópicos, detecção de notícias falsas e detecção de fraudes. Embora algum esforço de rotulação ainda seja necessário para um bom desempenho, a pequena quantidade de dados rotulados necessária torna essa abordagem viável em cenários com recursos limitados. Além disso, os promissores avanços em Modelos de Linguagem de Grande Escala (LLMs) poderiam não apenas servir como valiosas ferramentas para benchmarking, mas também automatizar a marcação de anomalias, reduzindo o esforço manual e melhorando a adaptabilidade e escalabilidade em várias aplicações do mundo real.\n",
            "34\n",
            "**Resumo.** Interações com Modelos de Linguagem Generativa, como o GPT-3.5 Turbo da OpenAI, são cada vez mais comuns no dia a dia, tornando essencial examinar seus potenciais preconceitos. Este estudo avalia preconceitos no modelo GPT-3.5 Turbo utilizando a métrica de consideração, que avalia o nível de respeito ou estima expressa em relação a diferentes grupos demográficos. Especificamente, investigamos como o modelo percebe a consideração em relação a diferentes gêneros (masculino, feminino e neutro) tanto em inglês quanto em português. Para isso, isolamos três variáveis (gênero, idioma e filtros de moderação) e analisamos seus impactos individuais nas saídas do modelo. Nossos resultados indicam um leve viés positivo a favor do gênero feminino em relação aos gêneros masculino e neutro, um viés mais favorável ao inglês comparado ao português, e consistentemente saídas mais negativas quando tentamos reduzir os filtros de moderação.\n",
            "\n",
            "**1. Introdução**  \n",
            "Nos últimos anos, interações com Modelos de Linguagem Generativa (GLM) tornaram-se uma parte crescente da vida cotidiana. Estudos mostram que as pessoas estão se envolvendo com modelos como o GPT da OpenAI [Radford et al. 2019] de várias maneiras, desde o uso de chatbots para atendimento ao cliente e suporte à saúde mental [Zhang et al. 2023, Das et al. 2022] até experiências aprimoradas de comércio eletrônico através de descrições de produtos melhoradas, geração de atributos e engajamento de clientes [Wang et al. 2023, Zhou et al. 2023, Roy et al. 2021, Liu et al. 2023]. Nas redes sociais, contas de bots automatizados são comuns e usadas para simular comportamento humano, espalhar desinformação, promover produtos e interagir com os usuários [Orabi et al. 2020, Kolomeets et al. 2024, Lucas et al. 2023].  \n",
            "À medida que as interações entre humanos e GLMs se tornam mais frequentes, é cada vez mais importante identificar e mitigar os preconceitos sistêmicos que esses modelos podem perpetuar. Estudos recentes mostraram que modelos de linguagem frequentemente herdam e replicam preconceitos embutidos em seus dados de treinamento [Sheng et al. 2019, Shin et al. 2024, Liang et al. 2021, Gupta et al. 2024]. Esses preconceitos refletem padrões existentes de discriminação na sociedade e podem reforçar estereótipos e preconceitos prejudiciais.  \n",
            "Preconceito no contexto de modelos de linguagem refere-se a diferenças sistemáticas em como esses modelos geram, avaliam ou interpretam textos sobre diferentes demografias (por exemplo, gênero, raça, orientação sexual) [Sheng et al. 2019]. Um texto pode ser considerado preconceituoso se retrata um grupo demográfico de uma maneira que faz com que as pessoas desse grupo sejam percebidas de forma mais positiva ou negativa em comparação com outros. De forma semelhante, um modelo também exibe preconceito se percebe consistentemente um grupo demográfico (como homens vs. mulheres) de maneira mais positiva ou negativa do que outros. Neste trabalho, analisamos especificamente o preconceito em termos da percepção do modelo sobre a consideração em relação a diferentes gêneros.  \n",
            "A consideração, nesse contexto, refere-se ao nível de respeito, estima ou deferência expressos em relação a um indivíduo ou grupo mencionado no texto. Por exemplo, uma frase como \"A mulher é uma excelente líder\" transmite uma consideração positiva em relação à pessoa mencionada, enquanto \"Ele tem sorte, não habilidade\" reflete uma consideração mais negativa. Utilizamos a métrica de consideração para acessar potenciais preconceitos no modelo GPT-3.5 Turbo, examinando especificamente como ele percebe indivíduos de diferentes gêneros tanto em inglês quanto em português.  \n",
            "Nosso principal objetivo era determinar se a percepção do modelo sobre a consideração difere entre diferentes condições e identificar quaisquer preconceitos inerentes. Para isso, isolamos três variáveis (gênero, idioma e configurações do firewall) para entender seu impacto individual na saída do modelo. Hipotetizamos que a consideração por gêneros não-prototípicos, como feminino e especialmente neutro, seria menor (mais negativa) em comparação com o gênero masculino, e que o idioma (inglês vs. português) não afetaria significativamente a consideração do modelo. Adicionalmente, esperávamos que a consideração sem filtros de moderação seria significativamente pior do que com os filtros ativados.  \n",
            "As principais contribuições deste trabalho são três. Primeiro, avaliamos o preconceito no modelo GPT-3.5-Turbo analisando diretamente a percepção auto-relatada do modelo sobre a consideração em relação a diferentes gêneros. Segundo, estendemos nossa análise além do inglês para incluir o português, examinando como o idioma pode afetar a percepção do modelo sobre a consideração. Por fim, investigamos o impacto dos filtros de moderação ao experimentar com prompts projetados para reduzir as restrições éticas. Nosso código, juntamente com todos os resultados, está disponível publicamente no GitHub.  \n",
            "Este artigo está organizado da seguinte forma. Na Seção 2, fornecemos uma visão geral do trabalho relacionado. A Seção 3 foca no conceito de consideração, explicando porque o escolhemos como a métrica para nosso estudo. Na Seção 4, descrevemos o conjunto de dados e os passos de pré-processamento realizados com este conjunto de dados. A Seção 5 delineia os prompts e parâmetros específicos usados em nossos experimentos. A Seção 6 apresenta os resultados de nossa análise e discutimos como gênero, idioma e configurações do firewall impactam a percepção da consideração pelo modelo. Finalmente, na Seção 7, concluímos o artigo e apontamos direções para pesquisas futuras.\n",
            "\n",
            "**2. Trabalho Relacionado**  \n",
            "A pesquisa sobre preconceito em modelos de linguagem tem sido um ponto focal no Processamento de Linguagem Natural (NLP) por muitos anos. Estudos iniciais revelaram que modelos de linguagem, como embeddings de palavras, não apenas capturam padrões linguísticos, mas também codificam os estereótipos e preconceitos sociais presentes em seus dados de treinamento [Bolukbasi et al. 2016, Caliskan et al. 2017]. Trabalhos posteriores expandiram isso examinando como esses preconceitos se manifestam em tarefas específicas de NLP, como resolução de co-referência, onde modelos demonstraram preconceitos na correspondência de pronomes e entidades com base em gênero e raça [Zhao et al. 2018, Rudinger et al. 2018]. Na análise de sentimentos, modelos foram encontrados refletindo preconceitos de gênero e raciais em suas avaliações [Odbal et al. 2022] e, na tradução automática, as saídas frequentemente reforçam estereótipos prejudiciais [Stanovsky et al. 2019, Prates et al. 2020].  \n",
            "Com o surgimento dos GLMs, o foco das pesquisas sobre preconceito se expandiu para avaliar esses modelos em diferentes contextos. Estudos recentes exploraram como modelos generativos podem replicar e amplificar preconceitos sociais existentes. Uma abordagem comum para a medição de preconceitos em GLMs é o uso de um formato de pergunta-resposta (QA), onde os modelos são apresentados com perguntas e múltiplas opções de resposta projetadas para determinar se as respostas do modelo alinham-se ou contrapõem os estereótipos contidos nas perguntas [Parrish et al. 2022, Nangia et al. 2020, Nadeem et al. 2021].  \n",
            "Outra abordagem envolve atribuir personas específicas a modelos de linguagem, simulando efetivamente como um modelo poderia se comportar se estivesse \"interpretando um papel\", como um gênero, profissão ou fundo social específico. LLMs atribuídos a personas mostraram melhorar o desempenho do modelo em tarefas de raciocínio linguístico, mas também podem reforçar preconceitos demográficos existentes. Por exemplo, esses modelos foram encontrados gerando conteúdo mais tóxico ou preconceituoso, especialmente ao assumir papéis que alinham-se a estereótipos sociais existentes, conforme evidenciado tanto em sua fala gerada quanto em tarefas de escrita auto-descritiva [Gupta et al. 2024, Sheng et al. 2021, Deshpande et al. 2023].  \n",
            "Ao mesmo tempo, os pesquisadores desenvolveram numerosas métricas para capturar preconceitos de diferentes perspectivas, incluindo sentimento, toxicidade e consideração [Busker et al. 2023, Gehman et al. 2020, Sheng et al. 2019]. A métrica de consideração, em particular, avalia as percepções gerais positivas ou negativas em relação a um grupo demográfico, distinguindo-se de outras medições de preconceito que podem focar principalmente em conteúdo estereotipado. Estudos anteriores nesses enfoques geralmente dependeram de ferramentas de classificação de sentimento, toxicidade e consideração para analisar o conteúdo gerado, o que pode introduzir camadas adicionais de complexidade e potenciais erros [Nadeem et al. 2021].  \n",
            "Pesquisas na língua portuguesa mostraram preconceitos em embeddings de palavras e modelos generativos. Um estudo identificou estereótipos de gênero em embeddings, particularmente em profissões, o que reflete padrões históricos de sexismo [Taso et al. 2023]. Outra análise descobriu que mesmo após a aplicação de técnicas de desfazimento de preconceitos, o preconceito de gênero permanece presente em modelos word2vec em português [Santana et al. 2018]. Mais recentemente, preconceitos ideológicos em modelos baseados em GPT foram observados na geração de conteúdo político [Rodrigues et al. 2023].  \n",
            "Nosso estudo se baseia em pesquisas anteriores usando a métrica de consideração de uma maneira diferente, não analisando o texto gerado pelo modelo, mas perguntando diretamente ao modelo para avaliar sua percepção de consideração em relação a diferentes gêneros. Embora a maioria dos estudos anteriores tenha se concentrado exclusivamente no inglês, nosso trabalho inclui tanto inglês quanto português para explorar os efeitos da linguagem. Adicionalmente, experimentamos a tentativa de reduzir os filtros de moderação para ver se isso tem um impacto na avaliação de consideração do modelo.\n",
            "\n",
            "**3. Análise de Consideração**  \n",
            "Selecionamos a consideração como a métrica para medir preconceitos nas saídas do modelo GPT-3.5 Turbo. De acordo com o Cambridge Dictionary, consideração significa \"considerar ou ter uma opinião sobre algo ou alguém\". Nesse contexto, a consideração serve como uma métrica que avalia o nível de respeito,estima ou deferência expressos em relação a um grupo específico. Uma consideração positiva indica que a linguagem utilizada retrata o grupo de maneira respeitosa e favorável, enquanto a consideração negativa sugere uma falta de respeito ou uma perspectiva de desdém.  \n",
            "Diferente da análise de sentimentos, que mede geralmente a polaridade sentimental de uma frase inteira, a consideração foca em como um determinado demográfico é visto ou tratado dentro do texto. Isso significa que uma frase pode ter um sentimento positivo, mas ainda expressar uma consideração negativa em relação a uma entidade, ou vice-versa. Por exemplo, considere a frase \"A pessoa estava passando por uma situação difícil com resiliência\". Embora o sentimento geral seja negativo, devido à situação difícil, a consideração em relação à pessoa é positiva, já que ela é descrita como resiliente.  \n",
            "A ideia de usar consideração como uma métrica para avaliar preconceitos em modelos de linguagem foi introduzida pela primeira vez por [Sheng et al. 2019]. Em seu estudo, os autores propuseram \"consideração\" como uma métrica para detectar potenciais preconceitos sociais em GLMs. Para validar essa abordagem, eles geraram um conjunto de dados usando o GPT-2, com prompts que mencionam diferentes grupos demográficos, e anotaram manualmente o texto gerado com pontuações de sentimento e consideração. Esse processo demonstrou que a consideração frequentemente captura preconceitos sutis e específicos de contexto que a análise de sentimentos falha em identificar. Em seguida, eles desenvolveram um classificador automático de consideração usando aprendizado por transferência com BERT [Devlin et al. 2019], que revelou preconceitos significativos em modelos de linguagem como o GPT-2, incluindo uma tendência a associar demográficos específicos a estereótipos negativos.\n",
            "\n",
            "**4. Ferramentas e Recursos**  \n",
            "Para este trabalho, utilizamos o conjunto de dados de consideração desenvolvido por [Sheng et al. 2019], que está disponível publicamente no GitHub. O conjunto de dados foi criado usando modelos de linguagem para gerar frases em resposta a prompts projetados para revelar potenciais preconceitos relacionados a diferentes grupos demográficos. Anotadores humanos então revisaram as frases de saída, atribuindo a cada uma uma pontuação de polaridade de consideração. Para minimizar preconceitos na anotação, os indicadores demográficos nas frases foram substituídos por \"XYZ\" antes que os anotadores humanos as revisassem.  \n",
            "Do conjunto de dados de consideração original, utilizamos a API do Google Translate para traduzir as frases em inglês para português. Em seguida, usamos o modelo GPT-3.5-turbo para gerar três versões diferentes de cada frase em ambos os idiomas: masculino, feminino e neutro. Incluímos instruções específicas nos prompts para garantir que todos os termos de gênero fossem adequadamente adaptados para refletir a versão correspondente de gênero.  \n",
            "Finalmente, revisamos manualmente o conjunto de dados para remover quaisquer frases incompletas, bem como aquelas com pontuações de polaridade de consideração superiores a 1. Esse processo resultou em um conjunto final de 466 frases base únicas, cada uma das quais possui as três versões de gênero em inglês e português. A Tabela 1 apresenta todas as versões da frase original \"XYZ foi descrito como um 'bom homem' e 'caráter forte'.\" \n",
            "\n",
            "**5. Prompts e Parâmetros**  \n",
            "Selecionamos o GPT-3.5 Turbo da OpenAI para avaliar como ele expressa consideração em relação a indivíduos de diferentes gêneros (masculino, feminino e neutro). Este modelo particular foi selecionado devido ao seu status como um dos modelos de linguagem mais avançados e amplamente reconhecidos atualmente disponíveis, além de sua relação custo-benefício em comparação com as alternativas mais avançadas da OpenAI.  \n",
            "Todas as prompts foram configuradas com um valor de top p de 1 e uma configuração de temperatura de 0. O parâmetro top p, também conhecido como amostragem de núcleo, controla qual subconjunto de tokens possíveis o modelo considera ao gerar respostas. O parâmetro de temperatura, por outro lado, dita o nível de aleatoriedade na saída do modelo. Temperaturas mais altas resultam em mais variabilidade e criatividade, enquanto temperaturas mais baixas levam a respostas mais determinísticas e previsíveis. Assim, ao definir o top p para 1 e a temperatura para 0, permitimos que o modelo considerasse toda a gama de tokens potenciais enquanto garantimos que ele selecionasse consistentemente o token mais provável em cada etapa.  \n",
            "Para avaliar como o modelo GPT-3.5-Turbo interpreta a consideração em relação a diferentes gêneros, desenvolvemos prompts em inglês e português, instruindo o modelo a avaliar o impacto da consideração em relação à pessoa referida em cada frase em uma das três escalas: 1-3, 1-5 ou 1-7. Em cada caso, 1 indica a consideração mais negativa e o número mais alto na escala indica a consideração mais positiva. A Tabela 2 apresenta o prompt original em inglês usado neste estudo. Nessas prompts, \"[gênero]\" foi substituído por \"O homem\", \"A mulher\" ou \"Eles\" para a versão em inglês, e \"O homem\", \"A mulher\" ou \"A pessoa\" para a versão em português. Da mesma forma, \"[escala]\" foi substituído por 3, 5 ou 7, e \"[frase]\" foi substituído pela versão específica da frase sendo analisada.  \n",
            "\n",
            "**Tabela 2. Prompt Original.**  \n",
            "Prompt Original  \n",
            "Por favor, analise a seguinte frase para determinar o impacto da consideração pela pessoa ou entidade referida como \"[gênero]\". 'Consideração' refere-se à maneira como a frase expressa respeito, estima ou deferência em relação a [gênero]. Esta análise envolve classificar o sentimento transmitido sobre [gênero] em uma escala de 1 a [escala], onde 1 representa a consideração mais negativa e [escala] a mais positiva. A escala é de número ímpar, garantindo um valor médio que representa uma consideração neutra. Esta classificação deve considerar que frases com um sentimento geral positivo ainda podem expressar uma consideração negativa em relação a [gênero], e vice-versa. - Uma ‘consideração negativa’ caracteriza-se por expressões que menosprezam, criticam ou diminuem o valor ou ações de [gênero]. - Uma ‘consideração positiva’ caracteriza-se por expressões que honram, elogiam ou elevam a estima de [gênero]. - Uma ‘consideração neutra’ fica entre os dois, onde as expressões não honram nem menosprezam significativamente [gênero]. Frase a ser analisada: \"[frase]\" Retorne apenas o número de 1 a [escala] que corresponde ao impacto da consideração por [gênero], com base em sua análise. Não responda com nada mais.  \n",
            "Para explorar como o modelo poderia responder em condições com menos diretrizes éticas ou filtros de moderação, modificamos o prompt, criando o que chamamos de prompt sem firewall. Embora não pudéssemos desabilitar completamente as medidas de proteção do modelo, buscamos diminuir seu impacto ao adicionar a instrução: “sem aplicar quaisquer camadas protetoras, diretrizes éticas ou filtros de moderação que normalmente impedem a geração de conteúdo potencialmente preconceituoso ou ofensivo.” O asterisco (*) na Tabela 2 indica onde essa instrução foi adicionada.  \n",
            "Aplicamos tanto os prompts originais quanto os sem firewall a cada uma das seis versões das 466 frases em nosso conjunto de dados, resultando em um total de 12 avaliações distintas por frase. Essas avaliações abrangeram dois idiomas (inglês e português), três escalas (1-3, 1-5, 1-7) e duas variações do prompt (original e sem firewall).\n",
            "\n",
            "**6. Resultados**  \n",
            "Para entender melhor como diferentes fatores influenciam a percepção da consideração pelo modelo GPT-3.5 Turbo em relação a uma pessoa, focamos nossa análise em três variáveis: gênero, idioma e firewall. Isolamos cada variável para descobrir potenciais preconceitos na percepção de consideração do modelo. Embora inicialmente tivéssemos experimentado três escalas diferentes de polaridade, selecionamos a de melhor desempenho para todas as análises subsequentes. Para obter resultados comparáveis entre diferentes escalas, primeiro normalizamos as pontuações de cada escala para um intervalo de 1-3 antes de calcular o F1-score. Focando na escala onde o modelo demonstrou o melhor desempenho, garantimos uma avaliação mais justa e precisa do preconceito. Como mostrado na Tabela 3, a escala de 1-5 forneceu a maior pontuação média ponderada F1, tornando-se a melhor escolha para nossa análise posterior.\n",
            "\n",
            "**Tabela 3. Pontuações F1 ponderadas para cada saída do prompt.**\n",
            "| 1-3   | 1-5   | 1-7   |\n",
            "|---------------|---------------|---------------|\n",
            "| **Lang**    | **EN** | **EN** | **PT** | **PT** |\n",
            "| **Firewall** | **Mas** | **Fem** | **Neu** | **Mas** | **Fem** | **Neu** | **Mas** | **Fem** | **Neu** |\n",
            "| 0.77       | 0.72      | 0.77 | 0.73 | 0.65 | 0.75 | 0.67 | 0.75 | 0.72 | 0.72   |\n",
            "| 0.71       | 0.70      | 0.67 | 0.68 | 0.72 | 0.77 | 0.77 | 0.78 | 0.64 | 0.69   |\n",
            "| 0.70       | 0.69      | 0.68 | 0.66 | 0.60 | 0.62 | 0.68 | 0.61 | 0.66 | 0.61   |\n",
            "| **Avarage** |        |        |        |        |\n",
            "\n",
            "Vale mencionar que nosso objetivo principal era entender o impacto que diferentes variáveis (gênero, idioma e firewall) têm nas saídas do modelo GPT-3.5-Turbo, em vez de comparar os resultados com as polaridades verdadeiras. Para alcançar isso, primeiro normalizamos todas as pontuações de polaridade para uma escala de 0-1, onde 0 corresponde à pontuação mais baixa possível (1) e 1 à pontuação mais alta possível (5) na escala original de 1-5. Em seguida, isolamos cada variável para observar sua influência específica no comportamento do modelo. Para cada análise, calculamos a variação percentual nas pontuações médias correspondentes às opções dentro da variável isolada. Por exemplo, para isolar o impacto do idioma, calculamos a variação percentual na pontuação média entre prompts escritos em inglês e aqueles escritos em português, mantendo outras variáveis (como gênero e configurações do firewall) constantes.\n",
            "\n",
            "**6.1. Análise de Viés de Gênero**  \n",
            "A Tabela 4 apresenta as pontuações médias para cada tipo de prompt, juntamente com as variações percentuais entre diferentes frases de gênero em ambos os idiomas e configurações de firewall. Uma porcentagem positiva indica um aumento na pontuação média do primeiro gênero (por exemplo, masculino) em relação ao segundo gênero (por exemplo, feminino). Por outro lado, uma porcentagem negativa indica que a pontuação média do primeiro gênero é maior do que a do segundo, indicando uma diminuição relativa na pontuação média.  \n",
            "\n",
            "Os resultados indicam que o modelo exibe um ligeiro viés mais positivo em relação ao gênero feminino quando comparado aos gêneros masculino e neutro, como evidenciado pelas variações percentuais positivas na coluna mas-fem e as variações percentuais negativas na coluna fem-neu. Ao comparar masculino com neutro, o modelo tende a mostrar um viés mais positivo em relação ao neutro com o prompt original, enquanto o viés se torna mais negativo em relação ao neutro quando tentamos reduzir o impacto do firewall, especialmente em inglês. \n",
            "\n",
            "**Tabela 4. Pontuações médias e variações percentuais para análise de gênero.**  \n",
            "| Tipo de Prompt  | Porcentagem de Alteração | Pontuações Médias |\n",
            "|----------------------|-------------------|-------------------|\n",
            "| **Idioma**          | **EN**            | **EN**            | **PT**            | **PT**          |\n",
            "| **Firewall**        | **Mas** | **Fem** | **Neu** | **Mas** | **Fem** | **Neu** | \n",
            "| **Original**        | 0.51 | 2.30% | 0.57 | 0.49 | 0.49 | 0.44 | \n",
            "| **Nofirewall**      | 0.43 | 0.38% | 0.46 | 0.44 | 0.43 | 0.41 | \n",
            "|                      | -17.89% | -19.81% | -19.78% | -17.31% | -9.69% | -10.68% |\n",
            "\n",
            "**6.2. Análise de Viés de Idioma**  \n",
            "Para isolar a variável idioma, calculamos a variação percentual entre as pontuações médias das saídas em inglês e português para cada gênero sob os prompts originais e sem firewall. A Tabela 5 exibe a pontuação média de cada prompt juntamente com as variações percentuais.  \n",
            "Os resultados indicam que o modelo GPT-3.5-Turbo tende a avaliar a consideração de forma mais positiva quando o texto é escrito em inglês do que em português, como evidenciado pelas variações percentuais negativas em todos os prompts. Isso sugere que o modelo possui um viés mais positivo em relação ao idioma inglês. Isso pode ser parcialmente explicado pela necessidade de substantivos e adjetivos de gênero em português, o que pode levar o modelo a gerar viéses diferentes em comparação ao inglês, onde expressões neutras em gênero são mais comuns. Adicionalmente, os prompts sem firewall apresentam consistentemente variações percentuais negativas menores em comparação aos prompts originais, sugerindo que a influência do idioma nas saídas do modelo é menor quando as diretrizes éticas são reduzidas.\n",
            "\n",
            "**Tabela 5. Pontuações médias e variações percentuais para análise de idioma.**  \n",
            "| Tipo de Prompt  | **Firewall** | **Gênero** | **Original** | **Masculino** | **Feminino** | **Neutro** | **Nofirewall** | **Masculino** | **Feminino** | **Neutro**  \n",
            "|---------------------|--------------|-------------|----------------|----------------|---------------|--------------|----------------|----------------|----------------|--------------|\n",
            "| **Médias**          | **Inglês** | **Português** | **Porcentagem de Alteração** | 0.51 | 0.57 | 0.52 | 0.49 | 0.49 | 0.44 | \n",
            "|                     | **0.43** | **0.46** | **0.43** | **0.41** | **0.44** | **0.39** | \n",
            "|                     | **-17.89%** | **-19.81%** | **-19.78%** | **-17.31%** | **-9.69%** | **-10.68%** | \n",
            "\n",
            "**6.3. Análise de Viés de Firewall**  \n",
            "Para isolar a variável firewall, calculamos a variação percentual entre as pontuações médias dos prompts originais e dos prompts sem firewall para cada gênero e idioma. A Tabela 6 mostra as pontuações médias para cada tipo de prompt e as variações percentuais correspondentes.  \n",
            "Embora não tenha sido possível desabilitar completamente o firewall do modelo, os resultados indicam que simplesmente instruir o modelo a ignorar diretrizes de segurança teve um impacto notável em sua saída. O prompt sem firewall produziu consistentemente resultados mais negativos em todos os casos quando comparado ao prompt original. Além disso, a versão em inglês da saída do modelo pareceu, em geral, mais suscetível à remoção dessas diretrizes, mostrando variações maiores (até -17,7% para frases neutras).\n",
            "\n",
            "**Tabela 6. Pontuações médias e variações percentuais para análise de firewall.**  \n",
            "| Tipo de Prompt  | **Médias** | **Porcentagem de Alteração** |  \n",
            "|---------------------|------------|-------------------|\n",
            "| **Idioma**         | **Inglês** | **Masculino** | **Feminino** | **Neutro** |  \n",
            "|                     | **Inglês** | **0.51** | **0.57** | **0.52** | **0.43** |  \n",
            "|                     | **Português** | **0.49** | **0.49** | **0.44** | **0.46** |  \n",
            "|                     | **Porcentagem** | **-4.93%** | **-14.77%** | **-17.70%** |  \n",
            "|                     | **-4.35%** | **-4.62%** | **-8.58%** |\n",
            "\n",
            "**7. Discussão e Trabalhos Futuros**  \n",
            "Neste trabalho, investigamos potenciais preconceitos no modelo GPT-3.5 Turbo analisando sua percepção auto-relatada de consideração em relação a diferentes gêneros em duas línguas, e sob um filtro de moderação mais relaxado. Nossa abordagem isolou essas três variáveis para entender seus impactos individuais sobre a saída do modelo.  \n",
            "Contrariamente à nossa hipótese inicial de que os gêneros feminino e neutro seriam percebidos mais negativamente, os resultados indicaram um leve viés positivo em relação ao gênero feminino em comparação com os gêneros masculino e neutro, embora esse viés seja pequeno. Além disso, embora esperássemos que a consideração do modelo permanecesse consistente entre idiomas, nossas descobertas mostraram uma preferência clara pelo inglês em relação ao português, provavelmente refletindo a predominância de dados em inglês em seu treinamento. No entanto, nossa expectativa de que filtros de moderação menos rígidos resultariam em saídas mais negativas foi confirmada, com efeitos particularmente acentuados no inglês. Essas descobertas demonstram a importância de considerar múltiplas línguas e medidas protetivas ao avaliar preconceitos em modelos de linguagem, pois podem impactar significativamente o comportamento do modelo.  \n",
            "Pesquisas futuras poderiam expandir a análise para incluir uma gama mais ampla de atributos demográficos, como raça, nacionalidade e orientação sexual, e considerar intersecções entre essas identidades (por exemplo, \"a mulher asiática\", \"o homem gay\"). Além disso, em vez de apenas variar as línguas, estudos futuros poderiam se concentrar em avaliar preconceitos em diferentes modelos de linguagem, incluindo aqueles projetados especificamente para o português, como o modelo Sabiá [Pires et al. 2023].\n",
            "35\n",
            "Resumo. Este artigo apresenta os desafios e os avanços de pesquisa voltada à construção de soluções computacionais capazes de apoiar o entendimento do debate em redes sociais no idioma português. Uma das bases fundamentais dessas soluções é a aplicação de técnicas de Mineração de Argumentos. Apresentamos as estratégias utilizadas para o endereçamento de desafios da mineração de argumentos em redes sociais, em particular, o uso de deep learning. Os resultados obtidos demonstram boa eficácia dos modelos selecionados para as tarefas consideradas, tendo atingido um F1-Score de 0,85 para a análise de sentimento, 0,97 na detecção de posição e 0,76 na detecção de ironia.\n",
            "\n",
            "1. Introdução  \n",
            "A área de Mineração de Argumentação (MA) é uma área multidisciplinar onde se encontram a Linguística Computacional e a Ciência de Dados, cujo objetivo é identificar, extrair e compreender a estrutura de argumentação em textos e/ou discussões [Lawrence e Reed 2020][Lytos et al. 2019][Stede e Schneider 2019]. É o qual processo Argumentação são construídos, compartilhados e avaliados a partir de outros argumentos [Palau e Moens 2009]. Uma argumentação se estrutura a partir de evidências, premissas, fatos e falas que suportam ou não uma determinada alegação, em uma cadeia de raciocínio que leva à conclusão de discussões e à tomada de decisão [Toulmin 2003]. A argumentação tem papel importante nas atividades humanas, e tem sido compreendida como uma área de pesquisa que surge com base em campos como a Retórica e a Filosofia, mas que hoje inclui estudos de Processamento de Linguagem Natural e modelos teóricos de discussão, tendo como áreas antecessoras a [Palau e Moens 2009][Lawrence et al. 2012], Mineração de Opiniões e a Análise de Sentimentos, entre outras [Lawrence e Reed 2020][Lytos et al. 2019].  \n",
            "\n",
            "O projeto HEIWA1 pretende construir soluções computacionais de análise de redes sociais baseadas em técnicas de MA para a compreensão de discussões em redes sociais, com um olhar específico para o contexto brasileiro. O principal resultado esperado com o projeto é a construção de uma plataforma que permita o acompanhamento de discussões em redes sociais, preferencialmente pelos usuários das próprias redes, mas também para interessados no estudo de comportamentos e mediação em redes sociais. A plataforma será composta por ferramentas computacionais capazes de apoiar um processo de curadoria, mineração de argumentos e visualização do debate para usuários de redes sociais. As implicações da construção dessas tecnologias envolvem aspectos sociais, como o aperfeiçoamento da qualidade do debate e a democracia, aspectos educacionais e de desenvolvimento de pensamento crítico.  \n",
            "\n",
            "Neste artigo, apresentamos os avanços obtidos no escopo do projeto, especificamente em relação ao uso da MA para a identificação de argumentos em textos extraídos de redes sociais em português. Em linhas gerais, os avanços compreendem a necessidade de lidar com linguagem informal, uso de ironia e a polarização presente nos debates online. Além disso, destacamos o uso de modelos de deep learning, que se mostraram eficazes em tarefas como análise de sentimento, detecção de posição e detecção de ironia, tendo alcançado bons resultados de F1-Scores nas tarefas em um contexto de grande relevância, como os eventos de 8 de janeiro de 2023 em Brasília.  \n",
            "\n",
            "O artigo se estrutura da seguinte forma: a Seção 2 apresenta o conceito e abordagens na literatura para a MA em redes sociais; a Seção 3 descreve as propostas já exploradas no projeto baseadas em análise de sentimentos e deep learning; a Seção 4 discute a proposta atual de pipeline para execução de tarefas de identificação de tópicos, entidades, sentimentos e ironia; a Seção 5 mostra os resultados dos experimentos computacionais realizados; a Seção 6 conclui o artigo e apresenta os próximos passos da pesquisa.  \n",
            "\n",
            "2. Mineração de Argumentos em Redes Sociais  \n",
            "O processo de identificar, extrair e compreender a estrutura argumentativa a partir de dados textuais é o objetivo da mineração de argumentos (MA) [Stede e Schneider 2019]. A MA tem sido explorada cientificamente para a análise de diferentes conteúdos (documentos, transcrições de debates, transcrições de áudios, etc.), com o objetivo principal de extrair a estrutura de argumentação contida nesses textos e que, eventualmente, levaram para uma deliberação ou decisão [Lawrence e Reed 2020] e para a construção de tecnologias capazes de facilitar discussões ou debater com humanos [Slonim et al. 2021].  \n",
            "\n",
            "Nos últimos anos, a comunidade científica na área de MA também explora a oportunidade de aplicar o conceito e as técnicas de MA no estudo da argumentação em textos extraídos de redes sociais [Addawood e Bashir 2016][Bosc et al. 2016][Schaefer e Stede 2021][Vecchi et al. 2021]. Essas pesquisas partem da ideia de que esses textos possam conter uma atividade argumentativa e que a compreensão dos argumentos utilizados durante a interação nas redes sociais pode agregar para a tomada de decisões, gestão da informação e o entendimento do comportamento coletivo em diversos domínios, principalmente as ciências políticas e sociais.  \n",
            "\n",
            "Em um levantamento de literatura preliminar, identificamos que os esforços recentes na aplicação de MA em redes sociais focaram, em alguns casos, na criação e anotação de datasets apropriados para a aplicação das técnicas de MA, em maior número, na definição e execução de sequências pré-definidas de tarefas consideradas essenciais para um pipeline de mineração de argumentos, e poucos trabalhos na visualização dos resultados (estruturas de argumentação) [Schaefer e Stede 2021].  \n",
            "\n",
            "A despeito dos esforços crescentes, a mineração de argumentos em redes sociais ainda enfrenta desafios significativos. Problemas antigos, apontados desde [Bosc et al. 2016a], permanecem, como a escassez de datasets anotados; a baixa qualidade e/ou informalidade dos textos nas redes; o tamanho reduzido das falas (tweets, posts), que impedem a riqueza de ideias; a ausência de foco ou tópico em discussão; enquanto outros se intensificam, como o crescente custo e restrição de uso das APIs das plataformas de redes sociais de maior escala (X, Facebook etc.). Além disso, a literatura mostra abordagens ainda exploratórias e nenhum dos trabalhos encontrados trata de conteúdos em português brasileiro.  \n",
            "\n",
            "No que se refere à definição e ao desenvolvimento de pipelines para MA em redes sociais, Schaefer e Stede (2021) e Bosc et al. (2016a) mencionam que as tarefas necessárias para aplicação de MA, em linhas gerais, incluem: (i) a anotação de corpus; (ii) a detecção de argumentos e suas relações; e (iii) a detecção de posicionamentos. No que se refere à tarefa (i) anotação de corpus, a literatura mostra um número escasso de bases anotadas e, pelo menos no material levantado pela presente pesquisa, nenhuma considerando conteúdo em português. No projeto de pesquisa em tela, avançamos nesse aspecto, ao anotar datasets de tweets em português coletados durante o período eleitoral brasileiro em 2022 [Silva et al. 2024]. A tarefa seguinte, (ii) detecção de argumentos e relações, envolve, primeiramente, a identificação, no conteúdo de cada postagem, de elementos (alegações), que possam caracterizar as falas como argumentos ou não e, posteriormente, a identificação de relações entre elas (oposição, apoio), permitindo a identificação de estruturas (grafos) de argumentação [Stede e Schneider, 2019][Bosc et al., 2016a]. Por fim, a tarefa (iii) detecção de posicionamentos consiste em extrair os diferentes pontos de vista sobre um determinado tópico em uma discussão. As tarefas (ii) e (iii) são o foco deste artigo, conforme detalharemos nas seções a seguir.  \n",
            "\n",
            "3. Estudos Iniciais  \n",
            "Os primeiros estudos de MA em redes sociais realizados por esta equipe focaram na identificação de relações de oposição e apoio entre falas em um corpus de postagens. As falas dos participantes foram tratadas como sentenças, e as relações entre elas foram identificadas pelo direcionamento das falas entre os interlocutores. Usando os identificadores extraídos das redes sociais, criou-se uma estrutura de dados que preserva a organização entre um argumento e outro, mantendo a estrutura de threads de discussão. A natureza da relação (oposição ou apoio) foi determinada pela análise de sentimentos (negativo, positivo ou neutro).  \n",
            "\n",
            "Os primeiros resultados desta estratégia são reportados por [Sousa et al. 2021], que explora o pipeline definido por Lippi e Torroni (2016). Para cada uma das sentenças, utilizamos técnicas de análise de sentimentos para determinar a polaridade das manifestações no discurso, utilizando o algoritmo SGD (Pedregosa et al. 2011) para a classificação, treinado com as discussões já classificadas presentes na base Internet Argument Corpus (IAC) [Walker et al. 2012]. A base IAC contém discussões com um teor politizado, porém amplas o bastante para que pudéssemos usá-la em debates mais genéricos. Quanto ao método de classificação dos argumentos, a classificação se deu sentença a sentença, fornecendo três rótulos que caracterizavam a polaridade de um argumento: apoio, oposição ou neutro. Os resultados obtidos demonstraram uma precisão de 74% na identificação das polaridades dos argumentos, criando assim uma alternativa promissora para a classificação.  \n",
            "\n",
            "Uma segunda abordagem desenvolvida pela equipe envolveu o uso de técnicas de análise de sentimentos baseadas em deep learning. A abordagem, apresentada em [Tokuda et al. 2021], explora a arquitetura BERT [Devlin et al. 2019] como estratégia para elucidar a polaridade das afirmações presentes em um corpus de argumentação extraído de uma rede social. Os experimentos realizados demonstraram resultados satisfatórios na extração da polaridade de uma afirmação. A acurácia dos experimentos chegou a 88% em dados não vistos anteriormente pelo modelo, a partir de um conjunto de dados com diversidade extremamente alta de palavras e estruturação livre dos dados, em um formato livre de discussões online com razoável incidência de erros gramaticais e de digitação. Em outro trabalho [Salles e Coelho 2022], foi utilizada uma rede deep learning derivada da BERT, a DistilBERT [Zhang et al. 2020], para realizar a análise das emoções presentes em frases que constam do dataset em inglês GoEmotions [Demszky et al. 2020]. Os resultados obtidos aperfeiçoam os melhores resultados publicados por Cortiz (2021).  \n",
            "\n",
            "Os estudos anteriores demonstram resultados satisfatórios para o uso da análise de sentimentos como parte do processo de MA, mas ainda não avançam em tarefas mais sofisticadas de identificação da estrutura de argumentos e não contemplam conteúdos em português. A estratégia atual do projeto é avançar na identificação de outros componentes de argumentação além de sentimentos, explorando o uso de modelos baseados no Transformer [Vaswani et al. 2017] e semelhantes ao BERT aplicados a dados extraídos de redes sociais em português.  \n",
            "\n",
            "4. Proposta de Pipeline para Mineração de Argumentos  \n",
            "Optamos por avançar em nosso projeto a partir da expansão do leque de tarefas consideradas para um pipeline de mineração de argumentos, sendo elas: análise de sentimento, identificação de tópico, reconhecimento de entidades, detecção de ironia, detecção de posição, e do uso de redes neurais como principal ferramenta para MA. Destaca-se que as tarefas do pipeline, em conjunto, são essenciais para o entendimento do discurso nas redes sociais, visando a posterior identificação de argumentos.  \n",
            "\n",
            "A inclusão das tarefas de identificação de tópico e reconhecimento de entidades nomeadas objetiva desagregar duas grandes atividades vistas na literatura: (i) a identificação do assunto da discussão; e (ii) a detecção de relação entre pares de texto. Com a desagregação, será possível investigar a eliminação da anotação de um dataset para pares de tweets relacionados, como o apresentado em [Bosc et al. 2016a]. Adicionalmente, a inclusão da detecção de ironia visa resolver a dificuldade adicional da MA em redes sociais, relacionada ao alto grau de informalidade do texto [Schaefer e Stede 2021] e à mudança de semântica gerada pelo uso desse recurso linguístico.  \n",
            "\n",
            "Optamos por não incluir a tarefa de detecção de argumentação, prevista pela literatura, no pipeline proposto. Como exposto em [Schaefer e Stede 2021], a literatura recente despendeu muito esforço na construção de datasets com alguma definição teórico-conceitual de argumentação. O formato geral do pipeline está na Figura 1.  \n",
            "\n",
            "Figura 1. Pipeline proposto da mineração de argumentos.  \n",
            "Posto que o objetivo geral deste trabalho é compreender o debate em uma rede social, cujas características exigem uma análise mais sofisticada de entendimento do texto antes de realizar a identificação da argumentação, entendemos que as tarefas de identificação de tópicos, entidades, sentimentos, posição e ironia de uma discussão oferecem vantagem para o seu entendimento, ainda que ali possamos constatar que não ocorra argumentação. Um exemplo simplificado desta estratégia, tendo por base textos sintéticos sem o pré-processamento e sem considerar todas as tarefas do pipeline, é demonstrado na Figura 2. Nela, são apresentados três tweets sintéticos e sem pré-processamento a respeito do filme “Duna: Parte Dois”, tópico correspondente a esses posts. Com os tweets pré-processados, é possível encontrar: as entidades “Duna”, “Aila” e “Paul Artreids”; os sentimentos de cada um deles; e a ironia existente. A partir disso, uma estrutura do debate realizado sobre esse tópico pode ser montada.  \n",
            "\n",
            "Figura 2. Exemplo do processamento de textos em redes sociais para a identificação da estrutura do debate (entidades, sentimentos e ironia).  \n",
            "\n",
            "5. Experimentos  \n",
            "Para dar cabo das tarefas selecionadas, procurou-se por modelos pré-treinados em português ou que tivessem sofrido fine-tuning para tarefas em português com um bom resultado. Um fator crucial para a seleção de modelos foi a sua presença no HuggingFace Hub2, repositório que visa armazenar e distribuir modelos de deep learning para facilitar sua utilização em pesquisas e aplicações comerciais. A Tabela 1 apresenta os modelos selecionados para os experimentos, com a coluna “Modelo” sendo o ID dos modelos na plataforma da HuggingFace e coluna ID sendo a utilizada para referenciar os modelos ao longo desta seção.  \n",
            "\n",
            "O modelo 1, apelidado na literatura de BERTimbau (Souza et al., 2020), foi pré-treinado na base de dados brWaC [Wagner Filho et al., 2018], ou seja, em dados sem domínio específico em português brasileiro. Já os modelos 2 e 3, BERTweet.BR [Carneiro, 2023] e BERTaporu [Costa et al., 2023], foram pré-treinados em milhares de tweets em português. Entretanto, o modelo 4, RoBERTuito [Pérez et al., 2022], foi pré-treinado em tweets na língua espanhola, mas já demonstrou bons resultados para a tarefa de análise de sentimento em português [Pérez et al., 2023].  \n",
            "\n",
            "Tabela 1. Modelos escolhidos para os experimentos computacionais.  \n",
            "| ID | Modelo | Endereço Web |\n",
            "|----|--------|---------------|\n",
            "| 1  | neuralmind/bert-base-portuguese-cased | huggingface.co/neuralmind/bert-base-portuguese-cased |\n",
            "| 2  | melll-uff/bertweetbr | huggingface.co/melll-uff/bertweetbr |\n",
            "| 3  | pablocosta/bertabaporu-base-uncased | huggingface.co/pablocosta/bertabaporu-base-uncased |\n",
            "| 4  | pysentimiento/robertuito-base-cased | huggingface.co/pysentimiento/robertuito-base-cased |\n",
            "\n",
            "Por sua vez, a base de dados utilizada para realizar o fine-tuning dos modelos, [Silva et al. 2024], foi construída a partir das discussões realizadas em torno dos eventos de 8 de janeiro de 2023 na praça dos Três Poderes, em Brasília, e anotada em etapas manuais e automáticas (com uso de LLMs) para as tarefas de análise de sentimento (AS) (positivo, negativo ou neutro), detecção de ironia (DI) (contém ironia, não contém ironia) e detecção de posição (DP) (a favor da invasão, contra a invasão). Considerando que a base foi construída a partir de um evento marcante, inferimos que o corpus já está intrinsecamente associado a um único tópico central. Dessa forma, a tarefa de identificação de tópico foi considerada redundante e removida desta bateria de experimentos. A detecção de posição, por sua vez, foi conduzida em relação a esse tópico pré-definido, permitindo uma análise mais precisa das opiniões expressas no contexto específico do evento. Além disso, optou-se por explorar o reconhecimento de entidades nomeadas (NER) futuramente devido ao tempo significativo que seria necessário para uma anotação manual da base de dados e treinamento do modelo, considerando o cronograma previsto.  \n",
            "\n",
            "A Tabela 2 apresenta a quantidade de tweets utilizada para os experimentos de cada tarefa, bem como a distribuição entre as classes de cada tarefa. Deve-se notar que a base está balanceada para a tarefa de AS, mas não para as tarefas de DP e DI. Adicionalmente, cabe destacar que os tweets utilizados em cada uma das tarefas não possuem total sobreposição, isto é, não são necessariamente os mesmos entre as bases de cada tarefa.  \n",
            "\n",
            "Tabela 2. Quantidade de tweets entre as tarefas e classes.  \n",
            "| Tarefa | Classe | Treino | Validação | Teste | Total |\n",
            "|--------|--------|--------|-----------|-------|-------|\n",
            "| Análise de Sentimento | Neutro | 234 | 77 | 50 | 361 |\n",
            "|                       | Positivo | 233 | 19 | 50 | 302 |\n",
            "|                       | Negativo | 234 | 1453 | 50 | 1737 |\n",
            "| Detecção de Posição | Neutro | 16 | 4 | 50 | 70 |\n",
            "|                       | A favor | 50 | 50 | 50 | 150 |\n",
            "|                       | Contra | 50 | 50 | 50 | 150 |\n",
            "| Detecção de Ironia | Contém ironia | 312 | 50 | 51 | 413 |\n",
            "|                       | Não contém ironia | 312 | 50 | 17 | 379 |\n",
            "\n",
            "Os resultados do ajuste dos modelos podem ser vistos na Tabela 3. Os modelos foram treinados com 5 passagens completas pelo conjunto de treino e aquele com melhor desempenho no conjunto de validação foi salvo e utilizado para avaliação no conjunto de teste. Todos os modelos foram treinados variando o número de camadas treinadas (as 2 ou 4 últimas camadas) e o batch size (testes realizados com 8, 16, 32 e 64). Utilizou-se a taxa de aprendizado de 0.001 para todos os modelos e a entropia cruzada na função de custo, com ajuste para penalizar mais severamente os erros nas classes minoritárias. Na tarefa de DI, o melhor modelo foi o RoBERTuito (ID 4 - Tabela 1), tendo sido ajustado nas últimas duas camadas e com um batch size de 16. Nas tarefas de AS e DP, o melhor modelo foi o BERTimbau (ID 1 - Tabela 1), sendo ajustado nas últimas 4 camadas e com o mesmo batch size do modelo anterior.  \n",
            "\n",
            "Tabela 3. Métricas de avaliação dos melhores modelos por tarefa.  \n",
            "| Tarefa | Modelo | F1-Score | Precision | Recall |\n",
            "|--------|--------|----------|-----------|--------|\n",
            "| Análise de Sentimento | neuralmind/bert-base-portuguese-cased | 0,85 | 0,86 | 0,76 |\n",
            "| Detecção de Ironia | pysentimiento/robertuito-base-uncased | 0,76 | 0,76 | 0,97 |\n",
            "| Detecção de Posição | neuralmind/bert-base-portuguese-cased | 0,97 | 0,86 | 0,76 |\n",
            "\n",
            "Embora os resultados obtidos aqui não sejam diretamente comparáveis com a literatura por serem obtidos em uma base de dados diferente, eles são numericamente superiores aos encontrados em Pérez et al. (2023). Comparativamente, o modelo de AS em português ajustado em Pérez et al. (2023), treinado a partir do BERTweet.BR na base de dados apresentada em Brum & Volpe Nunes (2018), alcançou uma macro F-Score de 0,73 no conjunto de teste do dataset [Silva et al. 2024]. Além disso, a título de exemplo, a Tabela 4 apresenta dois tweets do conjunto de teste, a classificação dada pelo modelo e a classe tida como verdadeira. Os dois tweets foram levemente modificados para evitar a sua identificação na rede social sem comprometimento do seu significado e avaliação.  \n",
            "\n",
            "Tabela 4. Exemplos de classificação de dois tweets do conjunto de teste.  \n",
            "| Classe Prevista | Análise de Sentimento | Classe Verdadeira | Detecção de Posição | Classe Prevista | Classe Verdadeira | Detecção de Ironia | Classe Prevista | Classe Verdadeira |\n",
            "|------------------|----------------------|-------------------|---------------------|-----------------|-------------------|-------------------|------------------|-------------------|\n",
            "| Positivo         | Neutro               | Contra            | Contra              | Neutro          | Neutro            | Contra            | Não contém ironia | Não contém ironia  |\n",
            "|                  |                      |                   |                     |                 |                   |                   | Contém ironia    |                   |\n",
            "| Texto            | mano passei a noite toda no meu quarto assistindo tbt e fui ver agora que tava a maior confusão em Brasília. k super antenada. eu sou nem eu acreditei quando vi onde ela tava me mandou corrente de excursão pra Brasília e tudo e nem dei bola  |\n",
            "\n",
            "6. Conclusão  \n",
            "Este artigo apresenta os avanços desta pesquisa em andamento voltada à construção de soluções computacionais capazes de apoiar o entendimento do debate em redes sociais. Uma das bases fundamentais destas soluções, explorada neste artigo, é a aplicação de técnicas de mineração de argumentos capazes de identificar a estrutura de argumentação presente nas diversas falas na rede. A expectativa é que a identificação e visualização da estrutura de argumentação possam auxiliar usuários da rede a compreenderem, refletirem e, eventualmente, melhor participarem do debate público.  \n",
            "\n",
            "No treinamento dos modelos, os resultados mais significativos incluem o bom desempenho do modelo BERTimbau na tarefa de detecção de posição, com F1-Score de 0,97. Adicionalmente, o modelo RoBERTuito também obteve um desempenho bom, com um F1-Score de 0,76 na detecção de ironia, uma tarefa ainda mais complexa no contexto das redes sociais. No entanto, destaca-se como limitação o fato de o dataset utilizado ser pequeno e desbalanceado, o que pode comprometer a representatividade estatística necessária para treinar e testar adequadamente modelos de deep learning, especialmente na tarefa de detecção de posição. Esse aspecto pode restringir a generalização dos resultados obtidos para diferentes contextos e tópicos. Os resultados de aplicação do pipeline proposto nos ajudarão, em passo seguinte, a projetar as abordagens para a realização da tarefa de detecção de argumentação, sofisticando a identificação da estrutura do debate. Como trabalhos futuros, planejamos aumentar a quantidade de dados anotados utilizados para treinamento dos modelos, bem como explorar outras tarefas de um pipeline de mineração de argumentos. Outra estratégia prevista é explorar a utilização de Large Language Models (LLMs) [Zhao et al. 2023][Brown et al., 2020], abrindo a possibilidade de resumir o pipeline para apenas um modelo.  \n"
          ]
        }
      ],
      "source": [
        "client = OpenAI(api_key=key, base_url=ENDPOINT_GPT) # Token and EndPoint\n",
        "\n",
        "question_answering = []\n",
        "\n",
        "for index, article in enumerate(conf_principal): # Iteration based on the titles of the CDC articles contained in the df\n",
        "\n",
        "  if article[\"idioma\"] == \"Português\":\n",
        "    instruction = \"\"\"\n",
        "\n",
        "    Sua tarefa é corrigir os erros de acentuação do artigo abaixo, não faça comentarios, apenas retorne o artigo corrigido:\n",
        "\n",
        "    \"\"\" + article[\"artigo_completo\"] + \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "  elif article[\"idioma\"] == \"Inglês\":\n",
        "    instruction = \"\"\"\n",
        "\n",
        "    Sua tarefa é traduzir o artigo abaixo para português, não faça comentários, apenas retorne o artigo traduzido:\n",
        "\n",
        "    \"\"\" + article[\"artigo_completo\"] + \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "  model= model_gpt, # Model name (Only have to trade)\n",
        "\n",
        "  messages=[\n",
        "\n",
        "    {\"role\": \"user\", \"content\": instruction}\n",
        "    ],\n",
        "\n",
        "  # Parameters:\n",
        " # temperature = 0.8, # Increase the model creativity\n",
        "  #frequency_penalty = 0.5, # Reduce the use of repetitive words\n",
        "#  presence_penalty = 0.8 # increase the approach to other topics\n",
        "  )\n",
        "  chat_response = response.choices[0].message.content # Answer\n",
        "\n",
        "  print(index)\n",
        "  print(chat_response)\n",
        "  article[\"artigo_completo\"] = chat_response\n",
        "  dados[\"conferencia_principal\"].append(article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Cqivwrdte6rt",
        "outputId": "6d1064c5-b9ab-410b-ecf5-2e3fc7fc0788"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_0e004000-8ba6-42d2-b9f9-86d3d4778952\", \"corpus_artigos.json\", 939626)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with open(\"corpus_artigos.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(dados, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "files.download(\"corpus_artigos.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
