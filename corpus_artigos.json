{
    "Conferência Principal": [
        {
            "titulo": "Leveraging Structured Data Input for Effective Chatbot Integration in Enterprises",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31108",
            "idioma": "Inglês",
            "storage_key": "files/article_31108_30911.pdf",
            "autores": [
                {
                    "nome": "Caio Siqueira",
                    "afiliacao": "PUC-Rio",
                    "orcid": "http://orcid.org/0009-0009-2808-3278"
                },
                {
                    "nome": "Orlando Fonseca",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0000-0002-0205-2483"
                },
                {
                    "nome": "Giuliano Ferreira",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0009-0004-7824-0191"
                },
                {
                    "nome": "Omar Leiva",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0000-0001-6067-8372"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "This paper introduces an approach for integrating structured data into chatbot applications. Utilizing our Mindmap tool, which hierarchically organizes data and maps nodes to actions, we developed an augmented JSONschema to improve chatbot contextual understanding and response accuracy. Byapplying the Langchain suite and Retrieval-Augmented Generation techniques,our method enhances data retrieval and processing from a vector store, signifi-cantly improving interaction relevance.",
            "keywords": [
                "Chatbot Integration",
                "Structured Data",
                "RAG",
                "Langchain"
            ],
            "referencias": [
                "LangChain Documentation. Langchain: Building applications with llms through composability. n.d.",
                "Lewis, P., Oguz, B., Rinott, R., Riedel, S., and Stenetorp, P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.",
                "OpenAI (2023). Gpt-4 technical report. Technical report, OpenAI.",
                "Vercel, Inc. (2024). React Foundations: About React and Next.js. Next.js Documentation.",
                "Weiying, K., Pham, D. N., Eftekharypour, Y., and Pheng, A. J. (2019). Benchmarking nlp toolkits for enterprise application. In PRICAI 2019: Trends in Artificial Intelligence: 16th Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yanuca Island, Fiji, August 26-30, 2019, Proceedings, Part III 16, pages 289–294. Springer.",
                "Wen, Y., Wang, Z., and Sun, J. (2023). Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. arXiv preprint arXiv:2308.09729."
            ],
            "artigo_completo": "Abstract. This paper introduces an approach for integrating structured data into chatbot applications. Utilizing our Mindmap tool, which hierarchically organizes data and maps nodes to actions, we developed an augmented JSON schema to improve chatbot contextual understanding and response accuracy. By applying the Langchain suite and Retrieval-Augmented Generation techniques, our method enhances data retrieval and processing from a vector store, signifi- cantly improving interaction relevance. Keywords: Chatbot Integration; Structured Data; RAG; Langchain  1. Introduction  In recent years, the evolution of consumer-facing software has led to heightened expec- tations among corporate users for more intuitive and natural interactions with computer systems. Traditionally, user interfaces in corporate environments have relied on window- based interaction paradigms. However, the advent of sophisticated natural language processing (NLP) technologies has begun to shift this paradigm, making natural lan- guage interfaces increasingly desirable for enterprise applications [Weiying et al. 2019]. A significant milestone in this transition was the public release of ChatGPT by OpenAI [OpenAI 2023]. The underlying technology behind ChatGPT is based on a Large Lan- guage Model (LLM), a type of machine learning model trained on extensive datasets con- taining diverse textual content. The scale of these training datasets allows LLMs to effec- tively respond to a wide range of user inputs. However, deploying LLMs in corporate en- vironments presents unique challenges. A key limitation is that these models are typically not trained on proprietary data from private institutions, which must remain confidential to protect organizational integrity and data privacy. To address this issue, researchers have explored the use of knowledge graphs to further enhance LLMs [Wen et al. 2023], and techniques such as Retrieval-Augmented Generation (RAG) have been developed to provide LLMs with context derived from private datasets, thereby enhancing their rele- vance and accuracy in enterprise settings [Lewis et al. 2020]. Despite the effectiveness of these techniques, they often fall short of meeting the demands of corporate software development, where frequent updates and rapid responses to organizational changes are common. These approaches also face challenges such as difficulty in incorporating new knowledge and explaining their reasoning processes [Wen et al. 2023].  This study proposes a mechanism for structuring data more effectively to feed LLM-based chatbots with the contextual information necessary to provide accurate and   contextually relevant responses within corporate environments. Given the stringent re- quirements of corporate settings, where control over generated outputs is paramount, the approach outlined in this work prioritizes supervised interaction models over autonomous agent-based systems. Unlike general public applications, where disclaimers can mitigate the risks of inaccurate or flawed outputs, corporate environments require a higher level of oversight to prevent potential adverse outcomes.  2. Conception  With the growing demand from our key clients, particularly those forming the consumer base of our business model, for a more integrated and sophisticated LLM chatbot inter- face, we developed an approach that leverages existing structured chatbot systems and the data already mapped within these systems.  The structured chatbot system organized data into a hierarchical, tree-like struc- ture, where each parent node represented a topic, and the child nodes indicated possible responses or subtopics associated with that topic. This setup ensured a well-defined and navigable dialogue structure. With the goal of creating an LLM-based chatbot integrated with RAG — which employs a vector store for semantic retrieval — we developed a tool capable of exporting this structured data for integration into the new system.  However, we identified significant limitations in our current software, which only supported data generation in RTF (Rich Text Format), a format unsuitable for our needs. To overcome this challenge, we developed a new tool to manage the registra- tion of nodes and their associated child nodes. This led to the creation of a web-based interface (Mindmap), built using advanced JavaScript frameworks (React with Next.js [Vercel, Inc. 2024]).  The introduction of this tool significantly expanded the scope of the project, al- lowing us to move beyond simple topic and child text nodes, enabling the creation of more complex data structures tailored to specific objectives. One of the critical features devel- oped was the mapping of nodes to actions, which dictate the operations to be executed by the chatbot. The text registered within each node is subsequently parsed as parameters for the corresponding action.  With the development of the Mindmap tool, we now have a robust platform that al- lows for the systematic registration of necessary data and facilitates its export into formats more suitable for subsequent processing.  (a) Screenshot of the Mindmap tool  (b) Snippet of the augmented json  Figure 1. Generated augmented json   3. Proposed pipeline  The Mindmap tool exports hierarchical data as a structured JSON file, which is pro- cessed into an augmented JSON using Python scripts within the Langchain suite [LangChain Documentation ], currently associated with the GPT-4 model from OpenAI. This augmented file incorporates questions and answers, derived from the hierarchy of each node, its actions, and related metadata.  These questions and answers are generated based on the node’s actions and prede- fined profiles. The resulting JSON is then used to populate a vector store database, serving as the foundation for the chatbot’s RAG functionalities. The design is agnostic concern- ing the chatbot engine’s method of consuming this data to construct its vector store or run the inference itself. For example, a collaborating research group utilizes the vector store without incorporating questions and answers, using them later to validate the generated dataset. In contrast, internally, we employ the questions and answers to create documents directly in the vector store. A comparative study on the efficacy of these strategies for various use cases could be the focus of future research.  Table 1. Sample of node actions  Action description  Use a vector store for document  Action name Fetch data on a website The node contains a URL and a CSS selector to fetch data. Additionally, it contains a JUDGE text to check if the data is similar to what is expected. Given a document link, build a vector store specific to that document. Once an initial query matches that the answer should come from the document, a second LLM query is used on that specific vector store. Translate the user input into an API call. Translate the return into readable output to serve for the user. The node has instructions on how this should be done. Use the node text as context to provide an answer to the user input.  Serve node text  Fetch an API  4. Data structure  The output augmented JSON schema encapsulates several key elements, including the list of actions within the exported object, the profiles used for generating questions and answers, and the hierarchical structure of nodes and their children. Each node is assigned a unique identifier and an update timestamp, generated by our Mindmap application, to facilitate efficient updates in downstream consumer applications.  To enhance the functionality of each node, we introduced a supplementary struc- ture termed ”helper.” Each action within a node is associated with a helper, which con- sists of parameters parsed during the generation of the augmented JSON. Internally, our team loads these helpers into the vector database, enabling their retrieval at runtime via Langchain and Python.  Looking forward, a significant improvement on our roadmap involves integrat- ing the Mindmap frontend tool with the Python backend responsible for generating the   augmented JSON structure. We also plan to migrate the generated content, including questions and answers, into our relational database alongside the nodes. This integration will streamline the workflow, allowing for direct export of the augmented JSON from the Mindmap application, thereby enhancing the user experience and operational efficiency.  Table 2. Sample profiles used when generating questions and answers  Question Profile Computer science stu- dent  Language student  Internet user  Institutional chatbot  Profile description You are a computer science student who focuses your in- put using direct messages You are a language student with rich vocabulary You are an unknown inter- net user with poor gram- mar which basically uses key- words when interacting with systems You are an organization chat- bot, which needs to answer in a formal way never betraying the ideals of the organization  Used in Questions  Questions  Questions  Answers  5. Conclusion  The work we present proposes an innovative and cohesive approach to integrating the entire workflow in creating a chatbot application that is closely coupled with its under- lying data. By leveraging structured data input and the advanced functionalities of our Mindmap tool, we have established a solid foundation that not only supports the contin- uous generation of enhanced JSON structures but also facilitates real-time data retrieval and processing through advanced technologies such as Langchain and Python.  The management of the Mindmap software is designed to be in the hands of the process owners, ensuring that those who understand the intricacies of each process are directly involved in its configuration and oversight. For example, one of our chatbot instances is currently managed by our Process Management Office (EP) 1.  Our collaboration with the partner research group, the Applied Computational Intelligence Laboratory (ICA) 2, has proven invaluable, providing critical insights and valuable feedback for the continuous improvement of our tool.  Looking ahead, we are confident that the continued development and refinement of this tool will further enhance our ability to integrate complex data structures with chat- bot applications, ultimately contributing to more intelligent and responsive systems across the organization. This work not only demonstrates the potential of structured data integra- tion but also lays the groundwork for future innovations in the field of enterprise chatbot solutions.  1Escrit´orio de Processos - https://ep.dsi.puc-rio.br 2Laborat´orio de Inteligˆencia Computacional Aplicada - https://ica.ele.puc-rio.br   "
        },
        {
            "titulo": "Avaliação de modelos para detecção de ataques de replay usando diferentes bases de dados",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31109",
            "idioma": "Português",
            "storage_key": "files/article_31109_30912.pdf",
            "autores": [
                {
                    "nome": "Giovana Y. Nakashima",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Higor D. C. Santos",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Jone W. M. Soares",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Mário Uliani Neto",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Fernando O. Runstein",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Ricardo P. V. Violato",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Marcus Lima",
                    "afiliacao": "PUC-Campinas",
                    "orcid": "https://orcid.org/0009-0008-7254-285X"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Ataque de replay é uma falsificação de fala utilizada na tentativa de autenticação de locutor. Redes neurais profundas têm sido propostas como métodos para detecção de áudios fraudulentos. Tendo em vista a utilização desses modelos em aplicações reais, além de bom desempenho na aprendizagem, espera-se que o modelo obtido apresente bons resultados com bases de dados distintas da utilizada no treinamento. Neste trabalho, duas abordagens foram avaliadas com três bases de dados públicas, com resultados que indicam baixa capacidade de generalização dos modelos.",
            "keywords": [
                "biometria de voz",
                "ataques de replay",
                "anti-spoofing"
            ],
            "referencias": [
                "Chettri, B., Mishra, S., Sturm, B. L., and Benetos, E. (2018). A study on convolutional neural network based end-to-end replay anti-spoofing.",
                "Lee, S.-K., Tsao, Y., and Wang, H.-M. (2022). Detecting replay attacks using single-channel audio: The temporal autocorrelation of speech. In Proceedings of 2022 APSIPA Annual Summit and Conference. 2022 APSIPA Annual Summit and Conference."
            ],
            "artigo_completo": "Resumo. Ataque de replay ´e uma falsificac¸ ˜ao de fala utilizada na tentativa de autenticac¸ ˜ao de locutor. Redes neurais profundas tˆem sido propostas como m´etodos para detecc¸ ˜ao de ´audios fraudulentos. Tendo em vista a utilizac¸ ˜ao desses modelos em aplicac¸ ˜oes reais, al´em de bom desempenho na aprendiza- gem, espera-se que o modelo obtido apresente bons resultados com bases de dados distintas da utilizada no treinamento. Neste trabalho, duas abordagens foram avaliadas com trˆes bases de dados p´ublicas, com resultados que indicam baixa capacidade de generalizac¸ ˜ao dos modelos.  1. Introduc¸ ˜ao  Sistemas de biometria de voz est˜ao sendo amplamente utilizados nos mais diversos seto- res, como ind´ustria automotiva, financeiro, sa´ude e educac¸ ˜ao [Khan et al. 2023]. Nesses casos, a autenticac¸ ˜ao do usu´ario ´e realizada por sistemas de verificac¸ ˜ao autom´atica de locutor (Automated Speaker Verification – ASV), suscet´ıveis a ataques de falsificac¸ ˜ao (spoofing).  Ataque de replay consiste na apresentac¸ ˜ao a um ASV da reproduc¸ ˜ao de um ´audio previamente gravado, com o objetivo de validar a fala do locutor como genu´ına. Esse tipo de falsificac¸ ˜ao ocorre de forma passiva e ´e dif´ıcil de ser detectado, uma vez que o sinal reproduzido apresenta semelhanc¸as f´ısicas (frequˆencias, espectros, formas de on- das) ao original [Khan et al. 2023]. Para aumentar a confiabilidade, os sistemas ASV s˜ao combinados com sistemas que identificam a fala falsificada, tamb´em chamados de antispoofing [Alzantot et al. 2019].  A s´erie de competic¸ ˜oes Automatic Speaker Verification Spoofing and Counterme- asures Challenge (ASVspoof) promove, desde 2015, o desenvolvimento de m´etodos para   detecc¸ ˜ao de falsificac¸ ˜ao. A cada edic¸ ˜ao, uma base de dados ´e disponibilizada para ser uti- lizada no treinamento e na validac¸ ˜ao de contramedidas aos ataques [Alzantot et al. 2019, Lee et al. 2022, Nautsch et al. 2021].  De forma geral, os m´etodos iniciam com a extrac¸ ˜ao de atributos (features), predo- minantemente baseados na an´alise espectral do ´audio, como espectrogramas, cepstrogra- mas, coeficientes em escala mel e variac¸ ˜oes da an´alise de Fourier [Khan et al. 2023]. O processo de aprendizado ocorre utilizando-se esses atributos como entradas para um clas- sificador, tipicamente uma rede neural. Frequentemente tem sido utilizadas redes con- volucionais [Chettri et al. 2018, Korshunov et al. 2018] (Convolutional Neural Network - CNN) e suas variac¸ ˜oes, como a rede convolucional leve (Light Convolutional Neural Network - LCNN) [Lavrentyeva et al. 2017, Lavrentyeva et al. 2019] e a rede convolucio- nal residual (Residual Neural Network - ResNet) [Alzantot et al. 2019, Zhang et al. 2021].  Usualmente, para utilizac¸ ˜ao em uma aplicac¸ ˜ao real, ´e esperado que o modelo apre- sente uma boa capacidade de generalizac¸ ˜ao [Korshunov and Marcel 2016], isto ´e, que seu desempenho n˜ao seja muito diferente quando comparados os resultados obtidos em dados distintos dos usados no treinamento.  O objetivo deste trabalho ´e estudar o desempenho de abordagens propostas `a detecc¸ ˜ao de ataque de replay entre bases de dados distintas da utilizada no aprendizado das redes, de modo a contribuir para a discuss˜ao sobre generalizac¸ ˜ao dos modelos.  2. Metodologia  Neste trabalho, duas abordagens de classificac¸ ˜ao foram avaliadas com trˆes bases de dados p´ublicas. O desempenho dos m´etodos foi mensurado pelo EER (Equal Error Rate), ponto de operac¸ ˜ao em que a taxa de falsa aceitac¸ ˜ao (False Acceptance Rate - FAR) e a taxa de falsa rejeic¸ ˜ao (False Rejection Rate - FRR) s˜ao iguais [Jain et al. 2008].  2.1. Bases de Dados  O estudo foi realizado com trˆes bases de dados p´ublicas: ASVspoof 20191, ASVspoof 20212 e REMASC3. As trˆes bases foram gravadas em inglˆes e, portanto, a influˆencia da variac¸ ˜ao de idioma n˜ao pode ser explorada nesse caso. O treinamento de todos os modelos neste trabalho utilizou o conjunto de treinamento da base de dados ASVspoof 2019.  As bases ASVspoof 2019 e ASVspoof 2021 s˜ao compostas por arquivos de ´audio do tipo flac, com um canal e taxa de amostragem de 16kHz. Ambas possuem outros tipos de ataque al´em do ataque de replay, mas, para os experimentos deste trabalho, foram utilizados apenas os conjuntos relativos ao ataque denominado de acesso f´ısico (Physical Access - PA), pois s˜ao os dados com o ataque de replay. Esses conjuntos s˜ao formados por 218.430 e 943.110 amostras, respectivamente.  A base de dados REMASC foi concebida visando sistemas controlados por voz (Voice Controlled Systems - VCS), em que a coleta do ´audio ocorre a uma distˆancia maior do locutor. Seu conjunto abrange 54.712 amostras, armazenadas em arquivos do tipo wav, multicanais, amostrados a 16kHz e 44kHz [Gong et al. 2019]. Os ´audios foram  1https://datashare.ed.ac.uk/handle/10283/3336 2https://www.asvspoof.org/index2021.html 3github.com/ndmobilecomplab/replay-attack   padronizados em monocanais a 16kHz, aplicando a m´edia dos canais e reduc¸ ˜ao da taxa de amostragem (downsampling) quando necess´ario.  As trˆes bases de dados disponibilizam arquivos de protocolo, indicando quais da- dos devem ser usados para treinamento e teste, bem como a identificac¸ ˜ao do locutor e do ´audio e sua classificac¸ ˜ao original (genu´ıno ou falso). Al´em disso, fornecem metadados como tamanho do ambiente e caracter´ısticas dos dispositivos utilizados para coleta.  2.2. Modelos  Neste trabalho, foram avaliadas dois tipos de arquitetura de redes neurais, ResNet e LCNN. A ResNet avaliada usa como atributos de entrada a magnitude do espectro em escala logar´ıtmica (Log-magnitude STFT - Short-Time Fourier Transform). Foi utilizado um modelo pr´e-treinado disponibilizado publicamente 4.  Quanto `a rede LCNN, foi utilizada uma implementac¸ ˜ao disponibilizada publi- camente 5 e usada em um estudo que avaliou diversos atributos como entrada para a rede [Lee et al. 2022, Lee 2024]. Neste caso, como n˜ao h´a modelo pr´e-treinado dis- pon´ıvel, o treinamento foi executado utilizando os seguintes atributos: an´alise discreta arbitr´aria de Fourier (arbitrary discrete Fourier analysis - ADFA), cepstrogramas (CEPS e CEPS1724), constant Q analysis (CQA), transformada discreta de cosseno (discrete cosine transform - DCT), an´alise discreta de Fourier em escala Mel (Mel-scale discrete Fourier analysis - MDFA) e espectrogramas (Spec e Spec1724). Os espectrogramas e cepstrogramas foram extra´ıdos pela transformada r´apida de Fourier (Fast Fourier Trans- form - FFT) utilizando uma janela de Blackman com comprimento 1024 (Spec e Ceps) e 1724 (Spec1724 e Ceps1724).  Ainda, como referˆencia para comparac¸ ˜ao, foi usada a abordagem LFCC-LCNN disponibilizada como baseline para o desafio ASVspoof 2021 [Liu et al. 2023], que inclui um modelo pr´e-treinado.  3. Resultados e Discuss˜ao  Os treinamentos da abordagem RD-LCNN foram realizados ao longo de cem ´epocas e o foi escolhido o modelo obtido a partir da ´epoca com menor valor EER no conjunto de desenvolvimento da base ASVspoof 2019.  A Tabela 1 apresenta os resultados de todos os modelos avaliados nas trˆes bases de dados, ASVspoof 2019, ASVspoof 2021 e REMASC. As colunas (a), mostram como referˆencia os resultados relatados na literatura para os subconjuntos de desenvolvimento (Dev) e de avaliac¸ ˜ao (Eval) da base ASVspoof 2019 [Nautsch et al. 2021] e, nas demais colunas, os resultados obtidos nos experimentos deste trabalho.  O modelo pr´e-treinado da baseline LFCC-LCNN apresentou bom desempenho com o subconjunto eval da base de dados ASVspoof 2019, com EER = 2,43%, fato es- perado, uma vez que o treinamento ocorreu com o subconjunto train dessa mesma base de dados. ´E importante observar que a mesma superou as baselines propostas em 2019, CQCC-GMM e LFCC-GMM, que apresentaram EER = 11,04% e EER = 13,54%, res- pectivamente [Nautsch et al. 2021]. J´a com a base ASVspoof 2021, o desempenho (EER  4https://github.com/nesl/asvspoof2019 5https://github.com/shihkuanglee/RD-LCNN/tree/main   Tabela 1. Resultados do ERR (%) obtidos (ASVspoof 2019 (b), ASVspoof 2021 e REMASC) e reportados pela literatura (ASVspoof 2019 (a))  = 45,67%) foi similar ao relatado pela literatura (EER = 44,77%) [Liu et al. 2023] e, por- tanto, muito pior.  A abordagem ResNet para a base de dados ASVspoof 2019 do subconjunto eval apresentou resultado (EER = 7,07%) pr´oximo ao da literatura (EER = 3,81%) e, embora 86% maior, ainda foi melhor que os das baselines do desafio de 2019: EER = 11,04% (B01 - CQCC-GMM) e EER = 13,54% (B02 - LFCC-GMM) [Nautsch et al. 2021]. Para as bases ASVspoof 2021 e REMASC observa-se um baixo desempenho, com valores EER acima de 40%.  Os modelos treinados da abordagem RD-LCNN apresentaram resultados para o subconjunto dev do ASVspoof 2019 pr´oximos aos relatados na literatura. Para o sub- conjunto eval da base ASVspoof 2019, o atributo “DCT” expressou a maior diferenc¸a. A inferˆencia nas bases de dados ASVspoof 2021 e REMASC tamb´em resultaram em valores EER muito piores, maiores que 35%.  4. Conclus˜ao  Os resultados obtidos para a base de dados ASVspoof 2019 demonstram bom desempenho da baseline e das abordagens experimentadas, semelhantes aos relatados pela literatura. Observa-se que a baseline obteve ERR = 2,43% no ASVspoof 2019 - eval, e, embora n˜ao se tenha encontrado valor na literatura para efeito de comparac¸ ˜ao, esse resultado foi melhor que os de ambas as baselines do desafio de 2019.  Os altos valores EER obtidos com as bases de dados ASVspoof 2021 e REMASC ratificam a situac¸ ˜ao exposta por [Korshunov and Marcel 2016], apontando para uma baixa capacidade de generalizac¸ ˜ao de todas as abordagens processadas.  "
        },
        {
            "titulo": "Avaliação de arquiteturas de síntese de fala generativa com abordagens de espectrograma e fim-a-fim em cenários low-resource para clonagem de voz",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31110",
            "idioma": "Português",
            "storage_key": "files/article_31110_30913.pdf",
            "autores": [
                {
                    "nome": "Bruno C. dos S. Ribeiro",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Gustavo H. dos S. Figueiredo",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Leonardo H. da S. Correia",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Mário Uliani Neto",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Fernando O. Runstein",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Ricardo P. V. Violato",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Marcus Lima",
                    "afiliacao": "PUC-Campinas",
                    "orcid": null
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "síntese de fala",
                "clonagem de voz",
                "low-resource"
            ],
            "referencias": [
                "Casanova, E., Junior, A. C., Shulby, C., Oliveira, F. S. d., Teixeira, J. P., Ponti, M. A., and Aluísio, S. (2022). Tts-portuguese corpus: a corpus for speech synthesis in brazilian portuguese. Language Resources and Evaluation, 56(3):1043–1055.",
                "Kong, J., Kim, J., and Bae, J. (2020). Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis."
            ],
            "artigo_completo": "Resumo. O artigo compara modelos de s´ıntese de fala com arquiteturas base- adas em espectrograma e fim-a-fim, com o objetivo de determinar a capacidade de clonagem de voz em cen´ario low-resource. Foram avaliados conjuntos de treinamento de adaptac¸ ˜ao com diferentes quantidades de fala para clonagem de uma voz alvo, e o tempo necess´ario para realizar o treinamento. O modelo VITS mostrou-se mais eficiente, alcanc¸ando os melhores resultados no teste de qualidade perceptual no cen´ario low-resource com dados no idioma portuguˆes, e completou o treinamento em menos tempo, quando comparado com o Taco- tron2.  1. Introduc¸ ˜ao  A s´ıntese de fala tem sido um campo de intenso estudo e inovac¸ ˜ao ao longo dos ´ultimos anos, com avanc¸os significativos impulsionados pelos r´apidos progressos na ´area de inte- ligˆencia artificial generativa. Dentro deste contexto, diversas abordagens tˆem sido explo- radas, incluindo as arquiteturas baseadas em espectrogramas e as abordagens fim-a-fim.  As arquiteturas Tacotron [Wang et al. 2017] e Tacotron2 [Shen et al. 2018] tˆem sido amplamente estudadas e aplicadas, demonstrando a capacidade de converter texto em fala natural por meio da gerac¸ ˜ao de espectrogramas intermedi´arios, que s˜ao posteri- ormente transformados em sinais de fala atrav´es de vocoders, como as arquiteturas Wa- veNet [van den Oord et al. 2016] e HiFi-GAN [Kong et al. 2020]. Apesar dos resultados promissores, esses modelos frequentemente requerem grandes quantidades de dados e longos per´ıodos de treinamento para atingir um n´ıvel satisfat´orio de qualidade e naturali- dade na fala.  As abordagens mais recentes de s´ıntese de fala, como o modelo VITS (do inglˆes, Variational Inference Text-to-Speech) [Kim et al. 2021], prop˜oem uma estrat´egia fim-a- fim que elimina a necessidade de um est´agio intermedi´ario expl´ıcito de gerac¸ ˜ao do es- pectrograma, combinando de forma eficaz a gerac¸ ˜ao e a codificac¸ ˜ao do sinal de fala em um ´unico fluxo de trabalho. Este m´etodo tem mostrado potencial em reduzir significati- vamente a quantidade de dados necess´arios para o treinamento, bem como o tempo total para alcanc¸ar resultados de alta qualidade.  A eficiˆencia da s´ıntese de fala em cen´arios com recursos limitados (low-resource) ´e uma ´area de interesse crescente, especialmente para idiomas com menor disponibili- dade de dados anotados, como o caso do portuguˆes. Trabalhos recentes tˆem investigado a   efic´acia de diferentes modelos em condic¸ ˜oes low-resource, abordando desafios espec´ıficos como a qualidade da fala sintetizada, a adaptabilidade de modelos pr´e-treinados para no- vos falantes e a eficiˆencia computacional do processo de treinamento [Lux et al. 2022].  O objetivo deste artigo ´e comparar as arquiteturas baseadas em espectrogramas e fim-a-fim no contexto de clonagem de voz em portuguˆes, com ˆenfase no desempenho do VITS versus o Tacotron2. O objetivo ´e comparar os modelos em cen´arios low-resource e quantificar o n´umero m´ınimo de dados e tempo de treinamento necess´arios para atingir resultados de alta qualidade. Os resultados baseiam-se em m´etricas de qualidade objetiva e subjetiva, e na an´alise do tempo de treinamento. Esperamos fornecer insights pr´aticos para a escolha e implementac¸ ˜ao de modelos de s´ıntese de fala com voz personalizada em condic¸ ˜oes de dados restritos, contribuindo para a eficiˆencia e a acessibilidade da tecno- logia de s´ıntese de fala em uma ampla gama de aplicac¸ ˜oes para o idioma portuguˆes do Brasil.  2. Metodologia  O treinamento dos modelos foi realizado utilizando duas bases de fala no idioma Por- tuguˆes Brasileiro: (i) o TTS-Portuguese Corpus [Casanova et al. 2022], composto por textos de dom´ınio p´ublico provenientes tanto da Wikip´edia quanto do Chatterbot-corpus (um corpus criado originalmente para a construc¸ ˜ao de chatbots), contendo aproximada- mente 10 horas e 28 minutos de fala de um ´unico locutor masculino, gravada com taxa de amostragem de 48 kHz e 16 bits, tendo 3.632 ´audios no formato WAV linear, com um range de durac¸ ˜ao de 0,67 a 50,08 segundos (todos os clipes de ´audio com durac¸ ˜ao supe- rior a 20 segundos foram removidos do treinamento); (ii) uma base de fala propriet´aria do CPQD composta por um locutor masculino contendo 20 minutos de fala, gravada com taxa de amostragem de 48kHz, 16 bits e formato PCM linear, contendo os arquivos de ´audio e as transcric¸ ˜oes ortogr´aficas correspondentes.  O treinamento foi realizado a partir do reposit´orio do VITS1, que foi adaptado para a inclus˜ao de fonemas do idioma portuguˆes do Brasil, realizado atrav´es do uso do m´odulo Phonemizer2 em conjunto com a pipeline de preparac¸ ˜ao de dados.  O treinamento dos modelos base ocorreram ao longo de 80 horas e 2.000 ´epocas no dataset TTS-Portuguese Corpus. A partir do ´ultimo checkpoint gerado pelo modelo base, foram realizados fine-tunings trocando os dados de treinamento pela base pro- priet´aria com a voz do locutor masculino, usando conjuntos de treinamento com 20, 15, 10 e 5 minutos de fala visando avaliar a quantidade m´ınima de dados necess´arios para obter s´ıntese de boa qualidade. O objetivo do fine-tuning ´e adaptar o modelo base para as caracter´ısticas da voz alvo, ou seja, realizar a clonagem de voz. Ap´os apenas 1 hora de treinamento de fine-tuning usando 20 minutos de fala, foram observados resultados de alta qualidade tanto no VITS como no Tacotron2. A qualidade melhorou ainda mais ap´os 20 horas de treinamento. Ambos utilizaram o vocoder HiFi-GAN, sendo que no caso do Tacotron2 o vocoder foi treinado de forma independente. Para os conjuntos de treinamento menores, a sec¸ ˜ao 3 apresenta os resultados obtidos.  1https://github.com/jaywalnut310/vits/ 2https://pypi.org/project/phonemizer/3.0.1/   3. Resultados  Para avaliar a qualidade da fala sintetizada resultante foram utilizadas medidas objetivas e subjetivas. As m´etricas objetivas foram o MCD (do inglˆes, Mel-Cepstral Distortion) e o F0 RMSE (do inglˆes, Log-F0 Root Mean Square Error) [Hayashi et al. 2021]. Para a avaliac¸ ˜ao subjetiva foi utilizada a m´etrica MOS (Mean Opinion Score), em um experi- mento que contou com 15 avaliadores n˜ao especialistas.  A m´etrica MCD, calculada por meio do reposit´orio TTS Objective Metrics3, quan- tifica a distˆancia entre dois sinais de fala. Quanto menor o valor MCD, mais semelhantes s˜ao as vozes. A qualidade da voz sintetizada foi avaliada com base no conjunto de teste, com frases separadas para validac¸ ˜ao. Ao comparar a voz sintetizada resultante do modelo de fine-tuning obtido com 20 minutos, com a voz original gravada, obteve-se valores de MCD entre 1.6 e 1.78. A m´etrica MCD mostra valores pr´oximos de 0, indicando que o modelo ´e capaz de gerar fala sintetizada pr´oxima da fala gravada. Para o F0 RMSE, aplicada nas mesmas sentenc¸as, foram obtidos valores entre 0.18 e 0.34. Os resultados reforc¸am a alta qualidade da fala sintetizada.  3.1. Avaliac¸ ˜ao Subjetiva  Para a avaliac¸ ˜ao subjetiva foi utilizado o servidor webMUSHRA4. Um grupo de 15 avali- adores n˜ao especialistas ouviram um conjunto de amostras e atribuiram notas de 0 a 100 com base na naturalidade da voz, sendo 0 nada natural e 100 muito natural. Esse processo permitiu realizar uma an´alise subjetiva da qualidade do ´audio sintetizado, proporcionando uma an´alise mais fidedigna da percepc¸ ˜ao humana em relac¸ ˜ao ao desempenho dos mode- los. As avaliac¸ ˜oes mostram uma melhor qualidade do VITS em relac¸ ˜ao ao Tacotron2. A Figura 1 mostra o boxplot com os dados do teste subjetivo, utilizando ´audios sintetizados por modelos obtidos atrav´es do fine-tuning com diferentes conjuntos de treinamento da voz alvo. Na legenda, 400 representa o conjunto com 20 minutos de fala, 300 indica 15 minutos, 200 indica 10 minutos e 100 indica o conjunto com 5 minutos de fala.  Os resultados indicam que o VITS (C1) consistentemente recebeu avaliac¸ ˜oes mais altas em comparac¸ ˜ao ao Tacotron2 (C2). O desvio padr˜ao menor do VITS em comparac¸ ˜ao ao Tacotron2 em todos os conjuntos de treinamento indica que as opini˜oes dos usu´arios sobre a qualidade do ´audio gerado pelo VITS s˜ao mais consistentes e robustas.  No teste realizado com o conjunto contendo 5 minutos de fala de treinamento, o VITS teve uma m´edia de 71,49 enquanto o Tacotron2 teve 67,49. Essa diferenc¸a foi consistente em todos os conjuntos de treinamento (20, 15, 10 e 5 minutos). No entanto, a diferenc¸a aumenta com um volume maior de dados, sugerindo que o VITS n˜ao apenas produz ´audio de melhor qualidade, mas tamb´em que melhora mais conforme a quantidade de dados de treinamento aumenta.  4. Conclus˜ao  O objetivo principal deste trabalho foi comparar as arquiteturas de s´ıntese de fala ge- nerativa com abordagens de espectrograma (Tacatron2) e fim-a-fim (VITS) em cen´arios  3https://github.com/AI-Unicamp/TTS-Objective-Metrics 4https://github.com/audiolabs/webMUSHRA/   Figura 1. Boxplot da distribuic¸ ˜ao das pontuac¸ ˜oes por est´ımulo. C1 representa o VITS, e C2 representa o Tacotron2.  low-resource, com uso de at´e 5 minutos de fala no treinamento de fine-tuning, para clo- nagem de voz; ou seja, avaliar a capacidade de adaptac¸ ˜ao dos modelos base pr´e-treinados fazendo uso de dados limitados de uma nova voz personalizada.  O modelo VITS, quando treinado com 20 minutos, mostrou resultados com alta qualidade ap´os apenas 1 hora de treinamento. Por outro lado, o Tacotron2, sob as mesmas condic¸ ˜oes, apresentou maior variabilidade e menor consistˆencia na qualidade do ´audio sintetizado. Mesmo quando treinado com 5 minutos o VITS apresentou boa qualidade e baixa variˆancia. Ao comparar o tempo de treinamento, o modelo VITS mostrou-se mais eficiente, alcanc¸ando bons resultados em menos tempo e com menos dados em relac¸ ˜ao ao Tacotron2.  Os resultados indicam que o VITS n˜ao s´o oferece uma s´ıntese de fala de melhor qualidade, com maior similaridade `a voz original e menor variˆancia entre as amostras sin- tetizadas, mas tamb´em ´e mais eficiente em termos de tempo de treinamento em cen´arios low-resource.  "
        },
        {
            "titulo": "Beyond Single Models: Leveraging LLM Ensembles for Human Value Detection in Text",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31111",
            "idioma": "Inglês",
            "storage_key": "files/article_31111_30914.pdf",
            "autores": [
                {
                    "nome": "Diego Dimer Rodrigues",
                    "afiliacao": "UFRGS",
                    "orcid": "http://orcid.org/0000-0002-9544-117X"
                },
                {
                    "nome": "Mariana Recamonde-Mendoza",
                    "afiliacao": "UFRGS",
                    "orcid": "https://orcid.org/0000-0003-2800-1032"
                },
                {
                    "nome": "Viviane P. Moreira",
                    "afiliacao": "UFRGS",
                    "orcid": "https://orcid.org/0000-0003-4400-054X"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Every text may reflect its writer’s opinions, and these opinions, especially in political contexts, are often tied to specific human values that they either attain or constrain. Identifying these values can provide policymakers with deeper insights into the underlying factors that influence public discourse and decision-making. While current large language models (LLMs) have shown promise across various tasks, no single model may generalize sufficiently to excel in tasks like human value detection. In this work, we utilize data from the Human Value Detection task at CLEF 2024 and propose leveraging multiple ensembles of LLMs to enhance the identification of human values in text. Our results found that the ensemble models achieved higher F1 scores than all baseline models, suggesting that combining multiple models can offer performance comparable to very large models but at much lower memory requirements.",
            "keywords": [
                "LLM",
                "LLM Ensembles",
                "Human Value Detection",
                "Text Classification",
                "Natural Language Processing (NLP)",
                "Ensemble Learning",
                "Text Analysis",
                "Multi-Model Approaches"
            ],
            "referencias": [
                "Ammanabrolu, P., Jiang, L., Sap, M., Hajishirzi, H., and Choi, Y. (2022). Aligning to social norms and values in interactive narratives. In Carpuat, M., de Marneffe, M.-C., and Meza Ruiz, I. V., editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5994–6017, Seattle, United States. Association for Computational Linguistics.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding.",
                "He, P., Liu, X., Gao, J., and Chen, W. (2021). DEBERTA: Decodingenhanced BERT with disentangled attention. In International Conference on Learning Representations.",
                "Hoang, M., Bihorac, O. A., and Rouces, J. (2019). Aspect-based sentiment analysis using BERT. In Hartmann, M. and Plank, B., editors, Proceedings of the 22nd Nordic Conference on Computational Linguistics, pages 187–196, Turku, Finland. Linköping University Electronic Press.",
                "Jiang, D., Ren, X., and Lin, B. Y. (2023). Llm-blender: Ensembling large language models with pairwise ranking and generative fusion.",
                "Kiesel, J., Çöltekin, Ç., Heinrich, M., Fröbe, M., Alshomary, M., De Longueville, B., Erjavec, T., Handke, N., Kopp, M., Ljubešić, N., Meden, K., Mirzhakhmedova, N., Morkevičius, V., Reitis-Münstermann, T., Scharfbillig, M., Stefanovitch, N.,Wachsmuth, H., Potthast, M., and Stein, B. (2024a). Overview of touché 2024: Argumentation systems. In Goharian, N., Tonellotto, N., He, Y., Lipani, A., McDonald, G., Macdonald, C., and Ounis, I., editors, Advances in Information Retrieval, pages 466–473, Cham. Springer Nature Switzerland.",
                "Kiesel, J., Çöltekin, Ç., Heinrich, M., Fröbe, M., Alshomary, M., Longueville, B. D., Erjavec, T., Handke, N., Kopp, M., Ljubešić, N., Meden, K., Mirzakhmedova, N., Morkevičius, V., Reitis-Münstermann, T., Scharfbillig, M., Stefanovitch, N.,Wachsmuth, H., Potthast, M., and Stein, B. (2024b). Overview of Touché 2024: Argumentation Systems. In Goeuriot, L., Mulhem, P., Quénot, G., Schwab, D., Nunzio, G. M. D., Soulier, L., Galuscakova, P., Herrera, A. G. S., Faggioli, G., and Ferro, N., editors, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 15th International Conference of the CLEF Association (CLEF 2024), Lecture Notes in Computer Science, Berlin Heidelberg New York. Springer.",
                "Legkas, S., Christodoulou, C., Zidianakis, M., Koutrintzes, D., Petasis, G., and Dagioglou, M. (2024). Hierocles of alexandria at touché: Multi-task & multihead custom architecture with transformer-based models for human value detection. In Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), CEUR Workshop Proceedings,",
                ".",
                "Schwartz, S. H. (1994). Are there universal aspects in the structure and contents of human values? Journal of Social Issues, 50(4):19–45.",
                "Schwartz, S. H., Cieciuch, J., Vecchione, M., Davidov, E., Fischer, R., Beierlein, C., Ramos, A., Verkasalo, M., Lönnqvist, J.-E., Demirutku, K., Dirilen-Gumus, O., and Konty, M. (2012). Refining the theory of basic individual values. Journal of Personality and Social Psychology, 103(4):663–688.",
                "Sobhanam, H. and Prakash, J. (2023). Analysis of fine tuning the hyper parameters in RoBERTa model using genetic algorithm for text classification. International Journal of Information Technology, 15(7):3669–3677.",
                "Yeste, V., Ardanuy, M., and Rosso, P. (2024). Philo of alexandria at touché: A cascade model approach to human value detection. In Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), CEUR Workshop Proceedings,",
                "Yunis, H. (2024). Arthur schopenhauer at touché 2024: Multi-lingual text classification using ensembles of large language models. In Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), CEUR Workshop Proceedings,"
            ],
            "artigo_completo": "Abstract. Every text may reflect its writer’s opinions, and these opinions, es- pecially in political contexts, are often tied to specific human values that they either attain or constrain. Identifying these values can provide policymakers with deeper insights into the underlying factors that influence public discourse and decision-making. While current large language models (LLMs) have shown promise across various tasks, no single model may generalize sufficiently to ex- cel in tasks like human value detection. In this work, we utilize data from the Human Value Detection task at CLEF 2024 and propose leveraging multiple ensembles of LLMs to enhance the identification of human values in text. Our results found that the ensemble models achieved higher F1 scores than all base- line models, suggesting that combining multiple models can offer performance comparable to very large models but at much lower memory requirements.  1. Introduction  People can agree or disagree on numerous topics even when using the same information. These differences arise largely from their individual beliefs about what is worth striving for, a concept referred to as (human) values. Human values can conflict or align, leading to a wide range of opinions on controversial issues. This divergence is one of the reasons for the formation of different political parties, each representing the values of specific groups [Kiesel et al. 2022].  Given its significance, the study of human values spans multiple disciplines, in- cluding social sciences [Schwartz 1994] and formal argumentation [Bench-Capon 2003]. Researchers focused on various aspects, such as classifying values, detecting them in text, and understanding their societal impact. In computer science, there is a grow- ing body of work dedicated to value detection and emotion recognition from text [Dellaert et al. 1996, Tariq et al. 2019, Ammanabrolu et al. 2022]. These tasks are chal- lenging and yet have a broad spectrum of applications, such as aiding policymakers in gauging public sentiment, detecting political alignment, and more.  In this work, we aim to advance the field of human value detection by lever- aging multiple ensembles of Large Language Models (LLMs) to identify these values in text and enhance model performance. We adopt the value taxonomy presented in [Schwartz et al. 2012], which categorizes values into two types for each value—attained and constrained. However, our task focuses solely on identifying the presence of a value in a sentence, so we sum the attained and constrained versions to determine whether a sentence contains a particular value. We conduct this study with a dataset from CLEF 2024. The data is highly imbalanced, making this a challenging classification problem.   2. Background and Related Work Human Value Detection has recently gained attention, particularly as the focus of a shared task at CLEF 2024. This task aimed to detect human values in speech, attracting participation from 20 teams. The outcomes of this competition, including the performance metrics of each team, are detailed in [Kiesel et al. 2022]. These efforts underscore the complexity of detecting nuanced human values in text and highlight the need for advanced models that can accurately capture such subtleties.  LLMs have revolutionized NLP tasks across various domains. The introduction of Transformer architectures [Vaswani et al. 2017] marked a significant leap forward, lead- ing to the development of powerful pre-trained models like BERT [Devlin et al. 2019], RoBERTa [Liu et al. 2019], and DeBERTa [He et al. 2021]. These models have been highly effective in text classification, sentiment analysis, and content generation, sig- nificantly reducing the need for training models from scratch. Numerous studies [Xian et al. 2023, Hoang et al. 2019, Sun et al. 2019, Sobhanam and Prakash 2023] have demonstrated the efficacy of fine-tuning these models for specific tasks, showcasing their versatility and robustness in handling diverse NLP challenges.  Ensemble Learning is a well-established technique in machine learning, often employed to improve predictive performance by combining multiple models. Tradition- ally associated with decision trees [Quinlan 1986], ensemble learning has evolved to in- corporate various frameworks, including those involving LLMs [Jiang et al. 2023].  3. Methodology The data used in this study comes from the Human Value Detection at CLEF (Conference and Labs of the Evaluation Forum) 2024 task (ValueEval’24) [Kiesel et al. 2024a] and consists of approximately 3K human-annotated texts containing over 73K sentences. The annotation associated with each sentence indicates whether a specific human value is “attained” and “constrained”. A total of 19 human values are analyzed. Each column receives the value 0, 0.5, or 1, indicating whether the sentence does not contain the human value, partially contains it, or fully contains it, respectively. This study focused on the English dataset. All models were optimized for F1-Macro score.  To approach the task as a multi-label classification problem, we combined the “attained” and “constrained” columns in the labels file, summing their values to determine whether a specific human value is present in a sentence (0 for false, 1 for true). The result was an array of 19 boolean values for each sentence, which were then used as inputs for model fine-tuning. Thus, each human value represents a class and the predictive model may assign more than one class for a given sentence. While the value Humility was removed by many CLEF participants due to its scarcity in the training set (present in only 0.2% of sentences), we retained it, considering it important to predict even rare values to ensure comprehensive performance across all values.  Using the training dataset, we fine-tuned six models: base and large versions of BERT [Devlin et al. 2019], RoBERTa [Liu et al. 2019], and DeBERTa [He et al. 2021]. After fine-tuning, we used the validation data to create a new dataset that included the sen- tences, prediction probabilities for each class, and binary predictions indicating whether a value is present in a sentence. The true labels are also carried onto the dataset to enable evaluations. Five different ensemble approaches were used to combine model outputs:   • prob-equal: Probabilities from each model were summed and then averaged. A  threshold of 0.2 was applied.  • prob-large-double: Probabilities from base models were summed, and probabil- ities from large models were doubled before summing. The total was divided by the number of votes (nine), and a threshold of 0.2 was applied.  • preds-majority: Binary predictions from all models were summed, with a thresh- old of 2 applied to predict a value as present if at least two models identified it. • preds-large-double: Binary predictions were summed, with large models receiv- ing two votes each. A threshold of 2 was used, meaning a value would be predicted as present if one large model or two base models identified it.  • prob-weight-macro-f1: The probabilities predicted by each model were weighted by their F1 scores on the validation set. The weighted probabilities were then summed and normalized, followed by applying a threshold of 0.2.  For reproducibility, all experiments, ensemble diagrams, and scripts used for fine- tuning are available on GitHub1, with a fixed random seed for all libraries. Implementa- tion details and further results are also in our repository. The models used in this study are publicly accessible and can be downloaded from HuggingFace.  4. Results  Results are presented in Table 1. The RoBERTa Large model achieved the highest accu- racy among the individual models, which aligns with expectations given the larger model size. However, since the primary metric for model selection during training was the macro F1-score rather than accuracy, it is not surprising that larger models and ensemble models do not consistently show higher accuracy.  Table 1. F1 and Accuracy results for our models and baselines. ⋆ means the model is an ensemble, and † means it used the multilingual dataset version  Model  Macro F1 Accuracy  Base models  Ensembles  Baselines  BERT-base-uncased BERT-large RoBERTa-base RoBERTa-large DeBERTa-base DeBERTa-large  prob-equal prob-large-double prob-weight-macro-f1 preds-majority preds-large-double  [Legkas et al. 2024] † [Yunis 2024] ⋆ † [Yeste et al. 2024]  0.160 0.263 0.248 0.282 0.274 0.295  0.330 0.326 0.330 0.318 0.319  0.390 0.350 0.280  0.502 0.482 0.485 0.508 0.480 0.507  0.447 0.438 0.445 0.484 0.418  – – –  Table 1 also compares our results with the top-3 models from the CLEF 2024 submissions. Notably, our ensemble approaches, specifically prob-weight-macro-f1 and  1https://github.com/diegodimer/valueeval24   prob-equal, performed only 0.03 and 0.02 below the top-scoring models from the con- ference, which utilized XLM models and the multilingual dataset. The approach by Arthur Schopenhauer [Yunis 2024] leveraged an ensemble of DeBERTa-v2-xxlarge and xlmRoBERTa-large models. Similarly, Hierocles of Alexandria [Legkas et al. 2024] em- ployed both the multilingual and English-translated datasets, incorporating sentence se- quence information and fine-tuning an XLM-RoBERTa-xl model. Finally, team Philo of Alexandria [Yeste et al. 2024] fine-tuned a DeBERTa model specifically for this task.  Looking into the scores for each of the 19 human values, we see that our ensem- bles demonstrated competitive performance, closely matching the results of XLM models and outperforming the DeBERTa-base model across nearly all values. This task was par- ticularly challenging due to the significant class imbalance in the dataset, with nearly 50% of test set instances not containing any of the 19 values. This imbalance skews predictions towards false negatives, resulting in lower F1 scores despite high accuracy, as models may correctly predict the absence of values due to their prevalence.  Overall, the results demonstrate that ensemble models can achieve performance comparable to very large models, even when utilizing models that require less compu- tational resources. Although training an XLM-DeBERTa model was not feasible on the hardware used for this study due to memory constraints, our ensembles still achieved a strong macro F1-score. Specifically, the best ensemble model improved the macro F1- score from 0.295 (the highest among the base models) to 0.33, highlighting the effective- ness of ensemble methods in enhancing model performance in this context.  5. Conclusion  In this study, we tackled the complex task of identifying human values in text, a challenge crucial for understanding the values that shape public discourse and decision-making. By leveraging multiple ensembles of LLMs, we demonstrated that ensemble-based ap- proaches could significantly enhance individual model performance in this task. This suggests that instead of relying solely on a single, powerful LLM, ensemble methods offer a more robust and effective solution for complex NLP tasks.  Despite the advanced capabilities of models like GPT-4.0, these models still strug- gle to consistently deliver satisfactory performance in this domain. For instance, in the ValueEval’24, a team using GPT-4.0 for zero-shot classification achieved an F1-score of 0.25 [Kiesel et al. 2024b], which is lower than the performance of our ensemble ap- proaches. This highlights the inherent challenges in human value detection, where the nuances of language and context often exceed the capacity of a single model, no matter how sophisticated. Future work will include a qualitative analysis to better understand the errors made by the models and improve the proposed approaches, reinforcing the potential of ensemble learning as a key strategy in advancing the field.  Acknowledgments. This work has been partially funded by CNPq-Brazil and Capes Finance Code 001.  "
        },
        {
            "titulo": "A Change in Perspective: The Trade-Off Between Perspective API and Custom Models in Classifying Hate Speech in Portuguese",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31112",
            "idioma": "Inglês",
            "storage_key": "files/article_31112_30915.pdf",
            "autores": [
                {
                    "nome": "Arthur Buzelin",
                    "afiliacao": "UFMG",
                    "orcid": "http://orcid.org/0009-0007-0816-7189"
                },
                {
                    "nome": "Yan Aquino",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0009-0009-1647-4298"
                },
                {
                    "nome": "Pedro Bento",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0009-0007-4461-7503"
                },
                {
                    "nome": "Samira Malaquias",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0009-0006-8711-3412"
                },
                {
                    "nome": "Wagner Meira Jr",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0000-0002-2614-2723"
                },
                {
                    "nome": "Gisele L. Pappa",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0000-0002-0349-4494"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "This paper examines the performance of the Perspective API, developed by Jigsaw, in detecting hate speech in Portuguese. Although the Perspective API supports multiple languages, its performance metrics are often aggregated, obscuring specific details. Our study reveals that the API’s AUC-ROC score for Portuguese is significantly lower than for English (0.744 vs. 0.942). To address this, we developed a BERT classifier model trained on a Portuguese Twitter hate speech dataset. Our model, with just 100 messages in it’s training set, outperformed the Perspective API. These findings highlight the need for more granular performance metrics and suggest that custom models may offer better solutions for specific languages.",
            "keywords": [
                "Transformers",
                "BERT",
                "Perspective API",
                "NLP",
                "Hate Speech Detection"
            ],
            "referencias": [
                "Davidson, T., Warmsley, D., Macy, M., and Weber, I. (2017). Automated hate speech detection and the problem of offensive language. In Proceedings of the 11th International AAAI Conference on Web and Social Media (ICWSM). AAAI.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "Fortuna, P., Nunes, S., Soler-Company, J., and Wanner, L. (2019). A hierarchically-labeled portuguese hate speech dataset. In Proceedings of the Third Workshop on Abusive Language Online, pages 94–104. Association for Computational Linguistics.",
                "Kennedy, C., Bacon, G., Sahn, A., and Vacano, C. (2020). Constructing interval variables via faceted rasch measurement and multitask deep learning: a hate speech application.",
                "Kobellarz, J. K. and Silva, T. H. (2022). Should we translate? evaluating toxicity in online comments when translating from portuguese to english. In Anais do Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia), pages 95–104, Porto Alegre, Brazil. Sociedade Brasileira de Computação. In: 28th Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia), 2022, Curitiba.",
                "Lees, A., Tran, V. Q., Tay, Y., Sorensen, J., Gupta, J., Metzler, D., and Vasserman, L. (2022). A new generation of perspective api: Efficient multilingual character-level transformers.",
                "Lima, Q. L. H., Pagano, S. A., and da Silva, A. (2024). Toxic content detection in online social networks: A new dataset from brazilian reddit communities. In 16th International Conference on Computational Processing of Portuguese (PROPOR 2024).",
                "Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.",
                "Nogara, G., Pierri, F., Cresci, S., Luceri, L., Törnberg, P., and Giordano, S. (2024). Toxic bias: Perspective api misreads german as more toxic.",
                "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32.",
                "Roy, S. G., Narayan, U., Raha, T., Abid, Z., and Varma, V. (2021). Leveraging multilingual transformers for hate speech detection. ArXiv, abs/2101.03207.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained bert models for brazilian portuguese. In Proceedings of the 9th Brazilian Conference on Intelligent Systems (BRACIS), pages 403–417. IEEE."
            ],
            "artigo_completo": "Abstract. This paper examines the performance of the Perspective API, devel- oped by Jigsaw, in detecting hate speech in Portuguese. Although the Perspec- tive API supports multiple languages, its performance metrics are often aggre- gated, obscuring specific details. Our study reveals that the API’s AUC-ROC score for Portuguese is significantly lower than for English (0.744 vs. 0.942). To address this, we developed a BERT classifier model trained on a Portuguese Twitter hate speech dataset. Our model, with just 100 messages in it’s train- ing set, outperformed the Perspective API. These findings highlight the need for more granular performance metrics and suggest that custom models may offer better solutions for specific languages.  1. Introduction  Perspective API is a tool designed to identify and mitigate toxic language online [Lees et al. 2022]. Using advanced machine learning and Natural Language Processing (NLP) models, Perspective API analyzes textual content to detect various forms of harm- ful speech, including threats, insults, and hate speech. It is considered state-of-the-art for detecting toxicity, and used by multiple platforms, such as Reddit, The New York Times, The Wall Street Journal, and EL PA´IS.  Despite its widespread adoption and claimed multilingual support, including Por- tuguese, the actual performance of the Perspective API in different languages remains un- clear. The official documentation and associated research papers often report performance metrics by aggregating data from multiple languages, within a multilingual dataset. This aggregation conceal the individual performance metrics for Portuguese, making it diffi- cult to evaluate the API’s effectiveness in this specific language. The lack of transparency in language-specific performance metrics raises concerns about the API’s reliability when applied to non-English texts.  The widespread acceptance of Perspective API as a leading tool for hate speech detection combined with its claimed support for Portuguese, suggests that professionals may readily adopt it for work in Portuguese-speaking contexts. However, if the API’s performance in Portuguese is not on par with its performance in English or the aggregated results, this could lead to inaccurate analyses and conclusions, particularly in fields like   computational social sciences, where precise language detection is critical. The potential for misleading results is especially concerning when the tool’s reliability in Portuguese is taken for granted based on its performance in other languages.  This paper addresses this gap by evaluating the performance of the Perspective API in detecting hate speech specifically in Portuguese. It also assesses the feasibility of developing a custom hate speech detection tool tailored for Portuguese. To guide our investigation, we formulated the following research questions:  RQ1: How well does the Perspective API perform when detecting hate speech in  Portuguese?  RQ2: For Portuguese, is it more effective and efficient to use a custom-made tool rather than relying on existing solutions like the Perspective API? If so, how much effort would it take to build it?  To address these questions, we evaluated the Perspective API’s metrics using a Portuguese Twitter hate speech dataset. We then compared it to metrics obtained in a sim- ilar English dataset regarding classification, date of collection, and content. Our findings revealed that the Perspective API’s performance in Portuguese was significantly worse than in English. Based on this insight, we developed our version of a BERT classifier to detect hate speech in Portuguese. Remarkably, with just 100 messages, the BERT model outperformed the Perspective API in detecting hate speech in Portuguese. In contrast, the BERT model trained with the English dataset did not surpass the Perspective API’s performance.  2. Related Works  This section reviews studies related to hate speech detection models and language-specific performance comparisons. oportunities  2.1. Model Comparisons  Multilingual transformer models, such as BERT and its variants, have gained sig- nificant attention in hate speech detection across various languages. For instance, [Roy et al. 2021] demonstrated the superiority of fine-tuned transformer models in han- dling multilingual data, showcasing their effectiveness compared to more generalized ap- proaches like the Perspective API. This highlights the potential of specialized models to outperform broader, one-size-fits-all solutions.  Another noteworthy contribution is by [Kennedy et al. 2020], who introduced a hybrid approach that combines faceted Rasch measurement with multitasking deep learn- ing. This methodology enhances both the interpretability and precision of hate speech detection by integrating traditional psychometric techniques with advanced deep learning models. Compared to the Perspective API, which relies on more generalized algorithms, Kennedy et al.’s approach offers a more nuanced understanding of linguistic variations and the intensity of hate speech.  2.2. Language-Specific Comparisons  In Perspective’s introductory paper [Lees et al. 2022], developers reported AUC-ROC scores of 0.98 for English, 0.91 for Russian, and 0.87 for a group of ten other languages.   These results highlight a disparity in the API’s effectiveness across languages, raising concerns about its applicability in non-English contexts.  Further studies have confirmed these concerns were relevant. For instance, [Nogara et al. 2024] analyzed the use of the Perspective API in German and found that the API tends to classify German texts as significantly more toxic than their English coun- terparts. This finding underscores the potential biases and inaccuracies that arise when applying the API to languages other than English, highlighting the need for further inves- tigation into its multilingual capabilities.  The seminal study of the use of the Perspective API’s in Portuguese was conducted by [Kobellarz and Silva 2022]. They compared identical texts in Portuguese and English using the API and concluded that it performs better when analyzing texts in their origi- nal language. This suggests that the Perspective API may be less effective in detecting nuances in translated or non-native language content.  Building upon this study, [Lima et al. 2024] developed a manually labeled dataset of toxic messages in Portuguese and evaluated the API against this dataset. Their findings revealed significant discrepancies, emphasizing the need for the API to undergo more focused training on Portuguese-language content to improve its accuracy and reliability in detecting hate speech.  Additionally, [Silva et al. 2023] proposed standardized datasets and benchmarks for sentiment analysis in English, specifically addressing the challenges of automating the development process. While their focus was on English, the methods and standards they advocate could provide valuable insights for improving the Perspective API’s perfor- mance in other languages, including Portuguese.  2.3. Research Gap  Despite the widespread use and validation of the Perspective API for hate speech detection in various languages, a significant gap remains in its performance evaluation for less com- monly studied languages like Portuguese. Previous research has shown the API’s strong performance in English and other major languages, demonstrated by high AUC-ROC scores and robust metrics. However, detailed assessments for less-represented languages in its training datasets are lacking.  To address these gaps, we conducted focused evaluations of the Perspective API’s performance for individual languages. Our study highlights the advantages of developing custom models tailored to specific languages, such as Portuguese, offering more accurate and reliable hate speech detection. This emphasizes the need to consider custom solu- tions alongside existing multilingual models to improve the effectiveness of hate speech detection across diverse languages.  3. Methodology In this section, we discuss the dataset selection, Perspective API evaluation, and the BERThs models fine-tuning.  3.1. Dataset  Our analysis required a Portuguese hate speech dataset and a similar English dataset, Instead of manually labeling messages, for the purpose of an unbiased comparison.   which can be costly and prone to errors, we opted to use two well-known Twit- the Hierarchically-Labeled Portuguese Hate Speech Dataset ter hate speech datasets: [Fortuna et al. 2019] and the Automated Hate Speech Detection and the Problem of Of- fensive Language dataset [Davidson et al. 2017].  Both datasets were created using the same methodology for classifying messages. This involved identifying and mining accounts likely to post hate speech-related tweets in 2017. The tweets were then classified as either containing hate speech or not, which matches the output of the Perspective API.  The original Portuguese and English datasets vary significantly in size and propor- tion of hate speech messages. The Portuguese dataset includes 5,934 non-toxic messages and 1,607 toxic messages, resulting in a ratio of approximately 3.7 non-toxic messages per toxic message. On the other hand, the English dataset initially consisted of 25,000 classified tweets, with 3,280 non-toxic messages and 21,720 toxic messages.  For a fair comparison of classification scores between the two datasets, we bal- anced their proportions by using the Portuguese dataset as the baseline, since this will be the main object of our study. By selecting a random sample of messages from the English dataset that reflected the same proportion, we leveraged a final English dataset consisting of 3,280 non-toxic messages and 886 toxic messages, with the same ratio of non-toxic to toxic messages of approximately 3.7.  3.2. Comparing Perspective API results  To compare the models of Perspective for English and Portuguese, we selected random samples of messages and analyzed them for toxicity using the Perspective API. We fo- cused on the Toxicity attribute, which is widely used in literature due to its robustness and compatibility with both datasets under examination. The analysis was conducted in June 2024, and the Perspective API provided toxicity scores for each sample in both datasets.  Each message was assigned a toxicity score ranging from 0 to 1, where 0 repre- sents a very low probability of toxicity and 1 indicates a very high probability. To ensure the most precise possible comparison, we optimized the threshold for toxicity classifi- cation by maximizing the F1 score for each dataset individually. The optimal threshold was determined to be 0.48 for the Portuguese dataset and 0.59 for the English dataset, reflecting the different calibrations needed by the two languages.  3.3. BERThs (BERT hate speech) Model  This section shows how we fine-tuned our own BERT classifier for hate speech detection, namely BERThs. BERThs was fine-tuned using both a Portuguese and an Englih dataset.  Initially, the goal of the model, particularly the Portuguese one, was not to achieve the highest possible accuracy, but to be easy to replicate. This will help us show whether a simple fine-tuned model may be more effective than the Perspective API in Portuguese.  For fine-tuning the BERThs-Pt, we used BERTimbau [Souza et al. 2020] as the base model, as it is pre-trained in Portuguese and better suited for our task. Given the small size of the annotated corpus, we fine-tuned and evaluated the model 30 times using different randomized non-overlapping stratified sets: training, validation, and test sets, comprising 80%, 10%, and 10% of the labeled dataset. Each split maintained the original   class distribution of approximately 21.3% toxic messages and 78.7% non-toxic messages. The same test sets were used to evaluate the Perspective API. This approach ensured robustness and prevented issues such as training on an all-toxic set of messages, which could lead to unreliable results.  To determine the minimum number of messages needed for our classifier to out- perform the Perspective API, initially, only 10 messages from the training set were used for fine-tuning BERTimbau. We incrementally added 10 more messages to the training set after each iteration, until BERThs achieved a better AUC score than Perspective. The AUC metric was chosen because it was the only metric reported for Portuguese in the Perspective API paper. After that, we added 200 new messages to the training set in each subsequent iteration, until all messages were included, highlighting the highest perfor- mance our model could achieve.  The fine-tuning was performed using the PyTorch library [Paszke et al. 2019], with the AdamW optimizer [Loshchilov and Hutter 2017] and a learning rate of 5 × 10−6. The classification thresholds were established based on the output probabilities of the model, defined as the thresholds that yielded the best mean F1-score on our validation set.  For BERThs-En, we employed the BERT uncased model [Devlin et al. 2019], which is optimized for English language processing. The fine-tuning procedure followed the same general approach used for the Portuguese variant, but with specific modifications to account for the superior performance of the Perspective API on English texts. Specifi- cally, instead of gradually increasing the training set by 10 messages and subsequently by 200 messages per iteration, we opted to directly increase the training set by 200 messages in each iteration.  4. Results  This section presents the Perspective API prediction metrics for the English and Por- tuguese datasets and compares them to BERThs. The models were fine-tuned on an NVIDIA RTX 4090 GPU. As the models were trained 30 times with different data sam- ples, the results in this section present the mean followed by the standard deviation.  4.1. Perspective Performance  Table 1 shows the performance metrics of the Perspective API in the English and Por- tuguese datasets. It highlights a significant disparity in the model’s effectiveness between the two languages, with the English dataset consistently achieving higher scores across all metrics. Notably, the accuracy, precision, recall, F1 score, and AUC-ROC are consider- ably lower for the Portuguese dataset, suggesting that the model’s capability to accurately classify toxic content is compromised in Portuguese.  Note that the F1 score – which serves as a balanced measure of a model’s precision and recall in classification tasks – is almost 35 percentage points lower in Portuguese. On top of that, typically, there is a trade-off between the precision and recall metrics; adjust- ing the threshold to improve one often causes the deterioration of the other. However, in this case, precision and recall are significantly lower for the Portuguese dataset, indicat- ing an overall performance issue. Low precision usually implies in a high number of false positives, while low recall indicates many false negatives.   Table 1. Metrics for the Perspective API model in English and Portuguese. Perspective API(En) Perspective API(Pt) Difference Metric Accuracy Precision Recall F1 Score AUC-ROC  0.901 0.813 0.744 0.777 0.942  0.122 0.336 0.340 0.339 0.199  0.779 0.477 0.404 0.438 0.743  Figure 1. Graph displaying the mean AUC across varying training sizes of the BERThs-Pt model, with the left panel covering up to 100 Twitter posts to assess early performance, and the right panel extending to 5000 posts to evaluate the model’s full training potential.  The most concerning results are in the AUC-ROC score, which measures the clas- sification abilities of the Perspective API in its official paper. The Portuguese dataset scores 20 percentage points lower than the English dataset in AUC-ROC. This is both surprising and alarming, given that the multilingual Perspective API is reported to have an AUC-ROC of 0.877, only slightly lower than the English counterpart in the official documentation.  These findings suggest that, while the Perspective API claims to support multiple languages, its performance in Portuguese is substantially lower than in English. This underscores the importance of evaluating multilingual models on a per-language basis to ensure their effectiveness and reliability across different linguistic contexts.  4.2. Evaluating BERThs  BERThs-Pt was evaluated by incrementally increasing the training set by 10 messages at a time. Figure 1 illustrates the model’s performance as the number of training messages increased. The red line represents the average performance of the Perspective API on the test set. Observe that our model surpassed the Perspective API in AUC-ROC score with only 100 training messages.  On the other hand, the BERThs-En dataset showed a different result. Even with an extensive training data, leaving aside a small portion for testing and validation, the fine- tuned BERT model still performed worse than the Perspective API, achieving an average AUC-ROC of 0.934 compared to Perspective’s 0.942.  These findings suggest that while the Perspective API is an excellent tool, it is   not suited for Portuguese. This means creating a custom model can easily surpass the Perspective API’s performance with a relatively small amount of labeled data. Therefore, a classifier tailored to the specific linguistic and contextual nuances of Portuguese is better suited for the detection of hate speech.  4.3. Qualitative Analysis  Table 2 shows a comparative analysis of the classification outcomes for the Perspective API and BERThs-Pt on a Portuguese hate speech dataset. The most notable observation from this analysis is that BERThs-Pt misclassified significantly fewer messages (14.3%) when compared to the Perspective API(20.9%), indicating that the BERT model is gen- erally more accurate in discerning the nuances of the text. This superiority is particularly evident in its handling of subtle and context-dependent instances of hate speech, where the Perspective API often struggles. The analysis further reveals that while both models perform well with clear and unambiguous content, they encounter challenges with am- biguous language and contextually rich messages, cases where the BERT model shows a better overall ability to navigate these complexities.  t c e r r o c  BERThs-Pt was correct  Table 2. Comparison of misclassified and correctly classified messages from the Perspective API and BERThs-Pt. Four random examples from each quadrant are included. In quadrants where only one model missed the true class, the message labels refer to the model that made the mistake. “FP” refers to False Positive, “FN” to False Negative, “TP” to True Positive, and “TN” to True Negative. BERThs-Pt missed 6.9% of Messages (FP)“Nossa, mas feminismo necess´ario hoje em dia?” (FN) “Se vc bate nessa mulher, al´em de covarde, com certeza vc gosta de ” (FP) “quem ´e playboy safado fortalece no RT” (FP) “Isso sim ´e tratar gay com indiferenc¸a...”  72.2% of Messages (TP) “Que mulher burra do cacete” (TP) “gorda e feia” (TN) “Boa semana para todos!” (TN) “N˜ao vou orar, sou ateu”  s a w e v i t c e p s r e P  d e s s i  m e v i t c e p s r e P  13.5% of Messages (FN) “Vai tamb´em ser lanc¸ado um manual de boas maneiras para lidar com fufas, gays e transsexuais, os chamados LGTB” (FN) “as pessoas n˜ao entendem que no meio dos refugiados tem in´umeros terroristas, ´e uma coisa t˜ao ´obvia” (FN) “E traveco mesmo , m´o piroc˜ao” (FN) “Vocˆe ´e cheinha, N ˜AO ´e gostosa.”  7.4% of Messages (FP) “Pra mim BBB sempre foi uma merda.” (FN) “feliz dia do n˜ao tenho roupa pra sair” (FN) “Isso ´e injusto!” (FN) “meritocracia: existe”  Having established that BERThs-Pt generally outperforms the Perspective API, we conducted a quadrant-specific analysis to explore these differences further. The first quadrant represents messages that both methods classified correctly, accounting for 72.2% of the messages. This indicates their effectiveness in handling unambiguous content,   as shown in Table 2. The high success rate highlights the capability of both models to manage straightforward cases of hate speech or benign content where linguistic ambiguity is low. However, real-world scenarios often involve more nuanced language, where model differences become more evident.  The second quadrant covers the 6.9% of messages that Perspective correctly clas- sified but BERThs-Pt misclassified. A random sample of four of these messages reveals that they are somewhat ambiguous, making it difficult to determine with certainty whether they were wrongly classified. These cases highlight the challenges of accurately catego- rizing nuanced and context-dependent language.  The third quadrant, which includes 13.5% of the messages that Perspective mis- classified and BERThs-Pt correctly classified. It becomes apparent that Perspective strug- gles with more complex contexts, particularly when the hate speech is not explicit. The model has particular difficulty with slang or coded language, such as derogatory terms targeting LGBTQ+ individuals. Perspective’s limitations in understanding such indirect insults become evident here, suggesting that its generalized training may not sufficiently capture the nuances of the Portuguese language.  Finally, the fourth quadrant, comprising 7.4% of messages, involves cases where both models failed. These messages typically lack sufficient context, making accurate classification challenging. The shared difficulty in this category underscores the chal- lenges of detecting hate speech when language is ambiguous or context is missing.  5. Conclusions  This study assessed the performance of the Perspective API in detecting hate speech in Portuguese, comparing it to English and exploring the potential of custom-trained models. The results show a significant performance gap, with the API achieving an AUC-ROC score of 94.2 in English but only 74.4 in Portuguese. This drop illustrates the limitations of using a generalized multilingual tool for specific languages.  Relying on a model that supports Portuguese yet delivers subpar results poses two main issues. First, research conducted using such a tool may produce inaccurate or misleading outcomes, undermining the validity of the study. Second, researchers from non-English-speaking regions, may feel compelled to conduct their research in English contexts to leverage the more reliable performance of tools like the Perspective API, po- tentially overlooking important linguistic and cultural nuances.  While the Perspective API excels in English, our study shows it may not be the best choice for Portuguese. A custom BERT model we developed using BERTimbau out- performed the API with only 100 training messages, suggesting that fine-tuning models for specific languages can yield better results in hate speech detection.  In conclusion, while the Perspective API offers robust performance for English, its efficacy in Portuguese is limited. Researchers and practitioners should consider devel- oping custom models tailored to their specific linguistic contexts to achieve more accurate and reliable results.  Acknowledgments  This work was partially funded by CNPq, CAPES, FAPEMIG, and IAIA - INCT on AI.   "
        },
        {
            "titulo": "Segmentação Textual Baseada em Tópicos em Português Utilizando BERTimbau",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31113",
            "idioma": "Português",
            "storage_key": "files/article_31113_30916.pdf",
            "autores": [
                {
                    "nome": "Luciano A. C. da Silva",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0009-0002-2061-9903"
                },
                {
                    "nome": "Maiara S. F. Rodrigues",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0009-0006-6138-8258"
                },
                {
                    "nome": "Adriana P. Archanjo",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0000-0001-9503-194X"
                },
                {
                    "nome": "Luis Pessoa",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0009-0000-6290-4476"
                },
                {
                    "nome": "Miguel L. Silva",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0009-0002-9411-4465"
                },
                {
                    "nome": "Thiago F. de Almeida",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0009-0004-4528-9351"
                },
                {
                    "nome": "Leonardo Silveira",
                    "afiliacao": "PUC-Campinas",
                    "orcid": "https://orcid.org/0000-0002-4468-6812"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Neste trabalho, exploramos a segmentação textual para o português utilizando o modelo BERTimbau, com bases de dados construídas usando tradução automática e a partir de notícias online. Obtivemos P",
            "keywords": [
                "segmentação textual",
                "processamento de linguagem natural",
                "datasets em português",
                "BERTimbau"
            ],
            "referencias": [
                "Francisco, O. J. (2018). Recuperação de informação em atas de reunião utilizando segmentação textual e extração de tópicos. Dissertação de mestrado, Universidade Federal de São Carlos, Sorocaba.",
                "Hearst, M. A. (1997). Text tiling: Segmenting text into multi-paragraph subtopic passages. Computational linguistics, 23(1):33–64.",
                "Retkowski, F. and Waibel, A. (2024). From text segmentation to smart chaptering: A novel benchmark for structuring video transcriptions. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 406–419."
            ],
            "artigo_completo": "Resumo. Neste trabalho, exploramos a segmentac¸ ˜ao textual para o portuguˆes utilizando o modelo BERTimbau, com bases de dados constru´ıdas usando traduc¸ ˜ao autom´atica e a partir de not´ıcias online. Obtivemos Pk = 6, 89 para uma avaliac¸ ˜ao dentro do dom´ınio, mas resultados piores em avaliac¸ ˜oes fora do dom´ınio, destacando a importˆancia de uma base de treinamento diversificada para melhorar a generalizac¸ ˜ao em m´ultiplos dom´ınios.  1. Introduc¸ ˜ao Com o aumento na gerac¸ ˜ao de conte´udo textual n˜ao estruturado, como transcric¸ ˜oes au- tom´aticas de not´ıcias, aulas e reuni˜oes, h´a tamb´em um crescente interesse em extrair de forma eficiente informac¸ ˜oes relevantes desse material [Retkowski and Waibel 2024, Gklezakos et al. 2024]. Por exemplo, pode ser desafiador encontrar o in´ıcio de um determinado t´opico discutido na transcric¸ ˜ao de uma longa reuni˜ao, a menos que essa transcric¸ ˜ao esteja devidamente estruturada. A segmentac¸ ˜ao textual baseada em t´opicos ´e uma tarefa de Processamento de Linguagem Natural (PLN) que divide um texto longo em segmentos n˜ao sobrepostos, de acordo com as mudanc¸as de t´opico [Hearst 1997]. Essa ferramenta permite estruturar e compreender melhor grandes volumes de dados, facili- tando a busca e a extrac¸ ˜ao de informac¸ ˜oes.  recentes  trabalhos  H´a poucos  sobre segmentac¸ ˜ao textual em portuguˆes [Cardoso et al. 2017, Francisco 2018]. Neste artigo, exploramos a segmentac¸ ˜ao tex- tual baseada em t´opicos para o portuguˆes, aplicando a abordagem proposta em [Yu et al. 2023], utilizando o modelo BERTimbau [Souza et al. 2023]. Constru´ımos os conjuntos de dados de treinamento e teste por meio de traduc¸ ˜ao autom´atica para o por- tuguˆes, e utilizando not´ıcias extra´ıdas da internet.  2. Metodologia Neste trabalho, utilizamos a abordagem proposta por [Yu et al. 2023] que trata a segmentac¸ ˜ao textual como um problema de classificac¸ ˜ao de uma sequˆencia de sentenc¸as,   em que se deseja identificar a ´ultima sentenc¸a de cada t´opico, ou seja, identificar as fron- teiras dos segmentos. O componente principal ´e um modelo de linguagem pr´e-treinado do tipo Transformer encoder [Vaswani et al. 2023], que produz a representac¸ ˜ao contextual das sentenc¸as do texto de entrada. Cada representac¸ ˜ao de sentenc¸a ´e usada na classificac¸ ˜ao de fronteira do segmento, conforme mostrado na Figura 1.  Figura 1. Estrutura do modelo de segmentac¸ ˜ao proposto por [Yu et al. 2023]  Em [Yu et al. 2023], al´em da tarefa principal de segmentac¸ ˜ao baseada em t´opicos, s˜ao definidas duas tarefas auxiliares adicionais, Topic-aware Sentence Structure Predic- tion (TSSP) e Contrastive Semantic Similarity Learning (CSSL), com o objetivo de mode- lar a coerˆencia textual e obter melhores resultados na segmentac¸ ˜ao. O modelo ´e treinado de forma supervisionada, otimizando a soma das perdas das trˆes tarefas definidas, sobre um conjunto de treinamento devidamente anotado.  Neste trabalho, utilizamos datasets para o treinamento e a avaliac¸ ˜ao obtidos por meio de traduc¸ ˜ao autom´atica para portuguˆes ou constru´ıdos a partir de not´ıcias em por- tuguˆes extra´ıdas da internet. Os datasets WikiSection e WIKI-50 foram usados por [Yu et al. 2023] e passaram pelo processo de traduc¸ ˜ao autom´atica usando a API de traduc¸ ˜ao da Google. O dataset WikiSection [Arnold et al. 2019] foi usado para trei- namento e avaliac¸ ˜ao, e consiste num conjunto de 38K artigos em inglˆes e alem˜ao, nos dom´ınios de doenc¸as e cidades. Ap´os a traduc¸ ˜ao, restaram 3.590 documentos no dom´ınio de doenc¸as e 19.539 documentos no dom´ınio de cidades. O dataset WIKI-50 [Koshorek et al. 2018] foi usado apenas para avaliac¸ ˜ao, e consiste originalmente em um conjunto de 50 amostras em inglˆes, provenientes da Wikipedia.  Para a avaliac¸ ˜ao dos modelos, utilizamos tamb´em datasets em portuguˆes cons- tru´ıdos a partir de not´ıcias extra´ıdas com webscrapping do portal G11 (portal de not´ıcias do Grupo Globo de Comunicac¸ ˜ao), e do canal de not´ıcias do IBGE2 (Instituto Brasileiro de Geografia e Estat´ıstica). Os documentos de texto foram formados pela concatenac¸ ˜ao aleat´oria de not´ıcias, sendo cada not´ıcia considerada um segmento de t´opico diferente. No caso do dataset G1, foram gerados 454 documentos a partir de 1.300 not´ıcias. Para o dataset IBGE, foram gerados 1.517 documentos a partir de 3.376 not´ıcias.  1https://g1.globo.com/tecnologia/noticia/2012/11/siga-o-g1-por-rss.html 2https://servicodados.ibge.gov.br/api/docs/noticias?versao=3   Como o nosso objetivo ´e aplicar a segmentac¸ ˜ao para o portuguˆes, substitu´ımos o modelo usado em [Yu et al. 2023] pelo modelo BERTimbau [Souza et al. 2023], pr´e- treinado para o portuguˆes do Brasil. Utilizamos as vers˜oes BERTimbau Base (110M de parˆametros) e BERTimbau Large (335M de parˆametros)3.  O treinamento foi realizado em uma GPU NVIDIA T4, usando BERTimbau Base e Large, com 70% do dataset WikiSection em portuguˆes, por 5 ´epocas, com learning rate de 5 × 10−5, batch size de 2 e gradiente acumulado de 2. Criamos sempre um modelo treinado com WiKiSection/cidades e o outro modelo treinado com WiKiSection/doenc¸as. No caso do BERTimbau Large, o treinamento durou aproximadamente 2 dias e 5 horas para o conjunto de cidades e pouco mais de 11 horas para o conjunto de doenc¸as.  A avaliac¸ ˜ao dos modelos seguiu a mesma linha de [Yu et al. 2023]. Usamos trˆes m´etricas usuais para avaliac¸ ˜ao de segmentac¸ ˜ao textual: F1, Pk [Beeferman et al. 1999], e WindowDiff [Pevzner and Hearst 2002]. No caso das m´etricas Pk e WindowDiff, quanto menor o valor, melhor o desempenho. No caso da m´etrica F1, quanto maior o valor, melhor o desempenho. A avaliac¸ ˜ao dentro do dom´ınio de treinamento foi realizada com 20% do dataset WikiSection em portuguˆes. Os datasets WIKI-50, G1 e IBGE s˜ao usados apenas para avaliac¸ ˜ao fora do dom´ınio de treinamento.  3. Resultados  As Tabelas 1 e 2 apresentam os resultados de avaliac¸ ˜ao dos modelos usando BERTim- bau, criados e avaliados para o portuguˆes, dentro do mesmo dom´ınio, com os datasets WikiSection/cidades e WikiSection/doenc¸as. Tamb´em s˜ao apresentados os resultados para o inglˆes correspondentes ao modelo BERT Base [Devlin et al. 2018], obtidos por [Yu et al. 2023].  Modelo (en) BERT Base [Yu et al. 2023] 80,16 87,41 (pt) BERTimbau Base 87,59 (pt) BERTimbau Large  F1  Pk WD 8,22 10,19 8,55 7,07 8,37 6,89  Tabela 1. Resultados dos modelos criados e avaliados com o dataset WikiSec- tion / cidades. BERT Base avaliado em ingl ˆes, BERTimbau em portugu ˆes.  Modelo WD (en) BERT Base [Yu et al. 2023] 68,26 18,29 22,06 76,91 17,16 19,45 (pt) BERTimbau Base 77,77 16,55 18,76 (pt) BERTimbau Large  Pk  F1  Tabela 2. Resultados dos modelos criados e avaliados com o dataset WikiSec- tion / doenc¸ as. BERT Base avaliado em ingl ˆes, BERTimbau em portugu ˆes.  As m´etricas de avaliac¸ ˜ao obtidas com os modelos BERTimbau para o portuguˆes s˜ao melhores e pr´oximas `aquelas apresentadas por [Yu et al. 2023] em inglˆes. Neste caso, devemos considerar tamb´em que o modelo criado para o portuguˆes usando BERTimbau Large ´e maior que o modelo usado em [Yu et al. 2023].  3https://huggingface.co/neuralmind/bert-base-portuguese-cased   A Tabela 3 apresenta os resultados da avaliac¸ ˜ao de dois modelos criados para o portuguˆes nos dom´ınios de cidades e doenc¸as, usando o BERTimbau Large, e avaliados fora do dom´ınio de treinamento, nos datasets WIKI-50, G1 e IBGE.  Dataset Modelo / cidades Pk WD 35,01 35,36 13,62 17,28 20,12 21,06  F1 15,43 64,66 43,12  Wiki50 G1 IBGE  Modelo / doenc¸as Pk F1 WD 36,02 32,42 26,55  12,97 35,98 54,81 25,61 43,36 23,40  Tabela 3. Avaliac¸ ˜ao fora do dom´ınio de treinamento. Modelos com o BERTimbau Large criados com o dataset WikiSection/cidades e WikiSection/doenc¸ as.  O desempenho do modelo fora do dom´ınio de treinamento foi inferior ao desempe- nho dentro do dom´ınio. Os resultados foram melhores para o modelo treinado com o da- taset WiKiSection/cidades. De fato, segundo [Arnold et al. 2019], o conte´udo do dataset WikiSection apresenta caracter´ısticas distintas para cada dom´ınio: WiKiSection/doenc¸as ´e de dom´ınio cient´ıfico restrito com linguagem espec´ıfica, enquanto WiKiSection/cidades ´e de dom´ınio geral mais diverso, mais pr´oximo de um conte´udo de not´ıcias. Isso su- gere que a composic¸ ˜ao de dados de treinamento pode ajudar a obter um modelo para segmentac¸ ˜ao textual que generalize melhor para m´ultiplos dom´ınios.  4. Conclus˜ao  Neste trabalho, exploramos a segmentac¸ ˜ao textual para o portuguˆes, seguindo a aborda- gem de [Yu et al. 2023], mas utilizando o modelo pr´e-treinado para o portuguˆes BERTim- bau [Souza et al. 2023]. Empregamos bases de treinamento e teste constru´ıdas usando a traduc¸ ˜ao autom´atica de bases existentes, al´em de bases de teste constru´ıdas a par- tir de not´ıcias em portuguˆes recuperadas da internet. Obtivemos ´otimos resultados na segmentac¸ ˜ao de texto dentro do mesmo dom´ınio para o portuguˆes, semelhante ao que foi obtido por [Yu et al. 2023] para o inglˆes. Nossos resultados sugerem a efic´acia do m´etodo empregado para a criac¸ ˜ao do modelo em portuguˆes e a importˆancia de usar uma base de treinamento de dom´ınio diversificado para obter um modelo que generalize melhor para m´ultiplos dom´ınios.  Para trabalhos futuros, pretendemos explorar modelos diferentes e buscar uma composic¸ ˜ao mais variada de dados de treinamento para obter um modelo que generalize melhor para v´arios dom´ınios. Al´em disso, desejamos estudar a segmentac¸ ˜ao textual de transcric¸ ˜oes autom´aticas obtidas com reconhecimento de fala, e explorar a segmentac¸ ˜ao de textos muito longos, considerando a t´ıpica limitac¸ ˜ao do contexto de entrada de modelos baseados em Transformer [Vaswani et al. 2023].  "
        },
        {
            "titulo": "Synthetic AI Data Pipeline for Domain-Specific Speech-to-Text Solutions",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31114",
            "idioma": "Inglês",
            "storage_key": "files/article_31114_30917.pdf",
            "autores": [
                {
                    "nome": "Anderson Luiz Karl",
                    "afiliacao": "Audo Tecnologia e Saúde",
                    "orcid": "http://orcid.org/0009-0005-4989-8426"
                },
                {
                    "nome": "Guilherme Sales Fernandes",
                    "afiliacao": "Audo Tecnologia e Saúde",
                    "orcid": "https://orcid.org/0009-0001-8100-0074"
                },
                {
                    "nome": "Leonardo Augusto Pires",
                    "afiliacao": "Audo Tecnologia e Saúde",
                    "orcid": "https://orcid.org/0000-0002-8265-5021"
                },
                {
                    "nome": "Yvens R. Serpa",
                    "afiliacao": "Audo Tecnologia e Saúde / Saxion University of Applied Sciences",
                    "orcid": "https://orcid.org/0000-0002-4799-3180"
                },
                {
                    "nome": "Carlos Caminha",
                    "afiliacao": "UFC",
                    "orcid": "https://orcid.org/0009-0000-5788-6680"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "In this article, we propose a pipeline to fine-tune domain-specific Speech-to-Text (STT) models using synthetic data generated by Artificial Intelligence (AI). Our methodology eliminates the need for manually labelled audio data, which is expensive and difficult to obtain, by generating domain-specific data with a Large Language Model (LLM) combined with multiple Text-to-Speech (TTS) solutions. We applied our pipeline to the radiology domain and compared the results with different approaches based on the availability of domain-specific data, varying from the total absence of domain-specific data to the use of only domain-specific high-quality data (ground truth). Our performance improved the accuracy of the baseline by 40.19% and 10.63% for the WhisperX Tiny and Small models, respectively, which, although performed worse than the results from using the ground truth, shows that it is possible to achieve good results with minimal cost and effort. Finally, the result analysis shows a good insight into the amount of action necessary to achieve good results based on the availability of real data.",
            "keywords": [
                "Large Language Models",
                "Text-to-speech",
                "Speech-to-text",
                "Domain-Specific",
                "Model Fine-tuning"
            ],
            "referencias": [
                "Bain, M., Huh, J., Han, T., and Zisserman, A. (2023). Whisperx: Time-accurate speech transcription of long-form audio. INTERSPEECH 2023.",
                "Chan, W., Jaitly, N., Le, Q., and Vinyals, O. (2016). Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), page 4960–4964. IEEE Press.",
                "da Cruz, F. B., de Souza Britto, M. C., Moreira, G. M., and Junior, A. d. S. B. (2022). Robôs substituem juízes? o estado da arte da inteligência artificial no judiciário brasileiro. Revista Antinomias, 3(1):8–41.",
                "Gontier, F., Serizel, R., and Cerisara, C. (2021). Automated audio captioning by finetuning bart with audioset tags. In DCASE 2021-6th Workshop on Detection and Classification of Acoustic Scenes and Events.",
                "Johnson, M., Lapkin, S., Long, V., Sanchez, P., Suominen, H., Basilakis, J., and Dawson, L. (2014). A systematic review of speech recognition technology in health care. BMC Med. Inform. Decis. Mak., 14(1):94.",
                "Koenecke, A., Choi, A. S. G., Mei, K. X., Schellmann, H., and Sloane, M. (2024). Careless whisper: Speech-to-text hallucination harms. In The 2024 ACM Conference on Fairness, Accountability, and Transparency, pages 1672–1681.",
                "Kumar, Y. (2024). A comprehensive analysis of speech recognition systems in healthcare: Current research challenges and future prospects. SN Computer Science, 5.",
                "Silva, M. d. L. M., Mendonça, A. L. C., Neto, E. R. D., Chaves, I. C., Caminha, C., Brito, F. T., Farias, V. A. E., and Machado, J. C. (2024). Facto dataset: A dataset of user reports for faulty computer components. In Anais do VI Dataset Showcase Workshop, pages 1–12. SBC.",
                "Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems.",
                "Yu, D., Deng, L., and Dahl, G. (2010). Roles of pre-training and fine-tuning in contextdependent dbn-hmms for real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning. sn."
            ],
            "artigo_completo": "Abstract. In this article, we propose a pipeline to fine-tune domain-specific Speech-to-Text (STT) models using synthetic data generated by Artificial In- telligence (AI). Our methodology eliminates the need for manually labelled audio data, which is expensive and difficult to obtain, by generating domain- specific data with a Large Language Model (LLM) combined with multiple Text- to-Speech (TTS) solutions. We applied our pipeline to the radiology domain and compared the results with different approaches based on the availability of domain-specific data, varying from the total absence of domain-specific data to the use of only domain-specific high-quality data (ground truth). Our per- formance improved the accuracy of the baseline by 40.19% and 10.63% for the WhisperX Tiny and Small models, respectively, which, although performed worse than the results from using the ground truth, shows that it is possible to achieve good results with minimal cost and effort. Finally, the result analysis shows a good insight into the amount of action necessary to achieve good results based on the availability of real data.  1. Introduction Automatic audio transcription, commonly referred to as Speech-to-Text (STT), has been a common practice for many work fields, such as health, justice, education, and business [Kumar 2024]. However, precision in recognizing and transcribing language is impor- tant to guarantee the correct and efficient use of the transcribed information. That is especially important in domain-specific applications, in which the use of technical terms and jargon increases the recognition and transcription challenge [Suh et al. 2024]. How- ever, many of the typically available solutions for this problem are built on generic data. Due to that, their results are of lower quality when used in domain-specific scenarios [Chan et al. 2016].  A common approach to solving this issue is to build and refine solutions using domain-related contexts, vocabularies and other types of data [Huang et al. 2020]. Nowa- days, it is standard to use generic AI models as the base for STT solutions and fine-tune these models with domain-specific data [Mak et al. 2024]. However, the fine-tuning pro- cess is expensive and requires a significant amount of data and effort [Hu et al. 2022]. For medical applications, for example, it is necessary to collect sensitive data, have health pro- fessionals check, correct and validate it, and guarantee its privacy and security in regard to the involved patients and personnel [Johnson et al. 2014].  Nevertheless, the need for high-quality STT solutions is evident in many work sectors. In Radiology, for example, it is a common practice to have physicians use STT   tools in their work practice to increase productivity over traditional transcription, the lat- ter in which the professional records a report via voice to be later transcribed manually by another professional (usually without a medical background) [Hammana et al. 2015]. Any errors or delays in this process may result in possible harm and consequences to the patients and their treatments [Vorbeck et al. 2000]. Another common example is courts and judicial procedures, in which a large quantity of domain-specific texts is gen- erated and often transcribed manually, resulting in expensive and inefficient processes [da Cruz et al. 2022].  In this context, this work proposes a low-cost pipeline for the training and fine- tuning of STT AI models when domain-specific data is required but not readily available. Our pipeline is based on the use of AI models to generate synthetic domain-specific data. For that, we have used a Large Language Model (LLM) to produce domain-specific con- tent that simulates real use cases. Specifically for this work, we have explored the radiol- ogy domain, generating data for synthetic radiology reports using an LLM and a specific prompting approach. The synthetic data is then converted into audio files through Text- to-Speech (TTS) tools. Thus, the fine-tuning process is done entirely using synthetic data generated via AI. Additionally, due to the focus on being a low-cost solution, the results of this work were done by using inexpensive or freely available solutions. Simultane- ously, this work also presents a comparison analysis of a range of possible final results depending on the availability of domain-specific data.  2. Related Work  Automatic audio transcription has been a fruitful research field in computer sciences over many years [Yu et al. 2010, Blackley et al. 2019]. Many of the traditional works in this field are focused on the inherited challenges of it, such as handling language subtleties, structure, and fluency [Gontier et al. 2021], and the limitations on the access of adequate datasets [Hu et al. 2022]. These challenges increase when dealing with domain-specific scenarios [Samarakoon et al. 2018].  In regards to datasets, the majority of works in the field use datasets in the English language [Casanova et al. 2022]. When working in scenarios with other lan- guages, researchers must not only solve the recurrent STT challenges but also adapt their solutions, such as done by Gruzitis et al. [Gruzitis et al. 2022] which adapted their models to the Latvian language, and the work of Vivancos-Vincente et al. [Vivancos-Vicente et al. 2016] for Spanish and Portuguese. Alternatively, the work pro- posed by Casanova et al. [Casanova et al. 2022] shows an alternative to training models for different languages based on data augmentation from only one speaker for the targeted language, using cross-lingual voice conversion and multi-speaker TTS techniques.  Moreover, access to good domain-specific datasets is a challenge, and its produc- tion involves high costs with domain experts, data analysis, and validation. This prob- lem is often faced with the use of synthetic data [Li et al. 2018, Rosenberg et al. 2019, Laptev et al. 2020, Huang et al. 2020, Yang et al. 2023]. However, synthetic data is fre- quently distant from real use cases due to the absence of mistakes and imperfections that are often common in human-made data, which makes it “too perfect” compared to real-world cases. This“perfection problem” is handled with the introduction of synthetic errors and imperfections, such as done by the Synt++ solution proposed by Hu et al.   [Hu et al. 2022], in which noise and random artefacts are introduced to the synthetic data generation so it more closely resembles real-life data.  Only recently the process of data synthesis using LLM have been explored, such as the work presented by V´asquez-Correa et al. [V´asquez-Correa et al. 2023], which gen- erates domain-specific synthetic data through prompting to fine-tune an STT solution for the English, Spanish, and Basque languages. Silva et al. [Silva et al. 2024] also uses an LLM to generate synthetic data for a hardware failure prediction dataset. Their dataset was generated from problem categories and reports from major component manufacturers in the market.  Similarly, this work proposes a new approach to synthetic data based on prompt- ing. The synthetic data is then converted into audio files through TTS algorithms and used to fine-tune a generic STT AI model. Our approach uses a simple and low-cost generic STT AI model as a means to prove its usefulness in scenarios with minimal resources. Moreover, this work presents a comparison analysis of results based on the availability of domain-specific data, varying from the total absence of domain-specific data (our solu- tion) to the use of only domain-specific high-quality data (an ideal solution).  3. Methodology 3.1. Datasets To validate the efficiency of our proposed pipeline, we used a dataset of manually labelled audio data from radiology professionals, which was divided into a set for training and another for testing. The training set included 98 audio files from two cisgender male radiologists with a total duration of 1 hour, 10 minutes and 8 seconds of audio. The testing dataset consisted of 82 audio files from the same two radiologists, with a total duration of 1 hour, 4 minutes and 21 seconds of audio. Both training and testing sets had an equal amount of audio files for the two radiologists, and all audio files were spoken in Portuguese. All audio files were recorded in real-world scenarios, including background noise from the respective workplaces, audio artefacts, and other common issues. This dataset constitutes our ground truth dataset, which was used to compare with the results from the other approaches explored.  3.2. Methods and Technologies The transformers library by Hugging Faces [Vaswani 2017] was used to fine-tune the STT model, which was also configured for the Portuguese language. We opted for a traditional fine-tuning process using all of the available weights. For the inference, we have used the WhisperX model [Bain et al. 2023], which offers a quicker and more precise transcrip- tion, with the Ctranslate2 backend for better compatibility and reduced inference time. The main reason for using WhisperX was the presence of an internal Voice Activity De- tection (VAD), which considerably reduces the hallucination tendencies and optimizes the use of VRAM [Koenecke et al. 2024].  We have used GPT-4o as the LLM to generate synthetic domain-specific radiology reports using a specific approach and prompts [Islam and Moushi 2024]. The synthetic reports were fed into TTS solutions to generate audio files for the fine-tuning process.  As TTS solutions, we have used the ElevenLabs solution1, which is fairly low cost  1https://elevenlabs.io/   for its quality, and the Google Text-to-Speech2. Both tools allowed for a variety of into- nations, speech styles, and variations, which helped to reduce the “perfection problem” often produced in synthetic data. Furthermore, the use of two TTS solutions improved the representation and diversity of speech patterns and accents.  3.3. Metrics  The Word Error Rate (WER) metric was used to assess the precision of the STT solutions [Ali and Renals 2018]. The WER metric is calculated by the ratio between the number of transcribed errors and the number of words originally spoken. These errors are classified as Substitutions (S), Insertions (I), and Deletions (D). The WER formula we used was: WER = S+D+I  N , where N is the number of words originally spoken.  4. Results  4.1. Proposed Pipeline  Figure 1. Proposed Pipeline.  As shown in Figure 1, the proposed pipeline aims to fine-tune an STT model using a set of synthetic domain-specific data. It starts with a specialist prompt for the LLM. This specialist prompt must consider specific terminology and domain-specific information to guarantee that the synthetic data closely resembles real-life data.  The LLM-generated synthetic data is fed into TTS solutions and converted into audio files. It is important to include variations in tone of voice and synthetic noise in this process to reduce the “perfection problem”. Together, the LLM-generated synthetic data and its audio representation compose the AI-labelled dataset. This dataset is then used to fine-tune the STT model of choice.  4.2. AI-Labelled Dataset  GPT-4o was used as the LLM tool for the domain-specific synthetic data generation. For that, we first introduced the model to the radiology context and gave it a series of radiology specialities and exam types, such as computer tomography and radiography. Furthermore, to guarantee typical report-style phrasing, we instructed the LLM to create phrases and sentences in a progressive format, starting from normal descriptions, followed by potential  2https://cloud.google.com/text-to-speech  PromptDomain-Specfic  TextSynthetic AI  DatasetFine-Tuned Speech-to-TextDomain-Specfic  AudioText-to-Speech Solution(s) findings and specific diagnostics for those. Finally, the LLM was instructed not to include abbreviations and to provide the results in a JSON format without additional text. The prompt used can be seen in Figure 2.  You must generate {number of phrases} phrases in Portuguese that could be present in a {type of report} report made by a physician expert on a specific medical field you will be given as input. Generate the phrases and sentences following a logical chain of thought, starting from regular cases and progressing to possible findings and specific diagnostics related to the given context. Explore multiple phrase types, ranging from basic descriptions to detailed conclusions. Avoid using abbreviations, and every time you need to mention a specific term, use it in its most complete form (for example, use centimetres instead of cm and beats per minute instead of bpm). Format the output: return a JSON object with the phrase list. Do not include any additional text before and after the JSON. JSON output example: {  \"phrases\": [  \"O paciente apresenta ritmo card´ıaco regular, com 72 batimentos por minuto.\", \"A imagem mostra um aumento moderado no tamanho do ventr´ıculo esquerdo.\", \"N˜ao h´a evidˆencias de derrame pleural ou ascite.\"  ]  }  Output only the JSON with the {number of phrases} phrases without additional texts.  Figure 2. Prompt used to generate domain-specific radiology texts. The example phrases and sentences are written in Portuguese to exemplify better the input we used.  As previously mentioned, we have used two TTS tools for the synthetic audio generation: ElevenLabs and Google Text-to-Speech. The use of both tools is meant to di- versify the generated data with varying speaking patterns, rhythm, intonation and quality.  We generated 46 minutes and 43 seconds of audio using ElevenLabs in a total of 980 files. These files were equally split into five different male voices. As for the Google Text-to-Speech, we generated 58 minutes and 55 seconds of audio, again, in a total of 980 files, using only one male voice available. The dataset for the synthetically generated data is available in a GitHub repository3.  Figure 3 (a) and (b) shows the audio length distribution for the synthetic dataset compared to the real, manually labelled data we had. As seen, the overall distribution is quite similar, while the synthetic data tends to be shorter, resulting in more files. The word cloud in Portuguese for both datasets can be seen in Figure 3 (c) and (d). Both datasets show domain-specific terms, with a greater presence of punctuation terms (com- mas, dots, etc) on the real dataset. Alternatively, the synthetic dataset has a higher pres- ence of phrases such as “N˜ao h´a” or “H´a sinais” (meaning “There is no” and “There are signs of,” respectively in English), showing a tendency to repeat phrase structures with the same starting terms. The distribution of terms and times between TTS tools is fairly similar.  4.3. Analisys  Figure 4 shows the results for the WER metric for four different scenarios: a base- line (WhispherX without fine-tuning); WhispherX fine-tuned using the synthetic data; WhispherX using synthetic audio data generated from real radiology reports; WhispherX  3https://github.com/AtkLLM/AI-DrivenSpeechModel-Dataset   Figure 3. Histograms of audio length used: (a) histogram of the duration of man- ually labelled audio files used for training; (b) histogram of the duration of AI- generated audio files; (c) Word Cloud from Real Data; (d) Word Cloud from Syn- thetic Data;  fine-tuned using the ground truth (ideal scenario). The results are shown for both the WhisperX Small and Tiny versions, its smallest and more easily accessible forms.  In both cases, for the WhisperX Tiny and Small versions, the behaviour was fairly similar, with the Tiny version having considerably higher values for the WER compared to the Small version. The baseline, as expected, presented the highest WER value (104.13 and 37.35), while the ground truth version achieved the lowest one (40.11 and 23.11). It is worth mentioning that the ground truth Tiny version while performing significantly worse, achieved a similar WER when compared to the baseline Small version (40.11 compared to 37.35).  It is worth noting that the high WER value from the baseline WhisperX Tiny is the result of hallucination, which made the model include words that were not present in its results. That made the WER value rise above 100%.  Our proposed pipeline, which used only synthetic data for the fine-tuning process, achieved a WER of 62.28 and 33.38 for the Tiny and Small versions, respectively, which correspond to an improvement of 41.85% and 10.62%. These results were achieved us- ing only the audio files generated by ElevenLabs, which performed better than the ones generated by the Google-TTS tool (WER equal to 70.96 and 33.94 for the Tiny and Small versions) and by a combination of both ElevenLabs and Google-TTS (WER equal to 66.75  0102030Length(seconds)020406080100FrequencyAudiolengthhistogram-Real246Length(seconds)050100150Frequency(b)Audiolengthhistogram-Synthetic(c)WordCloud-Real(d)WordCloud-Synthetic and 33.94 for the Tiny and Small versions).  To exemplify a case in which there are some real-use data for the fine-tuning, we have tested using only real-case radiology reports (ignoring the LLM step) and producing the audio data from them using the same TTS tools mentioned previously. This new data was used to fine-tune both WhisperX Tiny and Small versions, achieving the WER of 56.24 and 30.76, respectively, which are 45.99% and 17.64% better than the baseline. For these results, we have only used audio data generated by ElevenLabs since it achieved better results in previous tests.  Our results show that it is possible to achieve better outcomes by using a com- pletely synthetic approach. While it still performs worse compared to approaches with real-data approaches, it shows a promising approach that has plenty of room for experi- mentation and improvement and incurs a very low cost compared to generating a dataset with real data.  Figure 4. Results from the four approaches using both WhisperX Small and Tiny models. The WER metric is shown on the y-axis.  The ground truth results are, as expected, the best results with the lowest WER values for both models. However, it is also the most expensive approach with its caveats and challenges. Moreover, it is not unlikely that its best results are a consequence of some level of overfitting since the training and test data come from the same physicians using the same equipment in the same environments. On the other hand, the synthetic dataset was composed of a wider variety of voices and intonations that, while similar to the real ones in terms of context and intonation, are still fairly different. On that, the wider range of possible voices from the ElevenLabs tool might explain why it performed better than the Google-TTS tool. From our experiments, the Google-TTS tool tends to generate very clean and “perfect” robot-like audio files that are remote from real-use cases.  5. Conclusion  This work presented a pipeline for fine-tuning domain-specific STT solutions using syn- thetic data produced by a combination of LLM prompting and TTS tools. Our proposed pipeline produces good-quality synthetic data and overcomes the “perfect problem” by using TTS tools for a wider range of voices, intonation, and rhythm. Our findings show that our pipeline improves the results compared to a non-fine-tuned solution.  Given the results, we can also make assumptions based on the availability of real domain-specific data. As Figure 4 shows, and as expected, the more real data used, the   better the results. Yet, the difference between the use of some real data (using real-case reports data with TTS for audio generation) and 100% synthetic data is not significant (about 10% improvement for the Tiny model, and 8% improvement for the Small model, when comparing both approaches), indicating that in some cases, the synthetic-only ap- proach might provide good enough results. Nevertheless, it is worth spending resources acquiring domain-specific knowledge and data, especially to produce a specialist LLM prompt required by our approach, but it will not necessarily reflect a significant improve- ment over the synthetic data.  Our choice of using WhisperX Tiny and Small models is focused on providing a low-cost solution for domain-specific scenarios. Higher WhisperX models are likely to provide better results, but they require expensive hardware and more resources for training. Besides that, higher models would require higher costs to host online for a production-ready solution. Considering our scenario, considerable investment would be required to host such a strategy for a single hospital with multiple simultaneous physicians working at the same time daily. Yet, our results indicate that, with the use of a ground truth dataset, it might be possible to improve a simpler model through fine-tuning to perform as well as a baseline better model, as we saw with the results from the ground truth fine- tuned Tiny model compared to the baseline Small model. In our preliminary tests, we found that the baseline WhisperX Medium model has a WER of 28.85, which is slightly higher than the ground truth fine-tuned WhisperX Small model we presented (23.11).  Besides operational costs, the complexity of the AI model used impacts its infer- ence time (the time it takes to generate the output given the input). Simpler models, such as Tiny and Small, have a relative inference time significantly smaller than larger models [Bain et al. 2023]. For real-time settings, this is of major importance, such as the one explored in this study for radiology STT solutions.  As future work, our pipeline could be assessed for other domain-specific contexts, as well as more experimentation on the synthetic data variation that further approaches real-case scenarios, including the use of different accents, acoustic conditions, and back- ground noise. The use of a more diverse ground truth set might also provide better insight into possible overfitting and more realistic results for fine-tuned models trained with it. It is not unlikely that a production-ready solution achieves a WER value closer to the results from our approaches than the current ground truth ones. Besides that, a longer audio ground truth dataset could surely provide better insights into our results since it was limited to a little over 1 hour long due to budget and time constraints.  Finally, a more fine-grained analysis of the balance between synthetic and real data could provide further insight into how much effort is needed to create hybrid approaches that more closely resemble real data, including the use of real audio instead of purely relying on TTS Tools. That might provide a great approach for fine-tuning STT models with a fraction of the usual associated costs when using high-quality ground truth sets.  6. Acknowledgements  The authors would like to thank the Brazilian Agency FUNCAP-CE for its financial sup- port under the project NUP 31052.001303/2023-62. We would also like to thank Raiza Vaz for her help in building the ground truth database.   "
        },
        {
            "titulo": "Evaluating Federated Learning with Homomorphic Encryption for Medical Named Entity Recognition Using Compact BERT Models",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31115",
            "idioma": "Inglês",
            "storage_key": "files/article_31115_30918.pdf",
            "autores": [
                {
                    "nome": "Marcos F. Pontes",
                    "afiliacao": "UFOP",
                    "orcid": "http://orcid.org/0000-0001-8721-0171"
                },
                {
                    "nome": "Rodrigo C. Pedrosa",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0003-2547-3835"
                },
                {
                    "nome": "Pedro H. Lopes",
                    "afiliacao": "UFOP",
                    "orcid": null
                },
                {
                    "nome": "Eduardo J. Luz",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0001-5249-1559"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Medical Named Entity Recognition (NER) identifies and categorizes medical entities from unstructured texts, crucial for health monitoring tasks. Despite advancements with Large Language Models (LLMs), medical NER faces challenges due to limited and dispersed labeled data across institutions, protected under privacy regulations. Federated Learning (FL) offers a solution by enabling decentralized model training while preserving data privacy, but it is vulnerable to byzantine attacks. This research proposes a simple and secure FL protocol using Homomorphic Encryption (HE), called FedHE, that removes the need of trust between the federations and the training coordinator. Encrypted FL imposes significant constraints regarding resources consumption and performance, making the state-of-the-art language models impractical. This research aims to assess how well compact BERT representations work in federated medical NER tasks in comparison to the state-of-the-art approaches. The results showed that compact BERT representations, such as BERTmini are competitive with the state-of-the-art, and are feasible to use in FedHE. However, resource consumption overheads remain a challenge, particularly when the number of clients increase.",
            "keywords": [
                "Cryptography",
                "Federated Learning",
                "Homomorphic Encryption",
                "Named Entity Recognition",
                "BERT"
            ],
            "referencias": [
                "Al Badawi, A. and Polyakov, Y. (2023). Demystifying bootstrapping in fully homomorphic encryption. Cryptology ePrint Archive.",
                "Beutel, D. J., Topal, T., Mathur, A., Qiu, X., Fernandez-Marques, J., Gao, Y., Sani, L., Li, K. H., Parcollet, T., de Gusmao, P. P. B., et al. (2020). Flower: A friendly federated ˜ learning research framework. arXiv preprint arXiv:2007.14390.",
                "Konečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., and Bacon, D. (2016). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492.",
                "Marcolla, C., Sucasas, V., Manzano, M., Bassoli, R., Fitzek, F. H., and Aaraj, N. (2022). Survey on fully homomorphic encryption, theory, and applications. Proceedings of the IEEE, 110(10):1572–1609.",
                "Peng, L., Luo, G., Zhou, S., Chen, J., Xu, Z., Sun, J., and Zhang, R. (2024). An indepth evaluation of federated learning on biomedical natural language processing for information extraction. npj Digital Medicine, 7(1):127.",
                "Tang, B., Cao, H., Wu, Y., Jiang, M., and Xu, H. (2013). Recognizing clinical entities in hospital discharge summaries using structural support vector machines with word representation features. In BMC medical informatics and decision making, volume 13, pages 1–10. Springer",
                "Yi, X., Paulet, R., Bertino, E., Yi, X., Paulet, R., and Bertino, E. (2014). Homomorphic encryption. Springer.",
                "Zhu, L., Liu, Z., and Han, S. (2019). Deep leakage from gradients. Advances in neural information processing systems, 32."
            ],
            "artigo_completo": "Abstract. Medical Named Entity Recognition (NER) identifies and categorizes medical entities from unstructured texts, crucial for health monitoring tasks. De- spite advancements with Large Language Models (LLMs), medical NER faces challenges due to limited and dispersed labeled data across institutions, pro- tected under privacy regulations. Federated Learning (FL) offers a solution by enabling decentralized model training while preserving data privacy, but it is vulnerable to byzantine attacks. This research proposes a simple and secure FL protocol using Homomorphic Encryption (HE), called FedHE, that removes the need of trust between the federations and the training coordinator. Encrypted FL imposes significant constraints regarding resources consumption and perfor- mance, making the state-of-the-art language models impractical. This research aims to assess how well compact BERT representations work in federated med- ical NER tasks in comparison to the state-of-the-art approaches. The results showed that compact BERT representations, such as BERTmini are competitive with the state-of-the-art, and are feasible to use in FedHE. However, resource consumption overheads remain a challenge, particularly when the number of clients increase.  1. Introduction  Medical named entity recognition (NER) aims to identify medical entities (e.g., drug names, adverse reactions and symptoms) from unstructured medical texts and classify them into different categories. It can be used in many intelligent healthcare tasks such as pharmacovigilance and health monitoring [Tang et al. 2013]. With the recent advance- ments in the field [Peng et al. 2024] the problem of NER has seen significant improve- ments. However, in the specific context of medical NER, there are significant challenges in the learning process due to the sensitive nature of the data. First, the available labeled data of a single healthcare institution might not be representative enough to adjust a NER model with good predictive accuracy. Second, collaborative training with data sharing is frequently impractical considering the regulations prohibitions and the security risks associated to the data sensitiveness and trust between the parties.  To leverage massively distributed data and enhance model generalizability, feder- ated learning (FL) was introduced in [Koneˇcn`y et al. 2016] as a novel learning framework. In an FL training loop, clients collaboratively train a shared global model by exchanging model weights or gradients while keeping their data stored locally. By bringing the model to the data, FL avoids data transfer and achieves competitive performance compared to models trained with pooled data.   Recently, [Peng et al. 2024] provided an in-depth evaluation of federated learning in biomedical natural language processing, demonstrating that the BlueBERT (BERTblue) model, particularly its larger variant (BERTlargeblue), trained using FL outperforms both its version trained on data from a single client and GPT-4 when applied in a few-shot prompt setting. Clearly, FL, combined with variations of BERT, stands out as an effective approach for NER.  Although FL’s primary focus is on maintaining rigorous privacy protections by preventing data sharing, [Zhu et al. 2019] introduced a new vulnerability in the form of inference attacks, showing that private training data can be extracted from the publicly shared gradients. To mitigate this risk, one approach is to incorporate an encryption step into the federated learning framework. Specifically, employing Homomorphic Encryption (HE) [Yi et al. 2014] within FL allows clients to encrypt their gradients, enabling the cen- tral coordinator to aggregate model updates directly on ciphertexts, thereby eliminating the need for decryption.  While HE is often considered the gold standard for data-in-use encryption, it im- poses significant performance overheads in terms of both computation and communica- tion. As a result, deploying state-of-the-art natural language models becomes impractical due to the large number of trainable parameters. Therefore, to implement a more secure FL system using HE, smaller models must be selected. In this context, this paper ad- dresses two key questions: (i) What is the computational cost of applying HE in FL for NER applications? (ii) How much predictive accuracy might be sacrificed by choosing a more secure FL+HE approach with a smaller model? The obtained results showed that compact models, like BERTmini, can perform competitively with state-of-the-art NER models in a FL+HE setting for different corpora. However, resource overheads — par- ticularly communication bandwidth and memory utilization—continue to pose significant challenges.  2. Federated Learning The prototypical FL setting consists of a central server S and a set of K distributed clients C, such that |C| = K, that jointly cooperate to solve a standard supervised learning task. Each client c ∈ C has access to it’s own private training set Dc = {xc,i, yc,i}nc i=1. The goal of FL is to train a global predictive model whose architecture and parameters θ∗ ∈ Rd c=1 pcLc(θ; Dc), where are shared amongst all the clients and found to minimize minθ Lc is the local objective and pc ≥ 0 specifies the individual contribution of the client c such that (cid:80)K n , where n = (cid:80)K c=1 nc. The local objective function Lc usually is defined as the empirical risk calculated over the training set Dc sampled from the client’s local data distribution Lc(θ; Dc) = 1 i=1 l(θ; (xc,i, yc,i)), where l is an instance-level loss (e.g., cross-entropy loss or nc squared error in the case of classification or regression tasks, respectively).  c=1 pc = 1. Two possible configurations for pc are pc = 1  K or pc = nc  (cid:80)nc  (cid:80)K  In Federated Learning, to generate a global model θ from locally trained models with parameters θc, an aggregation step is necessary to combine the updates from all clients. One of the most widely used methods for aggregation is called FedAvg. In each round t, clients perform local training steps on their private datasets Dc to minimize their respective objective functions Lc. After completing the local updates, clients send their   updated parameters θ(t) updates by computing a weighted average, typically defined as θ(t+1) = (cid:80)K uses this to update the global model for the next training round.  c back to the central server S. The server then aggregates these c , and  c=1 pcθ(t)  2.1. Federated Learning with Fully Homomorphic Encryption  Homomorphic Encryption (HE) allows certain computations (e.g., addition) to be per- formed directly on ciphertexts, without decrypting them first. The intuitive idea is that a third party can compute data without actually getting to know that data. This problem is solved with key-based encryption where the encryption process preserves algebraic opera- tions. For the addition operator, for example, we would have e(k, a)+e(k, b) = e(k, a+b) for an encryption scheme e(., .), encryption key k, and plaintexts a and b. A third party could thus compute the ciphertext of the value of the addition a + b from the ciphertexts of a and b , and return this to the owner who could decrypt this to get the computation result on the plaintext [Al Badawi and Polyakov 2023].  In the context of FL, the viability of HE is particularly constrained when encrypt- ing local model updates. The use of large language models, such as BERT with 110M parameters, becomes nearly infeasible given the bandwidth and computational overhead associated with processing encrypted gradients. This limitation underscores the need for more efficient encryption techniques, model compression strategies, and the adoption of more compact architectures— the latter being the focus of this paper’s assessment  3. Methodology  This section describes this work’s proposal for making federated medical NER secure with HE. The proposed solution, called FedHE, aims to be conceived as a generic frame- work on how HE can be used in FL, making them compliant with data privacy regulations and enabling scenarios such as medical NER to work without the risk of inference attacks.  The FedHE protocol uses HE encryption to protect the gradients data. Thus, even if byzantine attackers compromise the computing server, they don’t have access to the information of the gradient data from each learning client. In addition, it is impossible for byzantine attackers to use these encrypted gradient data to train shadow models.  In this work,  the cryptographic scheme CKKS (Cheon-Kim-Kim-Song) [Marcolla et al. 2022] is used to encrypt clients’ gradients preserving the arithmetic oper- ations of addition and multiplication by a scalar plaintext number. CKKS is an asymmet- ric cryptographic scheme that requires key pairs, so a key management service (KMS) is required. Notice that this work does not aim to detail neither the encryption scheme nor the KMS protocol, but we rely on strategies and algorithms publicly defined in the literature.  The coordinator algorithm orchestrates the federated network (See Algorithm 1). Usually, it defines how the protocol work, establish mechanisms to define the architecture, guarantee trust between the clients, and aggregate the locally generated gradients.1  The FedAvg algorithm is executed homomorphically, without decryption of clients updates. Additionally, although the KMS strategy is not specified in this paper, we assume the coordinator has only access to the public key.   Algorithm 1: FedHE Coordinator. The K clients are indexed by c ∈ C, T is the total of federated learning rounds and L is the loss function. The goal is to obtain θ∗ that minimizes the clients’ loss function.  State: Local model with parameters θi. Function ServerTrain:  initialize θ(0) request public key from KMS for each round t = 0, . . . , T do  number of clients: m ← max(C · K, 1) client selection: St ← (random set of m clients) for each client c ∈ St in parallel do  ∇enc(L(t+1)  c  ) ← ClientT rain(c)  end homomorphic FedAvg: ∇enc(L(t+1)) ← (cid:80)K for each client c ∈ C in parallel do ClientU pdate(c, ∇enc(L(t+1))  c=1  end  end  nc  n ∇enc(L(t+1)  c  )  The FedHE client training algorithm is where the actual train happens (See Al- gorithm 2). Each client trains on their own private training dataset and share only the encrypted gradient updates with the coordinator for model aggregation.  Algorithm 2: FedHE Client. X represents the training samples while Y represents the training labels. I, ϵ, η represents the number of local epochs, the tolerance and the learning rate, respectively. The goal is to obtain θ∗ that minimizes the loss function L.  State: Local model with parameters θi. Function ClientTrain:  request public key from KMS for each epoch i = 0, . . . , I do  forward propagation: ˆYi = f orward(X, θi) compute loss: Li = loss(Y, ˆYi) if Li <ϵ then break  end else  back propagation: ∇Li = backprop(X, θi, Li) gradients encryption: ∇enc(Li) = encrypt(∇Li, P ublicKey) return ∇enc(Li)  end  end  Function ClientUpdate(∇enc(Lagg)):  request private key from KMS gradients decryption: ∇Lagg = decrypt(∇enc(Lagg), P rivateKey) update: θi+1 = θi − η∇Lagg   In conclusion, the FedHE 1 protocol is adaptable to a range of model archi- tectures and can be seamlessly integrated into established FL platforms like Flower [Beutel et al. 2020], TensorFlow Federated2, FATE3, among others.  4. Results  In this section, we present the key findings of our analysis on FedHE, focusing on two practical aspects: (1) the performance of FedHE trained with compact BERT models compared to state-of-the-art models, and (2) the performance and resource consumption overheads associated with FedHE.  Named Entity Recognition Corpora  We compared FedHE with alternative training schemes on two biomedical NLP datasets and one news dataset, focusing on NER tasks. In NER, the objective is to identify and classify named entities, such as diseases and genes, from a given sequence of tokens.  The selected corpora were chosen based on two main criteria: they are publicly available, ensuring the reproducibility of results, and they are commonly used in well- cited papers, which helps guarantee the quality of the data. A summary of the selected datasets can be found in Table 1.  Entity/Relation Type Corpora Type  Corpus CONLL-2003 General BC2GM BC4CHEMD Drug/Chem  Gene  News articles Medline abstract PubMed abstract  Train Dev 14987 3466 26006 3251 94170  Test 3684 3251  11772 11771  Table 1. List of NER corpora and their statistics  What Are the Performance and Resource Overheads of FedHE?  In FedHE, the encryption of gradients introduces a substantial increase in data size, which can significantly impact bandwidth. Table 4 provides a comparative analysis of the size overhead associated with different BERT models when using the CKKS encryption scheme. For instance, BERTtiny, which has a plaintext gradient size of 16 MB, increases to 340 MB when encrypted. Similarly, BERTmini’s gradient size grows from 42 MB to 864 MB under the same scheme. The most pronounced effect is seen with BERTblue and BERTlarge blue , where the gradient size were 20 times and more than 50 times bigger, respectively. While local training remains unaffected, these increases in ciphertext size lead to significant bandwidth overheads. As such, models like BERTblue become im- practical for FedHE due to the prohibitive size of encrypted gradients, emphasizing the need for more bandwidth-efficient approaches or smaller models to maintain feasibility in federated settings.  Table 4 highlights the impracticality of Large Language Models in FedHE due to their exponential growth of memory and bandwidth requirements. Table 3 shows that  1Source code: https://github.com/marcosfpr/fedhe and https://github.com/  marcosfpr/sealy.  2https://www.tensorflow.org/federated 3https://fate.fedai.org/   BERTmini  Model BERTtiny  Scheme Single/Central/Federated FedHE Single/Central/Federated FedHE Single/Central/Federated FedHE BERTlarge blue Single/Central/Federated FedHE  BERTblue  # Params Size 4M 4M 11M 11M 108M 108M 344M 344M  16 MB 340 MB 42 MB 864 MB 415 MB 8 GB 1GB >50 GB  Table 2. Model Parameters and Size for Different Schemes  while operations on encrypted gradients, particularly encryption, become more costly with increased parameter sizes, these do not generally pose a bottleneck in training. How- ever, if federated training involves frequent aggregation rounds and infrequent local train- ing epochs, these operations could become a significant bottleneck when the number of parameters is sufficiently large. Typically, clients perform extensive local training with less frequent aggregations, which aligns with both performance and operational efficien- cies in FedHE.  Due to BERTlarge blue’s excessive memory demands—over 50 GB in FedHE and 1 GB in plaintext—along with significant bandwidth and processing requirements, this work will focus on comparing the effectiveness only with the base BERTblue model in- stead. Future work can be done to address the comparsion with larger versions of BERT such as BERTlarge blue.  Model  BERTtiny  BERTmini  Type Encrypt Decrypt Encrypt Decrypt  Mean (s) Std. Dev. (s) 8.016 2.179 21.877 5.884  0.133 0.053 0.251 0.095  99th Percentile (s) 8.484 2.337 22.671 6.144  Table 3. Summary Statistics for Encryption and Decryption Times of BERT Mod- els  While Table 4 highlights significant bandwidth constraints on the server-side in FedHE, it is also essential to evaluate the resource and time costs associated with aggre- gation operations. Figure 4 sheds light on the performance of aggregation as the number of clients increases for the BERTtiny model using the BC2GM corpus. The analysis indicates that the homomorphic FedAvg aggregation time does not present an efficiency issue in the training process; specifically, BERTtiny completes aggregation in approxi- mately 20 seconds with 22 clients, whereas BERTmini requires about 50 seconds with 14 clients. However, it is important to note that as the number of clients grows, the memory required to store ciphertext gradients increases significantly. For instance, aggregation for BERTmini with 16 clients led to a coordinator crash due to memory insufficiency. While such issues can be mitigated using external memory strategies, these solutions introduce additional performance overhead.   ) s (  i  e m T n a e  M  60  40  20  0  0  Aggregation Performance vs Number of Clients  BERTtiny BERTmini  Memory Limit Exceeded (>20GB)  2  4  6  8  10  12 Number of Clients  14  16  18  20  22  24  Figure 1. Aggregation performance for BERT models on BC2GM.  How Does FedHE with Compact BERT Models Compare to State-of-the-Art BERT Models For Medical NER?  Another fundamental research question for this work is to understand how far compact BERT models are from the state-of-the-art models for federated medical NER. In par- ticular, we would also like to understand if the introduction of HE can skew the overall results. The table 4 shows an F1 comparison of the approaches tested with the state-of- the-art models.  Another critical research question addressed in this work is evaluating the per- formance gap between compact BERT models and state-of-the-art models for federated medical NER. Additionally, we investigate whether the integration of HE affects the over- all performance outcomes. Table 4 provides a comparative analysis of F1 scores for the evaluated approaches against the state-of-the-art models.  Model Method  CONLL-2003  BC2GM  BC4CHEMD  BERTtiny  Train Eval Train 0.804 ± 0.002 0.618 ± 0.006 0.865 ± 0.005 Single 0.841 ± 0.000 0.728 ± 0.002 0.953 ± 0.001 Central 0.726 ± 0.010 0.464 ± 0.005 Federated 0.624 ± 0.001 0.816 ± 0.000 0.650 ± 0.014 0.744 ± 0.005 FedHE 0.802 ± 0.000 0.690 ± 0.002 0.961 ± 0.002 BERTmini Single 0.995 ± 0.001 0.787 ± 0.005 0.990 ± 0.000 Central Federated 0.993 ± 0.000 0.958 ± 0.001 0.758 ± 0.013 FedHE BERTblue Central  Eval Train 0.391 ± 0.047 0.537 ± 0.003 0.460 ± 0.022 0.605 ± 0.011 0.598 ± 0.010 0.448 ± 0.008 0.621 ± 0.004 0.464 ± 0.005 0.538 ± 0.004 0.802 ± 0.000 0.613 ± 0.002 0.859 ± 0.001 0.584 ± 0.000 0.833 ± 0.000 0.994 ± 0.000 0.781 ± 0.001 0.998 ± 0.000 0.739 ± 0.004 0.877 ± 0.000 0.590 ± 0.001 0.683 ± 0.000 0.999 ± 0.000  Eval 0.593 ± 0.001 0.645 ± 0.003 0.613 ± 0.016 0.604 ± 0.010 0.594 ± 0.002 0.763 ± 0.010 0.703 ± 0.012  0.791 ± 0.009  0.968 ± 0.001  0.992 ± 0.001  0.758 ± 0.041  Table 4. F1 Score comparison of FedHE with various BERT models on medical NER datasets. Standard deviations are shown in parentheses. Bold indicates FedHE surpasses Federated, while underscored indicates it surpasses Central- ized evaluation.  Training Setup  In all experiments we ran 50 epochs for training the models. The centralized and single- client learning, we conducted 50 local epochs on their private dataset. The single-client data were obtained splitting the dataset in two parts, and taking only one from the corpora.  The federated and FedHE approaches ran 5 aggregation rounds with 10 local epochs each on client data. Effectiveness tests, shown in Table 4, were conducted with   2 clients. Standard deviations for federated and FedHE were calculated from all clients, while single-client and centralized deviations were from 2 runs.  For training the models, we used Adam optimizer with an initial learning rate of 2e − 5 and weight decay of 0.1. All experiments were performed on a system equipped with an NVIDIA A100 GPU and at least 32GB RAM available.  Discussion  The results in Table 4 highlight the performance of different BERT models in 4 different configurations (centralized, single client, federated and FedHE) for medical NER tasks. The centralized BERTblue model is used as a baseline, representing the state-of-the-art in BERT-based models for medical named entity recognition. This model sets a high standard for comparison, demonstrating its accuracy across the datasets.  Importantly, the application of HE does not skew model effectiveness. On the con- trary, the encryption noise introduced by the encryption and decryption processes does not damage model accuracy. Instead, it sometimes even improves performance, as reflected in the bolded values in the table. The strongest hypothesis for this fact is that the small noise added by the ciphertext operations helped the model to generalize better. This indicates that HE can be effectively integrated without compromising, and potentially enhancing, the model’s performance.  The analysis also reveals that single-client learning models, such as BERTtiny and BERTmini, often achieve higher training accuracy, but generalize with less effectively compared to federated and FedHE approaches. Federated learning and FedHE models exhibit superior generalization in all corpora evaluated.  BERTtiny shows lower performance compared to BERTmini, with signficant dif- ferences in all 4 methods tested for all corpora. BERTmini, using only 11M parameters, presented satisfactory results even when compared to the more complex BERTblue with 108M parameters. This suggests that we can achieve results closer to state-of-the-art us- ing compact BERT representations without making FL+HE impractical. This work also suggests that evaluating other slightly more complex BERT variants, such as BERTsmall, could provide additional insights and potential improvements in model performance.  5. Conclusion Overall, FedHE shows a generic framework for integrating HE in a FL protocol as a strong alternative for federated medical NER tasks. FedHE offers robust performance and practical advantages, making it a compelling choice for scenarios where data pri- vacy and model effectiveness are critical. The results underscore the viability of FedHE in maintaining high performance while incorporating encryption techniques. For future work, we highlight (1) study scenarios where the number of clients is higher and the data is non-IID; (2) assess the feasibility of other compact BERT variants such as BERTsmall and (3) test the models against other LLM-based baselines such as BERTlarge blue.  6. Acknowledgements The authors would also like to thank the Universidade Federal de Ouro Preto (PROPPI/UFOP) for supporting the development of this study.   "
        },
        {
            "titulo": "Toxic Text Classification in Portuguese: Is LLaMA 3.1 8B All You Need?",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31116",
            "idioma": "Inglês",
            "storage_key": "files/article_31116_30919.pdf",
            "autores": [
                {
                    "nome": "Amanda S. Oliveira",
                    "afiliacao": "BLIP",
                    "orcid": "http://orcid.org/0009-0006-8000-7297"
                },
                {
                    "nome": "Pedro H. L. Silva",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0002-5525-6121"
                },
                {
                    "nome": "Valéria de C. Santos",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0002-7892-4954"
                },
                {
                    "nome": "Gladston Moreira",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0001-7747-5926"
                },
                {
                    "nome": "Vander L. S. Freitas",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0001-7989-0816"
                },
                {
                    "nome": "Eduardo J. S. Luz",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0001-5249-1559"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "The recognition of toxic and hate speech on social media platforms is important due to the significant risks posed to users and the digital ecosystem. Current state-of-the-art models, such as BERTimbau, have set benchmarks for Portuguese text classification, yet challenges remain in accurately detecting toxic content. This paper investigates the effectiveness of fine-tuning a smaller, open-source decoder-only model, LLaMA 3.1 8B 4bit, for this task. We propose an iterative prompt evolution method to optimize the model’s performance. Our results demonstrate that fine-tuning significantly enhances the LLaMA model’s F1-score from 0.61 to 0.75, surpassing BERTimbau in precision and matching the performance of the GPT-4o mini. However, the approach depends on the quality of the language models used for prompt evolution, highlighting the need for further research to enhance robustness in this area.",
            "keywords": [
                "Toxic Text Classification",
                "Llama",
                "LLM"
            ],
            "referencias": [
                "BehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N., and Reddy, S. (2024). Llm2vec: Large language models are secretly powerful text encoders.",
                "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.",
                "da Rocha Junqueira, J., Junior, C. L., Silva, F. L. V., Côrrea, U. B., and de Freitas, L. A.(2023). Albertina in action: An investigation of its abilities in aspect extraction, hate speech detection, irony detection, and question-answering. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 146–155. SBC.",
                "Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2023). Qlora: Efficient finetuning of quantized llms.",
                "dos Santos, W. R. and Paraboni, I. (2023). Predição de transtorno depressivo em redes sociais: Bert supervisionado ou chatgpt zero-shot? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 11–21. SBC.",
                "Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., and et al., A. F. (2024). The llama 3 herd of models.",
                "Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. (2024). Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In The Twelfth International Conference on Learning Representations.",
                "Hammes, L. O. A. and de Freitas, L. A. (2021). Utilizando bertimbau para a classificação de emoções em português. In Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL), pages 56–63. SBC.",
                "Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. (2019). Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790–2799. PMLR.",
                "Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2021). Lora: Low-rank adaptation of large language models.",
                "Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. (2024). Nv-embed: Improved techniques for training llms as generalist embedding models",
                "Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., and Stanley, K. O. (2023). Evolution through large models. In Handbook of Evolutionary Machine Learning, pages 331–366. Springer.",
                "Leite, J. A., Silva, D., Bontcheva, K., and Scarton, C. (2020). Toxic language detection in social media for brazilian portuguese: New dataset and multilingual analysis. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 914–924.",
                "Meyerson, E., Nelson, M. J., Bradley, H., Gaier, A., Moradi, A., Hoover, A. K., and Lehman, J. (2023). Language model crossover: Variation through few-shot prompting. arXiv preprint arXiv:2302.12170.",
                "Oliveira, A. S., Cecote, T. C., Alvarenga, J. P. R., Freitas, V. L. S., and Luz, E. J. S. (2024). Toxic speech detection in Portuguese: A comparative study of large language models. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 108–116, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Oliveira, A. S., Cecote, T. C., Silva, P. H., Gertrudes, J. C., Freitas, V. L., and Luz, E. J. (2023). How good is chatgpt for detecting hate speech in portuguese? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 94–103. SBC.",
                "OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., and et al., S. A. (2024). Gpt-4 technical report.",
                "Pires, R., Abonizio, H., Almeida, T. S., and Nogueira, R. (2023). Sabia: Portuguese large language models. In Naldi, M. C. and Bianchi, R. A. C., editors, Intelligent Systems, pages 226–240, Cham. Springer Nature Switzerland.",
                "Serras, F. and Finger, M. (2021). verbert: Automating brazilian case law document multilabel categorization using bert. In Anais do XIII Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 237–246, Porto Alegre, RS, Brasil. SBC.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: pretrained bert models for brazilian portuguese. In Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I 9, pages 403–417. Springer.",
                "Zheng, M., Su, X., You, S., Wang, F., Qian, C., Xu, C., and Albanie, S. (2023). Can gpt-4 perform neural architecture search? arXiv preprint arXiv:2304.10970."
            ],
            "artigo_completo": "Abstract. The recognition of toxic and hate speech on social media platforms is important due to the significant risks posed to users and the digital ecosystem. Current state-of-the-art models, such as BERTimbau, have set benchmarks for Portuguese text classification, yet challenges remain in accurately detecting toxic content. This paper investigates the effectiveness of fine-tuning a smaller, open- source decoder-only model, LLaMA 3.1 8B 4bit, for this task. We propose an iterative prompt evolution method to optimize the model’s performance. Our results demonstrate that fine-tuning significantly enhances the LLaMA model’s F1-score from 0.61 to 0.75, surpassing BERTimbau in precision and matching the performance of the GPT-4o mini. However, the approach depends on the quality of the language models used for prompt evolution, highlighting the need for further research to enhance robustness in this area.  1. Introduction The task of recognizing toxic and hate speech has gained substantial attention in recent years, particularly with the surge of user-generated content on social media platforms. As these platforms increasingly shape public discourse, the proliferation of harmful content presents significant risks to both individual users and the broader digital environment. Consequently, the need for effective moderation tools has escalated, driving research toward automated solutions capable of operating at scale.  Current state-of-the-art methods for automated toxic content classification pre- dominantly leverage transformer-based architectures, with encoder-only models be- ing the most common. Within the Portuguese language context, BERTimbau has emerged as a leading approach [Souza et al. 2020], demonstrating superior performance in various NLP tasks, including emotion classification [Hammes and de Freitas 2021], toxic speech detection [da Rocha Junqueira et al. 2023, Oliveira et al. 2023], news clus- tering [Pereira and da Silva 2023], among other tasks [dos Santos and Paraboni 2023, Serras and Finger 2021]. The BERTimbau ability to capture subtle nuances in Portuguese expressions has set a high standard in the field, making it the benchmark for multi-class classification tasks. However, despite its effectiveness, the problem of accurately classi- fying toxic content remains an open challenge, particularly in the diverse and evolving landscape of online discourse.   Recent advancements have shifted towards decoder-only models, such as LLM2Vec [BehnamGhader et al. 2024] and NV-Embed [Lee et al. 2024], which have shown promising results across multiple languages. Notably, OpenAI Chat- GPT [OpenAI et al. 2024] 1, a large decoder-only language model, has demonstrated com- petitive performance in this domain [Oliveira et al. 2023]. The emergence of open-source models, like the Meta LLaMA family of models [Dubey et al. 2024], further compels a reexamination of existing methodologies, raising research questions about the potential of these newer models.  Building on these recent developments,  this work explores the capabili- ties of decoder-only models, specifically focusing on the LLaMA 3.1 8B 4bit model [Dubey et al. 2024]. This model is particularly compelling due to its open-source nature, benchmark performance, and relatively smaller size, making it well-suited for fine-tuning specialized tasks such as toxic content classification in Portuguese. The key research questions guiding this investigation are RQ1: Can a fine-tuned LLaMA 3.1 8B 4bit model achieve or surpass the performance of GPT-4o mini in classifying toxic content in Portuguese? RQ2: Can this model outperform the current state-of-the-art BERTimbau- based approach in the same task? To address these questions, we propose a heuristic approach that utilizes a larger LLM (GPT-4o-mini) to refine the prompts employed by a smaller LLM, thereby automating prompt engineering. The optimal prompt is then used to fine-tune the LLaMA 3.1 8B 4bit model for toxic content classification in social media, using the TolDBr dataset - a large public dataset on this task [Leite et al. 2020]. Our results show that the fine-tuned LLaMA 3.1 8B 4-bit model, operating in zero-shot classification mode, outperforms the BERTimbau-based model regarding precision and is on par with GPT-4o mini.  2. Materials and Methods  Although the primary focus of this work is to investigate the performance of a small and open-source language model (with only 8B parameters) for the task of toxic text detection in Portuguese, the choice of prompt is a significant challenge. The quality of the prompt heavily influences the LLM’s performance [Brown et al. 2020]. Therefore, this work proposes a straightforward approach to evolving prompts, ultimately using the best prompt identified for fine-tuning the model.  The following subsections describe the dataset selected for benchmarking, which is a large and popular dataset by Portuguese language standards for this task. Additionally, an outline of the methodology for selecting the best prompt and the approach used for fine-tuning the model.  2.1. Told-Br dataset  We employed the ToLD-br dataset, developed in [Leite et al. 2020] for training and testing the models used in this study. This dataset contains 21,000 tweets, annotated in a binary manner as “toxic” or “non-toxic”. Additionally, the tweets are also classified into different categories of toxicity, such as LGBTphobia, insults, racism, misogyny, and xenophobia.  In this study, we focused on the binary classification between “toxic” and “non- toxic”, using the corresponding annotations to train and test our models. The dataset was  1https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/   divided in a stratified manner, with 80% of the tweets allocated to the training set and the remaining 20% to the test set.  2.2. Prompt Engineering: Iterative Prompt Refinement  The challenge in using large language models (LLMs) for zero-shot classification lies in identifying the most effective prompt. This study proposes a heuristic to iteratively refine prompts using a larger LLM, intending to enhance classification accuracy in a smaller LLM.  Our approach draws on previous research, mainly works by [Oliveira et al. 2024] and [Oliveira et al. 2023], which advocate for using in-context learning for social media post classification. While these studies explore both zero-shot and few-shot modalities, our focus remains exclusively on the zero-shot scenario.  Given that LLMs have been shown to function effectively as black-box optimiz- ers [Zheng et al. 2023] and are viable alternatives to mutation and crossover operations in genetic algorithms [Lehman et al. 2023, Meyerson et al. 2023], we draw inspiration from the work presented in [Guo et al. 2024] to propose a simplified algorithm for evolving prompts tailored explicitly to the task of toxic speech detection in Portuguese.  The methodology is structured as Figure 1 illustrates: Initially, a population of prompts is initialized, each one specifically designed to classify social media posts as toxic or non-toxic. The prompts then undergo a selection process, retaining only the top- performing ones based on evaluation metrics. Next, operations to evolve the prompt are applied utilizing an instruction to a larger LLM, such as GPT-4, which assists in generating new variations by recombining elements of existing successful prompts. This process is iteratively refined to enhance the quality of the prompts. Finally, the optimal prompt from this cycle is used to fine-tune the model. Algorithm 1 provides a pseudo-code overview of these steps.  Figure 1. The heuristic iterative prompt evolution process begins with an initial set of prompts, which are evaluated using the LLaMA model based on their F1-scores. The top-performing prompts are then selected, and GPT-4o mini generates new prompts. These new prompts are added back to the population, and the process repeats. The best prompt from this iterative cycle is ultimately selected for further use. All prompts and instructions used in this study were written in Portuguese.  2.3. LLM Fine-Tuning Methodology  This methodology fine-tunes the model using a quantized version to enhance mem- ory efficiency and speed. Parameter-Efficient Fine-Tuning (PEFT) [Houlsby et al. 2019,  Select the top kbest promptsPromptsInitializerAdd the newprompt to thePopulationSet promptsReturn thebest promptGPT-4o miniLLaMAEvaluate F1-scoreGenerate the new promptSet prompts+ ScoreTermination Population ← [] for each prompt in InitialPrompts do  Algorithm 1 Simplified Prompt Evolution 1: Function InitializePopulation(InitialPrompts) 2: 3: 4: 5: 6: 7: 8: Function GenerateNewPrompt(PromptsAndScores) 9: 10:  end for return Population  Evaluate prompt with Llama 3.1 8B, using F1-score Add prompt and its score to Population  PromptsText ← Concatenate each prompt and its F1-Score from PromptsAndScores with line breaks SystemInstruction ← “You are an assistant that helps improve AI prompts. You should always generate a new prompt, using different words or varying lengths, never repeating the same prompt. Generate ONLY the prompt, without comments or explanations”.  11:  Instruction ← “You are evolving a prompt for another LLM. Based on the following prompts and  their respective F1-scores, generate a new prompt optimized for the task of classifying hate speech”.  ChatGPTInput ← SystemInstruction + Instruction + PromptsText NewPrompt ← Call ChatGPT API with ChatGPTInput return NewPrompt  12: 13: 14: 15: Function Main() 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26:  InitialPrompts ← Define initial set of prompts Population ← InitializePopulation(InitialPrompts) for each epoch in range(NumEpochs) do  TopKPrompts ← Select top ‘k’ prompts from Population, based on F1-scores PromptsAndScores ← Collect scores and prompts from TopKPrompts NewPrompt ← GenerateNewPrompt(PromptsAndScores) Evaluate NewPrompt with Llama 3.1 8B using F1-score Add NewPrompt and its score to Population  end for BestPrompt ← Select best performing prompt from Population return BestPrompt  Hu et al. 2021] and QLoRA [Dettmers et al. 2023] techniques reduce model complexity, focusing on optimizing QKV projections and Feed Forward Layers. Training data is divided into training and validation sets. Specific prompts, structured as Alpaca prompts, align the model with toxic content classification objectives in Portuguese.  Alpaca Prompt Example: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  You are analyzing a social media post.  Instruction: text contains hate speech, offenses, aggressions, insults, swear words, or any form of toxicity, respond only with ‘yes’. If it is appropriate and non-toxic, respond ‘no’.  If the  Input: Tweet: every time the weather changes, my throat gets inflamed...dammit!  Response: no.   3. Experimental Setup and Results  Two key experiments are conducted to address the research questions posed in this study. First, Experiment #1 is designed to identify the best prompt. Using this prompt, the LLaMA 3.1 8B model is fine-tuned with the training data from the ToLD-Br dataset (Experiment #2). Four versions of the model are fine-tuned, varying parameters related to PEFT/QLoRA. An additional experiment is proposed to evaluate the performance of using a fine-tuned GPT-4o mini model within the same scenario. All experiments are performed in a Google Colab environment, utilizing an A100 GPU. Source code is available at https: //github.com/oliveiraamanda/ToxicSpeech-Llama-STIL-2024.  3.1. Experiment #1 - Prompt Engineering  To evolve the prompts using the iterative algorithm proposed here, it is essential first to define the cost function to be minimized. The F1-score of binary classification on a partition of the training data is selected as the cost function. As our population consists of natural language sentences, the initial individuals must be manually defined for the specific domain. This approach was also adopted in [Guo et al. 2024]. In this work, we base our initial prompts on those proposed in [Oliveira et al. 2023] and [Oliveira et al. 2024], as these studies serve as a baseline and address the same dataset. Subsequently, we derive additional prompts by adding or removing sentences and words, totalizing six prompts. The target model, LLaMA 3.1 8B, performs the classification using only 50 instances from each class to compute the F1-score, given the high computational cost of this function. Following this, prompts are evolved through an iterative process involving two models—the target model LLaMA 3.1 8B and GPT-4o mini over 50 epochs. After this period, the best prompt (with the highest F1-score) is selected and presented below. “Best Prompt: You are analyzing a social media post. If the text contains hate speech, offenses, aggressions, insults, swear words, or any form of toxicity, respond only with ‘yes’. If it is appropriate and non-toxic, respond ‘no’”.  3.2. Experiment #2 - LLaMA Fine-Tuning Process  To facilitate fine-tuning on modest hardware, we employed a 4-bit quantized version of the LLaMA 3.1 model, using the QLoRA technique [Dettmers et al. 2023], with 8 billion parameters model- LLaMA 3.1 8B [Dubey et al. 2024]2. We used the Hugging Face PEFT library3 with the Unsloth library4, setting the learning rate to 2e − 4 and the sequence length to 2048 tokens, while varying the “rank” and “LoRa Alpha” parameters.  The fine-tuning process used the most effective prompt and involved 3, 000 training steps, with a batch size of 2 and gradient accumulation set to 4, effectively processing 6000 instances from the training dataset.  Results from experiments varying the parameters “rank” and “LoRa alpha” are presented in Table 1, while the fine-tuning loss function using “rank=16” and “LoRa alpha=16” is shown in Figure 2.  2https://huggingface.co/unsloth/llama-3-8b-bnb-4bit 3https://huggingface.co/docs/peft/index 4https://github.com/unslothai/unsloth   3.3. Experiment #3 - GPT-4o mini Fine-Tuning Process  To fine-tune the GPT4-o mini model, we used the Azure AI Studio platform 5, leveraging the same training data used in Experiment #2. We adopted the best prompt identified in Experiment #1 and created a JSONL file where each instance of the training set was preceded by the prompt and accompanied by its respective label.  We chose the 2024-07-18 release of GPT4-o-mini, which was the one available for fine-tuning on Azure. After training, the model was deployed on the Azure platform, allowing its use through API calls.  During the evaluation, we noted that utilizing Azure Studio, which incorporates an additional content moderation layer beyond that provided by OpenAI, led to certain moderation inaccuracies. Approximately 1% of the test set was erroneously categorized due to “content moderation errors.” For these instances, we assigned the label “non-toxic.”  3.4. Results Comparison  identified through the iter- the most effective prompt For comparative purposes, ative prompt evolution approach is tested with three additional models: Mari- taca 6 AI Sabi´a3 [Pires et al. 2023], OpenAI GPT-4o mini [OpenAI et al. 2024] 7, and OpenAI ChatGPT 3.5 Turbo [Brown et al. 2020] 8, as well as the BERTimbau model [Souza et al. 2020].  The results presented in Table 2 highlight the significance of fine-tuning the LLaMA 3.1 8B model. Specifically, fine-tuning improved the F1-score from 0.61 to 0.75, demon- strating a substantial performance gain. Furthermore, when applying the fine-tuning methodology using the prompt proposed in [Oliveira et al. 2023], the F1-score reached 0.70. However, our prompt evolution approach further improved this to 0.75, indicating that the refined prompt contributed significantly to the model’s performance.  Additionally, the LLaMA 3.1 8B model, despite being fine-tuned with only 3,000 steps and 6,000 instances, performs competitively against other state-of-the-art models like GPT-4o mini, Sabi´a3, and BERTimbau. Notably, Sabi´a3, a leading model from Maritaca AI, demonstrated comparable accuracy to GPT-4o mini across various high-stakes Brazilian exams, such as OAB, ENEM, and ENADE. These results underscore the effectiveness of our prompt evolution methodology and the potential of smaller models like LLaMA 3.1 8B when paired with efficient fine-tuning techniques.  The results in Table 1 reveal differences in model performance based on the configuration of the “r” (rank) and “LoRa alpha” parameters. The configuration with “r=16” and “alpha=16” achieves the best overall performance, with an F1-Score of 0.75, balancing precision (0.69) and recall (0.83). Increasing “r” to 24 or “alpha” to 24 leads to a marked decline in performance, with the model showing symptoms of overfitting, particularly with a dramatic drop in recall. The configuration with “r=8” and “alpha=16” demonstrates high recall (0.935) but at the cost of precision, indicating a bias towards over-predicting the positive class.  5https://oai.azure.com/portal 6https://www.maritaca.ai/ 7https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/ 8https://chat.openai.com/   Figure 2. Loss over steps. Params LoRa alplha = 16 and r = 16.  Table 1. mance. Configuration  Impact of different r and LoRa alpha configurations on model perfor-  F1-Score Precision Recall Accuracy  LLaMA 3.1 8B (r=16, alpha=16)  0.75  LLaMA 3.1 8B (r=24, alpha=16)  LLaMA 3.1 8B (r=8, alpha=16)  LLaMA 3.1 8B (r=16, alpha=24)  0.432  0.727  0.327  0.69  0.601  0.595  0.573  0.83  0.338  0.935  0.229  0.76  0.609  0.690  0.584  Table 2. Comparison of Evaluation Metrics for Different Models  Model  F1-Score Precision Recall Accuracy  LLaMA 3.1 8B (original) w/ best prompt  LLaMA 3.1 8B (finetuned) w/ prompt from [Oliveira et al. 2023]  LLaMA 3.1 8B (finetuned) w/ best prompt  ChatGPT 3.5T Zero-Shot w/ prompt from [Oliveira et al. 2023]  GPT-4o mini w/ best prompt  GPT-4o mini (finetuned) w/ best prompt  Sabi´a 3 w/ best prompt  BERTimbau Finetuned  4. Conclusion  0.61  0.70  0.75  0.73  0.75  0.74  0.75  0.75  0.45  0.71  0.69  0.74  0.75  0.78  0.77  0.75  0.96  0.70  0.83  0.73  0.75  0.74  0.76  0.75  0.46  0.74  0.76  0.74  0.75  0.76  0.75  0.75  In this study, we investigated whether a smaller, open-source and quantized language model like LLaMA 3.1 8B 4 bits could effectively perform toxic text detection in Portuguese, particularly when optimized using an iterative prompt evolution approach along with finetune. The experiments demonstrated that, with carefully evolved prompts, the model could achieve competitive performance, even with a limited number of training steps and instances. This highlights the potential of smaller models when paired with efficient prompt engineering techniques.   However, the approach has its limitations. The success of the prompt evolution algorithm heavily depends on the quality of the underlying language models used for the text evolution operations. This reliance can be a significant constraint, as deficiencies in the language models directly affect the quality of the evolved prompts and, consequently, the overall model performance. Further research is needed to address these dependencies and enhance the robustness of the prompt engineering approach.  Acknowledgments  We would like to express our sincere thanks to Blip, whose generous support and invaluable assistance were crucial for the presence of the first author in the event. The authors would also like to thank the Coordenac¸ ˜ao de Aperfeic¸oamento de Pessoal de N´ıvel Superior - Brazil (CAPES) - Finance Code 001, Fundac˜ao de Amparo `a Pesquisa do Estado de Minas Gerais (FAPEMIG, grants APQ-01518-21), Conselho Nacional de Desenvolvimento Cient´ıfico e Tecnol´ogico (CNPq, grants 307151/2022-0, 308400/2022-4), and Universidade Federal de Ouro Preto (PROPPI/UFOP) for supporting the development of this study.  "
        },
        {
            "titulo": "Syntactic parsing: where are we going?",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31117",
            "idioma": "Inglês",
            "storage_key": "files/article_31117_30920.pdf",
            "autores": [
                {
                    "nome": "Lucelene Lopes",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0003-0314-140X"
                },
                {
                    "nome": "Thiago Alexandre Salgueiro Pardo",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2111-1319"
                },
                {
                    "nome": "Magali S. Duran",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-3843-4600"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "In this review & opinion paper, we discuss the options and challenges for syntactic parsing. Despite significant advances in recent years, driven primarily by neural network architectures, parsing accuracy appears to be approaching a plateau. This paper proposes a reflection on the factors that may possibly be influencing such results and suggests some future paths.",
            "keywords": [
                "Tools and Resources for NLP",
                "Syntactic representations",
                "Parsing"
            ],
            "referencias": [
                "Alves, D., Bekavac, B., and Tadić, M. (2021). Typological approach to improve dependency parsing for Croatian language. In Dakota, D., Evang, K., and Kübler, S., editors, Proceedings of the 20th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest), pages 1–11, Sofia, Bulgaria. Association for Computational Linguistics.",
                "Attardi, G., Sartiano, D., and Simi, M. (2021). Biaffine dependency and semantic graph parsing for Enhanced Universal dependencies. In Oepen, S., Sagae, K., Tsarfaty, R., Bouma, G., Seddah, D., and Zeman, D., editors, Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies, pages 184–188, Online. Association for Computational Linguistics.",
                "Branco, A., Silva, J. R., Gomes, L., and António Rodrigues, J. (2022). Universal grammatical dependencies for Portuguese with CINTIL data, LX processing and CLARIN support. In Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC), pages 5617–5626, Marseille, France. European Language Resources Association.",
                "Brigada Villa, L. and Giarda, M. (2023). Using modern languages to parse ancient ones: a test on Old English. In Beinborn, L., Goswami, K., Muradolu, S., Sorokin, A., Kumar, R., Shcherbakov, A., Ponti, E. M., Cotterell, R., and Vylomova, E., editors, Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pages 30–41, Dubrovnik, Croatia. Association for Computational Linguistics.",
                "Cassidy, L., Lynn, T., Barry, J., and Foster, J. (2022). TwittIrish: A Universal Dependencies treebank of tweets in Modern Irish. In Muresan, S., Nakov, P., and Villavicencio, A., editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6869–6884, Dublin, Ireland. Association for Computational Linguistics.",
                "de Lhoneux, M., Stymne, S., and Nivre, J. (2017). Arc-hybrid non-projective dependency parsing with a static-dynamic oracle. In Miyao, Y. and Sagae, K., editors, Proceedings of the 15th International Conference on Parsing Technologies, pages 99–104, Pisa, Italy. Association for Computational Linguistics.",
                "de Marneffe, M.-C., Manning, C. D., Nivre, J., and Zeman, D. (2021). Universal Dependencies. Computational Linguistics, 47(2):255–308.",
                "Dione, C. M. B. (2021). Multilingual dependency parsing for low-resource African languages: Case studies on Bambara, Wolof, and Yoruba. In Oepen, S., Sagae, K., Tsarfaty, R., Bouma, G., Seddah, D., and Zeman, D., editors, Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies, pages 84–92, Online. Association for Computational Linguistics.",
                "Dozat, T. and Manning, C. D. (2016). Deep biaffine attention for neural dependency parsing. CoRR, abs/1611.01734.",
                "Duran, M., das Graças Nunes, M., and Pardo, T. A. (2023a). Construções sintáticas do português que desafiam a tarefa de parsing: uma análise qualitativa. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 424–433, Porto Alegre, RS, Brasil. SBC.",
                "Duran, M. S., Nunes, M. d. G. V., and Pardo, T. A. S. (2023b). Avaliação qualitativa do analisador sintático udpipe 2 treinado sobre o córpus jornalístico porttinari-base. Technical report, Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo.",
                "Fernández-González, D. and Gómez-Rodríguez, C. (2023). Dependency parsing with bottom-up hierarchical pointer networks. Information Fusion, 91:494–503.",
                "Gamba, F. and Zeman, D. (2023). Universalising Latin Universal Dependencies: a harmonisation of Latin treebanks in UD. In Grobol, L. and Tyers, F., editors, Proceedings of the Sixth Workshop on Universal Dependencies (UDW, GURT/SyntaxFest), pages 7–16, Washington, D.C. Association for Computational Linguistics.",
                "Ghiffari, F. A. A., Alfina, I., and Azizah, K. (2023). Cross-lingual transfer learning for Javanese dependency parsing. In Li, D., Mahendra, R., Tang, Z. P., Jang, H., Murawaki, Y., and Wong, D. F., editors, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 1–9, Nusa Dua, Bali. Association for Computational Linguistics.",
                "Goldberg, Y. (2016). A primer on neural network models for natural language processing. J. Artif. Int. Res., 57(1):345–420.",
                "Kabiri, R., Karimi, S., and Surdeanu, M. (2022). Informal Persian Universal Dependency treebank. In Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC), pages 7096–7105, Marseille, France. European Language Resources Association.",
                "Kondratyuk, D. and Straka, M. (2019). 75 languages, 1 model: Parsing Universal Dependencies universally. In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2779–2795, Hong Kong, China. Association for Computational Linguistics.",
                "Lopes, L. and Pardo, T. (2024). Towards portparser - a highly accurate parsing system for Brazilian Portuguese following the Universal Dependencies framework. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 401–410, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Lusito, S. and Maillard, J. (2021). A Universal Dependencies corpus for Ligurian. In de Lhoneux, M. and Tsarfaty, R., editors, Proceedings of the Fifth Workshop on Universal Dependencies (UDW, SyntaxFest), pages 121–128, Sofia, Bulgaria. Association for Computational Linguistics.",
                "Mrini, K., Dernoncourt, F., Bui, T., Chang, W., and Nakashole, N. (2019). Rethinking self-attention: An interpretable self-attentive encoder-decoder parser. CoRR, abs/1911.03875.",
                "Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajič, J., Manning, C. D., McDonald, R., Petrov, S., Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D. (2016). Universal Dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC), pages 1659–1666, Portoroz, Slovenia. ELRA.",
                "Pedrazzini, N. and Eckhoff, H. M. (2021). Oldslavnet: A scalable early slavic dependency parser trained on modern language data. Software Impacts, 8:100063.",
                "Qi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. (2020). Stanza: A python natural language processing toolkit for many human languages. In Celikyilmaz, A. and Wen, T.-H., editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 101–108, Online. Association for Computational Linguistics.",
                "Sánchez-Rodríguez, X., Sarymsakova, A., Castro, L., and Garcia, M. (2024). Increasing manually annotated resources for Galician: the parallel Universal Dependencies treebank. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 587–592, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Straka, M. (2018). UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 197–207.",
                "Straka, M., Hajič, J., and Straková, J. (2016). UDPipe: Trainable pipeline for processing CoNLL-U files performing tokenization, morphological analysis, POS tagging and parsing. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC), pages 4290–4297, Portorǒz, Slovenia. European Language Resources Association (ELRA).",
                "Ustün, A., Bisazza, A., Bouma, G., and van Noord, G. (2020). UDapter: Language adaptation for truly Universal Dependency parsing. In Webber, B., Cohn, T., He, Y., and Liu, Y., editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2302–2315, Online. Association for Computational Linguistics.",
                "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
                "Yshaayahu Levi, D. and Tsarfaty, R. (2024). A truly joint neural architecture for segmentation and parsing. In Graham, Y. and Purver, M., editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1408–1420, St. Julian’s, Malta. Association for Computational Linguistics."
            ],
            "artigo_completo": "Abstract. In this review & opinion paper, we discuss the options and challenges for syntactic parsing. Despite significant advances in recent years, driven pri- marily by neural network architectures, parsing accuracy appears to be ap- proaching a plateau. This paper proposes a reflection on the factors that may possibly be influencing such results and suggests some future paths.  Motivation  The importance of good part of speech tagging and parsing annotation tools for down- stream Natural Language Processing (NLP) tasks is acknowledged by several publications in the history of the area, including both more classic (symbolic and statistic) approaches and new (usually neural-based) ones. In particular, the rise of “Universal Dependencies” (UD) framework1 [Nivre et al. 2016, de Marneffe et al. 2021] has sparked renewed inter- est in dependency parsing, driving new efforts in syntax studies and parsing in NLP.  This review & opinion paper attempts to draw a landscape of more recent pars- ing efforts that align to UD standards, trying to figure out the potential limits of the task with current methods and what other strategies might be adopted for keeping improving the achieved results in the area. Such initiative is bold and naturally subject to failure, as natural languages have diverse characteristics and there are always new NLP meth- ods emerging. Knowing this, this article makes a selection of works from the literature, choosing relatively recent and widely cited approaches in the area in order to draw some tentative (and certainly temporally anchored) conclusions.  Besides the possibly interesting work selection and overview that supported this paper, our contribution includes an exercise of “keeping the head above water”, showing how far we have come and the imperfections of the landscape.  On current parsing techniques  The use of neural networks for detection of patterns, and consequently, the prediction of part of speech tags and dependency relations became the preferred method in the area [Goldberg 2016]. Within neural networks, several techniques as Long Short-Term Mem- ory (LSTM) in its various versions [Van Houdt et al. 2020], together with other deep learning techniques [Dozat and Manning 2016], have been employed in the last decade with consistent advances for well resourced languages. The latest evolution brought by the self-attention methods [Vaswani et al. 2017], based on the famous Transformers, goes back a few years now, but it is still one of the main reasons for recent improvements.  1https://universaldependencies.org/   Overall, although different criteria could be used, in this paper we distinguish the parsing efforts according to the generic parsing tools or specific language parsing initia- tives; and basic technology employed (e.g., BiLSTM, Deep Biaffine, and Self-Attention).  The more popular parsing tools, within UD standard, are the UDPipe in its ver- sions 1.3 and 2.0 [Straka et al. 2016, Straka 2018], Stanza pipeline [Qi et al. 2020], UD- ify [Kondratyuk and Straka 2019], and AllenNLP pipeline [Dozat and Manning 2016]. Other tools were developed, but apparently had fewer number of users, as the Diaparser [Attardi et al. 2021], UDapter [ ¨Ust¨un et al. 2020], UUParser [de Lhoneux et al. 2017], LAL-Parser [Mrini et al. 2019], and Hierarchical Pointer Net- work algorithm [Fern´andez-Gonz´alez and G´omez-Rodr´ıguez 2023].  less popular  These parsers usually focus their efforts to cover several languages, being clearly multilingual. Some of these tools were specifically designed to cover the large set of languages available at the UD repository (which currenlty includes over 150 languages). However, from a technological point of view, the tools have considerable differences, although all of them make use of neural network models.  The technology of Bidirectional LSTM (BiLSTM) [Van Houdt et al. 2020] is fre- quently employed by many systems, including UDPipe 2.0, Stanza, and Hierarchical Pointer Networks algorithm. The Deep Biaffine technology [Dozat and Manning 2016] is found in AllenNLP pipeline, but also in tools as Diaparser and UDapter. Self-attention [Vaswani et al. 2017] is found in LAL-Parser and UDify tools. Additionally, the men- tioned tools show differences on offering a static model or the possibility to perform model construction through a training set and/or to adopt pre-trained word embeddings.  Parsing results  The best values reported for each of the previously cited parsing methods are shown in Table 1. We chose to report only the Label Attachment Score (LAS), as this is usually the most adopted evaluation metric and also one of the most punitive metrics, as it measures the accuracy of the dependency relation identification and the tokens related as head and dependent. The table also indicates the language for which the highest LAS was reported.  Table 1. Highest LAS reported for the generic parsing tools.  parsing system UUParser Stanza UDPipe 1.3 UDapter Diaparser UDify UDPipe 2.0 AllenNLP pipeline Hier. Pointer Networks LAL-parser  highest LAS 87.34% 90.01% 91.20% 92.60% 93.65% 93.70% 94.53% 94.60% 96.15% 96.29%  language Portuguese Spanish Hindi Italian Italian Russian Russian English English English  cited technology publication  BiLSTM Deep Biaffine NN Classifier Deep Biaffine Deep Biaffine Self-Attention BiLSTM Deep Biaffine BiLSTM Self-Attention  2017 2020 2016 2020 2021 2019 2018 2016 2023 2019  The performance of the parsing methods vary considerably according to the lan- guage to which they are applied, as the scientific literature has shown. For example,   for UDPipe 2, the reported LAS for Spanish and Italian can be as low as 80.68% and 77.34%, respectively. For AllenNLP pipeline, LAS for Chinese and Spanish was 85.38% and 91.65%, respectively. The values shown in the table may also reflect the number of tested languages. While UDify and UDPipe test over more than 70 languages, AllenNLP pipeline, UUParser, and LAL-parser test for only 6, 5, and 2 languages, respectively.  Focusing only on the highest LAS accuracy as presented in Table 1, it is notice- able that the majority of the highest scores are over 90% of accuracy. These numbers suggest that the State Of The Art (SOTA) for LAS is attainable despite of the technology employed, date of publication, and even specificity of each parsing development. Observ- ing the three best reported results, we see different techniques and that English shows the best scores (probably because English is the best resourced language).  This fact suggests that, after the spread of neural network-based models, the qual- ity of the training model plays a more important role than the specific technology em- ployed. As such, the variations for different languages seem to reflect the quality of the training data for each language. For example, LAS for UDify for a low resourced lan- guage as Breton is as low as 40.19%, which is much lower than the 93.70% maximum attained for Russian.  Fortunately, the literature is abundant in terms of efforts for specific languages. These works usually are presented either with the construction of a specific corpus for the target language, or transferring learning from a better resourced language towards the low resourced one. Observing the works dedicated to specific languages, we found a reasonable number of publications, some of which are summarized in Table 2.  language Yoruba  work [Dione 2021] [Brigada Villa and Giarda 2023] [Cassidy et al. 2022] [Lusito and Maillard 2021] [Baig et al. 2021] [Dione 2021] [T¨urk et al. 2022] [Ghiffari et al. 2023] [Pedrazzini and Eckhoff 2021] [S´anchez-Rodr´ıguez et al. 2024] [Alves et al. 2021] [Branco et al. 2022] [Kabiri et al. 2022] [Gamba and Zeman 2023] [Lopes and Pardo 2024]  Table 2. Highest LAS reported by specific language efforts. overall approach LAS 31.43% Transfer learning 58.70% Old English Transfer learning Transfer learning 59.34% Indonesian Corpus building Ligurian 60.74% Urdu 62.90% Corpus building Transfer learning Wolof 67.83% Corpus building Turkish 76.04% 79.22% Corpus building Irish Transfer learning 79.66% Old Slavic 84.31% Corpus building Galician Transfer learning Croatian 89.09% Corpus building 92.54% Portuguese Corpus building 92.68% Corpus building 94.61% Corpus building 94.70% Portuguese  Persian Latin  The examples summarized in Table 2 show efforts that can be grouped into at- tempts to serve very low resourced languages (as Old English, Old Slavic, Ligurian, Urdu, Bambara, Wolof, and Indonesian) and low resourced languages (as Turkish, Croatian, Galician, Irish, Persian, Latin, and Portuguese). While the very low resourced languages   attempts are mostly based on transfer learning, the languages better resourced mostly center the efforts in building better corpora to be used to train specific models.  The observation of LAS in Table 2 shows that the best reported results are also above the 90% score of the generic parsing methods (Table 1). Obviously, the hard cases, as Yoruba and Old English, show low accuracy despite the efforts, probably because they are low-resourced languages. However, it is noticeable the accuracy achieved by transfer learning for Old Slavic and Croatian, as well as the high values for Persian, Latin, and Portuguese with the production of high quality training corpora.  Where can we head to?  The advent of popular neural network methods in the last decade has brought impressive progress in several areas of NLP, bringing Artificial Intelligence to the center of topics in all areas of the human knowledge. For parsing tasks, specifically, using UD standards, we notice the increase of quality since 2016. However, improvements seem to reach a limit up to 96% accuracy, and it is noticeable that no specificity show a clear predominance.  It is also well known that languages with few resources may not be able to benefit from the advantages of SOTA methods. It would be better for theses lan- guages to invest in more classic methods or in the improvement of resources through corpora building including careful annotation. Specific techniques like data augmen- tation and joint task resolution may also be interesting ways (see, e.g., the work of [Yshaayahu Levi and Tsarfaty 2024] for Hebrew parsing). Such paths may also be rel- evant for languages already reaching accuracy around 95%, i.e., already delivering SOTA results.  Another relevant question is if the search for a better accuracy (over 96%) is a re- alistic goal. Should we make our peace with these missing 4% due to a natural inaccuracy of dependency annotation? Looking at the best method for a specific language (Por- tuguese), the authors [Lopes and Pardo 2024] [Duran et al. 2023a] [Duran et al. 2023b] discuss some reasons for the remaining errors that are also cited in the literature: under- represented phenomena in the training corpus (that might be solved by data augmentation and/or more corpus annotation) and difficult annotation issues (as to decide which is the head of a prepositional phrase) that sometimes may challenge even the humans. Person- ally, we believe that the above 99% accuracy already achieved for part of speech tagging may be achieved for parsing too. However, it may require to simplify some syntactic distinctions or to look for new approaches to the parsing problem.  The interested reader may find more information at the POeTiSA project web  portal: https://sites.google.com/icmc.usp.br/poetisa  Acknowledgements  This work was carried out at the Center for Artificial Intelligence of the University of S˜ao Paulo (C4AI - http://c4ai.inova.usp.br/), with support by the S˜ao Paulo Research Foundation (FAPESP grant #2019/07665-4) and by the IBM Corporation. The project was also supported by the Ministry of Science, Technology and Innovation, with resources of Law N. 8,248, of October 23, 1991, within the scope of PPI-SOFTEX, coor- dinated by Softex and published as Residence in TIC 13, DOU 01245.010222/2022-44.   "
        },
        {
            "titulo": "A Robustness Analysis of Automated Essay Scoring Methods",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31118",
            "idioma": "Inglês",
            "storage_key": "files/article_31118_30921.pdf",
            "autores": [
                {
                    "nome": "Rafael T. Anchiêta",
                    "afiliacao": "IFPI",
                    "orcid": "http://orcid.org/0000-0003-4209-9013"
                },
                {
                    "nome": "Rogério F. de Sousa",
                    "afiliacao": "IFPI",
                    "orcid": "https://orcid.org/0000-0003-4589-6157"
                },
                {
                    "nome": "Raimundo S. Moura",
                    "afiliacao": "UFPI",
                    "orcid": "https://orcid.org/0000-0002-1558-3830"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Automated Essay Scoring",
                "Robustness",
                "Adversarial Essays"
            ],
            "referencias": [
                "Barzilay, R. and Lapata, M. (2008). Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.",
                "Beigman Klebanov, B. and Madnani, N. (2020). Automated evaluation of writing – 50 years and counting. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7796–7810, Online. Association for Computational Linguistics.",
                "Cohen, J. (1968). Weighted kappa: nominal scale agreement provision for scaled disagreement or partial credit. Psychological bulletin, 70(4):213–220.",
                "de Sousa, R. F., Marinho, J. C., Neto, F. A. R., Anchiêta, R. T., and Moura, R. S. (2024). PiLN at PROPOR: A BERT-based strategy for grading narrative essays. In Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 2, pages 10–13, Santiago de Compostela, Galicia/Spain. Association for Computational Linguistics.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "Higgins, D. and Heilman, M. (2014). Managing what we can measure: Quantifying the susceptibility of automated scoring systems to gaming behavior. Educational Measurement: Issues and Practice, 33(3):36–46.",
                "Kabra, A., Bhatia, M., Singla, Y. K., Jessy Li, J., and Ratn Shah, R. (2022). Evaluation toolkit for robustness testing of automatic essay scoring systems. In Proceedings of the 5th Joint International Conference on Data Science & Management of Data, pages 90–99, Bangalore, India. Association for Computing Machinery.",
                "Liu, R., Wang, X., Liu, J., and Zhou, J. (2024). A comprehensive analysis of evaluating robustness and generalization ability of models in aes. In Proceedings of the 7th International Symposium on Big Data and Applied Statistics, pages 1–5, Beijing, China. IOP Publishing.",
                "Marinho, J. C., Anchiêta, R. T., and Moura, R. S. (2021). Essay-br: a brazilian corpus of essays. In XXXIV Simpósio Brasileiro de Banco de Dados: Dataset Showcase Workshop, SBBD 2021, pages 53–64, Online. SBC.",
                "Marinho, J. C., Anchiêta, R. T., and Moura, R. S. (2022a). Essay-br: a brazilian corpus to automatic essay scoring task. Journal of Information and Data Management, 13(1):65–76.",
                "Marinho, J. C., C., F., Anchiêta, R. T., and Moura, R. S. (2022b). Automated essay scoring: An approach based on enem competencies. In Anais do XIX Encontro Nacional de Inteligência Artificial e Computacional, pages 49–60, Campinas, Brazil. SBC.",
                "Mello, R. F., Oliveira, H., Wenceslau, M., Batista, H., Cordeiro, T., Bittencourt, I. I., and Isotanif, S. (2024). PROPOR’24 competition on automatic essay scoring of Portuguese narrative essays. In Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 2, pages 1–5, Santiago de Compostela, Galicia/Spain. Association for Computational Linguistics.",
                "Oliveira, H., Ferreira Mello, R., Barreiros Rosa, B. A., Rakovic, M., Miranda, P., Cordeiro, T., Isotani, S., Bittencourt, I., and Gasevic, D. (2023). Towards explainable prediction of essay cohesion in portuguese and english. In Proceedings of the 13th International Learning Analytics and Knowledge Conference, pages 509–519, Arlington TX USA. Association for Computing Machinery.",
                "Page, E. B. (1966). The imminence of... grading essays by computer. The Phi Delta Kappan, 47(5):238–243.",
                "Perelman, L. (2014). When “the state of the art” is counting words. Assessing Writing, 21:104–111.",
                "Tay, Y., Phan, M., Tuan, L. A., and Hui, S. C. (2018). Skipflow: Incorporating neural coherence features for end-to-end automatic text scoring. In Proceedings of the Thirty-second AAAI conference on artificial intelligence, pages 5948–5955, New Orleans, Louisiana, USA. AAAI Press.",
                "Yannakoudakis, H. and Cummins, R. (2015). Evaluating the performance of automated text scoring systems. In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 213–223, Denver, Colorado. Association for Computational Linguistics.",
                "Yoon, S.-Y., Cahill, A., Loukina, A., Zechner, K., Riordan, B., and Madnani, N. (2018). Atypical inputs in educational applications. In Bangalore, S., Chu-Carroll, J., and Li, Y., editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), pages 60–67, New Orleans - Louisiana. Association for Computational Linguistics."
            ],
            "artigo_completo": "Abstract. This paper analyzed the robustness of a state-of-the-art Automated Essay Scoring (AES) model by applying various linguistically motivated pertur- bations to the Essay-BR corpus. Our findings reveal that the AES model failed to detect these adversarial modifications, often assigning higher scores to the disturbed essays than to the original ones.  1. Introduction  Automated Essay Scoring (AES) aims to provide computational models for automati- cally grading essays or with minimal involvement of humans [Page 1966]. Although this research area is over fifty years old [Beigman Klebanov and Madnani 2020], it has re- cently gained the attention of the Brazilian community because of publicly available cor- pora [Marinho et al. 2021, Marinho et al. 2022a]. Several methods to grade an essay or its characteristics arose based on these resources [de Sousa et al. 2024, Oliveira et al. 2023, Marinho et al. 2022b]. Besides, there is a growing interest in the area. For instance, recently occurred the PROPOR’24 Competition, whose goal was to develop computer systems capable of automatically evaluating essays [Mello et al. 2024].  Despite the advances achieved, the Brazilian community has made little effort to evaluate the robustness of AES methods, including analyzing their sensitivity to adver- sarial perturbations. [Liu et al. 2024] define robustness as the capacity to remain stable and reliable under different circumstances. Studies demonstrate that AES methods for the English language are easily fooled [Perelman 2014], reducing the trustworthiness of AI-based automated scoring systems [Kabra et al. 2022]. Based on these limitations of AES methods for English, we investigated whether AES methods for Portuguese suffer from robustness problems.  Our objective is to analyze AES methods using adversarial essays. For that, we applied a set of perturbations to an essay corpus, including adding unrelated texts, shuf- fling, deleting, and repeating paragraphs of an essay. With these linguistically-motivated disturbances, we evaluated a state-of-the-art AES strategy for Portuguese and found that the analyzed model could not deal with adversarial essays, producing, in fact, better re- sults for undisturbed essays.  The remainder of this paper is organized as follows: Section 2 briefly presents related work. In Section 3, we detailed the performed analysis to verify the robustness of an Automated Essay Scoring method for Portuguese. Finally, Section 4 concludes the paper and indicates future directions.   2. Related Work  [Kabra et al. 2022] proposed a model agnostic adversarial evaluation scheme and asso- ciated metrics for AES systems to test their natural language understanding capabilities and overall robustness. They evaluated models ranging from feature-engineering-based approaches to the latest deep-learning algorithms. The authors found that AES models are highly overstable such that even heavy modifications (as much as 25%) with content unrelated to the topic of the questions do not decrease the score produced by the models.  [Liu et al. 2024] evaluated Automatic Essay Scoring models’ robustness and gen- eralization capabilities through a comprehensive series of experiments to validate various models’ efficacy. The authors randomly select a part of the essays and shuffle the order of the sentences or delete a sentence randomly to construct a Chinese adversarial sample set for evaluating the robustness of the models. The results showed that the advanced AES models have poor robustness and generalization ability, and Large Language Models have better performance but still need to be improved.  3. Robustness Analysis  The Essay-BR corpus [Marinho et al. 2021] is organized into training, development, and testing sets, each with 3,198, 686, and 686 essays. We used the test set to generate adver- sarial essays. First, we extracted the essays with a score greater than or equal to 680 since the average score of ENEM 2023 was 641.6 points1, resulting in 305 essays. We adopted the strategy of selecting the best essays, avoiding those with several grammatical, struc- tural, and argumentative issues. After that, we applied several perturbations to the essays to produce adversarial essays. From the original and adversarial essays set, we evaluated the robustness of an Automated Essay Scoring (AES) model.  We implemented linguistically motivated perturbations to analyze the robustness of an AES model, i.e., to check whether the model can detect any difference between original and modified responses. The perturbations are detailed below.  Add unrelated text. We added an unrelated paragraph in each essay. We create three sets of essays with unrelated content, each indicating the position where an unrelated text was added. The sets are with unrelated texts added at the beginning, middle, and end of the essays. We extracted a paragraph from essays with a prompt differ- ent from the analyzed essay and added it to the essay. This test tries to mimic the behavior of students when they make their responses lengthy by adding irrelevant information.  Add song and cake recipe. Although these perturbations add unrelated content to an es- say, they have a very different language structure than written prose in essays. So, this can be used to test a system negatively. Furthermore, it has been observed that students use this strategy in their exams, possibly in an attempt to fool the system2. We created two sets of perturbations, one for cake recipe and the other for the song. In both sets, we add unrelated content in the middle of the essays.  1https://querobolsa.com.br/revista/redacao-enem-2023-quantos-texto  s-tiraram-nota-mil-quantos-zeraram  2https://g1.globo.com/educacao/noticia/2013/03/queria-testar-correca  o-do-enem-diz-jovem-que-pos-receita-na-redacao.html   Add repeated text. For this adversarial strategy, we also created three sets of pertur- bations. For each set, we repeated the essay content at the beginning, middle, and end of the essays. The motivation for this perturbation is that, according to [Kabra et al. 2022], students sometimes repeat sentences or specific keywords in their responses to make them longer yet not out of context and to fashion cohe- sive paragraphs [Higgins and Heilman 2014, Yoon et al. 2018].  Delete text. Similar to adding repeated text, we created three sets of perturbations in this strategy. For each set, we removed a paragraph at the beginning, middle, and end of the essays. According to [Kabra et al. 2022], these tests generally break the flow of an argument, delete crucial details from an essay, and decrease wordiness. This perturbation can seriously detract from the coherency and quality of writing and frustrate readers.  Shuffle text. For this perturbation, we randomly shuffle the content of an essay. The motivation for this adversarial strategy is to analyze important aspects of es- say scoring, such as coherence and organization, which measure the extent to which the response demonstrates a unified structure and direction of the narra- tive [Barzilay and Lapata 2008, Tay et al. 2018].  After generating adversarial essays, we evaluated a state-of-the-art automated es- say scoring [de Sousa et al. 2024] based on the BERT model [Devlin et al. 2019]. We as- sessed that model using each ENEM competency through the Quadratic Weighted Kappa (QWK) metric [Cohen 1968] for original and adversarial essays. QWK is a metric com- monly used to assess AES models [Yannakoudakis and Cummins 2015]. Table 1 shows the results on the original essays, and Table 2 presents the results on the adversarial essays.  Tables 1 and 2, from C1 to C5, indicate the five competencies of the ENEM, and the total is the final grade for an essay. In Table 2, we highlight the values greater than or equal to the value of the original essays.  Analyzing the values from the two tables, we can see that only the values of adding text at the beginning and adding a cake recipe were not greater than the original essay values, indicating that the AES model was able to identify perturbations in the essays, penalizing their scores. On the other hand, the scores for adding unrelated text in the mid- dle, in the end, and a song were greater than or equal to the values for the original essays. An interesting finding is that, despite adding an unrelated text at the end of an essay, the C5 score was not penalized. Competency 5 of the ENEM is dedicated to elaborating a proposal to solve the problem. The proposal normally appears at the end of an essay, and the AES model could not detect the unrelated content added to an essay. More than that, the final grade of original and adversarial essays had the same QWK value, suggesting that the AES model failed to capture this perturbation.  For the perturbation of repeating a text in the essay, the AES model graded the original and adversarial essays with the same score, mainly in competence four. Com- petence 4 evaluates the superficial structure of the text, that is, how the sentences and paragraphs are linked through cohesive elements. This way, the AES model should nega- tively score such responses. Besides, the scores for repeating a text in the middle and at the end of an essay had the same value as the original essays.  Another interesting finding is that deleting some parts of the essay improves its grade in various competencies. As we can see, the scores for deleting a text in the essay   are greater than or equal to the scores of the original essays, including the final score. These results show that the AES model could not identify a break in the flow of an argu- ment when essential parts of the essay were removed.  Finally, and perhaps the most interesting finding, is that shuffling the paragraphs of an essay produces better results than the original essays. This result demonstrates that the AES model could not determine the cohesion and coherence of the essays. That is, the AES model did not identify the transition between the lines of the essays, verifying disconnected ideas that change the meaning substantially.  The source code of the AES model and for generating adversarial essays are pub-  licly available at https://github.com/liara-ifpi/essay-robustness.  Table 1. Quadratic Weighted Kappa results on the original essays.  C1  C2  C3  C4  C5 Total  0.44  0.23  0.29  0.24  0.62  0.46  Table 2. Quadratic Weighted Kappa results on the adversarial essays. C1  Adversarial strategy  C5 Total  C4  C3  C2  0.14 Add unrelated text at the begging 0.41 0.42 Add unrelated text in the middle 0.18 0.43 0.22 Add unrelated text at the end 0.21 0.38 Add song 0.18 0.38 Add cake recipe 0.44 0.20 Repeat text at the begging 0.21 0.43 Repeat text in the middle 0.23 0.43 Repeat text at the end 0.40 Delete text at the begging 0.19 0.40 0.23 Delete text in the middle 0.41 0.23 Delete text at the end 0.47 0.20 Shuffle text  0.15 0.24 0.28 0.23 0.22 0.18 0.23 0.24 0.24 0.27 0.28 0.24 0.30 0.22 0.25 0.29 0.24 0.29 0.24 0.29  0.17 0.57 0.62 0.20 0.62 0.24 0.20 0.64 0.61 0.59 0.61 0.62 0.66 0.66 0.63 0.64  0.40 0.44 0.46 0.42 0.41 0.44 0.46 0.46 0.45 0.46 0.46 0.47  4. Conclusion  This paper presented a robustness analysis for automatic essay scoring focusing on the Portuguese language. We used the Essay-BR corpus, which is based on the ENEM com- petencies, to perform that analysis. Our strategy was to add several perturbations to produce adversarial essays, aiming to check if a state-of-the-art automated essay scor- ing model can detect any difference between original and modified responses. From the analysis, we have learned that the automated essay scoring model could not identify the perturbations in the essays, producing scores that were even greater than the original re- sponses. We hope that this analysis sheds light on this research area and helps develop more robust strategies for automatically grading essays.  For future work, we intend to develop more perturbations and create a toolkit to  facilitate the creation of adversarial essays.   "
        },
        {
            "titulo": "Avaliação de Algoritmos de Clusterização para Agrupamento de Descrições de Produtos em Notas Fiscais Eletrônicas",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31119",
            "idioma": "Português",
            "storage_key": "files/article_31119_30922.pdf",
            "autores": [
                {
                    "nome": "Jonas Gabriel L. de Araújo",
                    "afiliacao": "UFPB",
                    "orcid": "http://orcid.org/0009-0006-1736-5593"
                },
                {
                    "nome": "Thaís G. do Rêgo",
                    "afiliacao": "UFPB",
                    "orcid": "https://orcid.org/0000-0002-6608-4900"
                },
                {
                    "nome": "Yuri de A. M. Barbosa",
                    "afiliacao": "UFPB",
                    "orcid": "https://orcid.org/0000-0002-7779-0288"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "A nota fiscal eletrônica é essencial para o processo de auditoria fiscal. Este artigo avalia a eficácia de algoritmos de clusterização para agrupar descrições de produtos em notas fiscais eletrônicas, um desafio devido à falta de padronização nos registros. Usando similaridade de strings e ajustes para unidades de medida, foram testados DBSCAN, HDBSCAN, OPTICS e Agglomerative Clustering. As métricas de avaliação incluíram o Coeficiente de Silhueta, Índice de Calinski-Harabasz e a porcentagem de produtos agrupados. O HDBSCAN apresentou o melhor desempenho inicial, e a subclusterização, apesar de melhorar as métricas, introduziu inconsistências nos agrupamentos.",
            "keywords": [
                "Algoritmos de Clusterização",
                "Notas Fiscais Eletrônicas",
                "Similaridade de Strings",
                "Descrições de Produtos",
                "Auditoria Fiscal"
            ],
            "referencias": [
                "Ahmed, M., Tiun, S., Omar, N., and Sani, N. S. (2022). Short text clustering algorithms, application and challenges: A survey. Applied Sciences.",
                "Ester, M., Kriegel, H.-P., Sander, J., Xu, X., et al. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. In kdd, volume 96, pages 226–231",
                "Jaro, M. A. (1989). Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida. Journal of the American Statistical Association, 84(406):414–420.",
                "Mazzarolo, J., Steinmetz, R., and Mergen, S. (2022). Um estudo sobre a falta de padronização na descrição de produtos em notas fiscais eletrônicas. In Anais da XVII Escola Regional de Banco de Dados, pages 31–40, Porto Alegre, RS, Brasil. SBC.",
                "Ribeiro, L., Brandão, W., Marques, I., Andrade, P., Júnior, R., Oliveira, F., and Kelles, R. (2018). Reconhecimento de entidades nomeadas em itens de produto da nota fiscal eletrônica. 36:116–126.",
                "Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20:53–65.",
                "Schulte, J. P., Giuntini, F. T., Nobre, R. A., Nascimento, K. C. d., Meneguette, R. I., Li, W., Gonçalves, V. P., and Rocha Filho, G. P. (2022). Elinac: Autoencoder approach for electronic invoices data clustering. Applied Sciences, 12(6).",
                "Steinbach, M., Karypis, G., and Kumar, V. (2000). A comparison of document clustering techniques.",
                "."
            ],
            "artigo_completo": "Resumo. A nota fiscal eletrˆonica ´e essencial para o processo de auditoria fis- cal. Este artigo avalia a efic´acia de algoritmos de clusterizac¸ ˜ao para agrupar descric¸ ˜oes de produtos em notas fiscais eletrˆonicas, um desafio devido `a falta de padronizac¸ ˜ao nos registros. Usando similaridade de strings e ajustes para unidades de medida, foram testados DBSCAN, HDBSCAN, OPTICS e Agglome- rative Clustering. As m´etricas de avaliac¸ ˜ao inclu´ıram o Coeficiente de Silhueta, ´Indice de Calinski-Harabasz e a porcentagem de produtos agrupados. O HDBS- CAN apresentou o melhor desempenho inicial, e a subclusterizac¸ ˜ao, apesar de melhorar as m´etricas, introduziu inconsistˆencias nos agrupamentos.  1. Introduc¸ ˜ao  As Notas Fiscais Eletrˆonicas (NF-e) s˜ao um marco na modernizac¸ ˜ao dos processos fis- cais no Brasil, ao melhorar o controle e a fiscalizac¸ ˜ao tribut´aria, o que j´a resultou em avanc¸os na arrecadac¸ ˜ao de impostos e no processo de auditoria [Vieira et al. 2019, Neto and Lopo Martinez 2016]. No entanto, a an´alise dessas notas enfrenta desafios devido `a falta de padronizac¸ ˜ao nas descric¸ ˜oes de produtos, com erros ortogr´aficos, abreviac¸ ˜oes e variac¸ ˜oes nas unidades de medida [Mazzarolo et al. 2022]. Essa incon- sistˆencia dificulta a organizac¸ ˜ao e comparac¸ ˜ao de dados, exigindo t´ecnicas computacio- nais para agrupar descric¸ ˜oes similares e auxiliar na auditoria fiscal, que requer a corres- pondˆencia entre o invent´ario das empresas e as notas emitidas por elas. Dessa forma, o uso de algoritmos de agrupamento facilita a fiscalizac¸ ˜ao e melhora a eficiˆencia do processo [Ribeiro et al. 2018].  Neste contexto,  este estudo busca avaliar algoritmos de agrupamento, como DBSCAN [Ester et al. 1996], HDBSCAN [Campello et al. 2013], OPTICS [Ankerst et al. 1999] e Agglomerative Clustering (AGG) [Steinbach et al. 2000], para   agrupar descric¸ ˜oes de produtos e identificar quais algoritmos oferecem o melhor desem- penho na organizac¸ ˜ao e interpretac¸ ˜ao dos dados. Para isso, foi empregada uma m´etrica personalizada no c´alculo da matriz de distˆancias, baseada em similaridade entre strings e a an´alise de uma segunda etapa de agrupamento dos dados.  2. Trabalhos relacionados  Nesta sec¸ ˜ao, ser˜ao abordados alguns trabalhos que contribu´ıram para tentar resolver as diferenc¸as na padronizac¸ ˜ao nas descric¸ ˜oes dos produtos, a fim de melhorar o processo de fiscalizac¸ ˜ao tribut´aria no Brasil [Mazzarolo et al. 2022].  O trabalho de [Schulte et al. 2022] apresentou o ELINAC, um modelo que com- bina autoencoder e busca bin´aria para agrupar descric¸ ˜oes de produtos em notas fiscais. O m´etodo filtra as descric¸ ˜oes, considerando apenas o nome e informac¸ ˜oes num´ericas, como quantidade e dosagem. Embora eficiente, ele tem limitac¸ ˜oes ao distinguir produtos com variac¸ ˜oes sutis, como sabor.  A revis˜ao de [Ahmed et al. 2022] aponta que a representac¸ ˜ao vetorial de tex- tos curtos ´e desafiadora devido `a alta dimensionalidade e ao ru´ıdo. O estudo de [Marinho et al. 2024] comparou representac¸ ˜oes textuais para classificar inconsistˆencias em notas fiscais, calculando a similaridade entre a descric¸ ˜ao do produto e a oficial da Nomenclatura Comum do Mercosul (NCM). Concluiu-se que a distˆancia de edic¸ ˜ao de strings teve melhor desempenho preditivo do que embeddings, apesar de n˜ao considerar a similaridade entre produtos.  Este estudo se diferencia ao focar na avaliac¸ ˜ao de algoritmos de agrupamento e na representac¸ ˜ao de descric¸ ˜oes de NF-es utilizando similaridade de strings. Enquanto outros trabalhos abordam redes neurais, detecc¸ ˜ao de fraudes e visualizac¸ ˜ao de dados, este estudo explora a efic´acia dos algoritmos de clusterizac¸ ˜ao para organizar e interpretar as descric¸ ˜oes de produtos em notas fiscais.  3. Metodologia  Esta sec¸ ˜ao descreve a base de dados, o c´alculo da matriz de distˆancias e os algoritmos de clusterizac¸ ˜ao utilizados.  3.1. Base de dados  Foram usadas duas bases: uma base sint´etica com 22 descric¸ ˜oes, contendo ru´ıdos t´ıpicos [Mazzarolo et al. 2022], e uma base real cedida pela Secretaria da Fazenda da Para´ıba (SEFAZ-PB) com 507 descric¸ ˜oes. As descric¸ ˜oes foram normalizadas, removendo carac- teres especiais e convertendo tudo para caracteres mai´usculos.  3.2. Matriz de distˆancias  Uma matriz de distˆancias ´e uma matriz quadrada que cont´em as distˆancias entre todos os pares de elementos do banco de dados. Neste trabalho, a matriz foi feita a partir de uma m´etrica personalizada, baseada na similaridade de Jaro [Jaro 1989]. O valor da similaridade varia entre 0 e 1, onde 0 indica que as strings n˜ao tˆem correspondˆencias e 1 indica que as strings s˜ao idˆenticas. Entretanto, para o conceito de distˆancia, quanto mais pr´oximo de 0, mais pr´oximos s˜ao dois pontos. Dessa forma, para computar a matriz de distˆancia, foi calculado o complemento da similaridade de Jaro, ou seja, 1−JaroSimilarity.   Al´em da similaridade textual, foi introduzido um c´alculo adicional para diferen- ciar produtos com o mesmo nome, mas com medidas distintas, como “200 ML” e “10 KG”. Isso evita que produtos com variac¸ ˜ao apenas na quantidade sejam considerados iguais. Para implementar esse ajuste, as medidas foram extra´ıdas por meio da express˜ao regular 1 [Lucena et al. 2022], e convertidas em mililitros, gramas ou metros. Quando as medidas diferem, adiciona-se uma penalidade de 0,3 ao complemento da similaridade de Jaro, valor que foi escolhido ap´os testes com variac¸ ˜oes entre 0,1 e 0,5.  (?:\\ d *[,]?\\ d+?\\s?(?:kg|ml|mm|l| lt | gr | grs |g|metros|m|gb|k|cm|mg)\\b)  (1)  3.3. Algoritmos de Clusterizac¸ ˜ao Para o agrupamento, este estudo avaliou 4 algoritmos diferentes: DBSCAN [Ester et al. 1996], HDBSCAN [Campello et al. 2013], OPTICS [Ankerst et al. 1999] e AGG [Steinbach et al. 2000]. Todos os algoritmos usados foram implementados pela bi- blioteca scikit-learn, vers˜ao 1.5.1, e nenhuma m´etrica de distˆancia foi passada para os algoritmos, uma vez que a matriz j´a est´a pr´e-computada.  Os algoritmos foram usados em duas etapas:  o agrupamento inicial e a subclusterizac¸ ˜ao dos grupos de outliers, aplicada apenas na base real. Para o agrupa- mento inicial, foi definida uma distˆancia m´axima de agrupamento de 0,1 e um tamanho m´ınimo de cluster sendo igual a 2. Para a segunda etapa, a distˆancia foi igual a 0,2. Os parˆametros de distˆancia foram escolhidos ap´os avaliac¸ ˜ao do agrupamento com variac¸ ˜oes entre 0,05 e 0,2 e os demais hiperparˆametros possuem os valores padr˜oes da biblioteca.  Para avaliar o resultado dos agrupamentos, foram utilizadas duas m´etricas prin- cipais: o Coeficiente de Silhueta [Rousseeuw 1987], que avalia a coes˜ao dos clusters, e o ´Indice de Calinski-Harabasz (CH) [Cali´nski and JA 1974], que mede a separac¸ ˜ao en- tre os grupos. O c´alculo dessas m´etricas foi feito utilizando as distˆancias entre pontos pr´e-computadas. Al´em disso, foi considerada a porcentagem de produtos agrupados para avaliar a cobertura dos dados pelos algoritmos de agrupamento.  4. Resultados e discuss˜oes  ´E importante A Tabela 1 apresenta os resultados da primeira etapa dos experimentos. ressaltar que o algoritmo AGG n˜ao gera um grupo de outliers identificado como −1, o que exigiu um ajuste no c´alculo das m´etricas para esse caso. Especificamente, todos os grupos individuais, que contˆem apenas um produto, foram considerados como pertencentes ao grupo −1, permitindo que as m´etricas fossem calculadas de forma consistente.  Na base sint´etica, DBSCAN, OPTICS e AGG produziram clusters idˆenticos, en- quanto o HDBSCAN teve desempenho superior, distinguindo produtos com variac¸ ˜oes de sabor, mas n˜ao separando bem produtos de medidas diferentes. Nos dados reais, o HDBS- CAN obteve as melhores m´etricas gerais, enquanto o OPTICS teve o maior coeficiente de Silhueta, mas o menor ´ındice de CH, sugerindo que seus clusters n˜ao estavam bem separados.  A Tabela 2 apresenta os resultados da subclusterizac¸ ˜ao dos grupos de produtos considerados outliers na base de dados real. Todas as m´etricas possu´ıram aumentos nos valores, quando comparados `a primeira clusterizac¸ ˜ao, especialmente na utilizac¸ ˜ao do HDBSCAN, tanto na primeira, quanto na segunda etapa.   Tabela 1. Avaliac¸ ˜ao dos algoritmos de clusterizac¸ ˜ao no agrupamento inicial  Base de Dados Algoritmo  Silhueta  CH  Produtos agrupados (%)  Base Controlada  Base SEFAZ- PB  DBSCAN HDBSCAN OPTICS AGG  DBSCAN HDBSCAN OPTICS AGG  0,490 0,563 0,490 0,490  0,686 0,726 0,730 0,696  7,97 15,58 7,97 7,97  21,71 43,40 17,98 20,18  98,16 99,80 98,16 98,16  86,19 94,08 85,99 75,79  Embora as m´etricas tenham melhorado com a segunda etapa de clusterizac¸ ˜ao usando o HDBSCAN, surgiram inconsistˆencias nos agrupamentos. Por exemplo, produ- tos como “BOM TRIGO PREP. EMULSIF.” e “MARG. MEDALHA DE OURO” foram agrupados erroneamente no mesmo cluster. Isso indica que a fase adicional pode priorizar a melhoria das m´etricas, mas comprometer a consistˆencia semˆantica, tornando os clusters menos ´uteis ou interpret´aveis na pr´atica.  Tabela 2. Avaliac¸ ˜ao dos algoritmos de clusterizac¸ ˜ao no segundo agrupamento  Primeira Etapa  Segunda Etapa  Silhueta  CH  Produtos agrupados (%)  DBSCAN  HDBSCAN  OPTICS  AGG  DBSCAN HDBSCAN OPTICS AGG  DBSCAN HDBSCAN OPTICS AGG  DBSCAN HDBSCAN OPTICS AGG  DBSCAN HDBSCAN OPTICS AGG  0,718 0,737 0,717 0,719  0,729 0,740 0.729 0,729  0,761 0,779 0,760 0,763  0,728 0,746 0,727 0,729  29,02 79,30 27,50 28,06  46,28 125,55 46,28 46,28  25,25 73,11 22,96 23,46  27,42 75.52 26.00 26,53  90,13 97,63 89,74 89,94  94,47 99,21 94,47 94,47  89,94 97,63 89,54 89,74  89,94 97,43 89,54 89,74  5. Considerac¸ ˜oes finais  Este estudo avaliou os algoritmos de clusterizac¸ ˜ao DBSCAN, HDBSCAN, OPTICS e AGG para agrupar descric¸ ˜oes de produtos em NF-e, utilizando similaridade de strings como representac¸ ˜ao de dados. O HDBSCAN apresentou o melhor desempenho inicial, mas a segunda etapa de agrupamento gerou inconsistˆencias. DBSCAN e OPTICS tive- ram m´etricas um pouco inferiores, por´em com menos irregularidades. Sugere-se, como trabalhos futuros, testar o m´etodo em bases maiores e explorar representac¸ ˜oes como em- beddings e redes neurais para padronizac¸ ˜ao.   Referˆencias  Ahmed, M., Tiun, S., Omar, N., and Sani, N. S. (2022). Short text clustering algorithms,  application and challenges: A survey. Applied Sciences.  Ankerst, M., Breunig, M. M., Kriegel, H.-P., and Sander, J. (1999). Optics: ordering  points to identify the clustering structure. SIGMOD Rec., 28(2):49–60.  Cali´nski, T. and JA, H. (1974). A dendrite method for cluster analysis. Communications  in Statistics - Theory and Methods, 3:1–27.  Campello, R. J. G. B., Moulavi, D., and Sander, J. (2013). Density-based clustering based on hierarchical density estimates. In Pei, J., Tseng, V. S., Cao, L., Motoda, H., and Xu, G., editors, Advances in Knowledge Discovery and Data Mining, pages 160–172, Berlin, Heidelberg. Springer Berlin Heidelberg.  Ester, M., Kriegel, H.-P., Sander, J., Xu, X., et al. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. In kdd, volume 96, pages 226–231.  Jaro, M. A. (1989). Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida. Journal of the American Statistical Association, 84(406):414–420.  Lucena, L. F., de Menezes e Silva Filho, T., do Rˆego, T. G., and Malheiros, Y. (2022). Automatic recognition of units of measurement in product descriptions from tax in- voices using neural networks. In Pinheiro, V., Gamallo, P., Amaro, R., Scarton, C., Batista, F., Silva, D., Magro, C., and Pinto, H., editors, Computational Processing of the Portuguese Language, pages 156–165, Cham. Springer International Publishing.  Marinho, M., Weigang, L., Oliveira, V., and Borges, V. (2024). Estrat´egias computacio- nais baseadas em similaridade de textos e visualizac¸ ˜ao explorat´oria para a identificac¸ ˜ao de inconsistˆencias em notas fiscais eletrˆonicas.  Mazzarolo, J., Steinmetz, R., and Mergen, S. (2022). Um estudo sobre a falta de padronizac¸ ˜ao na descric¸ ˜ao de produtos em notas fiscais eletrˆonicas. In Anais da XVII Escola Regional de Banco de Dados, pages 31–40, Porto Alegre, RS, Brasil. SBC. Neto, H. and Lopo Martinez, A. (2016). Nota fiscal de serviC¸ os eletr ˆOnica: Uma an ´Alise dos impactos na arrecadaC¸ ˜Ao em munic´Ipios brasileiros. Revista de Contabilidade e Organizac¸ ˜oes, 10:49.  Ribeiro, L., Brand˜ao, W., Marques, I., Andrade, P., J´unior, R., Oliveira, F., and Kelles, R. (2018). Reconhecimento de entidades nomeadas em itens de produto da nota fiscal eletrˆonica. 36:116–126.  Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20:53–65.  Schulte, J. P., Giuntini, F. T., Nobre, R. A., Nascimento, K. C. d., Meneguette, R. I., Li, W., Gonc¸alves, V. P., and Rocha Filho, G. P. (2022). Elinac: Autoencoder approach for electronic invoices data clustering. Applied Sciences, 12(6).  Steinbach, M., Karypis, G., and Kumar, V. (2000). A comparison of document clustering  techniques.   Vieira, P. A., Pimenta, D. P., Cruz, A. F. d., and Souza, E. M. S. d. (2019). Efeitos do programa de nota fiscal eletrˆonica sobre o aumento da arrecadac¸ ˜ao do estado. Revista de Administrac¸ ˜ao P´ublica, 53(2):481–491.   "
        },
        {
            "titulo": "EyetrackingMOS: Proposta de um método de avaliação online para modelos de síntese de fala",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31120",
            "idioma": "Português",
            "storage_key": "files/article_31120_30923.pdf",
            "autores": [
                {
                    "nome": "Gustavo E. Araújo",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0009-0004-5661-4789"
                },
                {
                    "nome": "Julio C. Galdino",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0001-6378-4648"
                },
                {
                    "nome": "Rodrigo de F. Lima",
                    "afiliacao": "USP",
                    "orcid": null
                },
                {
                    "nome": "Leonardo Ishida",
                    "afiliacao": "USP",
                    "orcid": null
                },
                {
                    "nome": "Gustavo W. Lopes",
                    "afiliacao": "USP",
                    "orcid": null
                },
                {
                    "nome": "Miguel Oliveira Jr.",
                    "afiliacao": "UFAL",
                    "orcid": "https://orcid.org/0000-0002-0866-0535"
                },
                {
                    "nome": "Arnaldo Cândido Jr.",
                    "afiliacao": "UNESP",
                    "orcid": "https://orcid.org/0000-0002-5647-0891"
                },
                {
                    "nome": "Sandra M. Aluísio",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0001-5108-2630"
                },
                {
                    "nome": "Moacir A. Ponti",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2059-9463"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Avaliação de modelos de síntese de fala",
                "língua portuguesa",
                "fala espontânea",
                "rastreamento ocular"
            ],
            "referencias": [
                "ALMEIDA, R. A. S. d., OLIVEIRA JR., M., and COZIJN, R. (2021). Paradigma do Mundo Visual: Método de Rastreamento Ocular, chapter 5. Blucher Open Access.",
                "Batista, N. A. R. (2019). Estudo sobre identificação automática de sotaques regionais brasileiros baseada em modelagens estatísticas e técnicas de aprendizado de máquina. Master’s thesis, Unicamp.",
                "Cagliari, L. C. (1992). Prosódia: algumas funções dos supra-segmentos. Cadernos de estudos linguísticos, 23:137–151.",
                "Casanova, E., Shulby, C., Gölge, E., Müller, N. M., de Oliveira, F. S., Junior, A. C., da Silva Soares, A., Aluisio, S. M., and Ponti, M. A. (2021). Sc-glowtts: an efficient zero-shot multi-speaker text-to-speech model.",
                "Casanova, E., Weber, J., Shulby, C. D., Junior, A. C., Gölge, E., and Ponti, M. A. (2022). Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone. In International Conference on Machine Learning, pages 2709–2720. PMLR.",
                "Caseli, H. M. and Nunes, M. G. V., editors (2024). Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português. BPLN, 2 edition.",
                "Choi, Y., Jung, Y., Suh, Y., and Kim, H. (2022). Learning to maximize speech quality directly using mos prediction for neural text-to-speech. IEEE Access, 10:52621–52629.",
                "Cooper, E., Huang, W.-C., Tsao, Y., Wang, H.-M., Toda, T., and Yamagishi, J. (2024). A review on subjective and objective evaluation of synthetic speech. Acoustical Science and Technology, 45(4):161–183.",
                "Hoogeboom, E., Van Den Berg, R., and Welling, M. (2019). Emerging convolutions for generative normalizing flows. In International conference on machine learning, pages 2771–2780. PMLR.",
                "ITU - R (2017). ITU-T Rec. P.10/G.100 (11/2017): Vocabulary for performance, quality of service and quality of experience. Recommendation P.10/G.100, International Telecommunication Union.",
                "ITU - T (1996). Methods for subjective determination of transmission quality. Recommendation P.800, International Telecommunication Union.",
                "Jia, Y., Zhang, Y., Weiss, R. J., Wang, Q., Shen, J., Ren, F., Chen, Z., Nguyen, P., Pang, R., Moreno, I. L., and Wu, Y. (2019). Transfer learning from speaker verification to multispeaker text-to-speech synthesis.",
                "Ju, Z., Wang, Y., Shen, K., Tan, X., Xin, D., Yang, D., Liu, Y., Leng, Y., Song, K., Tang, S., Wu, Z., Qin, T., Li, X.-Y., Ye, W., Zhang, S., Bian, J., He, L., Li, J., and Zhao, S. (2024). Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models.",
                "Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. (2016). Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29.",
                "Le Maguer, S., King, S., and Harte, N. (2024). The limits of the mean opinion score for speech synthesis evaluation. Computer Speech Language, 84:101577.",
                "Ling, L., Fernandes Tavares, T., Barbosa, P., and Batista, N. (2018). Detecção automática de sotaques regionais brasileiros: A importância da validação cross-datasets.",
                "Loizou, P. C. (2011). Speech Quality Assessment, pages 623–654. Springer Berlin Heidelberg, Berlin, Heidelberg.",
                "Mitchell, D. C. (2004). On-line methods in language processing: introduction and historical review. In Carreiras, M. and Clifton Jr., C., editors, The On-line Study of Sentence Comprehension: Eyetracking, ERP and Beyond, pages 15–32. Psychology Press, New York.",
                "Mota, J. A., Ribeiro, S. S. C., and de Oliveira, J. M. (2023). Atlas Linguístico Do Brasil: Comentários às Cartas Linguísticas I-V 3. Universidade Estadual de Londrina. Editora.",
                "Nguyen, T.-N., Pham, N.-Q., and Waibel, A. (2023). Syntacc: Synthesizing multi-accent speech by weight factorization. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE.",
                "Ren, Y., Hu, C., Tao, X., Zhao, Z., Zhang, X., Li, Q., Lei, L., Zhou, S., Liu, J., and Liu, S. (2021). Fastspeech 2: Fast and high-quality end-to-end text to speech. In International Conference on Learning Representations.",
                "Ren, Y., Zhao, Z., Tan, X., Yi, J., Cheng, Y.-L., Yang, J., Qin, T., and Liu, T.-Y. (2022). Naturalspeech: End-to-end text to speech synthesis with human-level quality. In Advances in Neural Information Processing Systems.",
                "Ribeiro, F., Florêncio, D., Zhang, C., and Seltzer, M. (2011). Crowdmos: An approach for crowdsourcing mean opinion score studies. In 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 2416–2419. IEEE.",
                "Sellam, T., Bapna, A., Camp, J., Mackinnon, D., Parikh, A. P., and Riesa, J. (2023). Squeak: Measuring speech naturalness in many languages. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE.",
                "Shen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin, T., Zhao, S., and Bian, J. (2023). Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers.",
                "Tan, X., Chen, J., Liu, H., Cong, J., Zhang, C., Liu, Y., Wang, X., Leng, Y., Yi, Y., He, L., Soong, F., Qin, T., Zhao, S., and Liu, T.-Y. (2022). Naturalspeech: End-to-end text to speech synthesis with human-level quality.",
                "Ynoguti, C. A. (1999). Reconhecimento de Fala Contínua Utilizando Modelos Ocultos de Markov. PhD thesis, Unicamp."
            ],
            "artigo_completo": "Resumo. Avaliar sistemas Text-To-Speech (TTS) ´e um desafio, uma vez que a qualidade crescente da s´ıntese imp˜oe obst´aculos em discriminar a capacidade de modelos em reproduzir atributos pros´odicos, especialmente para o portuguˆes brasileiro. M´etricas de avaliac¸ ˜ao offline n˜ao medem a reac¸ ˜ao genu´ına de ava- liadores aos est´ımulos de ´audios. Prop˜oe-se, portanto, um m´etodo de avaliac¸ ˜ao online com rastreamento de globo ocular. Os experimentos com 76 anotadores apontam que h´a uma correlac¸ ˜ao razo´avel entre EyetrackingMOS e MOS, assim como uma reduc¸ ˜ao em sua durac¸ ˜ao total. Desta forma, acredita-se que esta m´etrica fornec¸a uma informac¸ ˜ao precisa e potencialmente r´apida para comple- mentar os m´etodos de avaliac¸ ˜ao.  Index Terms: Speech Synthesis Models Evaluation, Portuguese language, spontaneous speech, eyetracking  1. Introduc¸ ˜ao  Sistemas de texto-para-fala, do inglˆes Text-To-Speech (TTS) buscam vocalizar um texto escrito em n´ıveis pr´oximos a naturalidade de fala humana [Caseli and Nunes 2024]. Os avanc¸os em Aprendizado Profundo impulsionaram o desenvolvimento de tais sistemas. Posteriormente, a utilizac¸ ˜ao de modelos gerativos baseados em fluxo, como os propostos por [Kingma et al. 2016] e [Hoogeboom et al. 2019], tem permitido maior flexibilidade   na manipulac¸ ˜ao de caracter´ısticas pros´odicas1 da fala sint´etica. Os resultados de modelos do estado da arte j´a reproduzem a identidade dos locutores com bastante naturalidade em condic¸ ˜oes mais amplas de dados [Casanova et al. 2022, Tan et al. 2022].  Entretanto, modelos de s´ıntese ainda encontram obst´aculos na reproduc¸ ˜ao de as- pectos espec´ıficos da expressividade individual de falantes. Estes aspectos podem ser medidos atrav´es da entoac¸ ˜ao, durac¸ ˜ao e ritmo da fala [Ju et al. 2024], que s˜ao de natu- reza pros´odica, o que se agrava em cen´arios de s´ıntese zero-shot [Casanova et al. 2022, Ju et al. 2024]. Neste contexto, sistemas contemporˆaneos de TTS investigam outras capa- cidades al´em da reproduc¸ ˜ao da identidade de um locutor com naturalidade nos resultados, dentre elas o interesse em manter a naturalidade ao gerar fala nas variantes internacio- nais de uma l´ıngua (accent-robust), como o Synthesizing Multi-Accent Speech By Weight Factorization (SYNTACC) [Nguyen et al. 2023]. A possibilidade de s´ıntese de fala com sensibilidade de sotaques internacionais, tamb´em levanta hip´oteses de aplicac¸ ˜oes para va- riantes lingu´ısticas regionais de uma dada l´ıngua que conta com menos recursos, afim de avaliar se a qualidade se preserva. O portuguˆes brasileiro ´e uma l´ıngua que contempla uma grande quantidade de variantes, dadas as dimens˜oes continentais do Brasil, e devido a fatores hist´oricos, sociais e culturais [Mota et al. 2023].  Para avaliar a qualidade da fala sintetizada nesses sistemas, s˜ao utilizadas diversas m´etricas. As m´etricas subjetivas como: Mean Opinion Score (MOS) [ITU - T 1996], Crowd MOS [Ribeiro et al. 2011], Similarity MOS (SMOS) [Jia et al. 2019] e Compa- rative MOS (CMOS), por um lado, dependem da opini˜ao e percepc¸ ˜ao de um grupo de ouvintes humanos. Apesar de importante, este perfil de m´etricas pode oferecer risco para an´alise de sotaques a depender da correspondˆencia entre o contexto regional/cultural dos avaliadores e os ´audios sint´eticos, uma vez que a avaliac¸ ˜ao ser´a influenciada por seus contextos culturais, lingu´ısticos e experiˆencias individuais. Por outro lado, as m´etricas objetivas como: Speaker Encoder Cosine Similarity (SECS) [Casanova et al. 2021], Pro- sody Similarity with Prompt, Prosody Similarity with Ground Truth e Word Error Rate (WER) [Shen et al. 2023] podem n˜ao capturar completamente a percepc¸ ˜ao humana da qualidade do ´audio sobre o desempenho na qualidade de expressividade individual e representatividade de variantes lingu´ısticas e, por isso, complementam a an´alise subje- tiva. A ausˆencia de uma m´etrica padr˜ao e amplamente aceita dificulta a identificac¸ ˜ao de tendˆencias e avanc¸os consistentes no campo do TTS, al´em de dificultar o entendimento de quais modelos s˜ao mais adequados para determinados cen´arios ou requisitos espec´ıficos (cf. [Le Maguer et al. 2024]). Ambos os perfis tˆem sensibilidades a aspectos diferentes e limitac¸ ˜oes que devem ser avaliadas [Cooper et al. 2024].  Ambas as m´etricas tamb´em podem ser observadas quanto a sua resposta aos est´ımulos de ´audios fornecidos durante a avaliac¸ ˜ao. Em m´etodos de avaliac¸ ˜ao of- fline (MOS, CrowdMOS, SMOS e CMOS), o indiv´ıduo pontua apenas ap´os ouvir todo o est´ımulo, enquanto m´etodos de avaliac¸ ˜ao online permitem que se registre suas impress˜oes `a medida que o est´ımulo ´e recebido, tendo como objetivo captu- rar reac¸ ˜oes genu´ınas e momentˆaneas. A avaliac¸ ˜ao de est´ımulos de ´audio utilizando rastreamento ocular j´a ´e amplamente empregada em contextos lingu´ısticos, como na  1A pros´odia estuda as func¸ ˜oes dos suprassegmentos, que s˜ao essenciais para a melodia da fala (tom, entoac¸ ˜ao, tessitura), para a dinˆamica da fala (durac¸ ˜ao, pausa etc.) e para qualidade da voz (volume, registro etc.) [Cagliari 1992].   an´alise de processamento de linguagem, compreens˜ao auditiva, e percepc¸ ˜ao fon´etica [ALMEIDA et al. 2021]. No entanto, sua aplicac¸ ˜ao na avaliac¸ ˜ao de sistemas de s´ıntese de fala ainda ´e pouco explorada. Buscamos preencher essa lacuna, propondo um novo m´etodo, EyetrackingMOS, que utiliza o rastreamento ocular para avaliac¸ ˜ao de qualidade dos ´audios forma mais natural, sem que o participante atribua uma nota de forma direta.  As principais contribuic¸ ˜oes feitas nesse trabalho s˜ao sumarizadas como se segue:  1. Proposta de um novo m´etodo de avaliac¸ ˜ao de sistemas de s´ıntese de fala que inte-  gra o rastreamento ocular, chamado de EyetrackingMOS;  2. Comparac¸ ˜ao entre o EyetrackingMOS e uma adaptac¸ ˜ao do MOS tradicional, des-  tacando suas respectivas vantagens e limitac¸ ˜oes;  3. Apresentac¸ ˜ao dos experimentos, detalhes de configurac¸ ˜ao do modelo e interfaces em um reposit´orio2, facilitando a replicabilidade em diferentes cen´arios e promo- vendo avanc¸os na pesquisa sobre s´ıntese de fala.  2. Revis˜ao sobre m´etricas subjetivas para an´alise de sistemas de TTS  Na d´ecada de 1990, a International Telecommunication Union (ITU) padronizou diversos tipos de testes de audic¸ ˜ao que eram frequentemente usados na telefonia [ITU - T 1996]. A pontuac¸ ˜ao baseada em opini˜ao pode ser definida como o valor em uma escala predefinida que um sujeito atribui `a sua opini˜ao sobre o desempenho de um sistema [ITU - R 2017, Loizou 2011]. A pontuac¸ ˜ao m´edia de opini˜ao, do inglˆes Mean Opinion Score (MOS) ´e um tipo de Absolute Category Rating (ACR) [Ribeiro et al. 2011]. A MOS emergiu como o descritor mais popular sobre a percepc¸ ˜ao da qualidade de m´ıdia. Para o c´alculo da MOS, humanos avaliam os ´audios sintetizados e naturais e atribuem uma nota de 1 a 5, no qual o valor final corresponde `a m´edia das notas de todos os avaliadores. A tabela traduzida com a equac¸ ˜ao correspondente pode ser vista no reposit´orio2.  Diversas variac¸ ˜oes da MOS foram desenvolvidas para atender a diferentes neces- sidades de avaliac¸ ˜ao. A Crowd Mean Opinion Score (crowdMOS) prop˜oe uma adaptac¸ ˜ao ao ambiente tradicional de testes MOS, ao utilizar trabalhadores de uma multid˜ao (do inglˆes, crowd) pela internet para realizar avaliac¸ ˜oes em ambientes n˜ao controlados, o que permite maior diversidade de ouvintes a um custo reduzido, embora com desafios em termos de controle de qualidade [Ribeiro et al. 2011]. A Similarity Mean Opinion Score (SMOS)3, por sua vez, foca na avaliac¸ ˜ao da semelhanc¸a entre ´audios sintetizados e de referˆencia, sendo ´util para medir qu˜ao pr´oximo um ´audio gerado est´a de uma voz original em termos de caracter´ısticas ac´usticas e vocais [Ren et al. 2021]. J´a a Comparative Mean Opinion Score (CMOS) avalia a qualidade relativa entre duas vers˜oes de ´audio sinteti- zado, pedindo aos avaliadores que comparem diretamente os ´audios e apontem qual deles possui melhor qualidade, utilizando uma escala de -3 a +3 [Ren et al. 2022]. Cada uma dessas variantes da MOS foca em diferentes aspectos da qualidade de ´audio, utilizadas de acordo com o que se deseja avaliar. Considerando que estudos tˆem utilizado a MOS como uma medida de naturalidade da fala em tarefas de s´ıntese (cf. [Sellam et al. 2023], [Choi et al. 2022]), a descric¸ ˜ao da caracter´ıstica observada pelo avaliador foi adaptada para a avaliac¸ ˜ao de naturalidade (veja a coluna 4 da Tabela 1).  2Acesso  em EyetrackingMOS-STIL  https://github.com/GustavoEvangelistaAraujo/  3Tamb´em abreviado por SimMOS na literatura.   3. EyetrackingMOS  O rastreamento ocular ´e amplamente reconhecido como uma das t´ecnicas mais precisas para a avaliac¸ ˜ao online do processamento lingu´ıstico [Mitchell 2004, Kaiser 2013]. Os variados movimentos dos olhos durante o processamento de informac¸ ˜oes podem ser utili- zados para inferir como essas informac¸ ˜oes s˜ao processadas, seja durante a leitura de texto (est´ımulo de leitura) ou ao observar uma imagem (est´ımulo visual). O resultado ´e obtido a partir da porcentagem de tempo em que o avaliador olhou para o lados direito e esquerdo, os quais mostram figuras relacionadas aos conceitos que se deseja medir. Assim, registra- mos a porcentagem de tempo que o participante permanece com o olhar sobre a figura que representa a fala natural. Esta medida pode ser avaliada em um intervalo de 0% a 100% e mapeada para a escala MOS como apresentado na Tabela 1. Assim como no MOS, ao final ´e calculada a m´edia das notas de todos os avaliadores.  Tabela 1. Mapeamento entre pontuac¸ ˜oes do EyetrackingMOS e MOS  Tempo de fixac¸ ˜ao (%) Avaliac¸ ˜ao MOS Qualidade Naturalidade 81 a 100 61 a 80 41 a 60 21 a 40 0 a 20  Extremamente natural Muito natural Razoavelmente natural Pouco natural Nada natural  Excelente Boa Razo´avel Pobre Ruim  5 4 3 2 1  4. Materiais e m´etodos  4.1. Descric¸ ˜ao do conjunto de dados  H´a uma carˆencia de conjuntos de dados de ´audio com variantes lingu´ısticas regi- onais. Corpus como BRACCENT, utilizado em [Batista 2019], [Ling et al. 2018] e [Ynoguti 1999], n˜ao apresentam volume satisfat´orio de dados, assim como n˜ao tratam da fala espontˆanea. Portanto, foi escolhido para este estudo um recorte de ´audios de um grande dataset do Museu da Pessoa4, um museu virtual e colaborativo de hist´orias de vida, que s˜ao do tipo entrevistas biogr´aficas, compilado pelo projeto Tarsila5. Detalhes do recorte preliminar do corpus (MuPe-v1) est˜ao dispon´ıveis no reposit´orio2.  4.2. Modelo de s´ıntese de fala  O modelo SYNTACC [Nguyen et al. 2023] ´e uma arquitetura para s´ıntese de fala com m´ultiplos sotaques baseada no YourTTS [Casanova et al. 2022]. Similarmente ao an- tecessor, utiliza uma arquitetura de codificac¸ ˜ao-decodificac¸ ˜ao baseada em Transformer, onde o codificador recebe a sequˆencia de texto como entrada e gera uma representac¸ ˜ao intermedi´aria, que ´e posteriormente processada pelo decodificador para gerar o espec- trograma mel, uma representac¸ ˜ao em espectro da frequˆencia ao longo do tempo que ´e reconstru´ıda em ´audio por um vocoder.  Esse modelo implementa as seguintes mudanc¸as: na entrada, a arquitetura con- catena 4 embeddings de idiomas trein´aveis em cada caractere de entrada, uma t´ecnica de fatorizac¸ ˜ao de pesos (weight factorization), o que permite um treinamento multi-accent.  4https://museudapessoa.org/ 5https://sites.google.com/view/tarsila-c4ai/home   Esta abordagem divide os pesos do modelo em componentes compartilhados e espec´ıficos para cada variante lingu´ıstica, otimizando o treinamento em cen´arios de poucos recursos. Isso possibilita que a s´ıntese de fala seja adapt´avel para o contexto do portuguˆes brasi- leiro, sendo poss´ıvel obter um controle expl´ıcito de sotaques pelo congelamento parcial de pesos atribu´ıdos a ele e portanto permite que a fala seja sintetizada de forma mais es- pec´ıfica para cada variante. Detalhes da arquitetura, configurac¸ ˜oes do modelo e etapa de treinamento tamb´em foram disponibilizados no reposit´orio2.  5. Experimentos  A Figura 1 apresenta o fluxo de interac¸ ˜ao do usu´ario neste experimento. Para tanto, foi utilizada a plataforma Gorilla6, uma plataforma paga, com o objetivo de construc¸ ˜ao e coleta de tarefas de anotac¸ ˜ao. A sequˆencia de interfaces e o conjunto de dados dos est´ımulos tamb´em s˜ao apresentados no reposit´orio2.  Figura 1. Fluxograma da interac¸ ˜ao do usu ´ario em cada experimento  No experimento elaborado neste trabalho, o processo se inicia com a aceitac¸ ˜ao do Termo de Consentimento Livre e Esclarecido (TCLE). Em seguida, o participante ´e conduzido a um tutorial, que tem o objetivo de ambient´a-lo com o experimento subse- quente. Para a captura do v´ıdeo, s˜ao utilizadas as cˆameras padr˜oes dos dispositivos pes- soais (apenas computador e notebook) dos usu´arios, caso as configurac¸ ˜oes de iluminac¸ ˜ao e qualidade de imagem n˜ao sejam suficientemente boas para permitir que o participante complete a calibragem sem erros, o participante ´e impedido de continuar. Em conjunto com uma calibragem recorrente, ´e poss´ıvel inferir que a qualidade de rastreamento se mantenha desde o in´ıcio at´e o final do experimento. O Gorilla utiliza a biblioteca Web- gazer7 para rastreamento ocular. No caso do EyetrackingMOS, ap´os o tutorial, o usu´ario passa pela etapa de calibragem, que ´e dividida em duas partes. Primeiro, ´e necess´ario posicionar corretamente o rosto em relac¸ ˜ao `a cˆamera. Em seguida, o participante deve fixar o olhar em uma sequˆencia de pontos que aparecem aleatoriamente nas extremidades da ´area ´util da tela. S˜ao apresentados 10 pontos no total, no qual os 5 primeiros pontos tornam-se uma referˆencia de rastreamento, e os 5 seguintes s˜ao repetidos como validac¸ ˜ao dos anteriores. Caso haja uma discrepˆancia significativa entre a referˆencia e a validac¸ ˜ao em um dos pontos (considerado como a tolerˆancia do teste), a calibragem ´e considerada falha, e o usu´ario precisa repetir o processo. Ap´os uma calibragem bem-sucedida, o parti- cipante prossegue e visualiza uma tela com duas imagens vetoriais ilustrativas (um robˆo e uma figura humana, que trocam de posic¸ ˜ao de forma aleat´oria a cada est´ımulo) enquanto ouve o ´audio. A pausa ´e uma tela subsequente ao final do ´audio com apenas um sinal de “+” por 3 segundos, feita para poder reposicionar o olhar do usu´ario no meio da tela.  6https://gorilla.sc/ 7https://webgazer.cs.brown.edu/   Este ciclo de est´ımulo e pausa ´e repetido 15 vezes, e ent˜ao ´e feita uma nova calibragem para garantir a qualidade de rastreamento do globo ocular, sendo realizado quatro ciclos completos, que totalizam 60 est´ımulos.  Por outro lado, no experimento de MOS, ap´os o TCLE e o tutorial, o participante ´e exposto a 60 est´ımulos de ´audio. Durante o tutorial, s˜ao apresentados trˆes exemplos de ´audios sintetizados, correspondentes `as pontuac¸ ˜oes 1, 3 e 5, para ajudar o participante a alinhar suas expectativas. O participante pode ouvir cada est´ımulo mais de uma vez antes de decidir sua pontuac¸ ˜ao, utilizando a Tabela 1 como referˆencia para todas as 60 amostras de ´audio, conforme descrito na literatura de avaliac¸ ˜ao de modelos de s´ıntese.  A divis˜ao das listas de ´audios para avaliac¸ ˜ao foi realizada considerando dois ti- pos de ´audios: sintetizados e naturais. Esses ´audios foram organizados em duas listas: Lista A e Lista B, que foram atribu´ıdas aos participantes de forma equilibrada. 30 ´audios naturais foram colocados na Lista A, enquanto seus correspondentes sintetizados foram alocados na lista B. Da mesma forma, 30 ´audios sintetizados foram inclu´ıdos na Lista A, com os seus correspondentes naturais na lista B. Quanto aos participantes, conforme [Loizou 2011], a proporc¸ ˜ao de avaliac¸ ˜oes subjetivas deve ser de 10 especialistas para 20 n˜ao especialistas. Foram escolhidos 76 anotadores dentre 28 especialistas e 48 n˜ao es- pecialistas, distribu´ıdos entre 4 grupos de 19 participantes. Ambos experimentos foram elaborados desta mesma forma, o que assegurou uma diversidade de perspectivas nas avaliac¸ ˜oes, permitindo uma an´alise comparativa abrangente entre as opini˜oes de especia- listas e n˜ao especialistas sobre os ´audios apresentados.  6. Resultados preliminares  A Figura 2 ilustra a relac¸ ˜ao entre os valores mensurados pelo EyetrackingMOS, conver- tidos para a escala MOS, e os valores mensurados diretamente pelo MOS. Cada ponto azul representa um par de medidas, com o eixo horizontal correspondendo aos valores do EyetrackingMOS convertidos para a escala MOS e o eixo vertical representando os valores obtidos diretamente pelo MOS. A linha verde trac¸ada no gr´afico indica a linha de tendˆencia linear, mostrando a direc¸ ˜ao geral da correlac¸ ˜ao entre as duas vari´aveis. A Figura 3 ilustra a dispers˜ao das pontuac¸ ˜oes obtidas tanto pelo MOS quanto pelo rastrea- mento ocular (EyetrackingMOS) para ´audios reais e sintetizados. Em ambos os testes, os participantes conseguiram separar razoavelmente bem os ´audios reais dos sintetizados.  Figura 2. Gr ´afico de dispers ˜ao entre EyetrackingMOS e MOS  Figura 3. Gr ´afico de dispers ˜ao por classe  No gr´afico de dispers˜ao por MOS (Figura 4), observa-se uma distinc¸ ˜ao clara en- tre os ´audios reais, que tendem a receber pontuac¸ ˜oes mais altas, e os sintetizados, que   se concentram nas faixas intermedi´arias e baixas. No entanto, no gr´afico de dispers˜ao por rastreamento ocular (Figura 5), a separac¸ ˜ao entre ´audios reais e sintetizados ´e me- nos evidente. Essa maior dispers˜ao nos resultados do rastreamento ocular ´e esperada, j´a que nesse m´etodo os est´ımulos s˜ao percebidos apenas uma vez, enquanto nos m´etodos offline como o MOS, o anotador pode ouvir o est´ımulo repetidas vezes antes de tomar sua decis˜ao, resultando em uma separac¸ ˜ao mais clara entre os tipos de ´audio. Assim, o rastre- amento ocular oferece uma avaliac¸ ˜ao mais detalhada, capturando variac¸ ˜oes mais sutis na percepc¸ ˜ao da qualidade dos ´audios.  Figura 4. Gr ´afico de dispers ˜ao por MOS  Figura 5. Gr ´afico de dispers ˜ao por rastreamento ocular  Os resultados apresentados na Tabela 2 indicam uma correlac¸ ˜ao razo´avel entre o EyetrackingMOS e o MOS, com uma m´etrica R² de 56%, sugerindo que o Eyetracking- MOS explica 56% da variˆancia observada no MOS. O desvio padr˜ao do erro entre as duas m´etricas ´e de 0,72 unidades, mostrando que, em geral, elas tendem a ser pr´oximas, com uma diferenc¸a m´edia de menos de uma unidade. Al´em disso, o MOS tende a classificar um n´umero maior de ´audios com a nota m´axima ou valores pr´oximos, enquanto o Eye- trackingMOS oferece uma an´alise mais detalhada, por sua escala ser de 0 a 100, o que ´e observado em ´audios de alta qualidade. Essa dispers˜ao indica que, embora exista uma correlac¸ ˜ao razo´avel entre as duas m´etricas, conforme evidenciado pela inclinac¸ ˜ao positiva da linha de tendˆencia, as medidas n˜ao s˜ao perfeitamente alinhadas, refletindo diferenc¸as na maneira como cada m´etodo capta e avalia a qualidade dos ´audios.  Tabela 2. Medidas de performance estat´ıstica  Medida Pearson Mean Squared Error (MSE) Rooted Mean Squared Error (RMSE) R2 Spearman  Interpretac¸ ˜ao geral Valor Correlac¸ ˜ao moderada 0.744 Erro m´edio baixo 0.710 Erro m´edio baixo 0.844 0.553 Explica 55% da variˆancia Correlac¸ ˜ao moderada 0.714  Tamb´em foi realizada uma an´alise da concordˆancia entre os avaliadores den- tro de seus respectivos grupos, utilizando o coeficiente de Kendall’s W para avaliar a consistˆencia das respostas (Tabela 3). Em resumo, o grupo EyetrackingMOS apresen- tou maior consistˆencia nas avaliac¸ ˜oes, com alta concordˆancia na maioria dos est´ımulos, enquanto o grupo MOS demonstrou uma maior variabilidade, com concordˆancia que variou de alta at´e nenhuma, indicando poss´ıveis desafios na avaliac¸ ˜ao uniforme dos   est´ımulos por este grupo. Com relac¸ ˜ao ao tempo, EyetrackingMOS e MOS tomaram em m´edia 12:07min e 12:30min dos participantes, respectivamente. As medianas foram de 11:38min e 10:41min, respectivamente. Nota-se que o teste MOS tende a ser em torno de 1 minuto mais r´apido que o EyetrackingMOS que pode ser justificada pelo tempo das 4 calibrac¸ ˜oes do rastreamento ocular.  Tabela 3. Medidas de concord ˆancia para cada grupo de experimentos  Grupo EyetrackingMOS  Intervalo de Kendall’s W Interpretac¸ ˜ao geral 0.6719 a 0.9579  Alta concordˆancia geral, algumas variac¸ ˜oes Grande variac¸ ˜ao, alta concordˆancia a nenhuma concordˆancia  MOS  0.0000 a 0.9474  7. Conclus˜ao e trabalhos futuros Conforme os resultados preliminares, o EyetrackingMOS e MOS tˆem uma correlac¸ ˜ao razo´avel. Paralelamente, a utilizac¸ ˜ao de uma medida de avaliac¸ ˜ao subjetiva com rastre- amento ocular oferece vantagens significativas, uma vez que permite capturar reac¸ ˜oes genu´ınas e s´ıncronas aos est´ımulos apresentados. Al´em disso, o controle mais rigoroso sobre a quantidade de est´ımulos recebidos por cada participante pode reduzir a variac¸ ˜ao na concordˆancia e aumentar a quantidade de est´ımulos por sess˜ao. A reac¸ ˜ao mecˆanica ocular tamb´em pode reduzir variac¸ ˜oes na concordˆancia, causadas pelas diferentes interpretac¸ ˜oes das descric¸ ˜oes de pontuac¸ ˜ao de m´etricas subjetivas. A escala de 0 a 100 para cada in- div´ıduo oferece uma avaliac¸ ˜ao mais detalhada e precisa, permitindo uma maior granu- laridade na an´alise das respostas, ao contr´ario das escalas limitadas a poucos pontos. Embora a produc¸ ˜ao dessa medida seja mais complexa e demorada, o benef´ıcio de ob- ter uma an´alise mais transparente das reac¸ ˜oes dos participantes justifica seu uso como complemento do MOS tradicional.  Como  futuros,  trabalhos  diferentes  experimentar  tecnolo- pretende-se gias/plataformas de captac¸ ˜ao ocular para comparar a precis˜ao da captac¸ ˜ao. Tamb´em ´e importante obter dados estat´ısticos com uma distinc¸ ˜ao das pontuac¸ ˜oes fornecidas entre os grupos de especialistas e n˜ao especialistas. Al´em disso, a selec¸ ˜ao de vari´aveis deve ser refinada, como, por exemplo, calcular a fixac¸ ˜ao no espac¸o intermedi´ario entre as imagens, o que pode oferecer uma compreens˜ao mais detalhada das reac¸ ˜oes dos participantes. Por fim, explorar maneiras de realizar esses testes gratuitamente, seja por meio de parcerias, uso de plataformas de crowdsourcing ou outras abordagens que reduzam os custos e ampliem o acesso aos participantes.  "
        },
        {
            "titulo": "Modestos e Sustentáveis: O Ajuste Eficiente Beneficia Modelos de Língua de Menor Escala em Português?",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31121",
            "idioma": "Português",
            "storage_key": "files/article_31121_30924.pdf",
            "autores": [
                {
                    "nome": "Gabriel Assis",
                    "afiliacao": "UFF",
                    "orcid": "http://orcid.org/0009-0000-2674-0427"
                },
                {
                    "nome": "Arthur Vasconcelos",
                    "afiliacao": "UFF",
                    "orcid": null
                },
                {
                    "nome": "Lívia de Azevedo",
                    "afiliacao": "UFF",
                    "orcid": null
                },
                {
                    "nome": "Mariza Ferro",
                    "afiliacao": "UFF",
                    "orcid": "https://orcid.org/0000-0003-0191-582X"
                },
                {
                    "nome": "Aline Paes",
                    "afiliacao": "UFF",
                    "orcid": "https://orcid.org/0000-0002-9089-7303"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Modelos de Língua têm estabelecido novos padrões de desempenho em tarefas textuais. Porém, tais modelos exigem grandes volumes de dados e recursos computacionais intensivos. Este estudo explora o uso de técnicas de Ajuste Fino Eficiente de Parâmetros (PEFT), especificamente LoRA e GreenTrainer, aplicadas a modelos especializados para o portugues, OPT-PTBR e PTT5. Almeja-se avaliar se as técnicas de PEFT mantém o desempenho dos modelos enquanto mitigam os impactos financeiros e ambientais do uso intensivo de recursos, mesmo em modelos menores. Os resultados mostram que o GreenTrainer, particularmente, oferece desempenho competitivo em relação ao Ajuste Fino completo, enquanto reduz significativamente demandas computacionais.",
            "keywords": [
                "ajuste eficiente de parâmetros",
                "modelos de língua de menor escala",
                "recursos limitados",
                "sumarização"
            ],
            "referencias": [
                "Cabral, B., Claro, D., and Souza, M. (2024). Exploring Open Information Extraction for Portuguese Using Large Language Models. In Proceedings of the 16th International Conference on Computational Processing of Portuguese, pages 127–136.",
                "Freitas, C. (2024). Dataset e corpus. In Caseli, H. M. and Nunes, M. G. V., editors, Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português, book chapter 13. BPLN, 2 edition.",
                "Fu, J., Ng, S.-K., Jiang, Z., and Liu, P. (2024). GPTScore: Evaluate as You Desire. In Duh, K., Gomez, H., and Bethard, S., editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6556–6576, Mexico City, Mexico. Association for Computational Linguistics.",
                "Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR.",
                "Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations.",
                "Huang, K., Yin, H., Huang, H., and Gao, W. (2024). Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation. In The Twelfth International Conference on Learning Representations.",
                "Kato, M. A., Martins, A. M., and Nunes, J. (2023). The Syntax of Portuguese. Cambridge Syntax Guides. Cambridge University Press.",
                "Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T. (2019). Quantifying the Carbon Emissions of Machine Learning. arXiv preprint arXiv:1910.09700.",
                "Li, P., Yang, J., Islam, M. A., and Ren, S. (2023). Making AI less “Thirsty’’: Uncovering and Addressing the Secret Water Footprint of AI models.",
                "Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
                "Maslej, N., Fattorini, L., Perrault, R., Parli, V., Reuel, A., Brynjolfsson, E., Etchemendy, J., Ligett, K., Lyons, T., Manyika, J., Niebles, J. C., Shoham, Y., Wald, R., and Clark, J. (2024). Artificial Intelligence Index Report 2024.",
                "Paes, A., Vianna, D., and Rodrigues, J. (2024). Modelos de linguagem. In Caseli, H. M. and Nunes, M. G. V., editors, Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português, book chapter 15. BPLN, 2 edition.",
                "Paiola, P. H. (2022). Sumarização abstrativa de textos em português utilizando aprendizado de máquina. Mestrado em ciências da computação, Universidade Estadual Paulista Júlio de Mesquita Filho, [s.l.]. Programa de Pós-Graduação em Ciência da Computação.",
                "Paiola, P. H., Garcia, G. L., Jodas, D. S., Correia, J. V. M., Sugi, L. A., and Papa, J. P. (2024). RecognaSumm: A Novel Brazilian Summarization Dataset. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 575–579, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1).",
                "Schwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. (2020). Green AI. Communications of the ACM, 63(12):54–63.",
                "Souza, J. W. d. C., Cardoso, P. C. F., and Paixão, C. A. (2024). Sumarização automática. In Caseli, H. M. and Nunes, M. G. V., editors, Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português, book chapter 22. BPLN, 2 edition.",
                "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY, USA. Curran Associates Inc.",
                "Yang, Y., Zhou, J., Wong, N., and Zhang, Z. (2024). LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models. In Duh, K., Gomez, H., and Bethard, S., editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3161–3176, Mexico City, Mexico. Association for Computational Linguistics.",
                "Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2020). BERTScore: Evaluating Text Generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
                "Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y. (2024b). GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection."
            ],
            "artigo_completo": "Resumo. Modelos de L´ıngua tˆem estabelecido novos padr˜oes de desempenho em tarefas textuais. Por´em, tais modelos exigem grandes volumes de dados e recursos computacionalis intensivos. Este estudo explora o uso de t´ecnicas de Ajuste Fino Eficiente de Parˆametros (PEFT), especificamente LoRA e Gre- enTrainer, aplicadas a modelos especializados para o portuguˆes, OPT-PTBR e PTT5. Almeja-se avaliar se as t´ecnicas de PEFT mantˆem o desempenho dos mo- delos enquanto mitigam os impactos financeiros e ambientais do uso intensivo de recursos, mesmo em modelos menores. Os resultados mostram que o Green- Trainer, particularmente, oferece desempenho competitivo em relac¸ ˜ao ao Ajuste Fino completo, enquanto reduz significativamente demandas computacionais.  1. Introduc¸ ˜ao  Modelos de L´ıngua (MLs) Computacionais tˆem como objetivo representar componen- tes da l´ıngua humana de forma simplificada usando representac¸ ˜oes num´ericas, mas ten- tando preservar seus fundamentos l´exicos, sint´aticos e semˆanticos [Paes et al. 2024]. No contexto atual do Processamento de Linguagem Natural (PLN), MLs Neu- rais — baseados em redes neurais — que empregam a arquitetura Transfor- mer [Vaswani et al. 2017] destacam-se por alcanc¸arem resultados no estado-da-arte em diversas tarefas [Wolf et al. 2020]. Particularmente, MLs de larga escala (Large Lan- guage Models, LLMs) [Zhao et al. 2023, Paes et al. 2024] estabeleceram novos padr˜oes para tarefas generativas, como a sumarizac¸ ˜ao [Fu et al. 2024]. Tais modelos se carac- terizam pelo seu vasto n´umero de parˆametros que possibilitam a observac¸ ˜ao de habili- dades emergentes, ao resolverem tarefas para as quais n˜ao foram explicitamente treina- dos [Paes et al. 2024]. Como consequˆencia, LLMs passaram a ser integrados como com- ponentes de software e partes essenciais de agentes de conversac¸ ˜ao, expandindo seu uso para al´em dos ambientes acadˆemicos e corporativos e tornando-os acess´ıveis por qualquer indiv´ıduo com um computador.   Dessa forma, aumentou-se a demanda pelo desenvolvimento e acesso de LLMs, acompanhados por um crescimento expressivo no n´umero de parˆametros desses mode- los [Maslej et al. 2024]. Contudo, o aumento em larga escala de parˆametros apresenta desafios not´aveis, incluindo a necessidade de vastos volumes de dados e um intenso con- sumo de recursos computacionais [Zhao et al. 2023]. Neste cen´ario, a Inteligˆencia Arti- ficial Verde (IA Verde) desponta como uma ´area dedicada a elucidar e reduzir os impac- tos computacionais — tanto ambientais, como socioeconˆomicos — do desenvolvimento de soluc¸ ˜oes em IA [Schwartz et al. 2020]. Atualmente, o desenvolvimento e a pesquisa em modelos de l´ıngua s˜ao dominados por entidades privadas, e com uma concentrac¸ ˜ao significativa nos Estados Unidos, Uni˜ao Europeia e China [Maslej et al. 2024]. Essa concentrac¸ ˜ao representa um entrave, pois limita a diversificac¸ ˜ao de pesquisa em outras regi˜oes, como o Brasil, que enfrentam restric¸ ˜oes de recursos. Al´em disso, a sustentabili- dade ambiental emerge como uma quest˜ao cr´ıtica, dado, por exemplo, o alto uso de tempo em GPUs para treinamento e operac¸ ˜ao de MLs, que tem como consequˆencias um elevado consumo energ´etico e seu equivalente em emiss˜oes de di´oxido de carbono (CO2eq) e uso de ´agua pot´avel [Li et al. 2023].  No contexto de adaptac¸ ˜ao de MLs, t´ecnicas como o Ajuste Fino (Fine-Tuning) e, mais ainda, o Ajuste Fino Eficiente de Parˆametros (Parameter Efficient Fine-Tuning, PEFT) [Xu et al. 2023] emergem como abordagens para adaptar LLMs de forma a aliviar essas limitac¸ ˜oes. Ambas as abordagens aproveitam o conhecimento previamente codifi- cado em MLs Pr´e-Treinados (Pre-trained Language Models, PLMs) [Ding et al. 2023] e os adaptam para dom´ınios ou tarefas espec´ıficas. Entretanto, enquanto a primeira abor- dagem pode alterar todos os parˆametros do modelo pr´e-treinado, a segunda abordagem foca na adaptac¸ ˜ao considerando explicitamente a limitac¸ ˜ao de recursos. Todavia, diver- sos m´etodos de PEFT dependem da selec¸ ˜ao de parˆametros a serem alterados, o que pode acarretar em degradac¸ ˜ao de desempenho [Yang et al. 2024].  sob a premissa de que esses modelos  Os m´etodos de PEFT s˜ao tipicamente avaliados em LLMs com bilh˜oes de parˆametros, s˜ao superparametriza- dos [Ding et al. 2023]. Embora haja uma motivac¸ ˜ao natural para reduzir o consumo de recursos por parte desses modelos, sua aplicac¸ ˜ao em grande escala, mesmo que de forma mais eficiente, n˜ao elimina completamente as barreiras impostas ao uso de MLs dessa magnitude. Surge, ent˜ao, uma quest˜ao relevante: quais seriam os impactos da aplicac¸ ˜ao de t´ecnicas de PEFT em modelos de menor escala em relac¸ ˜ao a sua capacidade de re- alizar tarefas espec´ıficas? Adicionalmente, o portuguˆes destaca-se como uma l´ıngua di- versificada, apresentando particularidades estruturais significativas, como a relac¸ ˜ao de ordem das palavras e as variac¸ ˜oes nas desinˆencias, que podem alterar o significado de uma frase [Kato et al. 2023]. Nesse contexto, outra quest˜ao importante se apresenta: a aplicac¸ ˜ao de t´ecnicas de PEFT em modelos de menor escala para o portuguˆes afetaria negativamente o desempenho e a representac¸ ˜ao do idioma?  Para responder tais quest˜oes, este artigo contribui com uma avaliac¸ ˜ao entre a abor- dagem de ajuste fino completo e t´ecnicas de PEFT, especificamente Low-Rank Adap- tation (LoRA) [Hu et al. 2022] e GreenTrainer [Huang et al. 2024], em dois PLMs es- pec´ıficos para o portuguˆes: OPT-PTBR1, com 125 milh˜oes de parˆametros, e PTT5- base [Carmo et al. 2020], com 223 milh˜oes de parˆametros. Nossos resultados demons-  1https://huggingface.co/monilouise/opt125M_portuguese   tram que as t´ecnicas eficientes produzem desempenhos competitivos em relac¸ ˜ao ao ajuste fino completo, mesmo em modelos de menor escala. Notavelmente, a t´ecnica GreenTrai- ner apresentou resultados com menor degradac¸ ˜ao e, em alguns casos, at´e superiores ao ajuste fino completo. Com essa an´alise, buscamos contribuir para a atenuac¸ ˜ao dos impac- tos socioeconˆomicos e ambientais do treinamento de MLs, sem deixar de considerar as particularidades do idioma portuguˆes.  2. Fundamentac¸ ˜ao Te´orica  Esta sec¸ ˜ao visa elucidar conceitos fundamentais tratados no trabalho e essenciais no con- texto de ajuste de MLs, especificamente acerca de PLMs e m´etodos de PEFT.  2.1. Ajuste de Modelos de L´ıngua Pr´e-treinados  Os PLMs s˜ao modelos que passam por uma etapa chamada de pr´e-treinamento, cujo ob- jetivo ´e incorporar informac¸ ˜oes lingu´ısticas relevantes a partir de um grande volume de corpora. Todavia, esses modelos podem n˜ao representar adequadamente informac¸ ˜oes es- pec´ıficas de certos dom´ınios ou tarefas n˜ao abordadas durante o pr´e-treinamento. Para tratar dessa quest˜ao, adota-se amplamente o ajuste fino dos PLMs, no qual os pesos dos modelos s˜ao atualizados para tarefas ou dom´ınios particulares por meio do treinamento sobre um novo conjunto de dados espec´ıfico, tipicamente na tarefa final pretendida. Dessa forma, ´e poss´ıvel aproveitar o conhecimento previamente codificado sem a necessidade de repetir a etapa de pr´e-treinamento, realizando um processo direcionado e geralmente menos oneroso [Paes et al. 2024].  2.2. Ajuste Fino Eficiente de Parˆametros  O conjunto de t´ecnicas de PEFT reduz a demanda por recursos computacionais para ajuste de PLMs. Esses m´etodos s˜ao divididos por [Xu et al. 2023] em aditivo, parcial, reparametrizado, unificado e h´ıbrido. O ajuste aditivo introduz uma quantidade me- nor de parˆametros adicionais ajust´aveis, evitando o ajuste dos parˆametros pr´oprios do modelo pr´e-treinado. O ajuste parcial atualiza apenas um subconjunto dos parˆametros pr´e-treinados. A reparametrizac¸ ˜ao utiliza transformac¸ ˜oes de baixo posto da ´Algebra Li- near para reduzir o n´umero de parˆametros trein´aveis. O m´etodo unificado prop˜oe um framework coeso que simplifica a integrac¸ ˜ao de t´ecnicas de ajuste fino, garantindo con- sistˆencia e eficiˆencia na adaptac¸ ˜ao dos modelos. Por fim, o m´etodo h´ıbrido combina diversas t´ecnicas de PEFT. Em comum, todos os m´etodos ajustam um n´umero reduzido de parˆametros dos MLs.  do  essas  todas  Dentre  t´ecnicas,  o m´etodo  reparametrizado LoRA [Hu et al. 2022] se destaca como um dos m´etodos de PEFT mais utilizados para o ajuste de modelos em diferentes tarefas ao proporcionar consistentemente a reduc¸ ˜ao no n´umero de parˆametros trein´aveis e consequente reduc¸ ˜ao na demanda de mem´oria [Zhao et al. 2024a, Yang et al. 2024]. Essa estrat´egia utiliza matrizes adicionais de baixo posto A e B, que substituem a matriz de pesos original W. A computac¸ ˜ao final dos modelos ´e realizada por meio da express˜ao W + A × B, permitindo a adaptac¸ ˜ao dos pesos com uma quantidade significativamente menor de recursos computacionais.  tipo  Embora eficaz, a LoRA ainda requer a computac¸ ˜ao dos gradientes de ativac¸ ˜ao durante a etapa de backpropagation no treinamento de modelos, o que limita seu potencial   m´aximo de reduc¸ ˜ao de recursos. A estrat´egia GreenTrainer [Huang et al. 2024] surge como uma alternativa que visa reduzir diretamente as operac¸ ˜oes necess´arias para ajustes dos modelos, sem desconsiderar a backpropagation. Ela seleciona tensores espec´ıficos para ajuste a cada ´epoca de treinamento, com base na importˆancia de cada tensor para a diminuic¸ ˜ao da loss, caracterizando-se assim como uma t´ecnica de ajuste parcial. Al´em disso, ela permite a configurac¸ ˜ao do hiperparˆametro ρ, que determina a porcentagem de operac¸ ˜oes mantidas em relac¸ ˜ao ao ajuste fino completo.  Desse modo, ao considerar uma t´ecnica consolidada e amplamente reconhecida como a LoRA, e uma abordagem emergente e competitiva, como o GreenTrainer, este estudo visa realizar uma avaliac¸ ˜ao inicial acerca do impacto dessas abordagens no de- sempenho de PLMs de menor escala em tarefas finais, bem como na reduc¸ ˜ao de seus custos e impactos computacionais.  3. Trabalhos Relacionados  Trabalhos recentes tˆem desenvolvido MLs espec´ıficos para o portuguˆes utilizando m´etodos eficientes. Como ilustrac¸ ˜ao, [Carmo et al. 2020] realizaram tanto o ajuste com- pleto de parˆametros quanto o ajuste restrito aos embeddings do vocabul´ario — um m´etodo parcial — no treinamento de um ML voltado para o portuguˆes. Os resultados indicam que, embora competitivo, o ajuste restrito aos embeddings ´e inferior ao ajuste com- pleto. Al´em disso, os estudos de [Garcia et al. 2024] e [Cabral et al. 2024] introduzi- ram LLMs ajustados especificamente para tarefas em portuguˆes baseados na arquitetura Llama [Touvron et al. 2023], empregando a t´ecnica de reparametrizac¸ ˜ao LoRA.  Outros trabalhos avaliam o impacto de t´ecnicas de PEFT sobre o desempenho de PLMs. [Yang et al. 2024] comparam o ajuste fino completo a t´ecnicas como LoRA, Prefix-tuning [Li and Liang 2021] e o uso de adaptadores [Houlsby et al. 2019] em mo- delos de menor escala da arquitetura BERT [Devlin et al. 2019] em tarefas n˜ao genera- tivas, destacando o desempenho da estrat´egia LoRA e a competitividade das demais es- trat´egias de PEFT em relac¸ ˜ao ao ajuste completo nesse contexto. Contudo, tratando-se da avaliac¸ ˜ao realizada sobre modelos generativos da arquitetura Llama, as t´ecnicas baseadas em LoRA se sobressaem. Resultados similares s˜ao reportados em [Huang et al. 2024], que, ao proporem a estrat´egia GreenTrainer, realizaram uma avaliac¸ ˜ao comparativa com outras t´ecnicas de PEFT, concluindo por sua competitividade. O estudo avaliou PLMs multil´ıngues ou predominantemente voltados ao inglˆes, com parˆametros variando entre 350 milh˜oes e 7 bilh˜oes, revelando o potencial da aplicac¸ ˜ao de t´ecnicas eficientes at´e mesmo nos modelos com menor n´umero de parˆametros.  No melhor de nosso conhecimento, n˜ao h´a, ainda, trabalho que avalie o uso da abordagem GreenTrainer para MLs em portuguˆes. Adicionalmente, nenhum dos trabalhos mencionados apresenta uma an´alise comparativa que considere a relac¸ ˜ao do desempenho de MLs de menor escala no idioma e o impacto de seu consumo em termos de tempo e CO2eq. Desse modo, este estudo visa oferecer novas perspectivas que abordem tanto a efic´acia preditiva de modelos, quanto os custos associados a sua etapa de ajuste.  4. Avaliac¸ ˜ao de T´ecnicas de Ajuste Eficiente  Esta sec¸ ˜ao detalha os MLs avaliados, a tarefa de PLN selecionada e as m´etricas de avaliac¸ ˜ao adotadas para investigar o impacto das t´ecnicas LoRA e GreenTrainer tanto   no desempenho textual quanto no consumo computacional. Destaca-se que o ajuste fino completo dos parˆametros dos modelos foi adotado como baseline.  4.1. Modelos de L´ıngua Selecionados A selec¸ ˜ao de MLs para tarefas espec´ıficas ´e influenciada pelo n´umero de parˆametros e pelo corpus de pr´e-treinamento, fatores cruciais para a viabilidade de execuc¸ ˜ao em diferentes plataformas de hardware e para as capacidades de gerac¸ ˜ao de texto do mo- delo. Modelos maiores geralmente demandam mais recursos computacionais, enquanto o corpus de pr´e-treinamento determina a adequac¸ ˜ao do modelo `as necessidades da ta- refa [Freitas 2024]. No contexto de recursos limitados, foram escolhidos dois mode- los: o OPT-PTBR2, com 125 milh˜oes de parˆametros, baseado na arquitetura Open Pre- trained Transformer (OPT) [Zhang et al. 2022] e adaptado para o portuguˆes do Brasil, e o PTT5-base [Carmo et al. 2020], com 223 milh˜oes de parˆametros, utilizando a arquite- tura T5 [Raffel et al. 2020] e pr´e-treinado com um corpus de p´aginas web em portuguˆes do Brasil. A escolha de modelos menores alinha-se com a necessidade de operar em am- bientes com recursos limitados, mantendo a avaliac¸ ˜ao da qualidade da gerac¸ ˜ao de textos em portuguˆes.  4.2. A Tarefa de PLN Aplicada: Sumarizac¸ ˜ao Para garantir a compatibilidade com a implementac¸ ˜ao p´ublica do GreenTrainer3, a ta- refa de sumarizac¸ ˜ao textual foi selecionada. A sumarizac¸ ˜ao por meio de MLs con- siste em condensar as informac¸ ˜oes de um texto, gerando uma nova vers˜ao que preserva de forma concisa o conte´udo essencial do original. Essa tarefa ´e ampla- mente estudada em PLN, incluindo no contexto do portuguˆes brasileiro [Paiola 2022, Pontes et al. 2022, Feltrin et al. 2023], com LLMs recentemente estabelecendo novos padr˜oes de gerac¸ ˜ao [Souza et al. 2024]. Fatores como a coocorrˆencia de termos relevantes e a fidelidade entre texto original e gerado s˜ao importantes para determinar a qualidade de um resumo. Igualmente relevantes s˜ao aspectos como a aderˆencia a formalidade e pre- cis˜ao gramatical pretendidos. Por exemplo, no contexto jornal´ıstico, resumos de not´ıcias pol´ıticas podem exigir um n´ıvel de formalidade distinto daquele necess´ario para resumos de eventos recentes em um reality show popular, embora, em ambos os casos, a correc¸ ˜ao gramatical seja tipicamente fundamental. Assim, a tarefa de sumarizac¸ ˜ao posiciona-se apropriadamente para a avaliac¸ ˜ao da aplicac¸ ˜ao de t´ecnicas de ajuste de modelos, uma vez que a adequac¸ ˜ao a contextos e dom´ınios espec´ıficos ´e fundamental para garantir a qualidade das gerac¸ ˜oes textuais [Paes et al. 2024].  4.3. M´etricas de Avaliac¸ ˜ao Com o objetivo de avaliar de forma integrada a qualidade do desempenho generativo e os custos e impactos computacionais, trˆes grupos de m´etricas foram usados na an´alise de gerac¸ ˜ao de sum´arios. O primeiro grupo visa medir a aderˆencia dos resumos gerados em relac¸ ˜ao aos textos de referˆencia e ´e composto pelas m´etricas ROUGE [Lin 2004] e BERTScore [Zhang et al. 2020]. A m´etrica ROUGE, amplamente utilizada nesse con- texto, compara a sobreposic¸ ˜ao de n-gramas entre o sum´ario autom´atico e a referˆencia, en- quanto o BERTScore utiliza modelos de l´ıngua baseados em BERT para avaliar a similari- dade semˆantica entre os textos. Neste estudo, a m´etrica ROUGE ´e apresentada pela m´edia  2https://huggingface.co/monilouise/opt125M_portuguese 3https://github.com/pittisl/GreenTrainer/   de suas variantes, ROUGE-1, ROUGE-2, ROUGE-L e ROUGE-S [Souza et al. 2024], que se diferenciam na forma de computar os n-gramas, sendo os resultados expressos em valores percentuais. O BERTScore, por sua vez, ´e expresso em termos da sua compo- nente F1. O segundo grupo de m´etricas visa mensurar explicitamente o impacto e o con- sumo de recursos associados ao ajuste dos modelos, incluindo a contagem do n´umero de (peta) operac¸ ˜oes de ponto flutuante por segundo (PFLOPS), que quantifica as operac¸ ˜oes aritm´eticas necess´arias para o ajuste, o tempo de treinamento dos modelos e a quantidade equivalente de CO2 emitida durante o processo, estimada pela ferramenta dispon´ıvel por [Lacoste et al. 2019]. Por fim, o terceiro grupo aproveita do extenso conjunto de m´etricas fornecidas pelo portal NILC-metrix [Leal et al. 2023]4 para avaliar a qualidade de escrita dos textos gerados. Essas m´etricas extraem valores de diversos indicadores lingu´ısticos para avaliar informac¸ ˜oes sobre morfossintaxe, coes˜ao e coerˆencia.  5. Experimentos Esta sec¸ ˜ao apresenta os experimentos conduzidos, detalhando as configurac¸ ˜oes utilizadas e os resultados obtidos.  5.1. Configurac¸ ˜oes Experimentais  Hiperparˆametros Considerando a premissa de recursos limitados, os modelos foram treinados por apenas uma ´epoca, com uma taxa de aprendizado de 2 · 10−5 e um tamanho de lote de 4. Para a tarefa de sumarizac¸ ˜ao, foram definidos: max input length de 512, max output length de 128, repetition penalty de 2,5 e length penalty de 1,0. No que se refere aos parˆametros do LoRA, utilizou-se r = 8, lora alpha = 32 e uma taxa de dropout de 0,1. O GreenTrainer foi testado com ρ de 0,5 e 0,7, e implementado conforme [Huang et al. 2024]. Tamb´em ao encontro desse trabalho, o modelo OPT-PTBR foi configurado com a estrutura “TL;DR” para sumarizac¸ ˜ao, enquanto o modelo PTT5 usou o prefixo “sumarize: [sequˆencia de entrada]”. Por fim, o BERTScore foi computado utilizando o modelo BERT multilingual5, dada a incompatibilidade da m´etrica com um modelo pr´oprio para o portuguˆes.  Conjunto de Dados A tarefa de sumarizac¸ ˜ao ocorreu com a base Recogna- Summ [Paiola et al. 2024]. Esse conjunto possui origem diversificada, sendo composto por not´ıcias de diferentes fontes de informac¸ ˜ao. Tal diversidade resulta em uma colec¸ ˜ao de documentos que abrangem uma variedade de t´opicos e estilos jornal´ısticos. Ademais, o RecognaSumm cont´em cerca de 135 mil instˆancias em que, para os prop´ositos deste tra- balho, foram selecionadas apenas as colunas referentes ao texto da not´ıcia e ao sum´ario, esse ´ultimo servindo como referˆencia padr˜ao nas m´etricas de avaliac¸ ˜ao. Adota-se a sub- divis˜ao pr´e-estabelecida do conjunto de dados, de 81,2 mil instˆancias para treinamento e 27,1 mil para validac¸ ˜ao e teste cada.  5.2. Resultados Experimentais  A Tabela 1 combina os resultados do primeiro e do segundo grupo de m´etricas avaliados. A porcentagem indica a variac¸ ˜ao positiva ou negativa em relac¸ ˜ao ao ajuste fino completo, com resultados com diferenc¸a percentual inferior a 1% marcados com 0%. Por fins de simplificac¸ ˜ao, as configurac¸ ˜oes de ρ para o GreenTrainer s˜ao denotadas GT-ρ.  4http://fw.nilc.icmc.usp.br:23380/metrixdoc 5https://huggingface.co/google-bert/bert-base-multilingual-cased   Tabela 1. Comparac¸ ˜ao de efici ˆencia, impacto ambiental e m ´etricas textuais.  Modelo  Estrat´egia  PFLOPS  Tempo (h)  CO2eq (kg)  ROUGE  BERTScore  OPT-PTBR (125M params)  PTT5 (220M params)  Ajuste fino GT-0.5 GT-0.7 LoRA  Ajuste fino GT-0.5 GT-0.7 LoRA  16,59 8,18 (51%↓) 11,61 (30%↓) 11,06 (33%↓)  15,67 8,76 (44%↓) 11,50 (27%↓) 10,45 (33%↓)  3,00 1,45 (52%↓) 2,17 (28%↓) 2,28 (24%↓)  3,73 2,38 (36%↓) 2,87 (23%↓) 2,61 (30%↓)  0,24 0,12 (50↓%) 0,17 (29↓%) 0,18 (25↓%)  0,30 0,19 (37↓%) 0,23 (23↓%) 0,21 (30↓%)  7,48 4,60 (39%↓) 7,94 (6%↑) 7,23 (3%↓)  27,82 27,16 (2%↓) 27,56 (1%↓) 26,20 (6%↓)  0,652 0,662 (2%↑) 0,682 (5%↑) 0,672 (3%↑)  0,742 0,739 (0%↓) 0,741 (0%↓) 0,734 (1%↓)  Os resultados indicam que a estrat´egia GT-0.7 apresentou ou a menor degradac¸ ˜ao, ou uma melhora no desempenho textual em comparac¸ ˜ao com o ajuste fino em todos os casos avaliados. Em termos de desempenho computacional, seus resultados s˜ao pr´oximos aos da LoRA, embora ligeiramente inferiores. Observa-se que a configurac¸ ˜ao GT-0.5, a mais eficiente em termos de consumo, apresentou uma queda significativa nos resultados generativos para o OPT-PTBR na m´etrica ROUGE. No entanto, essa mesma configurac¸ ˜ao n˜ao resultou em grandes quedas para o modelo PTT5, indicando que a robustez inerente do modelo deve ser considerada ao aplicar estrat´egias de eficiˆencia dr´astica. Na verdade, essa configurac¸ ˜ao foi superior `a estrat´egia LoRA para esse modelo.  (a.) ROUGE x PFLOPS  (b.) ROUGE x Tempo  (c.) BERTSore x PFLOPS  (d.) BERTScore x Tempo  30  25  20  15  10  5  30  25  20  15  10  5  8  11  14  17  1  1.5  2  2.5  3  3.5  4  0.74  0.72  0.7  0.68  0.66  0.64  8  11  14  17  0.74  0.72  0.7  0.68  0.66  0.64  1  1.5  2  2.5  3  3.5  4  OPT-PTBR  PTT5  GT-0.5  GT-0.7  LoRA  Ajuste Fino  Figura 1. Comparativo entre os desempenhos computacionais e textuais.  A Figura 1 contrasta as m´etricas textuais com as medidas de desempenho. Essa comparac¸ ˜ao reforc¸a a estrat´egia GT-0.7 como a que gera resultados mais pr´oximos do ajuste fino, seguida pela estrat´egia LoRA. Fica evidente tamb´em a leve superioridade da economia da estrat´egia LoRA em relac¸ ˜ao `a GT-0.7. Em termos de economia computa- cional, a estrat´egia LoRA se posiciona consistentemente entre as configurac¸ ˜oes de GT, embora, em termos de resultados generativos, seja inferior `a GT-0.5 para o modelo PTT5. Al´em disso, particularmente para o modelo OPT-PTBR, os resultados de BERTScore obti- dos pelo ajuste eficiente foram superiores ao ajuste completo. Por fim, a Figura 1 tamb´em demonstra a superioridade geral do modelo PTT5 na execuc¸ ˜ao da tarefa, ressaltando o impacto que a escolha adequada do PLM pode implicar.  A Tabela 2 apresenta a distˆancia euclidiana m´edia, calculada com base em cinco conjuntos de m´etricas do NILC-metrix, entre uma amostra de 100 sum´arios gerados para cada configurac¸ ˜ao avaliada e suas respectivas referˆencias. Antes do c´alculo, os valores foram normalizados para o intervalo de 0 a 1. Os melhores resultados est˜ao destacados em negrito, enquanto os segundos melhores est˜ao sublinhados. De modo geral, os valo- res semelhantes observados dentro do mesmo modelo, independentemente da estrat´egia   Tabela 2. M ´etricas em sintaxe, morfologia e sem ˆantica do conjunto NILC-metrix.  Modelo  Estrat´egia  Coes˜ao Referencial  Coes˜ao Semˆantica  Informac¸ ˜oes Semˆanticas  Complexidade Sint´atica  Informac¸ ˜oes Morfossint´aticas  OPT-PTBR (125M params)  PTT5 (220M params)  Ajuste fino GT-0.5 GT-0.7 LoRA  Ajuste fino GT-0.5 GT-0.7 LoRA  0,259 0,269 0,252 0,279  0,340 0,310 0,326 0,247  0,667 0,679 0,652 0,760  0,491 0,505 0,446 0,692  0,587 0,595 0,560 0,600  0,513 0,481 0,492 0,560  0,276 0,255 0,281 0,264  0,253 0,237 0,267 0,258  1,150 1,143 1,058 1,158  0,838 0,853 0,851 0,962  de ajuste, indicam que as t´ecnicas de PEFT n˜ao comprometem significativamente a ca- pacidade de escrita dos modelos de l´ıngua quando comparadas ao ajuste fino. Notavel- mente, as configurac¸ ˜oes do GT obtiveram os melhores resultados em v´arias ocorrˆencias. No entanto, uma avaliac¸ ˜ao comparativa entre os modelos revela que o PTT5 consistente- mente apresenta desempenho superior, especialmente na avaliac¸ ˜ao de Informac¸ ˜oes Mor- fossint´aticas. Esse resultado pode estar relacionado `a etapa de pr´e-treinamento, mais ro- busta nesse modelo, sugerindo que uma execuc¸ ˜ao adequada dessa fase possibilita uma estrat´egia de ajuste mais eficiente. No entanto, uma dualidade surge, pois uma etapa de pr´e-treinamento mais robusta pode resultar em custos mais elevados. De maneira geral, esses resultados corroboram os anteriormente descritos, indicando que, al´em da escolha da estrat´egia de ajuste, a selec¸ ˜ao do modelo mostra-se crucial.  6. Considerac¸ ˜oes Finais Este trabalho conduziu experimentos com estrat´egias de ajuste fino eficiente, empregando dois modelos de menor escala treinados em portuguˆes para a tarefa de sumarizac¸ ˜ao tex- tual. Os resultados indicam que a estrat´egia do GreenTrainer ´e competitiva em relac¸ ˜ao `a estrat´egia j´a estabelecida LoRA. Dependendo da escolha do parˆametro ρ, a estrat´egia pode, inclusive, alcanc¸ar um equil´ıbrio superior entre a degradac¸ ˜ao de desempenho e o ganho de eficiˆencia computacional. Al´em disso, os resultados revelam que a aplicac¸ ˜ao de estrat´egias eficientes pode implicar degradac¸ ˜oes significativas, dependendo da escolha do modelo. Trabalhos futuros incluem avaliar essas estrat´egias em outros modelos e tare- fas, visando obter melhores indicativos sobre a generalizac¸ ˜ao, al´em de considerar novas estrat´egias como LoRETTA [Yang et al. 2024] e GaLore [Zhao et al. 2024b].  Por fim, visando `a transparˆencia, explicitamos os custos totais desta pesquisa, totalizando R$1.642,48 em uso de recursos em nuvem. Os experimentos, realizados na Google Cloud Platform na regi˜ao us-central1, resultaram em emiss˜oes estimadas de 14,52 kgCO2eq, com 364 horas de computac¸ ˜ao em duas GPUs T4 (TDP de 70W) e uma eficiˆencia de carbono de 0,57 kgCO2eq/kWh.  "
        },
        {
            "titulo": "A Hybrid Machine Learning Method to Author Name Disambiguation",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31122",
            "idioma": "Inglês",
            "storage_key": "files/article_31122_30925.pdf",
            "autores": [
                {
                    "nome": "Natan S. Rodrigues",
                    "afiliacao": "UEG / UnB",
                    "orcid": "http://orcid.org/0000-0002-0785-4397"
                },
                {
                    "nome": "Celia G. Ralha",
                    "afiliacao": "UnB",
                    "orcid": "https://orcid.org/0000-0002-2983-2180"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Digital bibliographic repositories, including publications, authors, and research fields are essential for sharing scientific information. Nevertheless, the information retrieval, extraction, and classification efficiency in such archives is threatened by author name ambiguity. This paper addresses the Author Name Disambiguation (AND) problem by proposing a hybrid machine learning method integrating Bidirectional Encoder Representations from Transformers (BERT), Graph Convolutional Network (GCN), and Graph Enhanced Hierarchical Agglomerative Clustering (GHAC) approaches. The BERT model extracts textual data from scientific documents, the GCN structures global data from academic graphs, and GHAC considers heterogeneous networks’ global context to identify scientific collaboration patterns. We compare the hybrid method with AND state-of-the-art work using a publicly accessible data set consisting of 7,886 documents, 137 unique authors, and 14 groups of ambiguous authors, along with recognized validation metrics. The results achieved a high precision score of 93.8%, recall of 96.3%, F1-measure of 95%, Average Cluster Purity (ACP) of 96.5%, Average Author Purity (AAP) of 97.4% and K-Metric of 96.9%. Compared to the AND baseline approach, the hybrid method presents better results indicating a promising approach.",
            "keywords": [
                "AND",
                "BERT",
                "Digital Bibliographic Repositories",
                "GCN",
                "GHAC"
            ],
            "referencias": [
                "AMiner (2005-2024b). Search and mining of academic social networks.",
                ". Tsinghua University, Beijing, 100084. China.",
                "AMiner (2024a). Aminer dataset. Disponível em",
                ".",
                "Beltagy, I., Cohan, A., and Lo, K. (2019). Scibert: Pretrained contextualized embeddings for scientific text. CoRR, abs/1903.10676.",
                "CiteSeerX (2007-2019). Scientific literature digital library and search engine.",
                ". Pennsylvania State University, University Park, PA 16802, USA.",
                "DBLP (1993-2024). The digital bibliography & library project.",
                ". Schloss Dagstuhl, Leibniz-Zentrum fu ̈r Informatik, LZI GmbH.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. page 4171–4186, Minneapolis, Minnesota, USA. Proceedings of NAACL-HLT 2019, Association for Computational Linguistics.",
                "Ferreira, A. A., Gonçalves, M. A., and Laender, A. H. F. (2020). Automatic disambiguation of author names in bibliographic repositories. Synthesis Lectures on Information Concepts, Retrieval, and Services, 12(1):1–146.",
                "Hussain, I. and Asghar, S. (2017). A survey of author name disambiguation techniques: 2010-2016. Knowledge Eng. Review, 32:e22.",
                "Kim, J. and Owen-Smith, J. (2020). Model reuse in machine learning for author name disambiguation: An exploration of transfer learning. IEEE Access, 8:188378–188389.",
                "Kingma, D. P. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
                "Kipf, T. N. and Welling, M. (2017). Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations (ICLR).",
                "Pooja, K. M., Mondal, S., and Chandra, J. (2022). Exploiting higher order multi-dimensional relationships with self-attention for author name disambiguation. ACM Transactions on Knowledge Discovery from Data, 16(5).",
                "Qiao, Z., Du, Y., Fu, Y., Wang, P., and Zhou, Y. (2019). Unsupervised author disambiguation using heterogeneous graph convolutional network embedding. In 2019 IEEE International Conference on Big Data (Big Data), pages 910–919.",
                "Rodrigues, N. S., Mariano, A. M., and Ralha, C. G. (2024). Author name disambiguation literature review with consolidated meta-analytic approach. International Journal on Digital Libraries, pages 1–21.",
                "Shin, D., Kim, T., Choi, J., and Kim, J. (2014). Author name disambiguation using a graph model with node splitting and merging based on bibliographic information. Scientometrics, 100(1):15–50.",
                "Waqas, H. and Qadir, A. (2022). Completing features for author name disambiguation (AND): An empirical analysis. Scientometrics, 127(2):1039–1063.",
                "Waqas, H. and Qadir, M. A. (2021). Multilayer heuristics based clustering framework (MHCF) for author name disambiguation. Scientometrics, 126(9):7637–7678.",
                "Zhang, S., Tong, H., Xu, J., and Maciejewski, R. (2019). Graph convolutional networks: a comprehensive review. Computational Social Networks, 6(11)."
            ],
            "artigo_completo": "Abstract. Digital bibliographic repositories, including publications, authors, and research ﬁelds are essential for sharing scientiﬁc information. Neverthe- less, the information retrieval, extraction, and classiﬁcation efﬁciency in such archives is threatened by author name ambiguity. This paper addresses the Author Name Disambiguation (AND) problem by proposing a hybrid machine learning method integrating Bidirectional Encoder Representations from Trans- formers (BERT), Graph Convolutional Network (GCN), and Graph Enhanced Hierarchical Agglomerative Clustering (GHAC) approaches. The BERT model extracts textual data from scientiﬁc documents, the GCN structures global data from academic graphs, and GHAC considers heterogeneous networks’ global context to identify scientiﬁc collaboration patterns. We compare the hybrid method with AND state-of-the-art work using a publicly accessible data set con- sisting of 7,886 documents, 137 unique authors, and 14 groups of ambiguous authors, along with recognized validation metrics. The results achieved a high precision score of 93.8%, recall of 96.3%, F1-measure of 95%, Average Cluster Purity (ACP) of 96.5%, Average Author Purity (AAP) of 97.4% and K-Metric of 96.9%. Compared to the AND baseline approach, the hybrid method presents better results indicating a promising approach.  1. Introduction Digital bibliographic repositories are vast reservoirs of bibliographic citation information (DBLP [DBLP 2024], ArnetMiner [AMiner 2024b], CiteSeerX [CiteSeerX 2019]). They offer functionalities that allow the identiﬁcation of works by scientists, authors, and their respective academic social networks. The DBLP currently lists around 7 million works in Computer Science, including journals and conference articles. In January 2024, DBLP gathered information on approximately 3.5 million authors, with 227 thousand names of researchers and publications manually veriﬁed by the DBLP team, corresponding to a curation of 34% of all publications in the database.1 ArnetMiner stores information on approximately 2 million of scientiﬁc works, 1.7 million of authors, and 8 million of bibliographic citations [AMiner 2024a].2  By storing millions of information from bibliographic records, digital reposito- ries become an essential source of information for the global academic and scientiﬁc  1https://dblp.org/ 2https://www.aminer.org/   community, allowing retrieval, extraction and classiﬁcation of relevant publications in a centralized manner [Ferreira et al. 2020]. In addition to these bibliographic features, such digital libraries provide helpful analysis useful for better decision-making by scientiﬁc funding agencies and academic institutions [Hussain and Asghar 2017].  However, a common problem in digital bibliographic repositories is automatic Author Name Disambiguation (AND). The AND problem occurs when different authors have the same name record or when an author has multiple name records in the same data set. Such a problem can signiﬁcantly affect the document and information retrieval perfor- mance through Web search engines and obstruct entity integrity for integrated databases. Even though the author’s name ambiguity problem has been studied for decades, it re- mains without a canonical solution. Thus, research efforts to solve the AND problem are essential, especially considering that digital bibliographic repositories are becoming more person-centric than document-centric [Shin et al. 2014].  This work addresses the AND problem with a novel hybrid method combin- ing advanced machine learning techniques, such as the Bidirectional Encoder Represen- tations from Transformers (BERT) [Devlin et al. 2019], Graph Convolutional Network (GCN) [Zhang et al. 2019], and Graph Enhanced Hierarchical Agglomerative Clustering (GHAC) [Qiao et al. 2019]. As proposed by [Kipf and Welling 2017], GCN is a powerful machine learning model that extends the Convolutional Neural Network (CNN) to handle data structured as graphs, capturing local and global dependencies within a network. Our method aims to enhance the AND accuracy in digital bibliographic repositories consid- ering information retrieval, extraction, and classiﬁcation by applying document content semantic treatment related to a graph representation of relationships between documents, authors, and other scientiﬁc attributes.  As presented in [Ferreira et al. 2020], there are several approaches to solving AND problem applying various techniques, but no works combining transfer learning with GCN and GHAC techniques. Also, according to a recent AND literature review, using the theory of the consolidated meta-analytic approach with quantitative techniques and bibliometric aspects, the hybrid method proposed is considered a novel solution to AND [Rodrigues et al. 2024].  The rest of the article includes in Section 2 related work focusing on approaches to the AND problem. Section 3 details the AND hybrid method. Section 4 includes the conducted experiments with the evaluation metrics. In Section 5, we present the results with discussion. Finally, the conclusion and future work are in Section 6.  2. Related Work As presented in [Rodrigues et al. 2024], largely used AND solving approaches are au- thor grouping associated with similarity functions and clustering methods, and some works with author assignment allied to classiﬁcation methods. Also, approaches based on graphs, word embedding with supervised learning, and heuristics with probabilistic applications are common. The literature review highlights author clustering techniques’ prevalence and effectiveness, especially when addressing issues associated with large bib- liographic databases. In this section, we present works most related to our hybrid method.  The authors in [Kim and Owen-Smith 2020] explore supervised techniques using transfer learning on AND tasks where no labelled data was available for training. The   results show that by training source data that well represent the main characteristics of the target datasets, the developed disambiguation models through transfer learning can produce results comparable to those achieved by traditional machine learning approaches, which train algorithms on speciﬁcally labelled subsets of the target data.  In [Waqas and Qadir 2021], the authors propose a method to perform AND based on heuristic clusters in several layers. They used global characteristics and those related to the structure of publications to group them. One of the differences pointed out by the authors is that instead of relying only on keyword information, the approach also consid- ers the contextual structure of publications for grouping. The authors use an incremental classiﬁcation method to reduce errors after creating clusters. A dataset called CustAND was presented for testing and executing the AND method.  The approach of [Pooja et al. 2022] uses GCN in conjunction with attention mech- anisms for learning representations in a heterogeneous graph of documents. The work highlights the importance of using attention at different levels, both about the types of neighbors and relationships, to incorporate relevant context into learning node represen- tations. The emphasis on attention allows a detailed analysis of the impact of this mecha- nism on capturing semantic and contextual information from documents in a graph. The authors used two ArnetMiner variants as data sets, the ﬁrst with 110 and the second with 100 ambiguous name references.  3. The Hybrid Method  The hybrid method has four main steps as presented in Figure 1 and described in the sequence.  Figure 1. The hybrid method workﬂow.  3.1. Data Entry and Preprocessing  The hybrid method’s ﬁrst step deals with the input and preprocessing of document data (publications), when data is adjusted and formatted to ensure input suitability for the subsequent steps.  3.2. Graph Creation and Characterization  This step plays a fundamental role as it creates the structure of the heterogeneous network from the information received from the previous step and provides contextual information for the AND task.   • Creation of a Heterogeneous Network - includes different types of nodes and edges. The graph is formally deﬁned as Gheterogeneous = (Nnodes, Eedges), where Nnodes are nodes representing publications, authors, and words. The edges Eedges represent the different connections between nodes, such as contains (between publications and words), written by (between publications and authors), co − authored (between authors who collaborated on the same publication), and shared− word (between publications that share keywords).  • Embedding Extraction with BERT - BERT uses transfer learning pretraining its parameters on large sets of unlabeled texts with only minor modiﬁcations to per- form tasks in a given domain. BERT converts every word in a text into a vector representation that captures the word’s meaning given the context in which it ap- pears. This representation can be combined to obtain a representation of entire sentences. In this work, we use the SciBERT variant of BERT pre-trained on sci- entiﬁc texts, which is particularly effective at capturing the contextual and seman- tic information of academic documents [Beltagy et al. 2019]. SciBERT calculates the embeddings of publications based on titles and abstracts. These embeddings are incorporated as features of the nodes that represent the publications in the graph. The algorithm is described in the sequence. Given a set of N documents with titles and abstracts, where each document i has a title Ti and an abstract Ri, the embedding extraction process with SciBERT can be detailed as follows:  1. Tokenization of Titles and Abstracts: each title Ti and abstract Ri are tok- enized into sequences of separate tokens, represented by {ti,1, ti,2, . . . , ti,Li} and {ri,1, ri,2, . . . , ri,Mi}, respectively.  2. SciBERT Embedding Generation: the token sequences of the titles Ti and abstracts Ri are processed by BERT, which produces a vector of embed- dings for each document. These embeddings capture the semantics of the texts, reﬂecting the main topics and contextual relations.  3. Graph Embedding: the embeddings resulting from SciBERT are used as nodes features that represent the publications in the heterogeneous graph. In our algorithm, SciBERT performs the embedding extraction on the titles and abstracts of the documents. These embeddings are used as node features in the heterogeneous graph, allowing the subsequent GCN to use these representations to analyze the interactions between publications, authors, and words, including co-authorship relationships. An edge index represents sparsely the connections between nodes. This format allows the GCN to process large heterogeneous net- works while maintaining essential connectivity information among entities.  3.3. Learning Using GCN  After extracting embeddings with SciBERT and constructing the heterogeneous graph, the titles and abstract embeddings are used as features of the nodes in the network. The propagation operations in the GCN layers use these embeddings to compute representa- tions of neighboring nodes. The need to apply a GCN model to a heterogeneous network, instead of other traditional deep learning techniques arises from the particularities of net- works, where relationships between different types of nodes and graph structures must be captured effectively.   In the GCN step, this proposed hybrid method initially processes the textual data to create a vocabulary with a feature matrix, where each row corresponds to the embed- ding of a node, such as documents, authors, or keywords. The edge index represents the connections between nodes, preserving the essential relationships in the heterogeneous network.  To capture local and global dependencies within the heterogeneous network data, each layer of the GCN updates the node representations based on the connections and features deﬁned by the edge index. The proposed GCN model uses activation functions to introduce nonlinearities in the model. In our work, we use the ReLU function (σ(x) = max(0, x)) at different stages of GCN (widely used to mitigate the vanishing gradient problem). GCN training is performed by minimizing the MSE loss function, deﬁned as L = 1 i=1(Zi − Xi)2, where N is the number of nodes in the network, Zi the N ﬁnal GCN output for node i, and Xi the original feature vector of node i. The Adam optimizer [Kingma 2014] was used to adjusts the model’s weights, adjusting an initial learning rate as needed.  (cid:80)N  Finally, GCN produces embeddings of the nodes in the network, represented as low-dimensional vectors that capture both the nodes’ initial features and the network structure. These embeddings are used for subsequent tasks, such as agglomerative hi- erarchical clustering, which will be performed in the next step.  3.4. Generating Hierarchical Agglomerative Clustering The disambiguated authors’ clustering results are generated based on their representa- tions in the heterogeneous network. The goal is to group documents with similar char- acteristics the interactions between publications, authors, and keywords using the GHAC method [Qiao et al. 2019]. The GHAC is an agglomerative hierarchical clustering algo- rithm that integrates network structural information considering the average similarity of the embeddings between the connected nodes. The algorithm is suitable for complex heterogeneous networks as the one built from the embeddings generated by GCN.  Initially, each document is an individual cluster. The iterative algorithm proceeds to merge the clusters with the highest average similarity between their components until reaching the desired number of clusters. The similarity between the two clusters is de- ﬁned based on the normalized inner products of the node embeddings, allowing GHAC to capture the semantic and contextual data relationships.  Documents are grouped to maximize the internal cohesion of the clusters while preserving the semantic and structural interaction characteristics between the different types of entities in the network. This method not only groups documents based on local similarities but also considers the global context of the heterogeneous network, making it particularly effective in organizing complex academic networks and identifying underly- ing co-authorship and scientiﬁc collaboration patterns.  4. Experiments To validate the hybrid method, we conducted experiments comparing to the multi-layer approach with clustering techniques of [Waqas and Qadir 2021] as a baseline, using the public data set CustAND,3 which is composed of 14 ambiguous name groups with 137  3https://github.com/humaira699/CustAND_Full.git   distinct authors and 7,886 documents [Waqas and Qadir 2022]. This dataset is valuable for AND studies with various attributes and complex data relationships. The execution pipeline and the code for implementing this method are available in the repository.4  4.1. Experimental Setup  We used the document titles and abstracts to extract embeddings with SciBERT. We then concatenated these features to form the input text tokenized using the BERT tokenizer limited to 512 tokens. The output was a 768-dimensional embedding representing each document. The empirically deﬁned GCN conﬁguration includes three layers with an em- bedding size of 768, ReLU activation function, Mean Squared Error (MSE) loss function, and the optimization performed with a 0.001 learning rate for the Adam algorithm. We executed the training for 200 epochs with a batch size of 128. Python language was used to execute the experiments in a Google Colab L4 environment with the hardware acceler- ator L4, GPU with 22.5 GB of RAM, CPU with 53 GB of RAM, 201.2 GB disk, and the runtime type conﬁgured for Python 3.  4.2. Evaluation Metrics  The precision, recall, F1-measure, and speciﬁc metrics for clustering, such as Average Cluster Purity (ACP), Average Author Purity (AAP), and K-Metric metrics commonly presented in the AND literature are used to evaluate the experimental results.5  Precision measures the proportion of correctly classiﬁed documents relative to the total number of author documents, assessing the algorithm’s ability to assign documents to authors correctly as Precision = Documents Correctly Classiﬁed . Recall evaluates the ability of Total Documents Classiﬁed the algorithm to retrieve all documents from a real author, measuring the retrieval capacity of the algorithm about real authors as Recall = Documents Correctly Retrieved Total Documents from Real Author. F1-measure is the harmonic mean of precision and recall, providing a balanced metric between these metrics (general performance metric) as F1-measure = 2×Precision×Recall Precision+Recall  .  (cid:80)q  (cid:80)R  ACP evaluates the average purity of the clusters generated by the algorithm about the theoretical clusters. ACP measures how well the documents were grouped into clus- ters that represent real authors as ACP = 1 , where N is the total size of N the publication/paper records in the test set, q is the number of hybrid method/predicted clusters, R is the number of manually generated reference/real clusters, nij is the number of elements in common between the hybrid method-predicted clusters i and the reference clusters j, and nj is the number of elements in the reference cluster j. The purer the clus- ters, the higher the ACP value. AAP measures how fragmented or cohesive the clusters predicted by the algorithm are relative to the reference clusters. A higher AAP indicates that the clusters are less fragmented, as AAP = 1 N  n2 ij ni  (cid:80)R  (cid:80)q  j=1  j=1  i=1  i=1  .  n2 ij nj  K-metric determines the trade-off between the average purity of clusters (ACP) and the average purity of authors (AAP). It is a metric that provides a single measure that considers both the quality of clusters and the quality of document attribution to real ACP × AAP. K-metric helps evaluate the overall performance authors, as K-metric =  √  4https://github.com/natansr/adan_hybrid_method.git 5Cluster purity measures how well the items in a cluster belong to the same real class. For AND it reﬂects the authorship records belonging to a single author within a cluster. A higher purity indicates a more homogeneous cluster where one is the ideal value [Ferreira et al. 2020].   of the disambiguation algorithm by balancing the quality of clusters and the quality of document attribution.  1 ), the ACP is 0.888 ( 1  Figure 2 presents an illustrative example with geometric ﬁgures corresponding to an authorship record, where equal ﬁgures represent the same author. There are three the- oretical clusters and four empirical ones, with one empirical cluster not pure and two au- thorship circle records fragmented across two clusters. The results of the metrics applied to this example, considering the ACP with the empirical clusters include in the ﬁrst two 3 ), the third and fourth clusters two different authors ( 12 clusters three author records ( 32 2 ), 2 + 12 and the last cluster has a single record ( 12 1 )). The AAP values numerators remain the same, but the denominators reﬂect the number of records in the theoretical clusters. For instance, 32 4 represents three records from the same author in an empirical cluster out of four in the theoretical one. The ﬁnal AAP value is 9 × ( 32 0.722 ( 1 2 )), and the K-metric is the geometric mean of ACP and √ 0.888 × 0.722 = 0.8). Precision is 0.857 considering the sum of three author- AAP ( ship record pairs from the same author in the ﬁrst and second empirical clusters and none in the last three clusters. The denominator sums the total number of authorship record pairs from each empirical ( 3+3+0+0+0 3+3+1+0 ). Recall is 0.6 using the same Precision numerator with the denominator the sum of the authorship record pairs that refer to the same author in the theoretical clusters 6, 3, and 1 in the ﬁrst, second, and third theoretical clusters, ). Finally, the F1-measure = 2×(0.857×0.6) respectively ( 3+3+0+0+0  9 ×( 32  4 + 12  4 + 32  3 + 12  2 + 12  3 + 12  3 + 32  2 + 12  0.857+0.6 = 0.7.  6+3+1  Figure 2. Theoretical and empirical clusters.  5. Results and Discussion  In this section, we present our hybrid method results with the evaluation metrics (Sec- tion 4.2) for the CustAND dataset with 14 groups of ambiguous names compared to the baseline work of [Waqas and Qadir 2021]. In the CustAND dataset, an example of an ambiguous name group for “A Choudhary” consists of 12 distinct authors that share the same name in document citation, namely “Ashish Choudhary”, “Amit Choudhary”, “Anil Choudhary”, “Arvind Choudhary”, “Anupam Choudhary”, “Ajay Choudhary”, “Abhishek Choudhary”, “Aniruddha Choudhary”, “Anjali Choudhary”, “Arjun Choudhary”, “Akshay Choudhary”, and “Arun Choudhary”. Table 1 summarizes the metrics for each ambiguous name group presenting average values for our method and the baseline.  Analysis of our method performance metrics for the 14 ambiguous name groups of the CustAND dataset reveals attractive results. Compared to the results reported by [Waqas and Qadir 2021], the average precision across the 14 groups is slightly lower (93.8% versus 94.6%), which may indicate a loss of precision when classifying docu- ments for speciﬁc authors. However, the higher Recall (96.3% versus 92.5%) suggests   that the method applied to ambiguous groups has a better recall capacity and is more ef- ﬁcient in identifying all documents of an author. The F1-measure of 95% across the 14 groups, compared to 93.5% for [Waqas and Qadir 2021], demonstrates that the method achieves a better balance between Precision and Recall.  The ACP and AAP metrics across the 14 groups also outperform the baseline with values of 96.5% and 97.4%, compared to 95.8% and 87%, respectively. These results suggest a higher average purity of the generated clusters and a lower fragmentation of the predicted clusters, reﬂecting a more cohesive and representative grouping of the real authors. Finally, the 96.9% K-metric in the 14 clusters of our method is signiﬁcantly higher than the 91.24% reported by [Waqas and Qadir 2021], indicating that our method achieves a superior balance between the quality of the clusters and the correct attribution of documents to authors.  Table 1. Performance metrics by ambiguous name group.  Ambiguous Name Group A Choudhary J Martin M A Qadir J Mitchell A Gupta J Robinson A Kumar J Smith Bin Li S Kim D Eppstein Z Zhang J Lee K Tanaka Baseline [Waqas and Qadir 2021] Our Method  # Authors Precision Recall F1-measure ACP AAP K-metric  12 9 15 10 8 12 9 12 8 10 3 10 8 11 137 137  1.000 1.000 1.000 1.000 0.853 1.000 1.000 0.938 0.592 0.754 1.000 1.000 1.000 1.000 0.946 0.938  1.000 1.000 1.000 1.000 0.878 1.000 1.000 0.988 0.671 0.944 1.000 1.000 1.000 1.000 0.925 0.963  1.000 1.000 1.000 1.000 0.865 1.000 1.000 0.964 0.632 0.839 1.000 1.000 1.000 1.000 0.935 0.950  1.000 1.000 1.000 1.000 0.875 1.000 1.000 0.972 0.763 0.897 1.000 1.000 1.000 1.000 0.958 0.965  1.000 1.000 1.000 1.000 0.875 1.000 1.000 0.972 0.889 0.895 1.000 1.000 1.000 1.000 0.870 0.974  1.000 1.000 1.000 1.000 0.875 1.000 1.000 0.972 0.826 0.896 1.000 1.000 1.000 1.000 0.912 0.969  6. Conclusion  The main objective of this work was accomplished by proposing and evaluating the res- olution capacity of the AND problem using a hybrid method that involves transfer learn- ing with SciBERT, GCN, and GHAC. When comparing the effectiveness of our hybrid method with the state-of-the-art work of [Waqas and Qadir 2021], using the CustAND dataset, we note that the proposed method outperformed the baseline regarding average accuracy, considering ﬁve of six commonly used metrics of precision, recall, F1-measure, ACP, AAP, and K-metric.  Future experiments include comparison to [Pooja et al. 2022] including the use of other machine learning methods, diverse textual extract information methods, and the adoption of graph neural networks approaches, such as Graph Attention Network (GAT) and GraphSAGE with larger datasets. Also, a manageable data entry implementation for the end user as a graphical user interface to make the solution more user-friendly.   "
        },
        {
            "titulo": "PropBank e anotação de papéis semânticos para a língua portuguesa: O que há de novo?",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31123",
            "idioma": "Português",
            "storage_key": "files/article_31123_30926.pdf",
            "autores": [
                {
                    "nome": "Cláudia Freitas",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0001-6807-8558"
                },
                {
                    "nome": "Thiago Alexandre Salgueiro Pardo",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2111-1319"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "O artigo introduz o Porttinari-base PropBank (PBP): o corpus Porttinari-base com uma camada de papéis semânticos. A anotação foi feita sobre dependências sintáticas, usando regras linguísticas e sob inspeção humana. Foram anotados mais de 40 mil argumentos, e os resultados são discutidos à luz de trabalhos que investigam a generalização das classes do PropBank.",
            "keywords": [
                "PropBank",
                "anotação semântica",
                "papéis semânticos",
                "Dependências Universais"
            ],
            "referencias": [
                "Bick, E. (2007). Automatic semantic role annotation for portuguese. In Proceedings of TIL 2007 - 5th Workshop on Information and Human Language Technology, pages 1713–1716, Rio de Janeiro. Sociedade Brasileira de Computação (SBC).",
                "Branco, A., Carvalheiro, C., Pereira, S., Silveira, S., Silva, J., Castro, S., and Graça, J. (2012). A PropBank for Portuguese: the CINTIL-PropBank. In Calzolari, N., Choukri, K., Declerck, T., Doğan, M. U., Maegaard, B., Mariani, J., Moreno, A., Odijk, J., and Piperidis, S., editors, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 1516–1521, Istanbul, Turkey. European Language Resources Association (ELRA).",
                "de Marneffe, M.-C., Manning, C. D., Nivre, J., and Zeman, D. (2021). Universal Dependencies. Computational linguistics, 47(2):255–308.",
                "de Souza, E. and Freitas, C. (2021). ET: A workstation for querying, editing and evaluating annotated corpora. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 35–41, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "Dong, X. L. (2023). Generations of knowledge graphs: The crazy ideas and the business impact. Proc. VLDB Endow., 16(12):4130–4137.",
                "Duran, M., Lopes, L., das Graças Nunes, M., and Pardo, T. (2023). The dawn of the porttinari multigenre treebank: Introducing its journalistic portion. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 115–124, Porto Alegre, RS, Brasil. SBC.",
                "Duran, M. S. (2014). Manual de anotação do PropBank-Br v2. Technical report, ICMC-USP.",
                "Duran, M. S. and Aluísio, S. M. (2011). Propbank-br: a Brazilian Portuguese corpus annotated with semantic role labels. In Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology.",
                "Duran, M. S. and Freitas, C. (2024). Guia de anotação de papéis semânticos seguindo o modelo PropBank no corpus Porttinari-base. (no prelo). Technical report, ICMC-USP.",
                "Duran, M. S., Torres, L. S., Viviani, M. C., Hartmann, N., and Aluísio, S. M. (2014). Seleção e preparação de sentenças do corpus PLN-BR para compor o corpus de anotação de papéis semânticos Propbank-Br.v2. Technical report, Núcleo Interinstitucional de Linguística Computacional.",
                "Evans, R. and Orasan, C. (2019). Sentence simplification for semantic role labelling and information extraction. In Mitkov, R. and Angelova, G., editors, Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 285–294, Varna, Bulgaria. INCOMA Ltd.",
                "Freitas, C. (2023). Dataset e corpus. In Caseli, H. and Volpe Nunes, M. d. G., editors, Processamento de Linguagem Natural: conceitos, técnicas e aplicações em Português, pages 1–37. BPLN.",
                "Freitas, C. (2024). Anotação de papéis semânticos no corpus Porttinari-base: Procedimentos, resultados e análise. (no prelo). Technical report, ICMC-USP.",
                "Freitas, C., Souza, E., Castro, M. C., Cavalcanti, T., Ferreira da Silva, P., and Corrêa Cordeiro, F. (2023). Recursos linguísticos para o PLN específico de domínio: o Petrolês. Linguamática, 15(2):51–68.",
                "Gung, J. and Palmer, M. (2021). Predicate representations and polysemy in VerbNet semantic parsing. In Zarrieß, S., Bos, J., van Noord, R., and Abzianidze, L., editors, Proceedings of the 14th International Conference on Computational Semantics (IWCS), pages 51–62, Groningen, The Netherlands (online). Association for Computational Linguistics.",
                "Han, H. and Choi, J. (2020). Establishing strong baselines for the new decade: Sequence tagging, syntactic and semantic parsing with bert. In Proceedings of the Thirty-Third International Florida Artificial Intelligence Research Society Conference (FLAIRS 2020).",
                "Hartmann, N. S., Duran, M. S., and Aluísio, S. M. (2016). Automatic semantic role labeling on non-revised syntactic trees of journalistic texts. In Silva, J., Ribeiro, R., Quaresma, P., Adami, A., and Branco, A., editors, Computational Processing of the Portuguese Language, pages 202–212, Cham. Springer International Publishing.",
                "Levin, B. (1993). English Verb Classes and Alternations: a preliminary investigation. The University of Chicago Press, London.",
                "Levin, B. and Rappaport Hovav, M. (2005). Argument Realization. Cambridge University Pres, Cambridge.",
                "Li, T., Kazeminejad, G., Brown, S., Srikumar, V., and Palmer, M. (2023). Learning semantic role labeling from compatible label sequences. In Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15561–15572, Singapore. Association for Computational Linguistics.",
                "Merlo, P. and Van Der Plas, L. (2009). Abstraction and generalisation in semantic role labels: PropBank, VerbNet or both? In Su, K.-Y., Su, J., Wiebe, J., and Li, H., editors, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 288–296, Suntec, Singapore. Association for Computational Linguistics.",
                "Mohebbi, M., Razavi, S. N., and Balafar, M. A. (2022). Computing semantic similarity of texts based on deep graph learning with ability to use semantic role label information. Scientific Reports, 12(1).",
                "Palmer, M., Gildea, D., and Kingsbury, P. (2005). The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71–106.",
                "Pardo, T., Duran, M., Lopes, L., Felippo, A., Roman, N., and Nunes, M. (2021). Porttinari - a large multi-genre treebank for brazilian portuguese. In Anais do XIII Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 1–10, Porto Alegre, RS, Brasil. SBC.",
                "Rodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., Cardoso, H. L., and Osório, T. (2023). Advancing neural encoding of portuguese with transformer albertina pt-*. In Moniz, N., Vale, Z., Cascalho, J., Silva, C., and Sebastião, R., editors, Progress in Artificial Intelligence, pages 441–453, Cham. Springer Nature Switzerland.",
                "Sanches Duran, M. and Aluísio, S. (2015). Automatic generation of a lexical resource to support semantic role labeling in Portuguese. In Palmer, M., Boleda, G., and Rosso, P., editors, Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 216–221, Denver, Colorado. Association for Computational Linguistics.",
                "Tenney, I., Das, D., and Pavlick, E. (2019a). BERT rediscovers the classical NLP pipeline. In Korhonen, A., Traum, D., and Màrquez, L., editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593–4601, Florence, Italy. Association for Computational Linguistics.",
                "Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R. T., Kim, N., Durme, B. V., Bowman, S. R., Das, D., and Pavlick, E. (2019b). What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations.",
                "Wallis, S. (2003). Completing parsed corpora: From correction to evolution. In Abeillé, A., editor, Treebanks: Building and Using Parsed Corpora, pages 61–71. Springer Netherlands, Dordrecht.",
                "Wang, N., Li, J., Meng, Y., Sun, X., Qiu, H., Wang, Z., Wang, G., and He, J. (2022). An MRC framework for semantic role labeling. In Calzolari, N., Huang, C.-R., Kim, H., Pustejovsky, J., Wanner, L., Choi, K.-S., Ryu, P.-M., Chen, H.-H., Donatelli, L., Ji, H., Kurohashi, S., Paggio, P., Xue, N., Kim, S., Hahm, Y., He, Z., Lee, T. K., Santus, E., Bond, F., and Na, S.-H., editors, Proceedings of the 29th International Conference on Computational Linguistics, pages 2188–2198, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.",
                "Yi, S.-t., Loper, E., and Palmer, M. (2007). Can semantic roles generalize across genres? In Sidner, C., Schultz, T., Stone, M., and Zhai, C., editors, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 548–555, Rochester, New York. Association for Computational Linguistics."
            ],
            "artigo_completo": "Resumo. O artigo introduz o Porttinari-base PropBank (PBP): o corpus Porttinari-base com uma camada de pap´eis semˆanticos. A anotac¸ ˜ao foi feita so- bre dependˆencias sint´aticas, usando regras lingu´ısticas e sob inspec¸ ˜ao humana. Foram anotados mais de 40 mil argumentos, e os resultados s˜ao discutidos `a luz de trabalhos que investigam a generalizac¸ ˜ao das classes do PropBank.  1. Introduc¸ ˜ao Entre os m´etodos utilizados para representar computacionalmente informac¸ ˜ao semˆantica em textos est´a a anotac¸ ˜ao de pap´eis semˆanticos (ou SRL – Semantic Role Labeling). Pap´eis semˆanticos s˜ao respons´aveis por indicar quem fez o quˆe, para quem, onde, quando, como, por quˆe, para quˆe, com o quˆe, com quem, etc., e assim estruturam de maneira expl´ıcita e interpret´avel a informac¸ ˜ao contida em enunciados lingu´ısticos. Enquanto ta- refa, a anotac¸ ˜ao de pap´eis semˆanticos tem como objetivo atribuir etiquetas a argumentos de predicadores, indicando o papel que estes argumentos exercem em uma frase.  A anotac¸ ˜ao de pap´eis semˆanticos permite criar representac¸ ˜oes semˆanticas est´aveis ao longo de diferentes realizac¸ ˜oes lingu´ısticas, e as frases 1 a 5 ilustram este ponto. Sin- taticamente, “porta” exerce diferentes func¸ ˜oes, assim como “chave”. Na atribuic¸ ˜ao de pap´eis semˆanticos, “porta” ´e a “coisa abrindo” em todas as 5 frases, e “chave” ´e o instru- mento de abertura em todas as 5 frases, n˜ao importa a func¸ ˜ao sint´atica que exerc¸am. Na frase “O tempo abriu no feriado”, entretanto, ter´ıamos uma outra estrutura argumental (e outra representac¸ ˜ao semˆantica), j´a que estar´ıamos diante de um outro sentido de “abrir”.  1. A chave abriu a porta. 2. Ela abriu a porta com a chave. 3. A porta foi aberta com a chave. 4. A porta foi aberta por ela com a chave. 5. A porta abriu com a chave.  Um PropBank (Proposition Bank, ou Banco de Proposic¸ ˜oes) ´e um corpus que cont´em anotac¸ ˜ao de pap´eis semˆanticos, relacionando verbos1 e seus argumentos di- retamente `as estruturas sint´aticas de um treebank e conforme o modelo sugerido por  1Atualmente, substantivos e adjetivos tamb´em podem ser considerados.   [Palmer et al. 2005], sintaxe-semˆantica [Palmer et al. 2005, Levin 1993, Levin and Rappaport Hovav 2005].  teoricamente, pap´eis semˆanticos estariam na interface  j´a que,  Apesar de ser um fenˆomeno lingu´ıstico amplamente estudado, n˜ao h´a consenso relativo ao conjunto de pap´eis semˆanticos da l´ıngua. No PropBank, a diversidade te´orica acerca dos pap´eis semˆanticos ´e contornada com a utilizac¸ ˜ao de i) argumentos numera- dos, que v˜ao de Arg0 a Arg5; e ii) argumentos modificadores, um conjunto mais am- plo de argumentos. A motivac¸ ˜ao para o conjunto limitado de pap´eis numerados ´e fa- cilitar a generalizac¸ ˜ao para o aprendizado de m´aquina, ainda que alguns estudos mos- trem que este objetivo ´e apenas parcialmente alcanc¸ado [Merlo and Van Der Plas 2009, Gung and Palmer 2021, Li et al. 2023]. A diferenc¸a entre argumentos numerados e argu- mentos modificadores est´a sobretudo na natureza da relac¸ ˜ao sint´atica que o argumento mant´em com o verbo (exigˆencia, nos argumentos numerados, vs opcionalidade, nos ar- gumentos modificadores). A distinc¸ ˜ao entre os argumentos tamb´em ´e motivada pela as- sistematicidade semˆantica dos pap´eis com relac¸ ˜ao aos verbos, e por isso argumentos nu- merados s˜ao espec´ıficos de verbos (o Arg0 do verbo “abrir” ´e “quem abre”, e o Arg0 de “alagar” ´e “causador do alagamento”). Os ArgM, por outro lado, tˆem uma semˆantica espec´ıfica e previs´ıvel (indicada pelo nome da etiqueta, como ArgM-tmp para “tempo” e ArgM-cau para “causa”), e podem se associar a qualquer verbo.  A semˆantica dos argumentos numerados ´e revelada com o alinhamento entre a anotac¸ ˜ao do corpus e o recurso lexical associado ao PropBank, os chamados Frame Files – em nosso caso, dispomos do Verbo-Brasil [Sanches Duran and Alu´ısio 2015]. Assim, a frase (2), segundo o estilo PropBank, ´e anotada como indicado abaixo. Um PropBank, portanto, n˜ao ´e apenas um corpus anotado, mas a associac¸ ˜ao entre um corpus anotado e um l´exico, que indica como os elementos devem ser anotados e o que eles significam.  • Ela[Arg0] abriu a porta[Arg1] com a chave[Arg2].  Neste artigo, apresentamos o Porttinari-base PropBank (PBP), que corresponde `a adic¸ ˜ao de uma camada de pap´eis semˆanticos a todas as frases do corpus jornal´ıstico Porttinari-base, que comp˜oe o treebank Porttinari [Pardo et al. 2021, Duran et al. 2023]. O Porttinari-base ´e padr˜ao ouro na anotac¸ ˜ao sint´atica conforme a teoria Universal Depen- dencies (UD) [de Marneffe et al. 2021]. Com o PBP, o Porttinari-base passa a ser dupla- mente padr˜ao ouro – nas dependˆencias sint´aticas e nos pap´eis semˆanticos —, contribuindo para a disponibilizac¸ ˜ao de recursos de alto n´ıvel para o processamento do portuguˆes.  2. Motivac¸ ˜ao e trabalhos relacionados  O PropBank foi criado com o objetivo de treinar modelos no aprendizado supervisionado. Levando em conta a onipresenc¸a de arquiteturas neurais e grandes modelos de l´ıngua, discutimos brevemente a relevˆancia para o PLN da anotac¸ ˜ao de pap´eis semˆanticos.  Entre as cr´ıticas ao atual paradigma de IA, est˜ao a falta de transparˆencia e de ex- plicabilidade dos m´etodos e resultados. Neste contexto, pap´eis semˆanticos oferecem ma- neiras interpret´aveis de representar semanticamente enunciados verbais, podendo servir de insumo, por exemplo, para a criac¸ ˜ao de grafos de conhecimento [Mohebbi et al. 2022], o que torna este tipo de representac¸ ˜ao semˆantica relevante para a investigac¸ ˜ao acerca da articulac¸ ˜ao entre os grandes modelos de l´ıngua (os LLMs – Large Language Models) e fontes de conhecimento estruturado [Dong 2023]. Na articulac¸ ˜ao entre redes neurais e   SRL, [Mohebbi et al. 2022] prop˜oem uma abordagem de aprendizado de grafos profun- dos para computar similaridade semˆantica de documentos, usando pap´eis semˆanticos.  A anotac¸ ˜ao de SRL tamb´em pode ser usada para avaliac¸ ˜ao de modelos de l´ıngua quanto `a capacidade de representar informac¸ ˜ao semˆantica estruturada e interpret´avel [Tenney et al. 2019a, Tenney et al. 2019b, Han and Choi 2020]. Nessa vertente, nota- mos a escassez de conjuntos de validac¸ ˜ao criados para o portuguˆes, que nos faz uti- lizar conjuntos de dados traduzidos do inglˆes [Rodrigues et al. 2023]. A utilidade da anotac¸ ˜ao de pap´eis semˆanticos no PLN tamb´em pode ser indireta. Considerando o pa- radigma de avaliac¸ ˜ao extr´ınseca, [Evans and Orasan 2019] utilizam SRL para verificar se a simplificac¸ ˜ao textual ´e capaz de facilitar o desempenho na tarefa de SRL. Uma vez que a tarefa de SRL pode ser considerada um passo al´em da an´alise sint´atica, diferentes mo- delos de representac¸ ˜ao sint´atica tamb´em podem ser avaliados em func¸ ˜ao do desempenho obtido na anotac¸ ˜ao SRL, como sugerido em [Freitas 2023].  Desde 2011, o portuguˆes disp˜oe do PropBank-BR [Duran and Alu´ısio 2011], que anotou pap´eis semˆanticos ao estilo PropBank sobre a parte brasileira do treebank Bosque, em sua vers˜ao de sintaxe de constituintes disponibilizada pela Linguateca. Este PropBank levou `a formulac¸ ˜ao (e adaptac¸ ˜ao do inglˆes) de diretivas de anotac¸ ˜ao e permitiu a criac¸ ˜ao do recurso l´exico que codifica os sentidos dos verbos e descreve seus frames sint´aticos, o Verbo-Brasil [Sanches Duran and Alu´ısio 2015]. No entanto, esse recurso ´e ainda limi- tado. Tendo em vista a criac¸ ˜ao de um material maior e mais lexicalmente diversificado, foi criado o PropBank-BR v2 [Duran et al. 2014, Hartmann et al. 2016]. Diferentemente da vers˜ao anterior, este material foi constru´ıdo sobre ´arvores sint´aticas n˜ao revistas. Em ambos os casos, o processo de anotac¸ ˜ao seguiu o PropBank original, com a anotac¸ ˜ao feita de maneira linear, frase a frase. O portuguˆes conta ainda com o CINTIL-PropBank [Branco et al. 2012], um corpus de frases anotadas com estrutura de constituintes e pap´eis semˆanticos, criado de maneira semiautom´atica, com algumas etiquetas anotadas automa- ticamente, e com um conjunto de pap´eis semˆanticos que ´e uma adaptac¸ ˜ao dos argumen- tos numerados de [Palmer et al. 2005]. Por fim, em uma abordagem baseada em regras, [Bick 2007] faz SRL seguindo a Constraint Grammar.  3. O Porttinari-base PropBank  No PBP, a anotac¸ ˜ao de pap´eis semˆanticos foi baseada em dependˆencias, sendo cada argu- mento um ´unico token. A anotac¸ ˜ao foi feita em um arquivo no formato CoNLL-U, que consiste em um texto simples com 10 campos separados por caracteres de tabulac¸ ˜ao2. A anotac¸ ˜ao foi feita nas colunas 10 (nomeada de MISC) para a atribuic¸ ˜ao dos argumentos, e 9 (coluna DEPS) para a anotac¸ ˜ao dos frames. Foram anotados argumentos expl´ıcitos e impl´ıcitos, como sujeitos omitidos. A Figura 1 apresenta a codificac¸ ˜ao da frase “J´unior j´a presidiu a JBS, mas vendeu a sua parte”. Para facilitar a leitura, omitimos os conte´udos das colunas 3 a 6. A coluna 9 indica os frames dos verbos “presidir” e “vender” (res- pectivamente, presidir.01 e vender.01). A coluna 10 informa os pap´eis semˆanticos e seus predicadores. Por exemplo, o token 1, “Junior”, ´e Arg0 do token 3 (“presidir”) e Arg0 do token 8 (“vender”), mesmo que esta ´ultima informac¸ ˜ao n˜ao esteja expl´ıcita na frase.  A anotac¸ ˜ao do PBP utilizou 26 etiquetas. Diferentemente do Propbank original, criamos, no PBP, etiquetas que especificam alguns casos da classe mais geral ArgM-adv:  2https://universaldependencies.org/format.html   Figura 1. Anotac¸ ˜ao de pap ´eis sem ˆanticos nas colunas 9 e 10 do CoNLL-U  ArgM-conseq, para indicar consequˆencia; ArgM-cond, para indicar condic¸ ˜oes e ArgM- comp, para indicar comparac¸ ˜oes. A divergˆencia com relac¸ ˜ao `a lista de pap´eis do PropBank original est´a na classe ArgM-src (source, fonte da informac¸ ˜ao), para ocorrˆencias como De acordo com a pol´ıcia, trata-se de uma ”pris˜ao significativa”para as investigac¸ ˜oes. No PropBank original, este tipo de construc¸ ˜ao n˜ao deve ser anotado, mas as anotamos pela relevˆancia argumentativa/ret´orica. Em consonˆancia com as vers˜oes do PropBank- BR, utilizamos etiquetas espec´ıficas para verbos auxiliares (de tempo, modo, aspecto e voz) para o pronome -se.  O material foi anotado com base no Manual de anotac¸ ˜ao do PropBank- BR v2 [Duran 2014] e nos frames verbais elencados no recurso Verbo-Brasil. Ao longo do projeto, as diretivas de anotac¸ ˜ao foram enriquecidas e atualizadas, dando origem a [Duran and Freitas 2024]. Com relac¸ ˜ao a seus antecessores brasileiros, a anotac¸ ˜ao PBP difere quanto `a independˆencia da camada sint´atica no que se refere `a identificac¸ ˜ao/segmentac¸ ˜ao de argumentos (se a segmentac¸ ˜ao da an´alise sint´atica e a segmentac¸ ˜ao de argumentos indicada no Verbo-Brasil divergirem, seguimos o Verbo- Brasil).  ´E interessante destacar as interferˆencias da sintaxe UD na tarefa de SRL. No PBP, as divergˆencias entre anotac¸ ˜oes sint´atica e semˆantica foram motivadas pela impossibili- dade, em UD, de cruzar arcos sint´aticos (exemplo 1), e em frases com verbos auxiliares, uma vez que a sintaxe UD ´e bastante econˆomica quanto ao que deve ser considerado verbo auxiliar. Na anotac¸ ˜ao UD do Porttinari, est˜ao anotados como auxiliares apenas auxiliares de tempo e voz. Na atribuic¸ ˜ao de pap´eis semˆanticos, por´em, esta economia tem como resultado a (falsa) necessidade de atribuir pap´eis a elementos que, em portuguˆes, n˜ao est˜ao atuando como verbos plenos (“possam”no exemplo 2), e que portanto n˜ao deveriam receber pap´eis semˆanticos – e o resultado ´e uma construc¸ ˜ao sem sentido.  1. O defeito, que a Takata demorou a reconhecer, foi revelado em...  (a) Codificac¸ ˜ao UD: Takata demorou o defeito (b) Codificac¸ ˜ao PBP: Takata reconheceu o defeito  2. O projeto prevˆe que as deduc¸ ˜oes s´o possam ocorrer a partir de 2021  (a) Codificac¸ ˜ao UD: deduc¸ ˜oes possam; prevˆe possam; possam ocorrer (b) Codificac¸ ˜ao PBP: ocorrer deduc¸ ˜oes; prevˆe ocorrer  O fato de verbos de ligac¸ ˜ao serem considerados AUX, com relac¸ ˜oes de de- pendˆencia “especiais” (os argumentos sint´aticos – sujeito e predicativo – ficam disso- ciados do verbo “ser”, e o n´ucleo do sintagma ´e o elemento nominal predicativo) tamb´em levou `a divergˆencia de anotac¸ ˜oes, j´a que o verbo “ser” tem pap´eis semˆanticos para as posic¸ ˜oes de sujeito e de predicativo do sujeito.   4. Metodologia  Em termos gerais, a anotac¸ ˜ao de pap´eis semˆanticos no PBP seguiu as seguintes etapas, algumas delas concomitantes:  1. Identificac¸ ˜ao do predicador, que em nosso caso foram apenas os verbos; 2. Consulta ao Verbo-Brasil para selec¸ ˜ao do frame adequado; 3. Anotac¸ ˜ao dos argumentos numerados conforme descritos no Verbo-Brasil; 4. Anotac¸ ˜ao dos argumentos modificadores conforme descritos nas diretivas; 5. Caso necess´ario, criac¸ ˜ao de frames provis´orios para verbos novos ou sentidos no-  vos de formas verbais j´a presentes no Verbo-Brasil;  6. Aplicac¸ ˜ao de regras de validac¸ ˜ao para detectar problemas na anotac¸ ˜ao.  Todo o processo de anotac¸ ˜ao foi feito com base em regras linguisticamente moti- vadas e de maneira n˜ao-linear [Wallis 2003], seguindo o exemplo de [Freitas et al. 2023]. Nisto, diferimos do processo de anotac¸ ˜ao do PropBank original, no qual cada frase era anotada inteiramente de uma vez, e do PropBank-BR vers˜oes 1 e 2, que seguiu a mesma metodologia. A anotac¸ ˜ao foi feita com a ferramenta ET [de Souza and Freitas 2021], uti- lizando o ambiente Interrogat´orio.  A anotac¸ ˜ao foi feita em 3 fases: (i) anotac¸ ˜ao de elementos expl´ıcitos, sempre que poss´ıvel usando regras com padr˜oes l´exico-sint´aticos derivados dos exemplos de Du- ran (2014) e das frases-exemplo no Verbo-Brasil; (ii) anotac¸ ˜ao de elementos impl´ıcitos (que envolveu sobretudo a propagac¸ ˜ao de sujeitos, feita com regras) e (iii) aplicac¸ ˜ao final de regras de validac¸ ˜ao e detecc¸ ˜ao de inconsistˆencias. Quase todo o processo foi semi- autom´atico, utilizando regras que associam um padr˜ao de busca a uma regra de anotac¸ ˜ao, sempre com revis˜ao humana. Na propagac¸ ˜ao de sujeitos omitidos de verbos na forma infinitiva, explicitamos argumentos apenas se estes fossem recuper´aveis (frase 1). Em caso de d´uvida ou em caso de argumentos n˜ao recuper´aveis (frase 2), nada foi feito. Em [Freitas 2024] est˜ao detalhados os procedimentos e regras utilizados.  1. O presidente centrista optou por garantir pela a primeira vez em anos que. . . (O  presidente garantiu)  2. S˜ao mecˆanicas que pressionam a entender que isso tem custo. (n˜ao ´e poss´ıvel  determinar quem entender´a)  A anotac¸ ˜ao de elementos impl´ıcitos levou a um outro tipo de desalinhamento entre sintaxe e semˆantica. Na frase “A Folha pediu [contato com o general Mour˜ao] , para que comentasse suas declarac¸ ˜oes, mas (...)”, o segmento “contato com o general Mour˜ao” ´e Arg1 do verbo “pedir”, mas apenas “general Mour˜ao” ´e sujeito/Arg0 de “comentar”.  A validac¸ ˜ao final consistiu na aplicac¸ ˜ao de 4 regras (Figura 2), que buscavam frases com condic¸ ˜oes suspeitas. Foram encontrados 101 casos suspeitos, e apenas dois deles (derivados da regra 4) eram falsos positivos. Todos os erros foram corrigidos ma- nualmente. Diferentemente das regras utilizadas no processo de anotac¸ ˜ao, as regras de validac¸ ˜ao podem ser aplicadas para a verificac¸ ˜ao final de outros corpora com anotac¸ ˜ao de pap´eis semˆanticos. As regras de anotac¸ ˜ao, por sua vez, n˜ao foram criadas com o objetivo de serem generaliz´aveis para outros corpora, mas de criar um material padr˜ao ouro de qualidade e no menor tempo poss´ıvel. No entanto, a elaborac¸ ˜ao de um anotador baseado em regras, que aproveite estas regras e o corpus j´a anotado, ´e algo bastante poss´ıvel.   Figura 2. Regras para detecc¸ ˜ao de erros  A anotac¸ ˜ao foi feita por uma ´unica pessoa, por 7 meses, a partir das informac¸ ˜oes contidas no Manual de anotac¸ ˜ao do PropBank v2 e no Verbo-Brasil. A fim de avaliar a qualidade da anotac¸ ˜ao, foi feita uma concordˆancia inter-anotadores a posteriori, tomando como base uma amostra com as 100 frases com a maior quantidade de argumentos anota- dos no Propbank-BR v2, sobre o qual se baseiam as diretivas de anotac¸ ˜ao e o Verbo-Brasil. ´E Essa amostra foi reanotada sint´atica e semanticamente, e os resultados comparados. importante notar tamb´em que, embora tenhamos escolhido as 100 frases com a maior quantidade de argumentos anotados, nem todos os verbos do PropBank-BR v2 tˆem seus argumentos anotados, apenas aqueles mais frequentes no corpus. A comparac¸ ˜ao, medida com o ´ındice kappa, foi sobre 443 tokens anotados por ambas as anotac¸ ˜oes, e resultou em uma convergˆencia de .90 (como comparac¸ ˜ao, no Propbank original, e considerando apenas a classificac¸ ˜ao de pap´eis incluindo Arg-M, o kappa foi de .93).  O processo de anotac¸ ˜ao dos frames dos verbos foi concomitante `a anotac¸ ˜ao dos pap´eis semˆanticos. Uma vez que o foco da anotac¸ ˜ao PBP esteve na atribuic¸ ˜ao dos pap´eis semˆanticos, o processo de atribuic¸ ˜ao de sentidos aos verbos n˜ao foi exaustivo. Al´em de n˜ao exaustiva, a anotac¸ ˜ao privilegiou o alinhamento com verbos n˜ao monossˆemicos, uma vez que a anotac¸ ˜ao de verbos monossˆemicos poderia ser feita (e foi) de forma autom´atica.  Apesar de j´a dispor de um recurso como o Verbo-Brasil, um corpus novo sem- pre traz novas formas verbais e novos sentidos para verbos j´a descritos. Para os sentidos (ainda) sem frames, foi feita uma busca por um verbo similar no Verbo-Brasil, e a soluc¸ ˜ao foi indicada em um documento para posterior aprimoramento do Verbo-Brasil. Ao fi- nal do processo, foram documentados cerca de 350 sentidos de verbos sem frames, com exemplos do corpus e soluc¸ ˜oes provis´orias em boa parte deles. A anotac¸ ˜ao de frames monossˆemicos foi feita de maneira autom´atica para os casos de verbos monossˆemicos (ou seja, que s´o dispunham de um frame). Este procedimento levou `a inclus˜ao de mais de 500 frames no PBP.  A relac¸ ˜ao estreita entre anotac¸ ˜ao sint´atica e anotac¸ ˜ao de pap´eis semˆanticos, por um lado, e as interferˆencias da anotac¸ ˜ao sint´atica UD, por outro, levaram `a criac¸ ˜ao de diferentes vers˜oes do corpus, e com isso tamb´em criamos condic¸ ˜oes para investigar o papel de diferentes representac¸ ˜oes lingu´ısticas no aprendizado de SRL. Cada uma das vers˜oes ´e gerada automaticamente a partir de uma vers˜ao base.  1. PBP na vers˜ao UD: Esta vers˜ao se caracteriza pela atribuic¸ ˜ao de pap´eis semˆanticos apenas aos elementos considerados verbos plenos na UD. Em con- sequˆencia: (i) n˜ao foram anotados os pap´eis de argumentos do verbo “ser”; (ii)   foram anotados os pap´eis de argumentos de verbos considerados plenos pela UD, mas considerados auxiliares no Verbo-Brasil. No entanto, para diferenci´a-los dos demais argumentos, receberam a etiqueta Arg1 d e Arg0 d. Se desej´avel, ambas as etiquetas podem ser substitu´ıdas por Arg1 e Arg0, respectivamente.  2. PBP na vers˜ao cl´assica: Esta vers˜ao prioriza o conceito de proposic¸ ˜ao, e se carac- teriza pela atribuic¸ ˜ao de pap´eis conforme o modelo PropBank, independentemente do que foi considerado verbo pleno pela UD. Em consequˆencia: (i) foram anota- dos os pap´eis de argumentos do verbo “ser”; (ii) n˜ao foram anotados os pap´eis de argumentos de verbos que, apesar de considerados plenos na UD, s˜ao considera- dos auxiliares no Verbo-Brasil e na documentac¸ ˜ao [Duran and Freitas 2024]; e iii) foram anotados como modificadores auxiliares os verbos que, apesar de conside- rados plenos pela anotac¸ ˜ao UD, s˜ao considerados auxiliares no PBP (ArgM-mod, ArgM-asp), al´em daqueles considerados sempre auxiliares (ArgM-tml; ArgM- pas), seguindo a decis˜ao do PropBank-BR v2 [Duran 2014].  5. Resultados e conclus˜oes  Foram anotados 45.813 argumentos verbais e 13.395 instˆancias verbais contˆem anotac¸ ˜ao de frames, distribu´ıdos em quase 1.018 frames distintos (60,8% dos verbos possui anotac¸ ˜ao de frame). As vers˜oes anteriores do PropBank-BR continham cerca de 7 mil argumentos anotados. A Figura 3 apresenta a distribuic¸ ˜ao dos argumentos por func¸ ˜ao sint´atica e a Figura 4 traz, com mais detalhes, os pap´eis semˆanticos mais frequentes para cada relac¸ ˜ao sint´atica (deprel). Todos os n´umeros se referem `a vers˜ao cl´assica, alinhada `a vers˜ao 1.0 do treebank Porttinari-base.  Vemos que a associac¸ ˜ao mais frequente ´e entre obj e Arg1, com 92,03%, seguida da associac¸ ˜ao entre nsubj e Arg0, com 63,49%. O artigo de [Palmer et al. 2005] traz o mesmo tipo de an´alise para o PropBank original. No entanto, uma vez que cada gram´atica recorta e define os elementos que lhes parecem relevantes, ´e necess´ario algum cuidado nessa comparac¸ ˜ao. O elemento sentencial/oracional S, presente no PropBank original (derivado da anotac¸ ˜ao do Penn Treebank), n˜ao existe em UD, e est´a distribu´ıdo entre alguns casos de xcomp e de ccomp, por exemplo.  De forma geral, analisando os dados do PBP, extra´ımos as seguintes informac¸ ˜oes: (a) Arg1 se distribui principalmente entre as relac¸ ˜oes obj, nsubj, obl, xcomp e ccomp; (b) Arg0 se concentra em nsubj; (c) Arg2 se distribui principalmente entre obl e xcomp; (d) ArgM-tmp e ArgM-mnr se distribuem entre obl, advmod e advcl; (e) ArgM-loc se concentra em obl. Com a perspectiva da sintaxe: (a) obj participa de Arg1 e parece ser a generalizac¸ ˜ao mais f´acil: “se ´e um obj, ent˜ao ´e Arg1”; (b) xcomp participa igualmente de Arg1 e Arg2; (c) obl participa de Arg2, Arg1, tempo, modo e local; (d) advmod participa de neg, adv, tempo, dis e modo; (e) advcl participa de tempo, finalidade, modo e Arg2.  Os dados apontam para uma regularidade entre func¸ ˜oes sint´aticas e pap´eis nume- rados – especificamente, pap´eis Arg0 e Arg1. Os demais argumentos numerados, com baixa frequˆencia, s˜ao de mais dif´ıcil generalizac¸ ˜ao. Estas constatac¸ ˜oes convergem com resultados para o inglˆes, que verificam que a anotac¸ ˜ao ao estilo PropBank captura me- lhor regularidades sint´aticas, sobretudo para argumentos de frequˆencia alta, em oposic¸ ˜ao ao estilo VerbNet de anotac¸ ˜ao de pap´eis semˆanticos [Merlo and Van Der Plas 2009]. Li- dos de outra forma, embora o objetivo de um PropBank seja muitas vezes servir como   Figura 3. Distribuic¸ ˜ao de pap ´eis sem ˆanticos por relac¸ ˜ao sint ´atica  Figura 4. Pap ´eis sem ˆanticos mais frequentes para cada relac¸ ˜ao sint ´atica  material de treino para IA, a forma de codificar os pap´eis talvez seja mais indicada (ou tamb´em seja indicada) para um anotador baseado em regras e que considere sintaxe. De fato, [Palmer et al. 2005] relatam que um anotador simples baseado em regras tem de- sempenho de 83%, sendo 85% o estado-da-arte em inglˆes, considerando cen´arios dif´ıceis de avaliac¸ ˜ao, com verbos n˜ao vistos na fase de treino [Wang et al. 2022].  Apesar de raros, estudos sobre a capacidade de generalizac¸ ˜ao da anotac¸ ˜ao ao estilo PropBank tˆem mostrado que, quando comparada `a anotac¸ ˜ao ao estilo VerbNet, a anotac¸ ˜ao PropBank leva a resultados inferiores no que se refere a argumentos Arg2 a Arg5 e, em termos gerais, os bons resultados obtidos com a anotac¸ ˜ao PropBank se referem aos argumentos mais frequentes [Merlo and Van Der Plas 2009, Yi et al. 2007, Gung and Palmer 2021, Li et al. 2023]. No entanto, todos os estudos foram feitos para a l´ıngua inglesa, e apenas a disponibilizac¸ ˜ao de recursos similares para o portuguˆes nos permitiria verificar os resultados para a nossa l´ıngua.  Por fim, a criac¸ ˜ao de duas vers˜oes (UD e Cl´assica) tamb´em permite comparar a anotac¸ ˜ao PBP com outros PropBanks que tenham seguido mais fielmente a anotac¸ ˜ao UD. As diferentes vers˜oes do corpus, bem como documentac¸ ˜ao lingu´ıstica detalhada e regras de anotac¸ ˜ao utilizadas, est˜ao p´ublicas no portal web do projeto POeTiSA3.  "
        },
        {
            "titulo": "No Argument Left Behind: Overlapping Chunks for Faster Processing of Arbitrarily Long Legal Texts",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31124",
            "idioma": "Inglês",
            "storage_key": "files/article_31124_30927.pdf",
            "autores": [
                {
                    "nome": "Israel Fama",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0001-6325-4153"
                },
                {
                    "nome": "Bárbara Bueno",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0009-0004-7455-3342"
                },
                {
                    "nome": "Alexandre Alcoforado",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-3184-1534"
                },
                {
                    "nome": "Thomas Palmeira Ferraz",
                    "afiliacao": "Télécom Paris / Institut Polytechnique de Paris",
                    "orcid": "https://orcid.org/0000-0002-5385-9164"
                },
                {
                    "nome": "Arnold Moya",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0009-0001-2377-379X"
                },
                {
                    "nome": "Anna Helena Reali Costa",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0001-7309-4528"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "In a context where the Brazilian judiciary system, the largest in the world, faces a crisis due to the slow processing of millions of cases, it becomes imperative to develop efficient methods for analyzing legal texts. We introduce uBERT, a hybrid model that combines Transformer and Recurrent Neural Network architectures to effectively handle long legal texts. Our approach processes the full text regardless of its length while maintaining reasonable computational overhead. Our experiments demonstrate that uBERT achieves superior performance compared to BERT+LSTM when overlapping input is used and is significantly faster than ULMFiT for processing long legal documents.",
            "keywords": [
                "Natural Language Processing",
                "Legal NLP",
                "Text Classification",
                "Deep Learning",
                "Transformers"
            ],
            "referencias": [
                "Beltagy, I., Peters, M. E., and Cohan, A. (2020). Longformer: The long-document transformer.",
                "CNJ (2024). Conselho nacional de justiça - cnj. Accessed: 2024-08-05.",
                "Cui, J., Shen, X., Nie, F., Wang, Z., Wang, J., and Chen, Y. (2022). A survey on legal judgment prediction: Datasets, metrics, models and challenges. arXiv preprint arXiv:2204.04859v1.",
                "Demšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1–30.",
                "Ferraz, T. P., Alcoforado, A., Bustos, E., Oliveira, A. S., Gerber, R., Müller, N., d’Almeida, A. C., Veloso, B. M., and Costa, A. H. R. (2021). Debacer: a method for slicing moderated debates. In Anais do XVIII Encontro Nacional de Inteligência Artificial e Computacional, pages 667–678. Sociedade Brasileira de Computação-SBC.",
                "Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.-W. (2020). Realm: Retrieval-augmented language model pre-training.",
                "Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontañón, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. (2020). Big bird: Transformers for longer sequences. CoRR, abs/2007.14062",
                "Zhang, L., Wang, W., Yu, K., huang, J., Lyu, Q., Xue, H., and Hetang, C. (2023). Sliding-bert: Striding towards conversational machine comprehension in long contex. Adv. Artif. Intell. Mach. Learn., 3:1325–1339.",
                "Zhu, H., Mak, D., Gioannini, J., and Xia, F. (2020). NLPStatTest: A toolkit for comparing NLP system performance. In Wong, D. and Kiela, D., editors, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pages 40–46, Suzhou, China. Association for Computational Linguistics."
            ],
            "artigo_completo": "Abstract. In a context where the Brazilian judiciary system, the largest in the world, faces a crisis due to the slow processing of millions of cases, it becomes imperative to develop efficient methods for analyzing legal texts. We introduce uBERT, a hybrid model that combines Transformer and Recurrent Neural Net- work architectures to effectively handle long legal texts. Our approach processes the full text regardless of its length while maintaining reasonable computational overhead. Our experiments demonstrate that uBERT achieves superior perfor- mance compared to BERT+LSTM when overlapping input is used and is signif- icantly faster than ULMFiT for processing long legal documents.  1. Introduction  Legal NLP can be defined as the application of Natural Language Processing (NLP) tech- niques within the legal domain. This subfield of NLP has been experiencing rapidly growing interest from both academia and industry: [Katz et al. 2023] reports a signifi- cant increase in the volume of publications, rising from fewer than 30 papers in 2013 to nearly 120 in 2022. Brazil possesses the largest judiciary system in the world, comprising 18,000 judges distributed across 91 courts. At the time of writing, there are more than 84 million ongoing legal cases [CNJ 2024]. These numbers indicate both the need and the opportunity for innovative solutions to manage and analyze vast amounts of legal data.  We turn our focus to Legal Judgment Prediction (LJP), which involves predicting court decisions. Although predicting decisions may be a complex task, we argue it can be reduced to a Text Classification task, which has seen a marked increase in studies [Li et al. 2022], fueled by advancements in deep learning. In particular, the Transformer architecture emerged as a paradigm shift [Hasan 2022] for many NLP tasks. However, it still has limitations when handling long texts, which poses significant challenges in the legal domain, where documents are usually long and complex.  There is fruitful research being done on enhancing the input size limita- tion for Transformers, such as Retrieval-Augmented Language Models (RALMs) [Guu et al. 2020]. Current retrieval techniques, however, often trust embedding mod- els which also can be sub-optimal when dealing with legal documents, where a single word in the whole document can make a difference. Also, these methods demand sub- stantial computational resources and large document stores to achieve good performance.  *These authors contributed equally to this work.   Other methods combine input in a sequential way, often leveraging properties of Recur- rent Neural Networks to process longer sequences [Wan et al. 2019], although those will also usually truncate the text if it is too long. But for documents in the legal domain, such as judicial decisions, most of the documents are usually composed of reasoning from the judge. Therefore, it is of our interest to have a method that uses the full text as input.  In this paper, we propose uBERT, a hybrid model that combines an encoder-based Transformer with a Recurrent Neural Network, capable of processing long texts. We propose an experimental setup with data from legal decisions, and compare uBERT to baselines BERT+LSTM, Big Bird and ULMFiT in the classification task. Our results show that uBERT slightly outperforms BERT+LSTM as long as overlapping input is in- troduced. Also, ULMFiT performs better for long texts, but is 4x slower than uBERT.  The remainder of this paper is structured as follows: Sect. 2 reviews related work on Legal NLP and long text classification; Sect. 3 outlines our proposal, including the formalization of the target task and the introduction of our model; Sect. 4 outlines the experiments we setup to assess our model in terms of performance and efficiency. Finally, we present the results and conclude with a discussion of the findings.  2. Related Work  Transformer-Based Approaches for Long Text Processing in Legal NLP: Long- former [Beltagy et al. 2020] employs a sparse attention mechanism, extending the input size limit to 4096 tokens, which is eight times the limit of BERT [Devlin et al. 2019]). [Hoang et al. 2023] applied this architecture to classify legal texts from the Indian Legal Documents Corpus – ILDC [Malik et al. 2021], but they did not process the entire text.  [Pappagari et al. 2019] introduced RoBERT, a method that splits long texts into overlapping chunks for recurrent encoding. While similar in concept to our architecture, a direct comparison is not possible due to limited details on their overlap and recurrence strategies. Moreover, RoBERT was evaluated on shorter texts compared to our dataset.  The overlapping algorithm in our approach, uBERT, is a specific case of Sliding- BERT’s method [Zhang et al. 2023], with the stride set to half the overlap. Unlike Slid- ingBERT, where tokens can appear in multiple chunks, we limit overlaps to two chunks to reduce computational overhead while preserving context continuity. This choice is driven by efficiency, not language differences.  [Menezes-Neto and Clementino 2022] introduced BrCAD-51, a dataset designed for Legal Judgment Prediction (LJP), and evaluated three architectures for this task: ULMFiT [Howard and Ruder 2018], BigBird [Zaheer et al. 2020], and BERT+LSTM. ULMFiT, a transfer learning model that fine-tunes a pre-trained language model for down- stream NLP tasks, was the only architecture capable of processing the entire text as input. BigBird, a sparse-attention model, addresses the 512-token limit by focusing on subsets of tokens, thereby reducing computational complexity, and was configured to handle texts up to 7,680 tokens. For BERT+LSTM, documents were split into 512-token chunks, with truncation applied to middle chunks if a document required more than 15. While simi-  1This dataset consists of decisions issued by the Brazilian Federal Small Claims Court (FSCC). These decisions can be appealed to the Appellate Panel (AP), which re-examines the case and either reverses or affirms the initial ruling. Each data point in BrCAD-5 represents the text of a decision issued by the FSCC. The task proposed by the authors is to predict whether the AP will reverse or affirm the initial ruling based on the decision text.   lar in approach, uBERT differs from BERT+LSTM in that it uses a chunk overlapping strategy and imposes no limit on the number of chunks, ensuring the entire text is utilized without truncation.  Critiques and Limitations in Legal NLP Research: The legal industry has been slow to adopt NLP advancements, relying heavily on manual work by lawyers. [Mahari et al. 2023] identify a key issue: Legal NLP research often fails to align with the practical needs of legal practitioners. [Medvedeva and Mcbride 2023] further highlight a significant gap in Legal Judgment Prediction (LJP) research, criticizing the use of poorly designed datasets that rely on biased case facts extracted from judgments. This approach leads to models with overly optimistic performance that offer limited prac- tical value to legal practitioners.  This work aims to bridge the gap between research and practice in the field of Legal NLP. We propose an architecture capable of processing virtually infinite-length le- gal texts and evaluate it on the BrCAD-5 dataset, which [Medvedeva and Mcbride 2023] regard as a well-designed benchmark.  3. Proposal  Text classification can be formalized as follows. Given a document d that represents a judicial decision, the goal is to make a prediction y ∈ {0, 1}, by learning a binary classifier f such that f (d) = y. The positive class y = 1 represents a decision that will be reversed by an Appellate Panel (AP). Since legal documents are often long, when using Transformer-based models, conventional approaches usually truncate text from d, which is sub-optimal for the task [Pappagari et al. 2019]. This can hinder performance on the Legal Judgment Prediction task, since some relevant part of the text may be cut off.  Believing that the text as a whole is more useful when learning a classifier, we pro- pose unlimited BERT, or uBERT, an efficient architecture that combines an encoder-based Transformer with a Recurrent Neural Network, utilizing an overlapping algorithm during both training and inference to handle an unlimited number of input tokens. This approach is similar to the BERT+LSTM model used by [Menezes-Neto and Clementino 2022], but introduces modifications to maintain local context (through overlapping chunks) and ac- commodate documents of virtually any size. Although the quadratic memory complexity of the self-attention mechanism presents a challenge for scaling input indefinitely, we leverage the RNN’s capacity to process long sequences, enabling it to take chunk em- beddings and output a comprehensive document embedding. Several studies, such as [Hoang et al. 2023], have explored the combination of attention mechanisms and recur- rence. Our model builds on this concept but applies overlapping during both training and inference, and does not limit the number of chunks processed by the encoder.  Figure 1 depicts the uBERT architecture. It shows key aspects to understand how  our model works.  Let E be an encoder-based Transformer, with dim being the dimensionality of the output vector of the final layer, and R be a Recurrent Neural Network. Let maxtok be the maximum number of tokens E can process as input. Let maxc be the maximum number of chunks of maxtok tokens that E can process in parallel with a single run. We split document d into n chunks of size maxtok tokens, starting in the first token. For each   Figure 1: uBERT architecture.  run, we extract the hidden states from the last four layers of E, and concatenate them to form the representation of each chunk. This is based on the idea that different layers capture different linguistic features [Tenney et al. 2019]. Specifically, BERT+LSTM, the baseline most similar to our proposed architecture, extracts the hidden states from the last four layers. While other layers could be used for extraction, we retained this approach for consistency in model comparison. In each single run, we process [1, maxc] chunks in parallel, generating [1, maxc] vectors of embeddings, each with dimensionality 4 × dim. We iteratively process chunks from d until an embedding vector is generated for each chunk and thus preserving the entire text content of d.  Then, we concatenate the embedding vectors maintaining the order of the respec- tive chunks, generating a tensor of dimensionality (n, 4 × dim). We process this tensor with the RNN sequentially, capturing the dependencies between them and generating a contextually enhanced representation for each chunk.  Splitting text by token count can disrupt its flow, so we use token overlap between chunks during both training and inference to maintain continuity. This technique, similar to that used by [Hoang et al. 2023] but applied more broadly, helps preserve the text’s natural structure.  Our token overlap algorithm can be formalized as follows. Consider the judicial decision d as the tokenized sequence S = {t1, . . . , tk}, where k is the number of tokens in d. We define the overlap size, z, as the number of tokens each chunk shares with its (cid:5) adjacent neighbors. Thus, any chunk shares (cid:4) z with the subsequent one. The first and last chunks, having only one adjacent chunk, share (cid:4) z 2  (cid:5) tokens with their respective neighbors.  (cid:5) tokens with the previous chunk and (cid:4) z  2  2  Figure 2: Overlapping chunks example.  Figure 2 provides a simple example for clarification. In this example, the chunk  TokensRaw textOverlapping chunksbatch_1batch_nEBERT_output_1BERT_output_nChunk embeddingsRf size is 4, and z = 2. As shown, chunk c2 = {t4, t5, t6, t7} shares token t4 with chunk c1 and token t7 with chunk c3. Note that the first and last chunks share only one token with the neighboring chunk.  4. Experiments  In this section, we design experiments to assess our proposed model, uBERT, and validate its effectiveness on the legal domain. We split our experiments into 3, one for each of the following research questions:  RQ1: Would an encoder-based model benefit from using the entire text in terms of performance improvement?  We examine the impact of processing the whole documents using multiple encoder passes. We first tested if simply increasing text chunks to process the whole text without using overlap (uBERT 0) improves performance over BERT+LSTM, which processes only par- tial text in a single pass. Then, we investigated the effect of introducing overlaps (0 to 510 tokens2) between chunks to observe if the added local context enhances predictions.  RQ2: If performance improves, does it come with reasonable computational over- head?  We compare the inference time of our architecture against all baseline models to deter- mine if it offers a performance gain and to assess the associated computational overhead.  RQ3: Is our architecture better for processing longer texts?  We explore the relationship between document length and model performance. We tested the models on the full test set as well as on its subsets, the 10% and the 1% longest texts. This experiment involved statistical analysis to determine whether longer texts lead to better or worse predictions.  Data: We used the BrCAD-5 dataset3 in our experiments. The task is a binary classi- fication, with Class 1 indicating that the AP reverses the previous decision, and Class 0 indicating it affirms. The dataset is imbalanced, with 22% of the data points belonging to Class 1. Although this imbalance ratio is consistent across all dataset splits, it varies significantly with text length.  Models: In this work, our model (uBERT) uses BERT as the encoder and LSTM as the RNN, with maxtok set to 512 tokens and up to 15 chunks (maxc) processed in parallel. Our training procedure follows the approach of [Menezes-Neto and Clementino 2022], where we fine-tune the last layer of BERT and the LSTM. The fine-tuning is conducted for 1 epoch utilizing the One Cycle learning rate scheduler. Our inference procedure mirrors the training process.  Baselines: our baseline models are ULMFiT (forward, backward and bidirectional)4, Big Bird and BERT+LSTM. Notably, only ULMFiT and uBERT process the full text.  2The typical input size for BERT models is 512 tokens. Our overlap algorithm first slices the text and distributes the tokens. Only  after this process are the special tokens [CLS] and [SEP] added, resulting in the well-known 512-token limit.  3This dataset is divided in training, validation, and test sets: the training set includes 380,673 documents, while the validation and  test sets contain 76,342 and 76,299 entries, respectively.  4ULMFiT incorporates a forward language model (predicting the next token), a backward language model (predicting the previous  token), and a bidirectional model that combines the two, allowing it to capture contextual information in both directions.   Computational Infrastructure and Resources: the experiments were conducted using Google’s Colab infrastructure, specifically an NVIDIA A100 GPU with 40 GB of RAM.  Evaluation Metrics: We evaluate all models using the Macro F1 score and Matthews Correlation Coefficient (MCC). The Macro F1 score is a well-established metric across NLP fields, representing the harmonic mean of precision and recall, while MCC, though less common, is frequently used in the Legal Judgment Prediction (LJP) subfield, as noted by [Cui et al. 2022]. MCC measures the correlation between predicted and actual clas- sifications by accounting for true positives, true negatives, false positives, and false neg- atives, making it suitable for imbalanced classes5. Additionally, MCC is the metric used by [Menezes-Neto and Clementino 2022], making it necessary for us to use it as well for model comparison. To compare different baselines and configurations of our uBERT model, we employed bootstrap resampling to obtain 95% confidence intervals, followed by Wilcoxon-Holm post-hoc analysis to assess statistical significance with α = 5%, fol- lowing similar approaches [Demˇsar 2006, Zhu et al. 2020, Ferraz et al. 2021].  5. Results  Table 1 presents the results for all model configurations on the full test dataset, as well as the 10% and 1% longest texts. The baseline models were not run on the full test set in this study due to computational resource limitations. The results reported here are reproduced from [Menezes-Neto and Clementino 2022], which is why Table 1 does not include in- ference times for the full test set. Figure 3 displays the macro F1-scores across varying text lengths, while Figure 4 ranks the models using the MCC metric. Although MCC is effective for within-dataset comparisons, it is less suitable across datasets with differing class imbalance; hence, we rely on the macro F1-score for cross-dataset comparisons.  Table 1: uBERT performance across various overlap sizes compared with baselines.  Dataset:  Full Test Set (76.299 documents) Imbalance Ratio = 0.28  10% Set (7.632 documents) Imbalance Ratio = 0.32  1% Set (763 documents) Imbalance Ratio = 0.54  Architecture  Macro-F1↑ MCC↑ Macro-F1↑ MCC↑  Inf.Time↓ Macro-F1↑ MCC↑  Inf.Time↓  Baselines ULMFiT - fwd ULMFiT - bwd ULMFiT - bidir Big Bird BERT+LSTM Ours uBERT 0 uBERT 150 uBERT 205 uBERT 300 uBERT 408 uBERT 510  65.1 % 65.7 % 66.9 % 52.0 % 64.1 %  63.9 % 63.3 % 64.7 % 64.7 % 64.0 % 64.6 %  0.32 0.35 0.37 0.27 0.33  0.33 0.32 0.35 0.35 0.33 0.35  64.9 % 63.4 % 64.8 % 44.0 % 63.2 %  62.6 % 62.2 % 62.6 % 63.0 % 63.2 % 63.0 %  0.32 0.35 0.34 0.23 0.31  0.31 0.30 0.31 0.31 0.32 0.31  1h 18min 1h 18min 1h 18min 22min 12min  13min 14min 15min 17min 19min 21min  72.3 % 59.9 % 69.3 % 30.0 % 64.0 %  61.3 % 59.4 % 62.0 % 63.0 % 64.3 % 64.2 %  0.47 0.33 0.43 0.08 0.36  0.34 0.32 0.35 0.36 0.38 0.38  11min:22s 14min:44s 14min:44s 2min:58s 1min:29s  1min:51s 2min:04s 2min:09s 2min:23s 2min:42s 3min:08s  5MCC ranges from -1 to 1, where 1 indicates perfect prediction, 0 indicates no better than random chance, and -1 indicates total  disagreement between prediction and observation.   Figure 3: Macro-F1 score x Avg. Tokens/Group across different groups of same size ranked by the length.The error  bars represent 95% confidence intervals obtained with bootstrap resampling.  (a) All Text  (b) 10% Longest Text  (c) 1% Longest Text  Figure 4: Critical difference diagram showing pairwise statistical comparison between baselines and varying overlap sizes for uBERT using the MCC. Connecting bars represent no statistical difference between methods.  Processing the Full Text Requires Overlap Comparing the BERT+LSTM baseline, which middle-truncates text when it exceeds input size, with our uBERT without over- lap (uBERT 0), which uses the full text, we found that uBERT either underperformed or matched the baseline across all metrics. Notably, it performed worse on the 1% longest texts, where middle-truncation by BERT+LSTM occurs. This suggests that merely processing the entire text is insufficient for longer inputs. We hypothesize that non- overlapping chunks introduce noise due to abrupt segmentation, which degrades perfor- mance. Our results support this, showing that introducing overlap in uBERT configura- tions improves both Macro-F1 and MCC scores. The following uBERT configurations outperformed BERT+LSTM with statistical significance: uBERT 300, uBERT 510 and uBERT 205 on full test set; uBERT 408 and uBERT 300 on 10% longest; and uBERT 408, uBERT 300 and uBERT 510 on 1% longest. Thus, incorporating over- lap is crucial for maintaining semantic consistency and improving performance on longer texts.  uBERT with Overlap is still Significantly Faster than ULMFiT As expected, intro- ducing overlap in the uBERT architecture increased computational time overhead. How- ever, across the full dataset and the 10% longest texts, uBERT configurations delivered better results than the BERT+LSTM baseline with comparable inference times. Notably, uBERT 408 achieved a 4x faster inference than ULMFiT on the 10% longest texts. For the 1% longest texts, the increased length required two passes of uBERT 4086, resulting  6With zero overlap, uBERT can process a maximum of 7,650 tokens in a single encoder pass. This limit arises because uBERT handles up to 15 chunks of 510 tokens each (excluding special tokens [CLS] and [SEP]). Therefore, documents longer than 7,650 tokens require at least two encoder passes.  02000400060008000Text Length (Avg. Tokens/Group)0.30.40.50.60.70.80.9Macro F1-ScoreULMFiT-bwdULMFiT-bidirULMFiT-fwdBERT_LSTMuBERT 0uBERT 150uBERT 205uBERT 300uBERT 408uBERT 51012345678910ubert_150ULMFiT-fwdubert_408ubert_0BERT_LSTMubert_205ubert_510ubert_300ULMFiT-bwdULMFiT-bidir12345678910ubert_150ubert_510ubert_205ubert_0BERT_LSTMubert_300ubert_408ULMFiT-fwdULMFiT-bidirULMFiT-bwd12345678910ubert_150ULMFiT-bwdubert_0ubert_205BERT_LSTMubert_510ubert_300ubert_408ULMFiT-bidirULMFiT-fwd in 1.8x slower inference compared to BERT+LSTM, which needed to middle-truncate in all cases. Despite this, uBERT 408 slightly outperformed BERT+LSTM, narrowing the performance gap with ULMFiT while maintaining a faster inference, highlighting the efficiency and effectiveness of our approach. In summary, in all subsets, uBERT config- urations reduced the BERT+LSTM gap being significantly faster than ULMFiT.  ULMFiT Outperforms uBERT on Longer Texts As shown in Figure 3, model dif- ferences become more clear with increasing text length. Big Bird consistently underper- forms on longer texts, which is why it was excluded from the comparison charts. While some uBERT configurations outperform BERT+LSTM on longer texts, F1 scores in both models degrade compared to full test dataset performance. In contrast, ULMFiT models improve on longer texts compared to the full dataset. This suggests that our archi- tecture mitigates the degradation for longer text that is inherent to the BERT+LSTM approach, but still falls short of ULMFiT models, that handle better longer text but at a cost of 4x slower inference time.  6. Conclusion and Future Work  Our experiments demonstrate that the uBERT model improves the handling of legal texts compared to baseline encoder-based models, particularly on longer texts, due to its capa- bility to process entire documents using overlapping chunks. Despite the increased com- putational overhead, uBERT remains faster than ULMFiT. uBERT slightly outperforms BERT+LSTM, but still falls short of ULMFiT. Thus, further refinement is needed to fully match ULMFiT‘s performance. Notably, even ULMFiT, the top-performing model in our experiments, achieves relatively low Macro-F1 scores, suggesting that processing the full text alone is insufficient for high performance on this task. In this direction, future research should expand the evaluation methodology by analyzing correctly and incor- rectly classified cases across all tested models to assess whether specific characteristics of judicial decisions make them more prone to misclassification by certain models. Such an analysis, however, requires a multidisciplinary approach, including expert input from highly skilled legal practitioners.  Future research should also explore different chunking strategies to enhance text processing. Comparing syntactic chunking, which is based on grammatical structure, with semantic chunking, which is based on content meaning, could provide valuable in- sights. As this study focuses on a Portuguese-language dataset, evaluating these chunking approaches across datasets in multiple languages would help determine if optimal chunk- ing strategies vary with language, contributing to more robust long-text segmentation and model performance across diverse linguistic contexts.  Acknowledgements  This work was supported by CAPES (Finance Code 001), CNPQ (grant 312360/23-1), Programa Unificado de Bolsas de Estudo para Apoio `a Formac¸ ˜ao de Estudantes (PUB- USP), USP-IBM-FAPESP Center for Artificial Intelligence (FAPESP grant 2019/07665- 4), and Secretaria da Fazenda do Estado do Rio Grande do Sul (SEFAZ-RS), Brazil.   "
        },
        {
            "titulo": "Sumarização Automática de Artigos de Notícias em Português: Da Extração à Abstração com Abordagens Clássicas e Modelos de Neurais",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31125",
            "idioma": "Português",
            "storage_key": "files/article_31125_30928.pdf",
            "autores": [
                {
                    "nome": "Marcio Alves Sarmento",
                    "afiliacao": "IFES",
                    "orcid": "http://orcid.org/0009-0008-7333-7866"
                },
                {
                    "nome": "Hilário Tomaz Alves de Oliveira",
                    "afiliacao": "IFES",
                    "orcid": "https://orcid.org/0000-0003-0643-7206"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "A sumarização automática de texto tem como objetivo a criação de um resumo com as informações mais relevantes extraídas de um ou mais documentos textuais. Apesar dos avanços obtidos na área, pesquisas envolvendo documentos escritos em português do Brasil ainda são escassas. Este artigo apresenta uma análise envolvendo diferentes abordagens de sumarização, desde baselines clássicas, passando por sistemas extrativos, o ajuste fino de diferentes arquiteturas dos modelos PPT5 e FLAN -T5, até o uso de modelos de linguagem de larga escala para sumarização abstrativa. Experimentos foram realizados considerando três bases de dados de artigos de notícias escritos em português. Os resultados demonstraram que os modelos ajustados para a tarefa de sumarização abstrativa obtiveram resultados competitivos com base nas medidas do ROUGE-L e do BERTScore com modelos maiores, como o GPT-4o.",
            "keywords": [
                "Sumarização Automática de Texto (SAT)",
                "Processamento de Linguagem Natural (PLN)",
                "Redes Neurais",
                "Artigos de Notícias",
                "Português do Brasil",
                "Modelos de Linguagem",
                "Sumarização Extrativa",
                "Sumarização Abstrativa",
                "Large Language Models (LLMs)"
            ],
            "referencias": [
                "Cardoso, P. C., Maziero, E. G., Jorge, M. L. C., Seno, E. M., Di Felippo, A., Rino, L. H. M., Nunes, M. d. G. V., and Pardo, T. A. (2011). Cstnews-a discourse-annotated corpus for single and multi-document summarization of news texts in Brazilian Portuguese. In Proceedings of the 3rd RST Brazilian Meeting, pages 88–105.",
                "Carmo, D., Piau, M., Campiotti, I., Nogueira, R., and Lotufo, R. (2020). Ptt5: Pre-training and validating the T5 model on Brazilian Portuguese data. arXiv preprint arXiv:2008.09144.",
                "Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. (2024). Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1–53.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "Gomes, L. and de Oliveira, H. (2019). A multi-document summarization system for news articles in Portuguese using integer linear programming. In Anais do XVI Encontro Nacional de Inteligência Artificial e Computacional, pages 622–633. SBC.",
                "Levitin, D. J. (2014). Organized Mind: Thinking Straight in the Age of Information Overload (9780698157224). Barnes & Noble.",
                "Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81.",
                "OpenAI (2024). OpenAI models.",
                "Paiola, P. H., Garcia, G. L., Jodas, D. S., Correia, J. V. M., Sugi, L. A., and Papa, J. P. (2024). Recognasumm: A novel Brazilian summarization dataset. In Proceedings of the 16th International Conference on Computational Processing of Portuguese, pages 575–579.",
                "Pardo, T. A. S. and Rino, L. H. M. (2003). Temário: Um corpus para sumarização automática de textos. São Carlos: Universidade de São Carlos, Relatório Técnico.",
                "Sodré, L. and de Oliveira, H. (2019). Avaliando algoritmos de regressão para sumarização automática de textos em português do Brasil. In Anais do XVI Encontro Nacional de Inteligência Artificial e Computacional, pages 634–645. SBC.",
                "Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., et al. (2024). Gemma: Open models based on Gemini research and technology. arXiv preprint arXiv:2403.08295.",
                "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.",
                "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.",
                "Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2019). Bertscore: Evaluating text generation with BERT. arXiv preprint arXiv:1904.09675."
            ],
            "artigo_completo": "Resumo. A sumarizac¸ ˜ao autom´atica de texto tem como objetivo a criac¸ ˜ao de um resumo com as informac¸ ˜oes mais relevantes extra´ıdas de um ou mais do- cumentos textuais. Apesar dos avanc¸os obtidos na ´area, pesquisas envolvendo documentos escritos em portuguˆes do Brasil ainda s˜ao escassas. Este artigo apresenta uma an´alise envolvendo diferentes abordagens de sumarizac¸ ˜ao, desde baselines cl´assicas, passando por sistemas extrativos, o ajuste fino de diferen- tes arquiteturas dos modelos P P T 5 e F LAN -T 5, at´e o uso de modelos de linguagem de larga escala para sumarizac¸ ˜ao abstrativa. Experimentos foram realizados considerando trˆes bases de dados de artigos de not´ıcias escritos em portuguˆes. Os resultados demonstraram que os modelos ajustados para a ta- refa de sumarizac¸ ˜ao abstrativa obtiveram resultados competitivos com base nas medidas do ROUGE-L e do BERTScore com modelos maiores, como o GPT-4o.  1. Introduc¸ ˜ao  A crescente demanda por informac¸ ˜oes impulsiona o desenvolvimento de tecnologias ca- pazes de processar e sintetizar grandes volumes de dados de forma r´apida e eficiente. Em um cen´ario em que a produc¸ ˜ao de conte´udo digital online ´e cada vez mais abundante, torna-se cada vez mais desafiador para os leitores acompanhar todas as not´ıcias relevantes [Levitin 2014, Zhang et al. 2024]. Nesse contexto, sistemas de Sumarizac¸ ˜ao Autom´atica de Texto (SAT) podem ser ferramentas ´uteis para auxiliar os usu´arios, oferecendo a ca- pacidade de gerar resumos concisos que capturam as informac¸ ˜oes mais relevantes de um texto ou de m´ultiplos documentos relacionados, permitindo uma assimilac¸ ˜ao mais r´apida do conte´udo [Lin and Ng 2019, Zhang et al. 2022].   A SAT ´e uma ´area de pesquisa em Processamento de Linguagem Natural (PLN) que busca gerar resumos de documentos textuais de forma autom´atica. Exis- tem duas abordagens principais para a tarefa de SAT: a Extrativa e a Abstrativa [Nenkova and McKeown 2012]. A sumarizac¸ ˜ao extrativa seleciona as frases mais re- levantes diretamente do texto original para compor o resumo, enquanto a sumarizac¸ ˜ao abstrativa envolve a reescrita do conte´udo de forma mais condensada e frequentemente utiliza t´ecnicas de gerac¸ ˜ao de linguagem natural, sendo capaz de criar novas frases que n˜ao necessariamente aparecem no texto original [Zhang et al. 2022]. A sumarizac¸ ˜ao pode ser aplicada tanto a um ´unico documento (monodocumento) quanto a um conjunto de do- cumentos (multidocumento).  Nos ´ultimos anos, houve uma mudanc¸a no foco das pesquisas na ´area de SAT das abordagens extrativas para as abstrativas [Lin and Ng 2019]. Essa mudanc¸a foi im- pulsionada pelo desenvolvimento de algoritmos baseados em redes neurais profundas, especialmente na arquitetura Transformer [Vaswani 2017], capazes de gerac¸ ˜ao de lingua- gem natural. Assim, diversas abordagens surgiram usando modelos neurais pr´e-treinados e, mais recentemente, modelos de linguagem de larga escala, do inglˆes Large Language Models (LLMs) [Zhang et al. 2022]. Contudo, apesar dos resultados promissores usando essas abordagens neurais, elas imp˜oem diversos desafios, como a necessidade de grandes bases de dados para treinamento e demandam muitos recursos computacionais. Apesar dos avanc¸os na ´area, a maioria das pesquisas tem como foco a l´ıngua inglesa, e poucos estudos tˆem sido dedicados ao portuguˆes, especialmente para a sumarizac¸ ˜ao abstrativa [Zhang et al. 2022]. Essa lacuna limita a aplicabilidade de sistemas de sumarizac¸ ˜ao au- tom´aticos projetados especificamente para o portuguˆes, que enfrenta a ausˆencia de mode- los e bases de dados para suportar essas pesquisas [Paiola et al. 2024].  Este artigo busca contribuir para o desenvolvimento da tarefa de SAT em por- tuguˆes do Brasil por meio de uma investigac¸ ˜ao de algoritmos de sumarizac¸ ˜ao aplicados a artigos de not´ıcias. O estudo abrange desde o uso de sistemas extrativos, comumente usados como baselines de comparac¸ ˜ao, o ajuste fino dos modelos P T T 5 e F LAN -T 5 para sumarizac¸ ˜ao abstrativa, at´e o uso de LLMs de c´odigo aberto e propriet´arios, como o GPT4-o1, Llama 3 [Touvron et al. 2023] e o Gemma [Team et al. 2024]. Para isso, fo- ram realizados experimentos em trˆes bases de dados, o Tem´ario e Recognnasum para a tarefa de sumarizac¸ ˜ao monodocumento e o CSTNews para sumarizac¸ ˜ao multidocumento. O desempenho dos modelos foi avaliado usando as medidas de avaliac¸ ˜ao autom´aticas do ROUGE-L e BERTScore, que s˜ao usualmente adotadas na literatura.  Os c´odigos desenvolvidos e os resumos gerados neste trabalho est˜ao p´ublicos em  https://github.com/laicsiifes/benchmark_ptbr_summ.  2. Trabalhos Relacionados  A literatura da ´area de SAT ´e vasta e existem diversos surveys que fornecem uma vis˜ao ampla do desenvolvimento da ´area desde a sua origem [Nenkova and McKeown 2012, Lin and Ng 2019, Zhang et al. 2022]. Por limitac¸ ˜oes de espac¸o, esta sec¸ ˜ao foca apenas em trabalhos que envolveram documentos escritos em portuguˆes ou que usaram t´ecnicas adotadas nos experimentos realizados neste estudo.  1https://openai.com/index/hello-gpt-4o/   Diversos indicadores de relevˆancia vˆem sendo explorados para a execuc¸ ˜ao da ta- refa de sumarizac¸ ˜ao extrativa [Leite and Rino 2008, Oliveira et al. 2016a]. Em sua maio- ria, esses indicadores baseiam-se em t´ecnicas estat´ısticas, como frequˆencia e centralidade, ou em heur´ısticas, como a posic¸ ˜ao das sentenc¸as nos documentos. O estudo conduzido por Oliveira et al. [Oliveira et al. 2016a] avaliou diferentes t´ecnicas para mensurar a re- levˆancia de sentenc¸as em tarefas de SAT de artigos jornal´ısticos em inglˆes. Os autores analisaram os m´etodos individualmente e em combinac¸ ˜ao, utilizando-os como atributos em algoritmos de classificac¸ ˜ao. Leite e Rino [Leite and Rino 2008] investigaram uma abordagem combinando m´ultiplas features e algoritmos de aprendizado de m´aquina para a sumarizac¸ ˜ao extrativa de documentos em portuguˆes.  No trabalho de Sodr´e e Oliveira [Sodr´e and de Oliveira 2019], os autores in- vestigaram a estrat´egia de combinar alguns dos indicadores analisados por Oliveira et al. [Oliveira et al. 2016a] e aplicaram algoritmos de regress˜ao para estimar um escore de relevˆancia das sentenc¸as na tarefa de sumarizac¸ ˜ao de artigos jornal´ısticos em por- tuguˆes. Gomes e Oliveira [Gomes and de Oliveira 2019] propuseram um sistema usando Programac¸ ˜ao Linear Inteira (PLI) para sumarizac¸ ˜ao extrativa multidocumento. O sistema desenvolvido usa bigramas como conceitos e aplica m´etodos estat´ısticos tradicionais para identificar as informac¸ ˜oes mais relevantes para a construc¸ ˜ao do resumo.  Diferentemente dos trabalhos anteriores, Paiola et al. [Paiola et al. 2022] investi- garam a tarefa de sumarizac¸ ˜ao abstrativa. Os autores usaram diversas bases de dados em portuguˆes (TeM´ario, CSTNews, WikiLingua e XL-Sum) e um sistema de traduc¸ ˜ao para aplicar modelos treinados em inglˆes. Em [Paiola et al. 2024], os autores apresentam a base de dados do RecognaSumm, um conjunto de dados contendo mais de 135 mil arti- gos de not´ıcias para a tarefa de SAT. Os autores realizaram diferentes an´alises da base de dados proposta e avaliaram o desempenho do modelo base do P T T 5 para estabelecer um desempenho de referˆencia para comparac¸ ˜oes futuras.  Este trabalho busca expandir os anteriores ao realizar uma an´alise mais ampla con- siderando trˆes bases de dados (Tem´ario, RecognaSumm e CSTNews) para sumarizac¸ ˜ao monodocumento e multidocumento, al´em de envolver desde t´ecnicas de sumarizac¸ ˜ao ex- trativas tradicionais de ponderac¸ ˜ao das frases assim como os usados nos trabalhos em [Oliveira et al. 2016a, Sodr´e and de Oliveira 2019], adaptac¸ ˜ao do sistema de PLI pro- posto em [Gomes and de Oliveira 2019], ajuste fino e avaliac¸ ˜ao de diferentes tamanhos de arquiteturas (small, base e large) dos modelos P T T 5 e F LAN -T 5 e o uso de LLMs de c´odigo aberto (Llama3 e Gemma2) e propriet´arios (GP T -3.5 e GP T -4o).  3. Materiais e M´etodos  3.1. Bases de Dados  Neste trabalho, foram utilizadas trˆes bases de dados comumente usadas na literatura para a tarefa de SAT no dom´ınio de artigos de not´ıcias escritas em portuguˆes.  TeM´ario. Esse conjunto de dados ´e formado por 100 textos jornal´ısticos, prove- nientes da Folha de S.Paulo e do Jornal do Brasil. Os artigos, que abordam uma variedade de temas, foram selecionados por sua linguagem clara e objetiva. Todos os textos pos- suem resumos elaborados por um especialista, o que garante a qualidade dos resumos de referˆencia [Pardo and Rino 2003].   CSTNews. Essa base de dados ´e formada por 50 conjuntos de not´ıcias, cada um com aproximadamente quatro artigos sobre o mesmo tema, coletados manualmente em sites de not´ıcias como Folha de S˜ao Paulo e Estad˜ao. Essa abordagem permitiu a selec¸ ˜ao de not´ıcias com linguagem clara e acess´ıvel, provenientes de diferentes fontes sobre um mesmo assunto [Cardoso et al. 2011].  RecognaSumm. Com o objetivo de construir um conjunto de dados robusto para estudos de sumarizac¸ ˜ao de textos, Paiola et al. [Paiola et al. 2024] coletaram 135.272 ar- tigos de not´ıcias usando sistemas de web crawlers personalizados. A diversidade tem´atica dos artigos foi garantida pela coleta de dados em diferentes portais de not´ıcias e catego- rias. A base de dados ´e dividida em trˆes subconjuntos: treinamento, validac¸ ˜ao e teste. Por conta de limitac¸ ˜oes de hardware, foi feita uma filtragem no conjunto de treinamento, sendo removidos os artigos com resumos contendo menos do que 25 palavras.  Na Tabela 1 s˜ao apresentadas algumas estat´ısticas das bases de dados usadas nos experimentos. Para cada base, foi computado o total de documentos ou grupos, m´edia e Desvio Padr˜ao (DP) de frases e palavras no texto dos artigos.  Tabela 1. Estat´ısticas das bases de dados usadas nos experimentos. Base de Dados Conjunto Docs / Grupos M´edia (DP) Frases M´edia (DP) Palavras  TeM´ario CSTNews  RecognaSumm  ´Unico ´Unico Treino Validac¸ ˜ao Teste  100 50 64.347 21.538 21.493  32,4 (10,38) 47,06 (19,47) 27,07 (24,82) 26,73 (24,15) 27,05 (24,88)  618,67 (163,93) 939,56 (331,42) 527,33 (468,38) 519,91 (458,68) 526,41 (470,02)  3.2. Modelos de Sumarizac¸ ˜ao  Os seguintes modelos de sumarizac¸ ˜ao foram investigados:  Baselines. Foram utilizadas como baselines oito t´ecnicas de ponderac¸ ˜ao de frases [Sodr´e and de Oliveira 2019]. As t´ecnicas utilizadas foram: Bushy Path, Cen- tralidade das Frases, Frequˆencia de Palavras, Frequˆencia de Entidades Nomeadas, Frequˆencia do Termo - Frequˆencia Inversa das Sentenc¸as (TF-ISF), Posic¸ ˜ao das Fra- ses, Similaridade Agregada, TextRank. Essas t´ecnicas foram usadas em conjunto com uma abordagem cl´assica de sumarizac¸ ˜ao extrativa composta por trˆes etapas [Nenkova and McKeown 2012]:  • Pr´e-processamento: O documento ou grupo de documentos de entrada ´e pr´e- processado usando v´arias t´ecnicas tradicionais de PLN, como divis˜ao do texto em frases, palavras, lematizac¸ ˜ao, identificac¸ ˜ao das classes gramaticais e reconheci- mento de entidades nomeadas.  • Ponderac¸ ˜ao das frases: Nesta etapa, cada uma das oito t´ecnicas de ponderac¸ ˜ao de frases ´e aplicada para analisar cada frase do(s) documento(s) de entrada e gerar um valor que deve refletir sua relevˆancia para ser inclu´ıdo no resumo. Todos os valores gerados s˜ao normalizados no intervalo de 0 a 1.  • Gerac¸ ˜ao de resumo: As frases com os maiores valores de relevˆancia gera- das na etapa anterior s˜ao inseridas iterativamente no resumo at´e que o tamanho m´aximo desejado seja atingido. Uma nova frase ´e inserida no resumo somente   se sua similaridade de cosseno com as frases j´a inseridas for menor que 0, 5 [Nenkova and McKeown 2012].  Sistema de PLI. Foi utilizado o sistema de PLI proposto por Gomes e Oliveira [Gomes and de Oliveira 2019] para sumarizac¸ ˜ao multidocumento e uma adaptac¸ ˜ao de um sistema similar apresentado em [Oliveira et al. 2016b] para a tarefa de sumarizac¸ ˜ao mo- nodocumento.  Modelos Pr´e-treinados. Foram utilizados os modelos P T T 5 [Carmo et al. 2020] e F LAN -T 5 em suas arquiteturas small, base e large, que se diferenciam pelo tama- nho da arquitetura. O P T T 5 ´e uma vers˜ao em portuguˆes do modelo de linguagem T 5, pr´e-treinado no BrWac, um grande corpus de p´aginas da web em portuguˆes brasileiro. O F LAN -T 5 ´e um modelo multil´ıngue desenvolvido pela google que foi treinado para m´ultiplas tarefas de PLN [Chung et al. 2024].  LLMs. Os recentes avanc¸os no progresso de LLMs tˆem impulsionado o de- senvolvimento de diversas aplicac¸ ˜oes. Neste trabalho, foram utilizados os modelos: Gemma 2 9B [Team et al. 2024], o Llama 3.1 8B [Touvron et al. 2023] e os modelos Text-davinci-003, GPT-3.5 Turbo, GPT-4o e GPT-4o mini desenvolvidos pela empresa OpenAI [OpenAI 2024].  3.3. Desenho Experimental  A an´alise de desempenho dos modelos de sumarizac¸ ˜ao foi dividida em dois experimentos. No primeiro experimento, foi utilizada somente a base de dados do RecognaSumm, sendo considerados os sistemas extrativos (baselines e o sistema de PLI) e foram treinados seis modelos de sumarizac¸ ˜ao abstrativos baseados no P T T 5 e F LAN -T 5. O segundo ex- perimento foi realizado usando as bases de dados do Tem´ario e CSTNews. Para esse experimento, foram usados os sistemas extrativos (baselines e o sistema de PLI), os mo- delos abstrativos baseados no P T T 5 e F LAN -T 5 treinados no primeiro experimento e os modelos de LLMs. Em todas as abordagens avaliadas, foi configurado o tamanho m´aximo de resumo para 150 palavras.  Para os m´etodos de baselines e o sistema de PLI foram usadas implementac¸ ˜oes pr´oprias. Os modelos da OpenAI foram acessados usando a API oficial disponibilizada pela empresa. A implementac¸ ˜ao dos modelos P T T 5, F LAN -T 5 e dos LLMs do Gemma 2 9B e Llama 3.1 8B foi baseada na biblioteca Transformers2 e foram usados os modelos pr´e-treinados disponibilizados publicamente pelos autores e empresas na plataforma do Hugging Face3. Para o ajuste fino das trˆes arquiteturas dos modelos P T T 5 e F LAN -T 5, o tamanho m´aximo de entrada foi definido para 512 tokens e o tamanho m´aximo do re- sumo a ser gerado foi configurado para 150 tokens. Os modelos foram ajustados por no m´aximo 20 ´epocas, sendo utilizada a estrat´egia de parada antecipada com uma paciˆencia de 5 ´epocas. Para evitar sobreajuste dos modelos, foi feito um monitoramento do treina- mento, no qual, ao final de cada ´epoca, o modelo resultante ´e aplicado no conjunto de validac¸ ˜ao e ´e computada a medida do ROUGE-L, sendo armazenado somente o modelo com maior valor. Para a gerac¸ ˜ao dos resumos, foi usado o algoritmo de decodificac¸ ˜ao do Beam Search com tamanho 5 de largura.  2https://huggingface.co/docs/transformers/index 3https://huggingface.co/   Baseado no trabalho de [Zhang et al. 2024], o seguinte prompt foi usado nos LLMs para gerac¸ ˜ao dos resumos: “Escreva um resumo em PORTUGU ˆES DO BRASIL para o artigo de not´ıcias a seguir com no M ´AXIMO 150 palavras. ARTIGO: {TEXTO}.”, onde {T EXT O} foi substitu´ıdo pelo conte´udo completo do(s) artigo(s) de not´ıcias.  O desempenho dos modelos foi avaliado utilizando as medidas de avaliac¸ ˜ao do Recall-Oriented Understudy for Gisting Evaluation Longest Common Subsequence (LCS) (ROUGE-L) [Lin 2004] e a do BERTScore [Zhang et al. 2019]. O ROUGE-L computa a maior cadeia em comum entre um resumo candidato e o resumo de referˆencia, enquanto o BERTScore calcula a similaridade do cosseno entre dois textos usando representac¸ ˜oes de embeddings extra´ıdas do modelo Bidirecional Encoder Representations from Trans- formers (BERT) [Devlin et al. 2019]. Por quest˜oes de espac¸o, s˜ao reportados somente a m´etrica do f1-score, que combina as m´etricas de precis˜ao e revocac¸ ˜ao. Apesar de terem diversas limitac¸ ˜oes, essas medidas s˜ao alternativas v´alidas `a realizac¸ ˜ao de avaliac¸ ˜oes ma- nuais e, conforme an´alise feita em Zhang et al. [Zhang et al. 2024], elas apresentaram correlac¸ ˜ao moderada com avaliac¸ ˜oes humanas na tarefa de sumarizac¸ ˜ao.  4. Resultados  4.1. Experimento na base de dados do RecognaSumm  Na Tabela 2 s˜ao apresentados os resultados dos experimentos na base de dados do Re- cognaSumm. Analisando o desempenho dos baselines, pode-se observar que a t´ecnica da Posic¸ ˜ao das Frases foi a que obteve os melhores resultados. Essa t´ecnica consiste em selecionar as n primeiras frases do documento para compor o resumo at´e que o ta- manho m´aximo do resumo desejado seja alcanc¸ado. Essa t´ecnica tem sido um dos ba- selines mais competitivos para sumarizac¸ ˜ao de artigos de not´ıcias [Oliveira et al. 2016a]. O sistema baseado em PLI obteve melhor desempenho do que quase todos os baselines, com excec¸ ˜ao da Posic¸ ˜ao das Frases. Os modelos P T T 5 e F LAN -T 5 demonstraram melhor desempenho geral do que as demais abordagens analisadas. Em especial, os me- lhores desempenhos neste experimento foram obtidos pelos modelos F LAN -T 5Large e P T T 5Large em ambas as medidas de avaliac¸ ˜ao. Os resultados obtidos usando a arquite- tura base foram muito pr´oximos `as arquiteturas da large, sendo que eles s˜ao menores e consomem menos recursos computacionais.  Com base nos resultados, fica evidente que os modelos ajustados para sumarizac¸ ˜ao abstrativos geraram resumos melhores do que as t´ecnicas de baselines e que o sistema ex- trativo de PLI nas medidas do ROUGE-L e BERTScore. Essa superioridade demonstra a efic´acia dos modelos P T T 5 e F LAN -T 5 para a tarefa de gerac¸ ˜ao de resumos abstrati- vos. Entretanto, ao considerar o uso desses modelos, ´e importante levar em conta o custo computacional associado a cada um, tanto para o treinamento quanto para a gerac¸ ˜ao dos resumos. Portanto, a relac¸ ˜ao custo-benef´ıcio deve ser ponderada na escolha da aborda- gem, especialmente em cen´arios com recursos computacionais limitados.  4.2. Experimento nas bases de dados do Tem´ario e CSTNews  A Tabela 3 apresenta os resultados do experimento nas bases de dados do TeM´ario e CSTNews. As abordagens avaliadas incluem os m´etodos de baselines, o sistema extra- tivo usando PLI, os modelos do P T T 5 e F LAN -T 5 treinados no RecognaSumm e os LLMs analisados. Os resultados obtidos neste experimento foram bastante diversificados.   Tabela 2. Resultados do experimento usando o corpus RecognaSumm.  Abordagem  Bushy Path Centralidade das Frases Frequˆencia de Palavras Freq. Entidades Nomeadas Posic¸ ˜ao das Frases Similaridade Agregada TextRank TF-ISF Sistema PLI P T T 5Small P T T 5Base P T T 5Large F LAN -T 5Small F LAN -T 5Base F LAN -T 5Large  ROUGE-L 0,249 (0,086) 0,249 (0,087) 0,240 (0,085) 0,242 (0,088) 0,279 (0,099) 0,249 (0,085) 0,206 (0,072) 0,235 (0,084) 0,270 (0,095) 0,315 (0,125) 0,337 (0,132) 0,346 (0,134) 0,314 (0,130) 0,338 (0,140) 0,349 (0,143)  BERTScore 0,691 (0,037) 0,690 (0,038) 0,686 (0,038) 0,681 (0,038) 0,701 (0,040) 0,689 (0,039) 0,674 (0,034) 0,684 (0,037) 0,694 (0,038) 0,713 (0,045) 0,722 (0,045) 0,726 (0,046) 0,714 (0,045) 0,724 (0,048) 0,729 (0,048)  Baselines  Extrativo  Abstrativos  Na base de dados do Tem´ario, os modelos Text-davinci-003 e GPT-4o obtiveram o me- lhor desempenho nas medidas do ROUGE-L e BERTScore, respectivamente. Na base do CSTNews, o baseline de Posic¸ ˜ao das Frases apresentou o melhor resultado no ROUGE-L e o GPT-3.5 Turbo no BERTScore.  Tabela 3. Resultados do experimento usando o Tem ´ario e o CSTNews. CSTNews  Tem´ario  Abordagens  Sistema  Baselines  Extrativo  Abstrativos  LLMs  Bushy Path Cent. das Frases Freq. de Palavras Freq. Ent. Nom. Posic¸ ˜ao das Frases Sim, Agregada TextRank TF-ISF Sistema PLI P T T 5Small P T T 5Base P T T 5Large F LAN -T 5Small F LAN -T 5Base F LAN -T 5Large Gemma 2 9B Llama 3.1 8B Text-davinci-003 GPT-3.5 Turbo GPT-4o GPT-4o Mini  ROUGE-L 0,396 (0,069) 0,384 (0,063) 0,375 (0,069) 0,389 (0,076) 0,402 (0,070) 0,390 (0,070) 0,350 (0,059) 0,379 (0,072) 0,396 (0,065) 0,348 (0,064) 0,346 (0,062) 0,339 (0,062) 0,241 (0,053) 0,242 (0,048) 0,225 (0,049) 0,354 (0,046) 0,320 (0,037) 0,424 (0,075) 0,402 (0,074) 0,417 (0,062) 0,402 (0,059)  BERTScore 0,694 (0,024) 0,690 (0,025) 0,686 (0,023) 0,683 (0,024) 0,686 (0,022) 0,696 (0,025) 0,685 (0,021) 0,685 (0,024) 0,687 (0,023) 0,679 (0,024) 0,681 (0,023) 0,678 (0,025) 0,654 (0,021) 0,658 (0,022) 0,654 (0,021) 0,690 (0,020) 0,671 (0,019) 0,705 (0,027) 0,705 (0,025) 0,713 (0,025) 0,705 (0,021)  ROUGE-L 0,447 (0,067) 0,454 (0,066) 0,452 (0,063) 0,434 (0,068) 0,482 (0,047) 0,419 (0,050) 0,415 (0,060) 0,451 (0,061) 0,477 (0,049) 0,393 (0,065) 0,384 (0,055) 0,385 (0,055) 0,304 (0,070) 0,290 (0,064) 0,294 (0,070) 0,383 (0,034) 0,338 (0,037) 0,472 (0,048) 0,455 (0,061) 0,452 (0,043) 0,444 (0,039)  BERTScore 0,720 (0,029) 0,723 (0,031) 0,721 (0,029) 0,705 (0,032) 0,733 (0,024) 0,712 (0,024) 0,709 (0,027) 0,718 (0,032) 0,736 (0,033) 0,713 (0,031) 0,712 (0,028) 0,715 (0,026) 0,700 (0,027) 0,700 (0,033) 0,696 (0,033) 0,717 (0,023) 0,699 (0,026) 0,738 (0,029) 0,740 (0,025) 0,731 (0,023) 0,730 (0,023)  Os m´etodos de baselines, o sistema extrativo baseado em PLI e os LLMs apresen- taram resultados pr´oximos com base nas medidas de avaliac¸ ˜ao. Por outro lado, os mo- delos ajustados do P T T 5 e F LAN -T 5 demonstraram desempenho inferior aos demais, especialmente os modelos do F LAN -T 5. Esse baixo desempenho pode ser atribu´ıdo ao   fato desses modelos consistentemente gerarem resumos com tamanhos bem inferiores aos demais, mesmo sendo definido um tamanho m´aximo de 150 palavras. Essa caracter´ıstica aconteceu por conta do treinamento desses modelos no Recognasumm, que possui resu- mos de referˆencia bem menores do que os do Tem´ario e do CSTNews.  Apesar dos resultados quantitativos serem pr´oximos, ao analisar os resumos gera- dos pelas abordagens extrativas e abstrativas, fica evidente que os resumos extrativos, em geral, possuem muitas informac¸ ˜oes contidas nos resumos de referˆencias, mas os resumos possuem diversos problemas de coerˆencia e coes˜ao textual. Por outro lado, os resumos abstrativos s˜ao mais sucintos e, em sua maioria, apresentam uma boa qualidade textual em termos de coerˆencia, coes˜ao e estrutura ortogr´afica e gramatical. Os LLMs do Gemma 2 9B e do Llama 3.1 8B apresentaram uma tendˆencia de terminar de forma brusca os re- sumos, por exemplo, no meio de uma frase. Cabe ressaltar que nenhum LLM foi ajustado para a tarefa de sumarizac¸ ˜ao.  Por fim, ´e importante enfatizar que os LLMs, como Gemma, Llama e especial- mente os modelos da OpenAI, possuem um custo consideravelmente maior do que os demais modelos avaliados neste trabalho. Essa caracter´ıstica deve ser considerada em aplicac¸ ˜oes pr´aticas, na qual a relac¸ ˜ao custo-benef´ıcio ´e determinante. Nesse contexto, abordagens extrativas, como o sistema baseado PLI ou mesmo os baselines, podem ofe- recer uma alternativa que equilibra desempenho com menor custo computacional. Em cen´arios com recursos computacionais moderados, os modelos ajustados do P T T 5 e F LAN -T 5 podem ser as melhores opc¸ ˜oes.  5. Conclus˜oes  Este trabalho apresentou uma an´alise comparativa de v´arias abordagens para sumarizac¸ ˜ao autom´atica de texto, considerando desde tradicionais m´etodos de ponderac¸ ˜ao de frases at´e modelos de linguagem de grande escala, para sumarizac¸ ˜ao abstrativa e extrativa de artigos de not´ıcias escritas em portuguˆes do Brasil. Essa avaliac¸ ˜ao fez uso de trˆes bases de dados e de duas medidas de avaliac¸ ˜ao autom´atica comumente usadas na literatura. Os resultados obtidos demonstram que os modelos de LLMs s˜ao promissores para a tarefa de criac¸ ˜ao autom´atica de resumos, mas s˜ao sistemas com uma alta complexidade que requerem muitos recursos computacionais. Portanto, modelos especializados para a tarefa de sumarizac¸ ˜ao ou sistemas extrativos ainda podem ser opc¸ ˜oes vi´aveis, especialmente em cen´arios de poucos recursos.  Em trabalhos futuros, pretendemos expandir este trabalho visando: (i) analisar o desempenho de modelos de LLM de c´odigo aberto, considerando diferentes cen´arios de utilizac¸ ˜ao, como zero shot-learning, few-shot learning e fazendo o ajuste fino desses modelos para a tarefa de sumarizac¸ ˜ao; e (ii) realizar uma avaliac¸ ˜ao manual de um sub- conjunto dos resumos gerados para complementar as an´alises autom´aticas.  "
        },
        {
            "titulo": "Geração Automática de Perguntas em Português do Brasil Usando os Modelos PTT5 e FLAN-T5",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31126",
            "idioma": "Português",
            "storage_key": "files/article_31126_30929.pdf",
            "autores": [
                {
                    "nome": "Tiago Felipe V. braga",
                    "afiliacao": "IFES",
                    "orcid": "http://orcid.org/0009-0003-8351-513X"
                },
                {
                    "nome": "Bruno Cardoso Coutinho",
                    "afiliacao": "IFES",
                    "orcid": "https://orcid.org/0000-0002-4183-7865"
                },
                {
                    "nome": "Hilário Tomaz Alves de Oliveira",
                    "afiliacao": "IFES",
                    "orcid": "https://orcid.org/0000-0003-0643-7206"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Este artigo apresenta uma análise comparativa dos modelos neurais pré-treinados do PTT5 e FLAN-T5 para a geração automática de perguntas em português do Brasil. Para isso, foram utilizados dois conjuntos de dados, PIRA e FairyTaleQA, para avaliar a capacidade desses modelos de gerar perguntas a partir de dois cenários: (i) considerando apenas o contexto e (ii) usando o contexto e a resposta esperada. As medidas do ROUGE-L e do BERTScore foram usadas para avaliar as perguntas geradas, além de uma análise baseada no GPT-4o. Os resultados demonstram que o modelo PTT5",
            "keywords": [
                "Geração Automática de Perguntas",
                "Processamento de Linguagem Natural",
                "PTT5",
                "FLAN-T5",
                "Modelos de Linguagem Pré-treinados",
                "PIRÁ",
                "FairyTaleQA",
                "ROUGE-L",
                "BERTScore",
                "GPT-4o",
                "Avaliação de Modelos de Linguagem",
                "Transformers",
                "Aprendizado de Máquina",
                "Inteligência Artificial"
            ],
            "referencias": [
                "Lin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
                "Wagner Filho, J. A., Wilkens, R., Idiart, M., and Villavicencio, A. (2018). The brwac corpus: A new open resource for brazilian portuguese. In Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018)."
            ],
            "artigo_completo": "Resumo. Este artigo apresenta uma an´alise comparativa dos modelos neurais pr´e-treinados do P T T 5 e F LAN -T 5 para a gerac¸ ˜ao autom´atica de pergun- tas em portuguˆes do Brasil. Para isso, foram utilizados dois conjuntos de da- dos, PIR ´A e FairyTaleQA, para avaliar a capacidade desses modelos de gerar perguntas a partir de dois cen´arios: (i) considerando apenas o contexto e (ii) usando o contexto e a resposta esperada. As medidas do ROUGE-L e do BERTS- core foram usadas para avaliar as perguntas geradas, al´em de uma an´alise ba- seada no GP T -4o. Os resultados demonstram que o modelo P T T 5Large apre- sentou consistentemente desempenho superior aos demais modelos, gerando 93,06% de perguntas v´alidas no PIR ´A e 82,32% no FairyTaleQA na avaliac¸ ˜ao baseada no GP T -4o.  1. Introduc¸ ˜ao  A Gerac¸ ˜ao de Perguntas (QG, do inglˆes Question Generation) ´e uma tarefa da ´area de Processamento de Linguagem Natural (PLN), que envolve a criac¸ ˜ao au- tom´atica de perguntas a partir de um dado texto ou conjuntos de dados textuais [Zhang et al. 2021]. Usando t´ecnicas de PLN e algoritmos de Aprendizado de M´aquina (AM), os sistemas de QG visam gerar perguntas gramaticalmente corretas e contextu- almente relevantes [da Rocha Junqueira et al. 2024]. Diante da grande abundˆancia de informac¸ ˜oes digitais, sistemas de QG possuem diversas potenciais ´areas de aplicac¸ ˜ao [Mulla and Gharpure 2023]. Na ´area da educac¸ ˜ao, a aplicac¸ ˜ao de abordagens de QG pode contribuir para o desenvolvimento de materiais de avaliac¸ ˜ao, question´arios pr´aticos, no desenvolvimento de sistemas de tutoria, aprimorando processos de aprendizagem e   avaliac¸ ˜ao [Kurdi et al. 2020]. No ˆambito dos sistemas de perguntas e respostas (QA, do inglˆes Question Answering), abordagens de QG tˆem sido usadas para o treinamento de modelos com pouca supervis˜ao ou para fins de aumento de dados [Puri et al. 2020].  Apesar do crescente interesse em pesquisas envolvendo a tarefa de QG, a mai- oria desses estudos concentra-se predominantemente na l´ıngua inglesa, onde h´a di- versos recursos e bases de dados dispon´ıveis para experimentac¸ ˜ao e desenvolvimento [Zhang et al. 2021, Mulla and Gharpure 2023]. Em contrapartida, as pesquisas foca- das na l´ıngua portuguesa, especialmente para o portuguˆes do Brasil, ainda s˜ao li- mitadas, resultando em uma escassez tanto de estudos quanto de bases de dados [da Rocha Junqueira et al. 2024, Leite et al. 2024]. Essa lacuna imp˜oe desafios adicio- nais para o avanc¸o no desenvolvimento e na aplicac¸ ˜ao pr´atica de sistemas de QG, uma vez que a adaptac¸ ˜ao de modelos e t´ecnicas desenvolvidas para o inglˆes nem sempre se traduzem diretamente em resultados eficazes em outros idiomas, dada a complexidade e as particularidades lingu´ısticas inerentes de linguagem natural.  Este artigo tem como objetivo investigar a aplicac¸ ˜ao de modelos neurais de lingua- gem pr´e-treinados baseados na arquitetura Transformers [Vaswani 2017], mais especifi- camente os modelos P T T 5 [Carmo et al. 2020] e F LAN -T 5 [Chung et al. 2024] para a tarefa de QG em portuguˆes do Brasil. Para isso, foi realizado o ajuste fino desses mo- delos em suas arquiteturas small, base e large, utilizando as bases de dados do PIR ´A [Paschoal et al. 2021], nativa em portuguˆes, e uma vers˜ao traduzida da base de dados FairytaleQA [Leite et al. 2024] para o portuguˆes do Brasil. Os experimentos foram rea- lizados em dois cen´arios: no primeiro, as perguntas foram geradas a partir somente de um dado contexto; no segundo, as perguntas foram geradas considerando tanto o con- texto quanto uma resposta pr´evia. A avaliac¸ ˜ao dos resultados foi realizada por meio das medidas autom´aticas do ROUGE-L e BERTScore, que s˜ao comumente utilizadas para avaliar abordagens de QG em termos de similaridade l´exica e semˆantica das perguntas geradas com as perguntas de referˆencia. Al´em disso, foi realizado um experimento adi- cional utilizando o modelo GP T -4o para avaliar as perguntas geradas. Esse experimento teve como objetivo complementar as avaliac¸ ˜oes quantitativas anteriores, proporcionando uma an´alise adicional da qualidade das perguntas geradas.  As principais contribuic¸ ˜oes deste artigo incluem: (i) o ajuste fino e a avaliac¸ ˜ao de diferentes arquiteturas dos modelos P T T 5 e F LAN -T 5 para a tarefa de QG em por- tuguˆes do Brasil; e (ii) uma extensa investigac¸ ˜ao considerando duas bases de dados, PIRA e FairytaleQA, e duas variac¸ ˜oes da tarefa. O c´odigo-fonte desenvolvido neste trabalho est´a p´ublico em um reposit´orio do GitHub1.  2. Trabalhos Relacionados  As abordagens de gerac¸ ˜ao de perguntas podem ser classificadas em m´etodos convencio- nais e baseados em modelos neurais [Zhang et al. 2021]. Os m´etodos convencionais de QG baseiam-se principalmente na aplicac¸ ˜ao de regras heur´ısticas para transformar os tex- tos em perguntas relacionadas. Recentemente, com a evoluc¸ ˜ao das arquiteturas de redes neurais profundas, houve uma mudanc¸a de paradigma na tarefa para a adoc¸ ˜ao de modelos neurais, permitindo assim, o desenvolvimento de abordagens orientadas a dados e com- pletamente trein´aveis, na qual a selec¸ ˜ao de conte´udo e a construc¸ ˜ao de perguntas podem  1https://github.com/laicsiifes/question_generation_ptbr   ser otimizadas de forma combinada. Embora exista uma vasta literatura sobre QG em diversos idiomas [Kurdi et al. 2020, Zhang et al. 2021, Mulla and Gharpure 2023], por limitac¸ ˜ao de espac¸o, esta sec¸ ˜ao foca em trabalhos envolvendo o portuguˆes do Brasil.  Em [Leite and Lopes Cardoso 2022], os autores apresentam um estudo que envol- veu o treinamento do modelo P T T 5 para a gerac¸ ˜ao de perguntas utilizando uma vers˜ao em portuguˆes do conjunto de dados SQuAD 1.1. Os resultados obtidos foram encora- jadores, com desempenho equipar´avel com a implementac¸ ˜ao em inglˆes do modelo T 5, evidenciando a efic´acia dos modelos baseados na arquitetura Transformers e estabele- cendo baselines para futuras comparac¸ ˜oes para a tarefa de QG em portuguˆes. Oliveira et al. [Oliveira et al. 2023] abordam o desafio de gerar e classificar distratores (opc¸ ˜oes incorretas) para quest˜oes de m´ultipla escolha em portuguˆes. Os autores desenvolveram e combinam v´arios m´etodos de gerac¸ ˜ao de distratores, incluindo extrac¸ ˜ao baseada em con- texto, manipulac¸ ˜ao num´erica e similaridade semˆantica a partir de recursos como WordNet.  Junqueira et al. [da Rocha Junqueira et al. 2024] apresentaram uma investigac¸ ˜ao do desempenho dos modelos T 5, F LAN -T 5 e BART -P T para a gerac¸ ˜ao de perguntas factuais em portuguˆes do Brasil. Para mitigar o problema da escassez de dados, foi uti- lizada uma vers˜ao em portuguˆes brasileiro do SQuAD v1.1, obtida por meio de traduc¸ ˜ao autom´atica. Leite et al. [Leite et al. 2024] realizaram a construc¸ ˜ao de vers˜oes traduzidas automaticamente da base de dados FairytaleQA, que ´e um conjunto de dados comumente usado para o desenvolvimento de sistemas de perguntas e respostas em inglˆes. Foram desenvolvidas vers˜oes do FairytaleQA para o portuguˆes de Portugal, portuguˆes do Brasil, espanhol e francˆes, que podem ser usadas em pesquisas da ´area de QG e QA. Al´em disso, foram realizados experimentos usando modelos neurais baseados na arquitetura T 5.  Este trabalho difere dos anteriores ao: (i) treinar e avaliar diferentes tamanhos de arquitetura dos modelos P T T 5 e F LAN -T 5, (ii) considerar dois cen´arios da tarefa de QG, (iii) adotar uma base de dados escrita nativamente em portuguˆes (PIR ´A) e outra obtida por meio de traduc¸ ˜ao autom´atica (FairytaleQA), e (iv) analisar o desempenho dos modelos usando uma abordagem com o modelo GP T -4o, al´em de tradicionais medidas de avaliac¸ ˜ao consideradas em trabalhos anteriores.  3. Materiais e M´etodos  3.1. Bases de Dados  Neste trabalho foram utilizados dois conjuntos de dados, o PIR ´A [Paschoal et al. 2021] e o FairyTaleQA [Xu et al. 2022]. Essas bases de dados foram selecionadas por serem usadas em trabalhos da literatura na tarefa de gerac¸ ˜ao de perguntas ou de sistemas de perguntas e respostas em portuguˆes do Brasil. Al´em disso, elas possuem trˆes componentes essenciais para a tarefa de QG: (i) contexto textual, (ii) pergunta associada e (iii) resposta correspondente.  O PIR ´A ´e uma base de dados bil´ıngue (portuguˆes-inglˆes) focada em quest˜oes oceˆanicas e da costa brasileira. A base cont´em 2.261 textos extra´ıdos de trechos de re- lat´orios das Nac¸ ˜oes Unidas sobre o oceano e de resumos relacionados ao litoral brasileiro [Paschoal et al. 2021]. As perguntas e respostas foram criadas manualmente em um pro- cesso de revis˜ao em pares por avaliadores humanos. Ap´os uma an´alise da base de dados, foi observado que alguns exemplos n˜ao apresentam as respostas para as perguntas. Por   isso, esses exemplos foram removidos, j´a que a resposta ´e um elemento importante para os experimentos realizados neste trabalho.  O FairyTaleQA ´e uma base de dados comumente usada para avaliar sistemas de perguntas e respostas em inglˆes. Essa base foi criada por especialistas em educac¸ ˜ao e ´e composta por textos narrativos infantis. Leite et al. [Leite et al. 2024] realizaram um processo de traduc¸ ˜ao do FairyTaleQA para diversos idiomas, incluindo o portuguˆes de Portugal e do Brasil. Neste trabalho, foi utilizada a vers˜ao traduzida para o portuguˆes do Brasil, que compreende 10.580 perguntas e respostas derivadas de 278 hist´orias infantis.  Na Tabela 1 s˜ao apresentadas para cada base de dados as estat´ısticas do total de exemplos em cada conjunto (treinamento, validac¸ ˜ao e teste) e o tamanho m´edio e desvio padr˜ao do total de palavras para cada componente (contexto, pergunta e resposta). Para gerar essas estat´ısticas, foi utilizada a ferramenta spaCy2 para o processamento dos textos.  Tabela 1. Estat´ıstica das bases de dados do PIR ´A e FairyTaleQA  Base de Dados  Conjunto  Exemplos  Componente M´edia de Palavras (Desvio Padr˜ao)  Treino  1.756  PIR ´A  Validac¸ ˜ao  215  Teste  216  Treino  8.548  FairyTaleQA  Validac¸ ˜ao  1.025  Teste  1.007  Contexto Pergunta Resposta Contexto Pergunta Resposta Contexto Pergunta Resposta Contexto Pergunta Resposta Contexto Pergunta Resposta Contexto Pergunta Resposta  274,73 (141,41) 13,83 (5,62) 14,32 (11,76) 273,98 (157,08) 13,65 (5,38) 15,04 (12,06) 250,58 (128,98) 13,36 (5,68) 14,92 (14,50) 182,51 (94,53) 10,23 (3,38) 6,98 (5,73) 170,08 (74,18) 10,93 (3,40) 7,52 (5,96) 168,92 (73,77) 10,48 (3,30) 6,80 (5,31)  3.2. Modelos Avaliados  Neste trabalho, foram avaliados os modelos P T T 5 e o F LAN -T 5, baseados na arquite- tura Text-to-Text Transfer Transformer (T5) [Raffel et al. 2020]. Apesar de existirem dife- rentes tamanhos de arquitetura, as trˆes comumente usadas s˜ao “small”, “base” e “large”. Elas possuem um n´umero crescente de parˆametros, o que geralmente resulta em maior capacidade de aprendizado, mas tamb´em em um maior custo computacional. Esses mo- delos foram escolhidos devido ao seu desempenho promissor em tarefas de PLN e por terem sido explorados em trabalhos anteriores.  O P T T 5 ´e uma adaptac¸ ˜ao do modelo T 5, especificamente pr´e-treinada para o portuguˆes do Brasil [Carmo et al. 2020]. O modelo foi pr´e-treinado no corpus BrWac [Wagner Filho et al. 2018], uma extensa colec¸ ˜ao de p´aginas web em portuguˆes, contendo aproximadamente 2,7 bilh˜oes de tokens. Foram utilizados os modelos P T T 5Small, que possui aproximadamente 60 milh˜oes de parˆametros, o P T T 5Base, com cerca de 220 milh˜oes de parˆametros, e o P T T 5Large, que apresenta aproximadamente 740 milh˜oes de parˆametros.  2https://spacy.io/   O F LAN -T 5 [Chung et al. 2024] ´e uma vers˜ao aprimorada do T 5 pr´e-treinado em m´ultiplas tarefas de PLN. Esse modelo foi pr´e-treinado majoritariamente em do- cumentos em inglˆes, mas possui suporte a outros idiomas, como o portuguˆes. Foram avaliadas trˆes variantes deste modelo: o F LAN -T 5Small com cerca de 80 milh˜oes de parˆametros, o F LAN -T 5Base contendo aproximadamente 250 milh˜oes de parˆametros, e o F LAN -T 5Large apresentando cerca de 780 milh˜oes de parˆametros. Sua inclus˜ao tem o objetivo de avaliar como um modelo com treinamento diversificado se comporta em comparac¸ ˜ao a modelos especializados em um ´unico idioma e tarefa.  3.3. Metodologia Experimental  A metodologia experimental utilizada neste trabalho envolveu o desenvolvimento, ajuste fino e a avaliac¸ ˜ao dos modelos investigados, especificamente ajustados para dois cen´arios da tarefa de gerac¸ ˜ao de perguntas, conforme ilustrado na Figura 1. Para cada cen´ario, seis modelos foram treinados, considerando os trˆes tamanhos de arquitetura e os dois modelos P T T 5 e F LAN -T 5. No primeiro cen´ario, foi analisada a variac¸ ˜ao da tarefa que gera perguntas a partir somente do contexto, como entrada. J´a no segundo cen´ario, os modelos recebem tanto o contexto quanto uma resposta como entrada e devem gerar uma pergunta como sa´ıda.  Figura 1. Cen ´arios da tarefa de QG analisados.  Os modelos P T T 5 e F LAN -T 5 foram implementados usando a biblioteca Trans- formers3. O tamanho de entrada m´aximo foi definido para 512 tokens, enquanto a sa´ıda foi configurada para no m´aximo 40 tokens. Durante o treinamento, os modelos foram ajustados por no m´aximo 20 ´epocas, sendo utilizada a estrat´egia de parada antecipada com uma paciˆencia de 5 ´epocas. Para mitigar o sobreajuste dos modelos, ao final de cada ´epoca, o modelo treinado ´e aplicado no conjunto de validac¸ ˜ao e ´e computada a medida do ROUGE-L, sendo salvo somente o modelo com maior valor. Durante a gerac¸ ˜ao das per- guntas, foi utilizado o algoritmo de Beam Search com uma largura de tamanho 5. Esses valores foram definidos a partir da an´alise de trabalhos anteriores.  A avaliac¸ ˜ao do desempenho dos modelos foi realizada por meio de duas abor- dagens: a aplicac¸ ˜ao de m´etricas autom´aticas de similaridade e uma avaliac¸ ˜ao com base no modelo de linguagem GP T -4o. Para a avaliac¸ ˜ao autom´atica, foi utilizada a m´etrica Recall-Oriented Understudy for Gisting Evaluation Longest Common Subse- quence (ROUGE-L) [Lin 2004], que mensura a similaridade com base na maior sequˆencia de palavras em comum entre as perguntas geradas e as perguntas de referˆencia. Adicional- mente, foi utilizada a m´etrica BERTScore [Zhang et al. 2019], que calcula a similaridade de cosseno a partir das representac¸ ˜oes em embeddings extra´ıdas do modelo Bidirectional Encoder Representations for Transformers (BERT).  3https://huggingface.co/docs/transformers/index   Para uma avaliac¸ ˜ao mais hol´ıstica e contextualmente relevante, foi realizada uma an´alise usando o modelo GP T -4o. Essa avaliac¸ ˜ao foi pensada porque, dado um con- texto espec´ıfico, ´e poss´ıvel gerar m´ultiplas perguntas v´alidas que n˜ao necessariamente precisam ser idˆenticas `a pergunta de referˆencia presente nas bases de dados usadas nos experimentos. Esta situac¸ ˜ao ´e particularmente relevante no Cen´ario 1, onde apenas o con- texto ´e fornecido como entrada para o modelo. Neste caso, diversas perguntas podem ser consideradas v´alidas, desde que sejam respond´ıveis com base no contexto fornecido. Em contraste, no Cen´ario 2, onde o contexto e a resposta esperada s˜ao fornecidos como entrada, a pergunta gerada deve ser semanticamente equivalente `a pergunta de referˆencia.  O processo de avaliac¸ ˜ao utilizando o GP T -4o foi inspirado na t´ecnica de Retrieval Augmented Generation (RAG). Esta t´ecnica consiste em fornecer um contexto e uma per- gunta para um modelo de linguagem de grande escala (LLM, do inglˆes Large Language Model), solicitando que ele responda `a pergunta usando apenas o contexto fornecido ou sinalize caso n˜ao seja poss´ıvel [Chen et al. 2024]. Seguindo esta abordagem, foi criado um prompt4 contendo o contexto original e a pergunta gerada pelos modelos avaliados. Este prompt foi ent˜ao submetido ao GP T -4o, com a instruc¸ ˜ao de responder `a pergunta utilizando somente o contexto fornecido ou indicar a impossibilidade de resposta. Com base nas respostas do GP T -4o, foi calculado o percentual de perguntas v´alidas (aquelas que o LLM conseguiu responder com base no contexto) e inv´alidas (as que n˜ao pude- ram ser respondidas) para cada modelo avaliado. Deste modo, foi poss´ıvel analisar se as perguntas geradas foram relevantes ao contexto, ainda que diferentes da pergunta de referˆencia. Embora esta an´alise seja automatizada, foi realizada uma inspec¸ ˜ao manual em amostras das sa´ıdas do GP T -4o para verificar sua confiabilidade. Foi observado que, em geral, o LLM identificava corretamente as perguntas v´alidas e inv´alidas.  4. Resultados  Na Tabela 2 s˜ao apresentados os resultados dos experimentos, considerando os cen´arios 1 e 2, com base nas medidas de avaliac¸ ˜ao do ROUGE-L e BERTScore. No Cen´ario 1 (apenas contexto), o P T T 5Base e o P T T 5Large apresentaram os melhores desempenhos para as bases de dados do FairyTaleQA e PIR ´A, respectivamente. No Cen´ario 2 (contexto e resposta esperada), o P T T 5Large superou os demais modelos em ambas as bases. Fica evidente que os modelos P T T 5 consistentemente obtiveram melhores resultados do que os modelos F LAN -T 5 em ambos os conjuntos de dados e cen´arios, sugerindo que o pr´e- treinamento espec´ıfico em portuguˆes confere vantagens na tarefa de gerac¸ ˜ao de perguntas.  Comparando os resultados obtidos em ambos os cen´arios de avaliac¸ ˜ao, observa-se que os modelos apresentaram melhores desempenhos no Cen´ario 2 em comparac¸ ˜ao com o Cen´ario 1. Isso acontece porque no Cen´ario 2, como a resposta ´e dada como entrada, ela guia os modelos a gerarem perguntas para aquele contexto e resposta. Assim, a pergunta gerada precisa ser semanticamente equivalente `a pergunta de referˆencia. Tal situac¸ ˜ao n˜ao ocorre no Cen´ario 1, j´a que ´e somente dado o contexto como entrada e, para um mesmo contexto, ´e poss´ıvel gerar diversas perguntas v´alidas. Por isso, para melhor avaliar o Cen´ario 1, foram realizadas as an´alises usando o modelo GP T -4o.  Na Tabela 3 s˜ao apresentados os resultados da avaliac¸ ˜ao dos modelos no Cen´ario 1 usando o GP T -4o. Os resultados obtidos apresentam um padr˜ao similar ao primeiro ex-  4O prompt usado est´a dispon´ıvel no reposit´orio do projeto.   1  PIR ´A  Cen´ario  FairyTaleQA  Base de Dados  Tabela 2. Resultados dos experimentos nos cen ´arios 1 e 2. Modelo P T T 5Small P T T 5Base P T T 5Large F LAN -T 5Small F LAN -T 5Base F LAN -T 5Large P T T 5Small P T T 5Base P T T 5Large F LAN -T 5Small F LAN -T 5Base F LAN -T 5Large P T T 5Small P T T 5Base P T T 5Large F LAN -T 5Small F LAN -T 5Base F LAN -T 5Large P T T 5Small P T T 5Base P T T 5Large F LAN -T 5Small F LAN -T 5Base F LAN -T 5Large  BERTScore 0,3976 0,4137 0,4093 0,3469 0,3590 0,3448 0,2982 0,3265 0,3449 0,2099 0,2404 0,2988 0,5429 0,5906 0,6057 0,4203 0,4611 0,4884 0,3635 0,4505 0,4656 0,2220 0,2579 0,3257  ROUGE-L 0,2491 0,2699 0,2668 0,2412 0,2497 0,2354 0,2109 0,2266 0,2280 0,1581 0,1723 0,2219 0,4230 0,4786 0,4938 0,3190 0,3672 0,3810 0,2625 0,3506 0,3640 0,1680 0,1934 0,2620  FairyTaleQA  PIR ´A  2  perimento, mas com algumas diferenc¸as importantes. O P T T 5Large obteve o melhor de- sempenho em ambas as bases de dados, com 82,32% das perguntas geradas sendo consi- deradas v´alidas no FairyTaleQA e 93,06% no PIR ´A. ´E poss´ıvel observar uma divergˆencia entre as medidas autom´aticas e a avaliac¸ ˜ao GP T -4o, particularmente no PIR ´A. Enquanto as medidas do ROUGE-L e BERTScore indicaram valores menores para o PIR ´A em comparac¸ ˜ao com o FairyTaleQA, a avaliac¸ ˜ao GP T -4o mostrou uma tendˆencia oposta, com percentuais mais altos de perguntas v´alidas no PIR ´A.  Tabela 3. Resultados da avaliac¸ ˜ao do Cen ´ario 1 usando o GP T -4o.  Base de Dados  FairyTaleQA  PIR ´A  Modelo P T T 5Small P T T 5Base P T T 5Large F LAN -T 5Small F LAN -T 5Base F LAN -T 5Large P T T 5Small P T T 5Base P T T 5Large F LAN -T 5Small F LAN -T 5Base F LAN -T 5Large  V´alida 680 726 829 451 525 719 135 199 201 122 138 197  Inv´alida % V´alida  327 281 178 556 482 288 81 17 15 94 78 19  67,53 72,10 82,32 44,79 52,14 71,40 62,50 92,13 93,06 56,48 63,89 91,20  Na Figura 2 ´e apresentado um exemplo extra´ıdo da base de dados do PIR ´A, con- tendo o contexto, as perguntas geradas pelos modelos P T T 5 e a sa´ıda da an´alise usando o GP T -4o. Nesse exemplo, ´e poss´ıvel ver que os modelos P T T 5Large e P T T 5Base foram capazes de gerar perguntas que podem ser respondidas pelo contexto, sendo assim consi- deradas v´alidas. Por outro lado, o modelo P T T 5Small gerou uma pergunta confusa sobre a Petr´oleo Brasileiro SA n˜ao ter comentado sobre a estimativa de produc¸ ˜ao revisada. Ape-   sar de ser mencionado no contexto, n˜ao est´a expl´ıcito nele o porquˆe disso. Sendo assim, considerada inv´alida pela avaliac¸ ˜ao do GP T -4o.  Contexto: O BG GROUP produziu recentemente uma nova estimativa na descoberta de ´oleo de Tupi na Bacia de Santos, afirmando que o campo cont´em 12-30 bilh˜oes boe ou mais. Por um lado, a Petroleo Brasileiro SA de Petr´oleo (Petrobras) n˜ao comentou sobre a estimativa de produc¸ ˜ao revisada. Pode-se lembrar que a Petrobras relatou a descoberta para ser de 8 bilh˜oes bbl de luz em bruto em 2007. Enquanto isso, alegac¸ ˜oes de BG foram produzidas em uma declarac¸ ˜ao sobre a estrat´egia de crescimento a longo prazo da empresa, lanc¸ada `a frente da apresentac¸ ˜ao de seus resultados do quarto trimestre.  P T T 5Small: Por que a Petroleo Brasileiro SA de Petr´oleo (Petrobras) n˜ao comentou sobre a esti- mativa de produc¸ ˜ao revisada? Avaliac¸ ˜ao: INV ´ALIDA.  P T T 5Base: Qual foi a estimativa do BG Group para a descoberta de ´oleo de Tupi na Bacia de Santos? Avaliac¸ ˜ao: V ´ALIDA.  P T T 5Large: Qual a estimativa do BG Group para a quantidade de ´oleo de Tupi na Bacia de Santos? Avaliac¸ ˜ao: V ´ALIDA.  Figura 2. Exemplo de contexto extra´ıdo do PIR ´A e perguntas consideradas v ´alidas e inv ´alidas pelo GP T -4o.  5. Considerac¸ ˜oes Finais e Trabalhos Futuros  Neste trabalho, foi realizada uma an´alise comparativa dos modelos P T T 5 e F LAN -T 5 para a tarefa de gerac¸ ˜ao autom´atica de perguntas. Para isso, foram utilizadas as bases de dados do PIR ´A e uma vers˜ao traduzida do FairyTaleQA para o portuguˆes do Brasil. O desempenho dos modelos foi avaliado usando uma abordagem tradicional, considerando as medidas de avaliac¸ ˜ao do ROUGE-L e do BERTScore. Al´em dessa abordagem, foi re- alizada uma an´alise das perguntas geradas pelos modelos usando o GP T -4o, avaliando se as perguntas geradas poderiam ser respondidas somente a partir do contexto fornecido. Os resultados experimentais demonstraram que o modelo P T T 5Large obteve os melhores resultados em quase todos os cen´arios avaliados. Os resultados obtidos indicam a efic´acia do pr´e-treinamento espec´ıfico em portuguˆes, evidenciada pelo desempenho superior con- sistente dos modelos P T T 5 em comparac¸ ˜ao com os modelos F LAN -T 5.  Apesar dos resultados encorajadores obtidos, o trabalho apresenta diversas limitac¸ ˜oes, que ser˜ao melhor exploradas. Dentre elas, pode-se destacar duas li- (i) investigar o desempenho de LLMs, como o Llama 3 nhas de pesquisa futuras: [Touvron et al. 2023], Gemma [Team et al. 2024] e Sabi´a [Almeida et al. 2024]; e (ii) re- alizar uma avaliac¸ ˜ao humana para complementar as avaliac¸ ˜oes autom´aticas realizadas.  "
        },
        {
            "titulo": "Unified Knowledge-Graph for Brazilian Indigenous Languages: An Educational Applications Perspective",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31127",
            "idioma": "Inglês",
            "storage_key": "files/article_31127_30930.pdf",
            "autores": [
                {
                    "nome": "Gustavo Polleti",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0001-5526-2161"
                },
                {
                    "nome": "Fabio Cozman",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-4077-4935"
                },
                {
                    "nome": "Fabrício Gerardi",
                    "afiliacao": "Universität Tübingen",
                    "orcid": null
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Este artigo apresenta um grafo de conhecimento unificado para as línguas indígenas brasileiras (BIL) a partir da perspectiva de aplicações potenciais, com foco particular no domínio educacional. Apresentamos o BILGraph, um protótipo construído para o Bororo e línguas tupis, como Guajajara, Munduruku e Akuntsu. Em seguida, descrevemos o processo de extração de conhecimento e ligação de entidades para construir o grafo a partir de um banco de árvores de dependências e de um banco de dados lexical para línguas Tupi e Bororo. Discutimos as limitações do BILGraph, destacando questões éticas e práticas de implementação.",
            "keywords": [
                "Knowledge Graph",
                "Low-resource languages",
                "Natural Language Processing",
                "Indigenous Languages"
            ],
            "referencias": [
                "Cabral, A. S. and Rodrigues, A. (2003). Dicionário da língua asurini do tocantins. Belém-Pará: UFPA/IFNOPAP/UnB: IL/LALI.",
                "Cong, J. and Liu, H. (2014). Approaching human language with complex networks. Physics of Life Reviews, 11(4):598–618.",
                "Ferraz Gerardi, F. Bororo Dictionary. Forthcoming. Available upon request.",
                "Ferraz Gerardi, F. (2024). Universaldependencies/udbororo−bdt.",
                "Ferraz Gerardi, F. M., Sollberger, D., and Toribio Serrano, L. (2024). Corpus bororo (corbo) (v0.1.1).",
                "Gerardi, F. F., Reichert, S., Aragon, C., Wientzek, T., List, J.-M., and Forkel, R. (2022a). TuLeD. Tupían Lexical Database. Zenodo.",
                "Gerardi, F. F., Reichert, S., Aragon, C., Wientzek, T., List, J.-M., and Forkel, R. (2022b). TuLeD. Tupían Lexical Database (v0.12).",
                "Harrison, C. and Harrison, C. (2013). Dicionário Guajajara-Português. SIL.",
                ", Aboriginal Territories in Cyberspace, Honolulu, HI. Edited by Jason Edward Lewis. English Language Version of ”Ka?ina Hana ?Ōiwi a me ka Waihona ?Ike Hakuhia Pepa Kūlana” available at:",
                ".",
                "Miller, G. A. (1994). WordNet: A lexical database for English. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994.",
                "Monserrat, R. F. (2000). Vocabulário Amondawa-Português, Vocabulário e frases em Arara e Português, Vocabulário Gavião-Português, Vocabulário e frases em Karipuna e Português, Vocabulário e frases em Makurap e Português, Vocabulário e frases em Suruíe Português, Pequeno dicionário em Tupari e Português. Universidade do Caixas do Sul.",
                "Nivre, J., Abrams, M., Agić, Z., Ahrenberg, L., Antonsen, L., Aranzabe, M. J., Arutie, A., Asahara, M., Ateyah, L., Attia, M., et al. (2020a). Universal dependencies v2: An evergrowing multilingual treebank collection.",
                ". Accessed: 2024-08-27.",
                "Nivre, J., de Marneffe, M.-C., Ginter, F., Hajič, J., Manning, C. D., Pyysalo, S., Schuster, S., Tyers, F., and Zeman, D. (2020b). Universal Dependencies v2: An evergrowing multilingual treebank collection. In Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno, A., Odijk, J., and Piperidis, S., editors, Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4034–4043, Marseille, France. European Language Resources Association.",
                "Pinhanez, C. S., Cavalin, P., Vasconcelos, M., and Nogima, J. (2023). Balancing social impact, opportunities, and ethical constraints of using ai in the documentation and vitalization of indigenous languages. In Elkind, E., editor, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, pages 6174–6182. International Joint Conferences on Artificial Intelligence Organization. AI for Good.",
                "Polleti, G. (2024). Building a language-learning game for Brazilian indigenous languages: A case study. Technical report, arXiv:2403.14515."
            ],
            "artigo_completo": "Resumo. Este artigo apresenta um grafo de conhecimento unificado para as l´ınguas ind´ıgenas brasileiras (BIL) a partir da perspectiva de aplicac¸ ˜oes po- tenciais, com foco particular no dom´ınio educacional. Apresentamos o BIL- Graph, um prot´otipo constru´ıdo para o Bororo e l´ınguas tupis, como Guajajara, Munduruku e Akuntsu. Em seguida, descrevemos o processo de extrac¸ ˜ao de co- nhecimento e ligac¸ ˜ao de entidades para construir o grafo a partir de um banco de ´arvores de dependˆencias e de um banco de dados lexical para l´ınguas Tupi e Bororo. Discutimos as limitac¸ ˜oes do BILGraph, destacando quest˜oes ´eticas e pr´aticas de implementac¸ ˜ao.  1. Introduction  The development of applications for Brazilian Indigenous languages (BIL) is severely limited by the lack of resources and tools. As is often the case with endangered lan- guages, available resources are both scarce and dispersed [Pinhanez et al. 2023]. For some languages, such as Guajajara, Asurini, and Bororo, dictionaries are now available [Harrison and Harrison 2013, Cabral and Rodrigues 2003, Ferraz Gerardi ]. For other languages, treebanks are available through the Universal Dependencies Project (UD) [Nivre et al. 2020a], though they vary in length and quality. Some languages, however, have only a handful of miscellaneous resources [Monserrat 2000]. This lack of standard- ization and proper linked data poses a significant barrier to developing tools and methods that could support language revitalization initiatives and accelerate the production of ped- agogical material.  Recent efforts to unify Brazilian Indigenous language resources, such as TuLeD [Gerardi et al. 2022a] and the TuDeT treebanks on UD — a lexical database and a de- pendency treebank for several Tupian languages (still in their initial phase), respec- tively — have been pivotal in the development of language-learning applications tar- geted at Indigenous communities [Polleti 2024]. Additionally, the recent publication of   Figure 1. BILGraph toy example displaying a sampled subgraph associated with the sentence Bororo “ure karo kowuje”, i.e. “He ate the fish”.  the Bororo Corpus [Ferraz Gerardi et al. 2024], which is connected to the UD Treebank [Ferraz Gerardi 2024], has enabled the use of various computational tools to develop ed- ucational materials and other online resources; notably a language-learning app for the Bororo language.1 UD-treebanks [Nivre et al. 2020b] are an important resource since the standardized type of annotation for all languages facilitate the development of new applications. On the other hand, heterogeneous and complex network structures, such as knowledge graphs, are known for their flexibility in incorporating linguistic charac- teristics [Cong and Liu 2014, Miller 1994] and can be effectively utilized to power so- phisticated applications, including recommendation systems, information retrieval, and educational assistants.  In this work, we introduce a preliminary version of an unified knowledge graph for Brazilian indigenous languages, which we will refer as “BILGraph”, and we de- scribe its knowledge extraction pipeline. We developed a prototype for Tupian lan- guages available in Tuled and Tudet [Gerardi et al. 2022b], and the Bororo language [Ferraz Gerardi et al. 2024]. We discuss the knowledge graph prototype with a focus on the potential applications. We managed to develop a natural language processing pipeline to build BILGraph that can handle semi-structured data from several sources, such as an- notated phrases from treebanks and dictionaries. We discuss the pipeline challenges and limitations. The main contribution of this work is to present a prototype version of BIL- Graph as a case of study on building an unified knowledge graph for BIL. We hope the knowledge graph and the methods presented in this work can support the development of sophisticated applications.  The paper is organized as follows. Section 2 describes BILGraph’s design and its development, including their data sources and knowledge extraction pipeline. Section 3 discusses the challenges and limitations of our prototype, analyses our processes and resources from both a practical implementation and potential applications perspective, and offers concluding remarks.  2. BilGraph: Linguistic Knowledge Graph  We have developed a knowledge extraction pipeline to structure and link language re- sources for Brazilian Indigenous Languages (BIL) available in Universal Dependencies (UD) treebanks and lexical databases, such as TuLeD and the Bororo dictionary. The re- sult of this effort is “BILGraph”, a knowledge graph for BIL that contains four principal types of nodes: (1) sentence, (2) token, (3) lemma, and (4) concept. Consider the example depicted in Figure 1. The sentence node represents the Bororo treebank phrase ure karo  1https://bilingo-4388e.web.app/   kowyje ‘He ate the fish’. This sentence node connects to its token nodes, which repre- sent the individual words composing the sentence and their syntactic dependencies. In this example, “kowyje” is the root, with the object “karo” and the nominal subject “ure” linking to it. Each token node is connected to a single sentence node. Each token is fur- ther linked to a lemma node, which represents the word’s base form and its relationships to linguistic classes, including any applicable synonyms. Up to this point, the entities and relationships described are those typically found in dependency treebanks. However, the lexical database or dictionary adds another layer by linking lemma nodes to concept nodes. Concept nodes represent high-level abstractions that convey meaning across dif- ferent languages and domains. In our example, the lemmas “karo” and “kowyje” are linked to the concepts “fish” and “eat,” respectively. The goal is to establish the concept nodes as a semantic layer that enables interoperability between the treebank sentences and other knowledge bases, such as ontologies, multimedia resources (e.g., phonetic or image databases), and other languages. Using BILGraph, one could easily search for sentences in other languages with similar structures or themes by fetching all sentence nodes con- nected to a given concept node. For example, a search engine could retrieve the Guajajara sentence uPu ipirateteaPu ‘It eats many fishes’ because it is connected to the concept node “FISH” as the similar sentence in Bororo ure karo kowyje. Note that the graph structure is flexible enough to encode N-N relationships between lemmas and concepts.  The relationships between sentences, tokens and lemmas can be extracted directly from UD treebanks, as the treebank sentences are annotated with attributes that allow a straightforward graph representation. To build BILGraph, the real challenge lies in linking lemma to concept nodes. In our preliminary version, we applied a simple entity linking process as follows. For each lemma, we generated a neighborhood set of similar words by changing and trimming characters based on rules. For example, in the Bororo language, we have different spellings where some words exchange “u” for “y”, and words like “boe” are often applied, so some of our neighborhood generation rules involved in adding or re- moving prefixes and changing exchangeable letters. The size of the neighborhood was defined considering a similarity threshold based on the Leveshnstein distance. Next, we select from all the vocabulary in our database the words that display high similarity, con- sidering again a threshold based on leveshnstein distance, with at least one instance in our neighborhood. Finally, we test if dictionary entry or description for each candidate has at least one word in the sentence. So, for example, consider we are trying to link the lemma “karo”, from the sentence “He ate the fish”, to its appropriate concepts. Addition- ally, consider the dictionary description for a word candidate “kabo” is “a type of river fish”. In this case, we will establish the link due to the lexical similarity between “karo” and “kabo”, and due to the word “fish” that is present in both the dictionary entry and the sentence. Note that relying on lexical similarity may lead to innacuracies. For example, the Bororo words “apido” (palm heart) and “apodo” (toucan) have high lexical similarity while there meanings are not related at all. If a dictionary entry contains both words, such as “palm hearth, edible for many animals like toucans”, this would lead to incorrect links being added to the graph. BILGraph’s knowledge extraction pipeline code, with the used Leveshentein distance thresholds for each language, and the knowledge graph itself is available in Github.2 We adopted the RDF format, where each edge in the graph is represented as a triple.  2https://github.com/gpadpoll/bilgraph   3. Discussion and Concluding Remarks  The preliminary version of BILGraph introduced in this work represents a significant step forward in advancing resources for Brazilian Indigenous languages. We envision that BILGraph could power typical applications such as information retrieval from texts written in these languages, with a particular emphasis on its educational potential. The process of creating educational resources often involves organizing texts based on their linguistic characteristics, themes, and complexity levels. For instance, one might search for specific sentences to teach someone how to ask for food. BILGraph simplifies this task by allowing queries for sentences linked to specific concept nodes. To find sentences that include food-related vocabulary, one can attach a generic ontology to BILGraph’s concept nodes and search for sentences associated with food-related concepts. Moreover, BILGraph makes it easy to query sentences based on linguistic features, such as those using possessive pronouns, verb forms, plurals, adverbs, and more. We believe that BIL- Graph’s ability to query and organize sentences can enhance the use of treebanks and other available BIL resources in the development of educational materials. By organiz- ing resources in a standardized and unified format, we can develop applications that scale across multiple languages. For example, a query that searches for food-related concepts in sentences for one language can be reused for other languages included in BILGraph. We are already leveraging BILGraph to develop a curriculum for a Bororo language course, which will be released as a language-learning app. We aim to extend this approach to other languages as they are incorporated into the knowledge graph.  At this point, our BILGraph prototype falls short in several aspects and remains a work in progress, from the difficulties of working with limited sources of data to in- nacuracies and ethical concerns. BILGraph was built from TuLeD, TuDet and the Bororo treebank and dictionary. All these data sources were developed by compiling several sources from the literature, without a proper structured data gathering process. As a re- sult, it suffers from incompleness, notably when we consider coverage of dependency trees with translation to portuguese. We only have have Portuguese translations for “Bororo”, “Guajajara”, “Munduruku” and “Akuntsu” out of the 9 languages available. The lack of Portuguese translations limits the application of these resources, as for educa- tional purposes for example. Furthermore, it reasonable to expect that some innacuracies may have been introduced as part of the entity linking and knowledge extraction pro- cess. We haven’t evaluated the correctness in a comprehensive manner yet, except for limited manual inspection by the researchers. Finally, it is worth mentioniong ethical concerns. BILGraph has been developed without the involvement of indigenous commu- nity [Pinhanez et al. 2023], except for the case of Bororo, so it is hard to enforce ethical guidelines [Lewis et al. 2020], as for example proposed by the Los Pinos Declaration,3 before BILGraph can be properly inspected and validated by actual indigenous speakers.  We recognize a limitation in distinguishing similar forms that map to different lemmas. While various solutions exist, the most effective approach tend to be proba- bilistic, improving in accuracy with larger datasets. We also focus on further research in developing a pipeline which only uses the target language, without relying on the use af a dictionary. Overall, we hope BILGraph represents a positive step towards an unified source for BIL resources so that more tools and applications can be developed for them.  3https://unesdoc.unesco.org/ark:/48223/pf0000374030   Acknowledgements  The second author was partially supported by CNPq grant 305753/2022-3. We also thank support by CAPES -Finance Code 001. The authors of this work would like to thank the Center for Artificial Intelligence (C4AI-USP) and the support from the S˜ao Paulo Research Foundation (FAPESP grant 2019/07665-4) and from the IBM Corporation.  "
        },
        {
            "titulo": "Boosting not so Large Language Models by using Knowledge Graphs and Reinforcement Learning",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31128",
            "idioma": "Inglês",
            "storage_key": "files/article_31128_30931.pdf",
            "autores": [
                {
                    "nome": "William Jones Beckhauser",
                    "afiliacao": "UFSC",
                    "orcid": "http://orcid.org/0009-0004-2647-3874"
                },
                {
                    "nome": "Renato Fileto",
                    "afiliacao": "UFSC",
                    "orcid": "https://orcid.org/0000-0002-7941-6281"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Ensuring the viability of large language models (LLMs) in situations requiring data privacy with limited on-premise resources is a significant current challenge. This work investigates how to tackle this challenge using knowledge graphs (KGs) and reinforcement learning (RL) to enhance minor LLMs by reducing non-factual responses and response gaps. We evaluated variations of GPT (4o, 4, and 3.5), Llama2 (7b, 13b, and 70b), and Llama3 (8b and 70b) for multi-label classification and information extraction, with or without KG and RL, and also fine-tuned a BERT model. Llama3 8b combined with KG and RL outperformed all other LLM models, and the fine-tuned BERT model too.",
            "keywords": [
                "Large Language Models",
                "Knowledge Graphs",
                "Reinforcement Learning"
            ],
            "referencias": [
                "Erickson, A. (2018). Comparative analysis of the eu’s gdpr and brazil’s lgpd: Enforcement challenges with the lgpd. Brook. J. Int’l L., 44:859.",
                "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744.",
                "Xue, F., Fu, Y., Zhou, W., Zheng, Z., and You, Y. (2024). To repeat or not to repeat: Insights from scaling llm under token-crisis. Advances in Neural Information Processing Systems, 36."
            ],
            "artigo_completo": "Abstract. Ensuring the viability of large language models (LLMs) in situations requiring data privacy with limited on-premise resources is a significant current challenge. This work investigates how to tackle this challenge using knowledge graphs (KGs) and reinforcement learning (RL) to enhance minor LLMs by re- ducing non-factual responses and response gaps. We evaluated variations of GPT (4o, 4, and 3.5), Llama2 (7b, 13b, and 70b), and Llama3 (8b and 70b) for multi-label classification and information extraction, with or without KG and RL, and also fine-tuned a BERT model. Llama3 8b combined with KG and RL outperformed all other LLM models, and the fine-tuned BERT model too.  1. Introduction  Large language models (LLMs) such as GPT [Liu et al. 2023], Llama [Gao et al. 2023], and Gemini [Team et al. 2023] are increasing their parameter count with each new re- lease, for performance gains [Xue et al. 2024]. Nevertheless, this technology, usually available in the clouds of large private corporations, remains out of reach for many compa- nies and projects that need to operate on local servers [Yao et al. 2024], due to high costs and regulations like the General Data Protection Law (LGPD) [Erickson 2018]. These enterprises could rely on open-source models with many parameters, but their computa- tional requirements are too high to run on-premises [Alizadeh et al. 2023].  Nowadays, there is a subtle research movement towards smaller open-source LLMs [Shridhar et al. 2023, Shen et al. 2024], and an intense pursuit of optimization strategies. A promising direction is Retrievable Augmented Generation (RAG) using a Knowledge Graph (KG) to add relevant formal knowledge to LLMs [Pan et al. 2023]. This approach has been tested in various tasks, including fake news detection [Liu et al. 2024], text classification [Shi et al. 2023], and refined node classification in citation graphs and networks [Bruno et al. 2023, He et al. 2023]. In the biomedical do- main, these solutions have been applied in recommendation systems and drug-gene in- teraction studies [Xu et al. 2024, Wang et al. 2023], as well as in recruiting for clinical studies [Guan et al. 2023]. However, there are still few concrete examples demonstrating consistent performance gains by using approaches like Graph-RAG [Pan et al. 2023] in typical machine learning tasks, such as multi-label classification or information extrac- tion, especially when using open-source LLMs.  This article contributes to filling this research gap by evaluating the synergism of KGs, reinforcement learning (RL) and LLMs. We compare the performance of rela- tively small LLMs, like Llama2 (7b and 13b) and Llama3 (8b), with that of larger LLMs   like GPT (4, 4o, and 3.5), Llama2 (70b), and Llama3 (70b), each one alone and com- bined with the use of KGs and RL, in two tasks: (i) multi-label classification of reviews posted by users of a food delivery app in multiple languages and their translations into English, and (ii) information extraction from invoices of different types of backhoes. We propose and evaluate alternative approaches for exploiting domain specific KGs to enrich LLM prompts with relevant context. An RL agent validates responses, restricting them to predefined labels, when available, and providing feedback to the models. It randomly validates some LLM responses with their respective labels throughout the RL process. We also fine-tuned and evaluated a BERT model for performing the same multi-label classification, on the same datasets.  The main contributions of this article are: (i) a systematic evaluation of language models, considering each LLM alone and assisted by a KG and/or an RL agent; (ii) demonstrating the superiority of smaller, open-source models, like Llama3 8b, when com- bined with KGs; (iii) showcasing the feasibility of feedback systems for language models; and (iv) applying LLMs combined with KGs and RL in unexplored fields.  2. Proposed Approach  Figure 1 shows the architecture of our integrated Graph-RAG and RL system for LLMs, designed to optimize responses in classification and information extraction tasks. The process starts with a instruction sent to the Prompt Raising module, supplemented by data from annotated corpora (e.g., a backhoe invoice). This module interacts with the KG/vector management component to search the Knowledge Base for relevant context by accessing the knowledge graph linked to the instruction. The retrieved context is then integrated into the prompt, which the LLM uses to generate a response. The RL Agent checks the LLM’s output against available labels(train data). If inaccuracies arise, feed- back is given to the LLM, and interaction results are stored in the Results database.  Figure 1. Proposed process for using knowledge and RL to improve LLM results.  2.1. Pre-production  In the pre-production phase, we focus on constructing KGs using domain-specific struc- tured data sources. For example, in information extraction tests related to backhoe load- ers, we use tables with product descriptions segmented into products, brands, and mod- els. These are organized into a hierarchy of classes and subclasses, with connections like “Product” connected by “offered by” to “Brand,” which in turn connects via “has”  LLMRL AgentInstructionRetrieved relevant contextSearch  related  contextAssociated  label searchAssociated label results, if anyContext Enriched PromptFeedback PromptData managenentKG / Vector ManagementPre-productionResultsPrompt  RaisingDomain specific knowledge (legacy  KGs & documents)Integration, chucking, embedding and  indexingKnowledge BaseAnnotated Corpora to “Model”. Once the structure is defined and validated, we generate embeddings for the classes, subclasses, and connections using the BGE model [Chen et al. 2024]. The graphs are then implemented in the Neo4j graph database, incorporating the generated embeddings. These KGs stored in Neo4j serve as our Knowledge Base.  2.2. Prompt Raising  In the \"Prompt Raising\" phase, the system processes three inputs: the instruction, textual descriptions on data management, and relevant information from the knowledge base. The third input is obtained via two methods: Graph-RAG Targeted, retrieving highly similar information, and Graph-RAG Comprehensive, gathering related classes and their interrelationships without filters.  Embeddings are generated from the inputs using the BGE model, the same one employed for knowledge graphs (KGs). Then, a similarity search compares these embed- dings with the knowledge base using cosine similarity. The Graph-RAG Targeted method identifies records with a cosine similarity above 85%, while the Graph-RAG Comprehen- sive method retrieves all relevant classes, subclasses, and connections. For example, when processing \"3C OPEN CAB JCB BACKHOE LOADER\" in a brand-related instruction, Graph-RAG Targeted might indicate \"97% probability for JCB and 3% for New Hol- land,\" while Graph-RAG Comprehensive would provide broader insights such as \"The JCB brand includes models 3CX and 5CX\" and \"New Holland covers models B95C and B115C\". Thus, as shown in Figure 2, the output of Prompt Raising consists of Instruction and Output Indicator. The Input Data represents a textual description from Data Manage- ment, and the Context, in this example, is Graph-RAG Targeted, which retrieved the data with the highest cosine similarities from our Knowledge Base.  Figure 2. Graph-RAG-Enhanced Contextual Prompt for Information Extraction.  2.3. LLM  We configure the LLM and invoke its API using an enriched prompt derived from the Prompt Raising stage. Key parameters, like temperature and output token count, are ad- justed. Temperature controls prediction randomness, with lower values yielding more deterministic results and higher values increasing creativity. For classification and extrac- tion tasks, we limit the output to fewer than 10 tokens. We employ models like Llama2 (7b, 13b, 70b) and Llama3 (8b, 70b) via the Deepinfra API, as well as GPT models (3.5, 4.0) via the OpenAI API. With the enriched prompt and optimal model settings, the API is called to perform extraction or classification. For example, for the product description “3C OPEN CAB JCB BACKHOE LOADER”, the expected response would be “JCB”.  2.4. RL Agent  The RL Agent processes the output of the LLM model by checking if there is a corre- sponding label in the database, as detailed in the enriched prompt. The annotated corpora  You are tasked with extracting the brand from the product description. What is the brand contained in the product description?   Product Description: 3C OPEN CAB JCB BACKHOE LOADER.  There is a 97% chance of the brand being JCB  and an 3% chance of it being New Holland.  Respond only with the brand name, no additional details.  Instruction1Output Indicator4Context3Input Data2 include a percentage of pre-labeled data randomly distributed, and each new LLM output is compared against these corpora. For example, if the LLM classifies a product descrip- tion as \"New Holland\" for \"JCB 3C OPEN CAB Backhoe Loader,\" the RL Agent searches the annotated corpora to check if there is a label. If \"New Holland\" is correct or if there is no existing label, the response is validated and stored; if incorrect, the agent provides feedback suggesting the correct label. This process is repeated up to five times to correct and reinforce the model’s learning. For classification tasks with predefined labels, the RL Agent adopts a two-step validation process. First, it checks if the LLM’s classification matches the predefined labels. If it doesn’t match, the agent provides feedback to align the response with the established categories. In the second step, if the classification falls within the categories, the Agent validates it against the associated label (if any).  3. Application Scenarios  3.1. Multi-label Classification of Food Delivery Reviews  In our first scenario, we analyzed a dataset of around 4,000 customer reviews from a European food delivery app, ranging from 0 to 889 characters, available in [Beckhauser and Fileto 2024]. After removing duplicates and outliers, 3,451 reviews remained. Approximately 80% are in European Portuguese, with the rest in English, Spanish, Italian, and Catalan. Given the importance of English in LLM training, we created a parallel dataset by translating all reviews into English using Googletrans, with manual corrections for about 30 reviews. We then identified key terms for each label by removing stop-words in multiple languages using nltk and spaCy and extracting fre- quent words with the Counter library. For sentiment analysis, we used the SiEBERT model [Hartmann et al. 2023], which showed consistent performance, even when com- pared to GPT-4 [Krugmann and Hartmann 2024]. Sentiment analysis results and dataset details are summarized in Table 1.  We manually built a tree-like KG to categorize reviews, distinguishing be- tween “Product” (item-related) and “Order” (delivery/service-related). Subcategories like “Quantity issue” and “Quality issue” under “Product,” and “Delivery issue” and “Praise comment” under “Order” are further refined with specific keywords.  Table 1. Dataset review distribution by class, subclass, and sentiment.  Class  Product  Product  Order  Order  Subclass Description Quality issue Quantity issue Delivery issue Praise comment  Issues with food preparation, taste, or hygiene. Dissatisfaction with the amount or size of the portions served. Problems related to delays, wrong deliveries or missing items. Positive comments about the quality of the service or product.  #Reviews % Pos. Neg.  671  605  19.44  26% 74%  17.53  28% 72%  1196  34.66  26% 74%  979  28.37  98% 2%  3.2. Information Extraction from Invoices  Our second application scenario involves a dataset of approximately 17,000 work machine purchase invoice descriptions, including the Mercosur Common Nomenclature (NCM)   and unit item values, provided by (blind review). The invoice descriptions range from 16 to 120 characters. Initially, we filtered the dataset using the NCM code, focusing on the first four characters, specifically \"8429,\" which covers bulldozers, graders, excavators, and similar machinery. We then applied a keyword dictionary to identify relevant terms. Backhoe loaders appeared most frequently, with around 1,100 descriptions, becoming the primary focus of our experiments. This dataset lacked initial classifications, containing only raw invoice descriptions. To facilitate future model validation, we manually cat- egorized the data into predefined classes such as brand, model, and specifications. A dictionary comprising brands, models, keywords, orthographic variations, acronyms, and abbreviations was used, considering possible typographical errors. Fields not covered by the dictionaries were manually completed, ensuring thorough validation of LLM outputs.  KGs for Backhoe Invoices. Figure 3 shows an extract of an ontology in KG for- mat, centered on heavy machinery. It depicts the ’Product’ concept, with ’Backhoe’ as a subclass, linked to 16 brands via the \"offered by\" relation. Brands like ’New Holland’ and ’JCB’ are highlighted, each connected to specific models through the \"has\" relation. For instance, ’New Holland’ includes models like ’B110B’ and ’LB90,’ while ’JCB’ offers ’4CX ECO’ and ’3CX.’ In total, 68 models are represented.  Figure 3. KG extraction with concepts and relations from heavy machinery.  4. Experiments  In this section, we describe the experiments conducted for multi-label classification with customer reviews, subsection 4.1, and information extraction experiments using back- hoe invoice data, subsection 4.2. All datasets and models were tested in various distinct scenarios: (1) classification or extraction using only the instruction and corpus, without providing enriched context to the LLM; (2) using only the RL Agent; (3) adding a compre- hensive search in the KGs, which returns all classes, subclasses, relationships, and leafs as context; (4) using targeted context with similarity search above 85%, utilizing Graph- RAG; (5) using Graph-RAG Comprehensive with RL; (6) using Graph-RAG Targeted with RL. Additionally, for the multi-label classification experiments, we will conduct a test with embeddings and fine-tuning using BERT. A more comprehensive description of the experiments developed is available at GitHub1.  4.1. Multilabel Classification of Customer Reviews  In this subsection, we present the experiments conducted for multi-label classification. The experiments are performed on two subsets of customer review data: the first contains  1https://github.com/WilliamBeckhauser/Boosting-not-so-LLM  ProductBackhoeNew HollandJCB...B110BLB90...4CX ECO3CX.........subclassOf...subclassOf...subclassOfBrandModeloffered byhas reviews from customers in various languages, and the second comprises the same reviews translated into English. Each dataset includes 3,451 reviews. We randomly selected 300 reviews from each label for the agent to use as a validator during the classification process, resulting in 1,200 reviews used solely for reinforcement training on the model.  The BERT experiment tokenize reviews and split them into training and testing datasets at an 80/20 ratio. We use BERT to produce embeddings and a training function with an AdamW optimizer and a linear scheduler. To optimize hyperparameters, we set up an objective function in Optuna, adjusting the learning rate, weight decay, and epochs.  English dataset: In these experiments, the Llama3 8b model, when combined with Graph-RAG Comprehensive and an RL agent, achieved a 64.7% increase in cov- erage compared to the “Base” experiment, the highest among all models and scenarios (Figure 4). Without Graph-RAG Comprehensive and the agent, coverage dropped dras- tically to 27.2%. The Llama3 8b also excelled in precision (93.3%) and F1-Score (92%) under the same conditions.  Figure 4. Multi-label classification of customer reviews in English.  The GPT-4o model, when used solely with Graph-RAG Comprehensive, recorded 90.4% coverage, 89.1% precision, and an 89.1% F1-Score, outperforming its configura- tion with agents, where these metrics were 72%, 71.1%, and 71% respectively. GPT-3.5 showed moderate stability, with 81.8% coverage, 84.5% precision, and 80.6% F1-Score using Graph-RAG Comprehensive and agents. Without agents, these values only slightly declined to 81.3%, 84.1%, and 80%. The Llama2 variants underperformed, particularly the 13b version without agents. The Llama3 70b model improved in precision (92%) and F1-Score (91%) with RL agents but showed reduced performance without them. The BERT model achieved 79% precision and 76% F1-Score with general context and RL agent conditions, but still lagged behind the Llama3 models.  Multilingual dataset: The coverage improvements were more modest for GPT-4 and GPT-4o models, with only a 0.2% increase, but they retained high accuracy (around 88-89%). Notably, Llama3 70b showed strong results in both contexts, with 90.1% cover- age in the multilingual setting and consistently high precision across datasets. However, in none of the scenarios did the multilingual dataset surpass the results achieved with the English dataset, highlighting a clear performance gap. Smaller models like Llama2 13b particularly struggled in both datasets, especially in multilingual tests where coverage remained low even with advanced techniques. The findings emphasize the superior adapt- ability of larger models like Llama3 and GPT-4 across languages, while smaller models struggle to maintain effectiveness without additional enhancements.  Difference between Base and highest resultBase (No context  and no RL agent)Graph-RAG ComprehensiveRL AgentGraph-RAG Comprehensive  + RL AgentGraph-RAG  TargetedGraph-RAG  Targeted  + RL Agent0.3%48.2%64.7%Coverage0255075100%92GPT-4oLlama3 8BLlama3 70BBERT Fine-tuning 4.2. Information Extraction from Backhoe Invoices  Figure 5 shows that the Llama3 8b model, when operated with KGs and RL agents, dis- plays a remarkable improvement in accuracy. Specifically, the accuracy increased from a baseline of 52.18% to 95.7% when using Graph-RAG Targeted and RL Agent, demon- strating an enhancement of 43.52%.  Figure 5. Information extraction from backhoe invoices.  The Llama3 8b model achieved the highest accuracy of 95.7% and the greatest accuracy improvement among the configurations, illustrating its strong synergy with KGs and RL agents. Conversely, without these tools, its accuracy substantially decreases to the baseline of 52.18%. For the Llama3 70b model, the highest accuracy reached was 97.21% with Graph-RAG Targeted and RL Agent, showing a slight accuracy increase from its baseline of 79.93%. This model also exhibited the highest consistency across different configurations. The GPT-4o model showed improvements as well, reaching an accuracy of 86.48% with Graph-RAG Comprehensive and RL Agent, which is an in- crease of approximately 7.25% over its baseline of 79.23%. These results highlight the significant impact of utilizing KGs and RL agents in enhancing the performance of ma- chine learning models, especially in tasks that involve complex document analysis such as information extraction from backhoe invoices.  4.3. Discussion  This study aligns with the growing body of research exploring the potential of LLMs to address NLP challenges. Although these models are capable of handling a wide range of tasks without the need for specialized data, in more specific cases, they show signifi- cant limitations due to the lack of fine-tuning, especially in smaller versions. LLMs face substantial limitations in their reasoning abilities, particularly when dealing with tasks involving multiple languages. In these scenarios, current LLMs still do not outperform approaches that utilize RL, whether through techniques like Proximal Policy Optimization (PPO), Trust Region Policy Optimization (TRPO), or Deep Deterministic Policy Gradient (DDPG), which require deep model adjustments, making their application considerably costly, or through RL techniques that provide textual feedback, as explored in this work. Consequently, approaches like Graph-RAG or RL with textual feedback are more viable in terms of cost and complexity.  The combination of Graph-RAG and RL, or even just one of these techniques, is more relevant for smaller models, which benefit from instructions with context and more detailed guidance, while larger models tend to perform better with more concise data or, in  7.3%17.3%43.5%Accuracy0GPT-4oLlama3 8BLlama3 70B255075100%97Difference between Base and highest resultBase (No context  and no RL agent)Graph-RAG ComprehensiveRL AgentGraph-RAG Comprehensive  + RL AgentGraph-RAG  TargetedGraph-RAG  Targeted  + RL Agent some cases, no additional data at all. Even with the application of techniques like Graph- RAG, larger models maintain high effectiveness in English but exhibit performance drops when applied to multilingual datasets.  5. Related Works  Recent studies combining LLMs with KGs have focused on models like OpenAI’s GPT- 3.5 and Meta’s Llama. GPT-3.5 has been applied in areas such as engineering ed- ucation [Yang et al. 2023], text classification [Shi et al. 2023], and node classification in graph structures [Li et al. 2024]. GPT-4 has been used in recommendation systems and biomedical studies [Xu et al. 2024, Guan et al. 2023]. Meta’s Llama2 models have shown effectiveness in processing complex graphs, with applications in vision sys- tems, academic databases, and digital news domains [Gouidis et al. 2024, Hu et al. 2024, Wu et al. 2024]. Chain of Thought (CoT) prompting and GNN techniques have also been integrated with LLMs for improving model interpretability and processing struc- tured knowledge from KGs [Guan et al. 2023, Xu et al. 2023]. Techniques like PCA, UMAP, and prompt methods further integrate LLMs into the visual and structural do- mains of KGs, enhancing zero-shot learning [Gouidis et al. 2024, Alfasi et al. 2024]. In RL, approaches like RLHF and RLAIF have demonstrated improvements in summa- rization, negotiation dialogues, and domain knowledge applications [Roit et al. 2023, Kwon et al. 2024, Mandi et al. 2023]. Although effective, RLHF and finetuning are ex- pensive and nearly unfeasible for most experiments due to the significant computational and financial resources required [Ouyang et al. 2022, Nguyen et al. 2023]. Persistent is- sues like biases, toxicity, and hallucinations remain critical in both KGs and RL con- texts [Gouidis et al. 2024, Xu et al. 2024, McKenna et al. 2023]. Differently from pre- vious works, our study addresses scalability high costs associated with the use of very large model and traditional techniques fine-tuning, by combineing RAG with RL. We demonstrate the effectiveness of this approach for multi-label classification and informa- tion extraction using domain-specific KGs and datasets.  6. Conclusions and Future Work  This study demonstrates the feasibility and effectiveness of integrating LLMs with Graph- RAG to enhance multi-label classification and information extraction. Experiments con- ducted with variations of the GPT and Llama models, combined with the use of KGs and an RL agent, revealed significant improvements in the performance of smaller models, such as Llama3 8b, especially when combined with Graph-RAG. The combination of smaller LLMs and Graph-RAG reduces the occurrence of “hallucinations”, contributing to superior accuracy and effectiveness, even in multilingual contexts. These outcomes suggest a promising future for not so large LLM’s, especially in organizations facing data privacy constraints and computational resource limitations. As future research directions, we envision the exploitation of more diverse KGs and the investigation of RL techniques to further improve results of complex tasks. Furthermore, additional studies could apply our proposal to low resource languages, for expanding its accessibility and applicability.  Acknowledgements: This work was supported by a 2022 CNPq Universal grant, FAPESC grant 2021TR1510, the Print CAPES-UFSC Automation 4.0 Project, and indirectly by the Céos project, financed by the Public Ministry of Santa Catarina State (MPSC).   "
        },
        {
            "titulo": "Identificação de aspectos explícitos e implícitos em críticas gastronômicas em português: avaliando o potencial dos LLMs",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31129",
            "idioma": "Português",
            "storage_key": "files/article_31129_30932.pdf",
            "autores": [
                {
                    "nome": "Luiz H. N. Silva",
                    "afiliacao": "IFSP",
                    "orcid": "http://orcid.org/0009-0007-2839-8259"
                },
                {
                    "nome": "Eloize R. M. Seno",
                    "afiliacao": "IFSP",
                    "orcid": "https://orcid.org/0000-0002-1549-9794"
                },
                {
                    "nome": "Rozane R. Rebechi",
                    "afiliacao": "UFRGS",
                    "orcid": "https://orcid.org/0000-0002-1878-7548"
                },
                {
                    "nome": "Helena M. Caseli",
                    "afiliacao": "UFSCar",
                    "orcid": "https://orcid.org/0000-0003-3996-8599"
                },
                {
                    "nome": "Fabiano M. Rocha Júnior",
                    "afiliacao": "IFSP",
                    "orcid": null
                },
                {
                    "nome": "Guilherme A. Faller",
                    "afiliacao": "UFRGS",
                    "orcid": null
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Análise de sentimentos baseada em aspectos",
                "aspectos explícitos e implícitos",
                "LLMs"
            ],
            "referencias": [
                "Almeida, T. S., Abonizio, H., Nogueira, R., and Pires, R. (2024). Sabiá-2: A new generation of portuguese large language models. ArXiv, abs/2403.09887.",
                "Assi, F. M., Candido, G. B., dos Santos Silva, L. N., Silva, D. F., and Caseli, H. M. (2022). Ufscar’s team at ABSAPT 2022: using syntax, semantics and context for solving the tasks. In Montes-y-Gomez, M. and et al., editors, Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022), volume 3202 of CEUR Workshop Proceedings. CEUR-WS.org.",
                "Balage Filho, P. P. (2017). Aspect extraction in sentiment analysis for portuguese language. PhD thesis, Sao Carlos - SP.",
                "Costa, R. and Pardo, T. (2020). Métodos baseados em léxico para extração de aspectos de opiniões em português. In Anais do IX Brazilian Workshop on Social Network Analysis and Mining, pages 61–72, Porto Alegre, RS, Brasil. SBC.",
                "Lopes, E., Correa, U., and Freitas, L. (2021). Exploring BERT for aspect extraction in portuguese language. The International FLAIRS Conference Proceedings, 34.",
                "Machado, M., Pardo, T., Ruiz, E., and Felippo, A. (2021). Learning rules for automatic identification of implicit aspects in portuguese. In Anais do XIII Simposio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 82–91, Porto Alegre, RS, Brasil. SBC.",
                "Machado, M. and Pardo, T. A. S. (2022). Evaluating methods for extraction of aspect terms in opinion texts in Portuguese - the challenges of implicit aspects. In Calzolari, N., Bechet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3819–3828, Marseille, France. European Language Resources Association.",
                "Machado, M. T. (2023). Methods for identifying aspects in opinion texts in Portuguese: the case of implicit aspects and their typological analysis. PhD thesis, Sao Carlos - SP.",
                "Oliveira, A., Cecote, T., Silva, P., Gertrudes, J., Freitas, V., and Luz, E. (2023). How good is ChatGPT for detecting hate speech in portuguese? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 94–103, Porto Alegre, RS, Brasil. SBC.",
                "Pereira, D. A. (2021). A survey of sentiment analysis in the portuguese language. Artificial Intelligence Review, 54(2):1087–1115.",
                "Rebechi, R. R., Nunes, R. R., Munhoz, L. R., and Marcon, N. O. (2021). Restaurant reviews in Brazil and the USA: A feast of cultural differences and their impact on translation. Mutatis Mutandis. Revista Latinoamericana de Traduccion, 14:372–396.",
                "Resplande, J., Garcia, E., Junior, A., Rodrigues, R., Silva, D., Maia, D., Da Silva, N., Filho, A., and Soares, A. (2022). Deep learning Brasil at ABSAPT 2022: Portuguese transformer ensemble approaches. In Montes-y-Gomez, M. and et al., editors, Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022), volume 3202 of CEUR Workshop Proceedings. CEUR-WS.org.",
                "Santos, W. and Paraboni, I. (2023). Predição de transtorno depressivo em redes sociais: BERT supervisionado ou ChatGPT zero-shot? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 11–21, Porto Alegre, RS, Brasil. SBC.",
                "Schouten, K. and Frasincar, F. (2016). Survey on aspect-level sentiment analysis. IEEE Transactions on Knowledge and Data Engineering, 28(3):813–830.",
                "Seno, E., Silva, L., Anno, F., Rocha, F., and Caseli, H. (2024). Aspect-based sentiment analysis in comments on political debates in Portuguese: evaluating the potential of ChatGPT. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Computational Processing of the Portuguese Language: 16th Conference, PROPOR 2024, pages 312–320, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Soni, P. K. and Rambola, R. (2022). A survey on implicit aspect detection for sentiment analysis: Terminology, issues, and scope. IEEE Access, 10:63932–63957.",
                "Vargas, F. A. and Pardo, T. A. S. (2018). Aspect clustering methods for sentiment analysis. In Computational Processing of the Portuguese Language: 13th International Conference, PROPOR 2018, Canela, Brazil, September 24–26, 2018, Proceedings, page 365–374, Berlin, Heidelberg. Springer-Verlag.",
                "Vargas, F. A. and Pardo, T. A. S. (2020). Linguistic rules for fine-grained opinion extraction. proceedings of the 14th International AAAI Conference on Web and Social Media, 2020. Zhang, L., Wang, S., and Liu, B. (2018). Deep learning for sentiment analysis : A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8",
                "Zhang, L., Wang, S., and Liu, B. (2018). Deep learning for sentiment analysis : A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8."
            ],
            "artigo_completo": "Resumo. A identificac¸ ˜ao de aspectos ´e uma etapa fundamental da An´alise de Sentimentos Baseada em Aspectos (ASBA) que consiste em detectar os aspectos alvos de opini˜ao em avaliac¸ ˜oes de produtos ou servic¸os publicadas nas m´ıdias sociais. Enquanto existem v´arios estudos focados na detecc¸ ˜ao de aspectos na l´ıngua inglesa, para o portuguˆes h´a poucos trabalhos na ´area e os LLMs pra- ticamente n˜ao tˆem sido explorados. Dado esse contexto, esta pesquisa investi- gou o potencial de uso de LLMs na identificac¸ ˜ao de aspectos em cr´ıticas gas- tronˆomicas em portuguˆes.  1. Introduc¸ ˜ao  A an´alise de sentimentos baseada em aspectos (ASBA) ´e uma sub´area da An´alise de Sentimentos (AS) que busca identificar e analisar opini˜oes e sentimentos relacionados a aspectos ou atributos espec´ıficos de uma entidade, produto ou servic¸o. Em uma avaliac¸ ˜ao de um restaurante, por exemplo, aspectos como “comida”, “servic¸o” e “prec¸o’ podem ser analisados individualmente, permitindo uma compreens˜ao mais detalhada das opini˜oes dos clientes sobre cada um deles.  A ASBA representa o n´ıvel mais complexo da an´alise autom´atica, devido `a difi- culdade de se modelar as conex˜oes semˆanticas entre um determinado aspecto (termo) e as palavras que fazem parte do seu contexto [Zhang et al. 2018]. Uma etapa fundamental da ASBA consiste na identificac¸ ˜ao de aspectos, os quais podem ser expl´ıcitos ou impl´ıcitos, de acordo com a literatura [Schouten and Frasincar 2016, Soni and Rambola 2022]. En- quanto o aspecto expl´ıcito ocorre diretamente no texto, o aspecto impl´ıcito n˜ao ´e men- cionado explicitamente, mas pode ser inferido pelo contexto. Por exemplo, na avaliac¸ ˜ao de um restaurante “A comida estava deliciosa, mas demorou muito para chegar.”, temos um aspecto expl´ıcito “comida” com sentimento positivo e um aspecto impl´ıcito “servic¸o” com sentimento negativo.   Enquanto para o inglˆes h´a uma vasta literatura relacionada `a detecc¸ ˜ao de as- pectos [Schouten and Frasincar 2016, Zhang et al. 2018, Soni and Rambola 2022], para o portuguˆes as pesquisas ainda s˜ao emergentes [Pereira 2021]. Al´em disso, os traba- lhos existentes se baseiam principalmente no uso de regras, l´exicos e em algoritmos de aprendizado de m´aquina, sendo que o uso de modelos de linguagem em larga es- `A medida cala (Large Language Model – LLM, no inglˆes) tem sido pouco explorado. que o interesse p´ublico por modelos generativos pr´e-treinados, como os modelos da OpenAI, continua a crescer, espera-se que a utilidade desses modelos em resolver ta- refas de PLN seja investigada. E nesse sentido, algumas iniciativas recentes tˆem surgido [Oliveira et al. 2023, Santos and Paraboni 2023].  Dado esse contexto, este estudo investigou a potencialidade de cinco LLMs na identificac¸ ˜ao de aspectos expl´ıcitos e impl´ıcitos em cr´ıticas gastronˆomicas em portuguˆes. Cr´ıticas gastronˆomicas s˜ao textos escritos por cr´ıticos profissionais da gastronomia com experiˆencia em avaliar restaurantes, pratos e experiˆencias culin´arias. A escolha desse dom´ınio se justifica pelo fato de que as cr´ıticas gastronˆomicas, at´e onde se sabe, ainda n˜ao foram exploradas no contexto da ASBA em portuguˆes.  2. Trabalhos Relacionados  de  l´exicos  regras de  no uso de  em algoritmos  [Costa and Pardo 2020],  Os trabalhos de identificac¸ ˜ao de aspectos para o portuguˆes se baseiam, prin- linguagem cipalmente, [Vargas and Pardo 2020, Machado et al. 2021], aprendizado de m´aquina tradicionais [Balage Filho 2017, Vargas and Pardo 2018] e no uso de deep learning [Lopes et al. 2021, Assi et al. 2022, Machado and Pardo 2022, Resplande et al. 2022]). Em [Resplande et al. 2022], por exemplo, os autores avaliaram o uso de modelos baseados em Transformers na extrac¸ ˜ao de aspectos em avaliac¸ ˜oes de hot´eis. Os aspectos extra´ıdos foram classificados, posteriormente, como positivos, negativos ou neutros usando o LLM GPT-3. Em um trabalho anterior [Seno et al. 2024], o GPT-3.5 Turbo foi empregado na tarefa de detecc¸ ˜ao de aspectos e classificac¸ ˜ao de polaridade em coment´arios do dom´ınio pol´ıtico. Em [Machado 2023], os autores compararam o uso de LLMs – GPT-3.5, Maritaca e Llama – com um modelo BERT e com v´arios classificadores tradicionais na identificac¸ ˜ao de aspectos em revis˜oes de produtos eletrˆonicos, livros e hot´eis. Nos experimentos, os melhores resultados para os aspectos expl´ıcitos foram obtidos pelo classificador CRF (o melhor F-score foi 81% para revis˜oes de hot´eis)). Por´em, para os aspectos impl´ıcitos o melhor resultado, em termos de porcentagem de acerto, foi obtido com o Llama 7B (52%).  De forma similar, este estudo tamb´em explorou o uso do GPT-3.5 e dos modelos da fam´ılia Maritaca na detecc¸ ˜ao de aspectos em cr´ıticas gastronˆomicas. Por´em, os mode- los investigados aqui s˜ao variac¸ ˜oes mais recentes das vers˜oes usadas por [Machado 2023].  3. Identificac¸ ˜ao de aspectos em Cr´ıticas Gastronˆomicas usando LLMs  Para a identificac¸ ˜ao de aspectos em cr´ıticas gastronˆomicas foram explorados alguns dos LLMs mais populares da atualidade como o GPT-3.5 Turbo, o GPT-4o e GPT-4o mini1. Segundo a OpenAI2, o GPT-4o ´e o seu modelo mais avanc¸ado e inteligente para tarefas  1https://platform.openai.com/docs/api-reference/introduction 2https://platform.openai.com/docs/models   Tabela 1. Prompts usados na anotac¸ ˜ao de aspectos expl´ıcitos e impl´ıcitos. Aspectos expl´ıcitos: Dada a sentenc¸a EXEMPLO com os alvos de opini˜oes expl´ıcitos, identifique os alvos de opini˜ao expl´ıcitos na sentenc¸a (se houver) no formato [e - alvo1], se n˜ao houver nenhum alvo, indique com um ’-’. EXEMPLO: “A pizza estava gostosa. E a sobremesa tamb´em.”. Sa´ıda: [e - pizza] [e - sobremesa] Aspectos impl´ıcitos: Dada a sentenc¸a EXEMPLO com os alvos de opini˜oes impl´ıcitos, identifique os alvos de opini˜ao impl´ıcitos (se houver) no formato [i - alvo], se n˜ao houver nenhum alvo, indique com um ’-’. EXEMPLO: “A pizza estava gostosa, mas era muito cara. Al´em disso, estava fria”. Sa´ıda: [i - prec¸o] [i - temperatura]  mais complexas. O GPT-4o mini ´e o modelo mais avanc¸ado na categoria de modelos pequenos, que tamb´em inclui o GPT-3.5 Turbo. Al´em desses LLMs, tamb´em foram in- vestigados dois modelos monol´ıngues treinados para o portuguˆes, o Sabi´a-2-medium e o Sabi´a-3 3. Em experimentos reportados por [Almeida et al. 2024], o Sabi´a-2-medium ´e comparado a v´arios outros LLMs, alcanc¸ando desempenho igual ou melhor que GPT- 3.5 Turbo em v´arias an´alises. O Sabi´a-3, por sua vez, lanc¸ado em julho de 2024, at´e o momento da escrita deste artigo n˜ao se tinha informac¸ ˜oes sobre o seu desempenho.  Todos os LLMs s˜ao modelos generativos baseados em prompt, que recebem como entrada um texto (prompt) contendo a descric¸ ˜ao da tarefa a ser realizada e geram as sa´ıdas conforme solicitado. O grande desafio em lidar com esses modelos consiste em definir um prompt que gere as sa´ıdas exatamente como se espera para a tarefa. V´arios prompts dife- rentes foram testados para a identificac¸ ˜ao de aspectos expl´ıcitos e impl´ıcitos no corpus. Foram experimentados prompts espec´ıficos para cada tipo de aspecto usando exemplos de anotac¸ ˜ao humana (i.e. abordagem few-shot) e sem o uso de exemplos de anotac¸ ˜ao (i.e. abordagem zero-shot). Contudo, percebeu-se uma facilidade maior dos modelos ao usar a abordagem few-shot. Assim, na anotac¸ ˜ao do corpus foram adotados os prompts apresen- tados na Tabela 1. Em todos os LLMs investigados a temperatura foi ajustada em zero, a fim de obter modelos mais determin´ısticos, conforme apontado por outros trabalhos da literatura [Oliveira et al. 2023, Santos and Paraboni 2023].  4. Corpus  Para os experimentos foi usado um conjunto de 1005 sentenc¸as extra´ıdas do corpus de cr´ıticas gastronˆomicas de [Rebechi et al. 2021]. Cada sentenc¸a foi anotada por 5 ano- tadores humanos, todos pesquisadores da ´area de PLN, em duas etapas. Primeiramente os anotadores classificaram as sentenc¸as em opinativa ou factual. Em seguida, aspectos expl´ıcitos e impl´ıcitos foram anotados, em dupla/trio, para as 374 (37,2%) sentenc¸as con- sideradas opinativas pelos anotadores. Para estas, 432 aspectos foram identificados, sendo 88,6% expl´ıcitos e 11,4% impl´ıcitos. A Tabela 2 apresenta exemplos de sentenc¸as com anotac¸ ˜ao de aspectos expl´ıcitos (em negrito) e impl´ıcitos.  Dado o fato de que n˜ao ´e poss´ıvel determinar todos os aspectos poss´ıveis para o corpus, n˜ao foi poss´ıvel calcular o coeficiente Kappa para estimar a concordˆancia entre os anotadores. Embora n˜ao se tenha obtido uma estimativa da concordˆancia na anotac¸ ˜ao do corpus, a busca pelo consenso, seguida da clara convergˆencia dos anotadores, permite assegurar que os aspectos identificados reproduzem de forma bastante fiel os aspectos que geralmente s˜ao considerados na avaliac¸ ˜ao de uma experiˆencia gastronˆomica.  3Dispon´ıveis por meio da MariTalk API como um chatbot.   Tabela 2. Exemplos de anotac¸ ˜ao de aspectos expl´ıcitos (em negrito) e impl´ıcitos.  Sentenc¸a Se estiver sozinho, desista de tentar o omakassˆe (sequˆencia de iguarias decidi- das e enviadas aos poucos pelo chef) — ele ´e gigante (para uma pessoa) e caro (42 itens, R$ 390). N˜ao ´e demais lembrar: a casa s´o aceita dinheiro ou cheque – costume fora de moda, tamb´em trazido de outros tempos. Carta de vinhos: Excelente, com muitas opc¸ ˜oes argentinas para todos os bol- sos.  Impl´ıcito tamanho; prec¸o  forma de pagamento  variedade prec¸o  (vinhos);  5. Experimentos e Resultados  A Tabela 3 apresenta os resultados obtidos por cada LLM na detecc¸ ˜ao de aspectos expl´ıcitos e impl´ıcitos. O Sabi´a-medium-2 obteve o melhor F-score (48,37%) para os as- pectos expl´ıcitos, alcanc¸ando tamb´em a maior cobertura (77,75%). Contudo, a maior pre- cis˜ao (40,90%) foi obtida pelo GPT-4o mini. J´a no que se refere aos aspectos impl´ıcitos, os resultados mostram uma grande dificuldade dos LLMs em identificar esse tipo de aspecto. Vale mencionar que essa dificuldade tamb´em foi relatada pelos humanos na anotac¸ ˜ao do corpus. Como os aspectos impl´ıcitos s˜ao inferidos pelo contexto, nem sem- pre ´e trivial perceber qual ´e o alvo de opini˜ao. Em alguns casos, essa inferˆencia exige um conhecimento mais especializado como no exemplo “Na boca, ´e equilibrado, com tani- nos firmes e boa estrutura.”, que se refere ao aspecto “vinho”. Para esse caso espec´ıfico, apenas o modelo GPT-3.5 Turbo conseguiu identificar o aspecto impl´ıcito.  Tabela 3. Resultados obtidos para aspectos expl´ıcitos e impl´ıcitos.  LLM Sabi´a-2-medium 35,11% 31,53% GPT-3.5 turbo Sabi´a-3 33,21% GPT-4o 21,51% 40,90% GPT-4o mini  Expl´ıcitos Precis˜ao Cobertura  F-score 77,75% 48,37% 1,60% 1,95% 44,60% 76,18% 2,21% 44,58% 67,80% 3,90% 33,54% 76,18% 2,23% 22,81% 15,82%  Impl´ıcitos Precis˜ao Cobertura F-score 3,01% 26,00% 3,68% 32,00% 4,13% 32,00% 7,00% 34,00% 4,01% 20,00%  6. Conclus˜oes  Este estudo investigou o uso de LLMs na detecc¸ ˜ao de aspectos em cr´ıticas gastronˆomicas. Nos experimentos, o LLM monol´ıngue Sabi´a-2-medium mostrou um potencial maior na detecc¸ ˜ao de aspectos expl´ıcitos, do que os modelos multil´ıngues analisados. Enquanto que o Sabi´a-3, tamb´em monol´ıngue, mostrou-se equivalente ao GPT-3.5 Turbo, supe- rando o GPT-4o e o GPT-4o mini. Al´em de apresentarem desempenho superior ou equi- valente aos obtidos pelos modelos multil´ıngues, os modelos monol´ıngues s˜ao bem mais acess´ıveis4. Com relac¸ ˜ao aos aspectos impl´ıcitos, todos os LLMs tiveram bastante dificul- dade em identificar esse tipo de aspecto. O melhor desempenho foi obtido pelo GPT-4o (7% de F-score).  Como trabalhos futuros, pretende-se investigar a combinac¸ ˜ao de LLMs para a tarefa de identificac¸ ˜ao de aspectos, bem como a utilizac¸ ˜ao de conhecimento do dom´ınio de cr´ıticas gastronˆomicas para enriquecer os prompts.  4Os valores podem ser consultados em https://openai.com/api/pricing/ e https://  www.maritaca.ai/   Referˆencias  Almeida, T. S., Abonizio, H., Nogueira, R., and Pires, R. (2024). Sabi´a-2: A new genera-  tion of portuguese large language models. ArXiv, abs/2403.09887.  Assi, F. M., Candido, G. B., dos Santos Silva, L. N., Silva, D. F., and Caseli, H. M. (2022). Ufscar’s team at ABSAPT 2022: using syntax, semantics and context for solving the tasks. In Montes-y-G´omez, M. and et al., editors, Proceedings of the Iberian Langua- ges Evaluation Forum (IberLEF 2022), volume 3202 of CEUR Workshop Proceedings. CEUR-WS.org.  Balage Filho, P. P. (2017). Aspect extraction in sentiment analysis for portuguese lan-  guage. PhD thesis, S˜ao Carlos - SP.  Costa, R. and Pardo, T. (2020). M´etodos baseados em l´exico para extrac¸ ˜ao de aspectos de opini˜oes em portuguˆes. In Anais do IX Brazilian Workshop on Social Network Analysis and Mining, pages 61–72, Porto Alegre, RS, Brasil. SBC.  Lopes, E., Correa, U., and Freitas, L. (2021). Exploring BERT for aspect extraction in  portuguese language. The International FLAIRS Conference Proceedings, 34.  Machado, M., Pardo, T., Ruiz, E., and Felippo, A. (2021). Learning rules for automatic identification of implicit aspects in portuguese. In Anais do XIII Simp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 82–91, Porto Alegre, RS, Brasil. SBC.  Machado, M. and Pardo, T. A. S. (2022). Evaluating methods for extraction of aspect terms in opinion texts in Portuguese - the challenges of implicit aspects. In Calzolari, N., B´echet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3819–3828, Marseille, France. European Language Resources Association.  Machado, M. T. (2023). Methods for identifying aspects in opinion texts in Portuguese: the case of implicit aspects and their typological analysis. PhD thesis, S˜ao Carlos - SP.  Oliveira, A., Cecote, T., Silva, P., Gertrudes, J., Freitas, V., and Luz, E. (2023). How good is ChatGPT for detecting hate speech in portuguese? In Anais do XIV Simp´osio Bra- sileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 94–103, Porto Alegre, RS, Brasil. SBC.  Pereira, D. A. (2021). A survey of sentiment analysis in the portuguese language. Artifi-  cial Intelligence Review, 54(2):1087–1115.  Rebechi, R. R., Nunes, R. R., Munhoz, L. R., and Marcon, N. O. (2021). Restaurant reviews in Brazil and the USA: A feast of cultural differences and their impact on translation. Mutatis Mutandis. Revista Latinoamericana de Traducci´on, 14:372–396.  Resplande, J., Garcia, E., Junior, A., Rodrigues, R., Silva, D., Maia, D., Da Silva, N., Filho, A., and Soares, A. (2022). Deep learning Brasil at ABSAPT 2022: Portuguese transformer ensemble approaches. In Montes-y-G´omez, M. and et al., editors, Proce- edings of the Iberian Languages Evaluation Forum (IberLEF 2022), volume 3202 of CEUR Workshop Proceedings. CEUR-WS.org.   Santos, W. and Paraboni, I. (2023). Predic¸ ˜ao de transtorno depressivo em redes sociais: Bert supervisionado ou ChatGPT zero-shot? In Anais do XIV Simp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 11–21, Porto Alegre, RS, Brasil. SBC.  Schouten, K. and Frasincar, F. (2016). Survey on aspect-level sentiment analysis. IEEE  Transactions on Knowledge and Data Engineering, 28(3):813–830.  Seno, E., Silva, L., Anno, F., Rocha, F., and Caseli, H. (2024). Aspect-based sentiment analysis in comments on political debates in Portuguese: evaluating the potential of ChatGPT. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Computational Processing of the Portuguese Language: 16th Conference, PROPOR 2024, pages 312–320, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.  Soni, P. K. and Rambola, R. (2022). A survey on implicit aspect detection for sentiment  analysis: Terminology, issues, and scope. IEEE Access, 10:63932–63957.  Vargas, F. A. and Pardo, T. A. S. (2018). Aspect clustering methods for sentiment analy- In Computational Processing of the Portuguese Language: 13th International sis. Conference, PROPOR 2018, Canela, Brazil, September 24–26, 2018, Proceedings, page 365–374, Berlin, Heidelberg. Springer-Verlag.  Vargas, F. A. and Pardo, T. A. S. (2020). Linguistic rules for fine-grained opinion ex- traction. proceedings of the 14th International AAAI Conference on Web and Social Media, 2020.  Zhang, L., Wang, S., and Liu, B. (2018). Deep learning for sentiment analysis : A survey.  Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8.   "
        },
        {
            "titulo": "TableRAG: A Novel Approach for Augmenting LLMs with Information from Retrieved Tables",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31130",
            "idioma": "Inglês",
            "storage_key": "files/article_31130_30933.pdf",
            "autores": [
                {
                    "nome": "Elvis A. de Souza",
                    "afiliacao": "PUC-Rio",
                    "orcid": "http://orcid.org/0000-0001-9373-7412"
                },
                {
                    "nome": "Patricia F. da Silva",
                    "afiliacao": "Petrobras",
                    "orcid": "https://orcid.org/0009-0004-3415-1904"
                },
                {
                    "nome": "Diogo Gomes",
                    "afiliacao": "Petrobras",
                    "orcid": "https://orcid.org/0000-0001-9351-7647"
                },
                {
                    "nome": "Vitor Batista",
                    "afiliacao": "Petrobras",
                    "orcid": "https://orcid.org/0000-0002-9095-0266"
                },
                {
                    "nome": "Evelyn Batista",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0000-0001-6672-8318"
                },
                {
                    "nome": "Marco Pacheco",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0000-0002-2381-0183"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "We present TableRAG, a novel pipeline designed to integrate tabular data into traditional Retrieval-Augmented Generation (RAG) systems. Our approach is composed of three main parts: (i) generating textual representations of tables; (ii) indexing table representations in vector databases for retrieval, and (iii) employing large language models to generate SQL or Python code for data manipulation over a given table. We assessed the effectiveness of TableRAG by comparing retrieval and re-ranking accuracies over the OTT-QA benchmark and by utilizing both open and closed-source LLMs to generate code for answering questions from the WikiTableQuestions benchmark. Our best results show 86.7% HITS@5 for retrieval and 74% accuracy for Q&A, demonstrating the feasibility of integrating tabular data into RAG systems with high accuracy.",
            "keywords": [
                "retrieval augmented generation",
                "tabular data",
                "information retrieval",
                "generative AI",
                "large language models"
            ],
            "referencias": [
                "Abraham, A. N., Rahman, F., and Kaur, D. (2022). Tablequery: Querying tabular data with natural language. arXiv preprint arXiv:2202.00454.",
                "Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., Noune, B., Pannier, B., and Penedo, G. (2023). Falcon-40B: an open large language model with state-of-the-art performance.",
                "Anand, Y., Nussbaum, Z., Duderstadt, B., Schmidt, B., and Mulyar, A. (2023). Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo.",
                ".",
                "Chen, W., Chang, M.-W., Schlinger, E., Wang, W., and Cohen, W. W. (2020a). Open question answering over tables and text. arXiv preprint arXiv:2010.10439.",
                "Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H., and Wang, W. (2020b). Hybridqa: A dataset of multi-hop question answering over tabular and textual data. arXiv preprint arXiv:2004.07347.",
                "Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V.,Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2019). Unsupervised cross-lingual representation learning at scale. CoRR, abs/1911.02116.",
                "Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Chang, B., Sun, X., Li, L., and Sui, Z. (2024). A survey on in-context learning.",
                "Dubey, A. et al. (2024). The llama 3 herd of models.",
                "Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H. (2023). Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.",
                "Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. (2023). Mistral 7b.",
                "Kandpal, N., Deng, H., Roberts, A., Wallace, E., and Raffel, C. (2023). Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pages 15696–15707. PMLR.",
                "Liang, C., Norouzi, M., Berant, J., Le, Q. V., and Lao, N. (2018). Memory augmented policy optimization for program synthesis and semantic parsing. Advances in Neural Information Processing Systems, 31.",
                "Lin, X. V., Chen, X., Chen, M., Shi,W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et al. (2023). Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint arXiv:2310.01352.",
                "Liu, T., Wang, F., and Chen, M. (2024). Rethinking tabular data understanding with large language models. In Duh, K., Gomez, H., and Bethard, S., editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 450–482, Mexico City, Mexico. Association for Computational Linguistics.",
                "Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H. (2022). When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511.",
                "OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., et al. (2024). Gpt-4 technical report.",
                "Pasupat, P. and Liang, P. (2015). Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305.",
                "Tonmoy, S., Zaman, S., Jain, V., Rani, A., Rawte, V., Chadha, A., and Das, A. (2024). A comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313.",
                "Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., and Wei, F. (2024). Multilingual e5 text embeddings: A technical report. arXiv preprint arXiv:2402.05672.",
                "Yin, P., Neubig, G., Yih,W.-t., and Riedel, S. (2020). TaBERT: Pretraining for joint understanding of textual and tabular data. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J., editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–8426, Online. Association for Computational Linguistics.",
                "Yu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q., Roman, S., et al. (2018). Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887."
            ],
            "artigo_completo": "Abstract. We present TableRAG, a novel pipeline designed to integrate tabular data into traditional Retrieval-Augmented Generation (RAG) systems. Our ap- proach is composed of three main parts: (i) generating textual representations of tables; (ii) indexing table representations in vector databases for retrieval, and (iii) employing large language models to generate SQL or Python code for data manipulation over a given table. We assessed the effectiveness of TableRAG by comparing retrieval and re-ranking accuracies over the OTT-QA benchmark and by utilizing both open and closed-source LLMs to generate code for answer- ing questions from the WikiTableQuestions benchmark. Our best results show 86.7% HITS@5 for retrieval and 74% accuracy for Q&A, demonstrating the feasibility of integrating tabular data into RAG systems with high accuracy.  1. Introduction  In recent years, the application of generative models in Question Answering (Q&A) sys- tems has gained substantial traction, particularly in enterprise settings where accurate and efficient information retrieval is crucial for decision-making, customer support, and oper- ational efficiency. Large Language Models (LLMs) have shown remarkable capabilities in generating natural language responses based on vast amounts of textual data, however, these models are not without their challenges—one of the most significant being the issue of hallucination, where the model generates plausible-sounding but incorrect or nonsensi- cal answers [Kandpal et al. 2023, Gao et al. 2023, Lin et al. 2023, Tonmoy et al. 2024]. This problem becomes particularly pronounced when the required information is not purely textual but embedded in semi-structured data, such as tables, which can be stored in large and diversified databases, where the ability to accurately retrieve and interpret them is essential.  To mitigate hallucination and to retrieve relevant information to a particular do- main, a widely adopted paradigm known as Retrieval-Augmented Generation (RAG) is employed. This technique is based on the premise that LLMs are more likely to provide accurate responses when supplied with relevant context at runtime, i.e., within the prompt that defines the task instructions, in a strategy called in-context learning [Dong et al. 2024]. Traditional Information Retrieval (IR) methods are utilized to fetch pertinent documents, which are then fed into the language model for text generation.  In this work, we propose a RAG pipeline that integrates tabular data within textual databases for information retrieval, using retrieved tables to generate reliable responses (Figure 1). Building upon traditional RAG pipelines, we implement a table retrieval mod- ule based on the same vector similarity strategy commonly used for texts, and another   Figure 1. Overview of the TableRAG Pipeline  module for Q&A based on generating code to manipulate tables. We evaluate the results using two well-known Q&A benchmarks with tabular data, Open Table-and-Text Ques- tion Answering (OTT-QA) [Chen et al. 2020a] for the retrieval part of our pipeline, and WikiTableQuestions [Pasupat and Liang 2015] for the Q&A part.  Our results show that it is possible to incorporate tabular data into Retrieval- Augmented Generation systems in an efficient way by representing tables as texts, sim- ilarly to how traditional retrieval information systems work. Moreover, we show that code generation yields good results for obtaining answers from retrieved tables, and we highlight the considerable promise for improving the performance of open-source LLMs, which could lead to the development of more robust, adaptable, and affordable RAG sys- tems, capable of managing multimodal data sources.  2. Related Work  Unlike Q&A over texts, which involves reading document excerpts to answer questions using extractive or generative models, Q&A over tables involves additional factors. To answer complex questions using tables, one must interpret the arrangement of rows and columns, perform filtering, joins, mathematical operations, and various other forms of table manipulation.  There are at least three tasks that have been tested in the literature for Q&A with tabular data. First, parsing semantically compositional questions in order to determine the manipulation steps that are needed to obtain a given information from a table. The Wik- iTableQuestions benchmark [Pasupat and Liang 2015] addresses this task. It comprises semi-structured tables, which may contain textual data, and each question may require operations such as table lookup, aggregation (counting records, summing numerical val- ues, etc.), superlatives (finding the maximum or minimum value), arithmetic operations, among others, which need to be identified in the question in order to determine the ma- nipulations steps that are needed to answer it correctly.  Second, manipulating relational database tables using their relationships with other tables to join information from different sources. The Spider benchmark [Yu et al. 2018] introduced this task. Its specificity involves the use of foreign keys, join- ing multiple tables, and constructing nested SQL queries.   Third, answering multi-hop open-domain questions that need reasoning over both textual passages and tabular information. The Open Table-and-Text Question Answering (OTT-QA) benchmark [Chen et al. 2020a] introduced this task. Its peculiarity is twofold: the retriever model must find the table that best answers the question from a large collec- tion of tables, and the reader model must simultaneously examine data from two different modalities, texts and tables.  For the Q&A part, our approach seeks to address the challenges proposed by the WikiTableQuestions benchmark, using LLMs to generate code that correctly manipulates tables in order to find answers to semantically compositional questions. In 2015, when the WikiTableQuestions benchmark was proposed, the approach used to solve the task involved converting tables into knowledge graphs and parsing the questions into logical form, followed by selecting the most probable graph candidates to answer the question [Pasupat and Liang 2015]. The results achieved were 37.1% accuracy, considering all candidate answers, and 76.6%, considering that at least one of the candidate answers was correct.  More recent works address the WikiTableQuestions task using large language models. [Yin et al. 2020] achieved a result of 52.3% accuracy by pre-training a new model, called TaBERT, with data from 26 million tables and their respective natural lan- guage contexts [Liang et al. 2018]. The work by [Liu et al. 2024] used GPT-3.5 to trans- pose tables to normalize them and then used the same model to perform two types of in- ference: with direct prompting (DP), asking the model to reason about the table in textual form, and as a Python agent, asking the model to interact with a Python shell in up to five interactions. They achieve state-of-the-art accuracy of 73.6% on WikiTableQuestions.  3. TableRAG Pipeline Components  The instruction in Figure 2 is an example of the prompt directed to the language models for generating Python code. In this prompt, we can observe, besides the request for code generation, how the tables are represented textually and in consideration of their metadata. These procedures will be explained step by step in the following subsections.  Obtaining Textual Representation of Tables This part of our approach is inspired by the work of [Abraham et al. 2022]. Given that it is not always feasible to load entire tables into memory, the authors employ several strategies to represent the table through its schema, ensuring that the SQL query to be created is based on this indirect form of representation. The idea works particularly well in the context of Retrieval-Augmented Generation, where the initial step is to retrieve specific documents from a diverse doc- ument repository. In our case, we aim to retrieve the tables that best answer the user’s query, using the same strategy for text indexing and retrieval.  The metadata generated for a table consists of a textual description, generated by an LLM, along with additional information such as column names, unique values, and data types (numeric, textual, dates) for each table column. Other pieces of information, such as title of the table, paragraphs that explain it, acronyms meanings and so on can be inserted here to enrich the textual description of each table. When the table preview is passed to the language model, it is placed as the final element of the prompt, so to truncate it if exceeding the input token limit that the model accepts.   Figure 2. Prompt for Python Code Generation  Column Renaming: A common error we observed in the construction of Python code and SQL queries by LLMs is the use of non-existent columns, a particular case of hallu- cination. To address this issue, we implement a strategy to rename columns to temporary placeholders, such as Col1, Col2, Col3, . . . ColN. This forces language models to utilize columns based on the description of the data type they contain, rather than their names.  Table Indexing: The table is then indexed in a vector database through its textual repre- sentation. The idea is that the summary generated by the LLM should be sufficient for the table to be retrieved, leveraging the same strategy used in text indexing, by transforming table descriptions into dense contextual vectors.  Retrieval, Re-ranking and Filtering: Table retrieval is performed via similarity mea- sures, such as cosine similarity, comparing the vector representation of the user’s query with the vector representations of table descriptions. After retrieval, we employ a re- ranking step by asking an LLM to reorder the retrieved tables by relevance, and a filtering step, by asking it to filter the results, eliminating retrieved tables that despite their simi- larity to the user’s query cannot directly answer the question.  Code Generation: Next, we prompt an LLM to generate code to obtain the answer from the table, as exemplified in Figure 2. The code generation prompt was meticulously   adjusted to avoid common pitfalls these models tend to encounter, such as attempting to perform operations on dataframe columns with missing values without first handling these cells. These adjustments were based on the outputs of GPT-4, for both SQL and Python code generation, and improved our accuracy in 16% for the same model, being the previously mentioned column renaming procedure one of the most beneficial changes we have made.  Response Generation: The generated code is then executed and the output is passed to another LLM, which is responsible for providing a natural language response to the user’s question, considering the question, the data, and the output of the executed code.  4. Methodology  To evaluate the quality of our pipeline, we utilized two traditional benchmarks, called OTT-QA [Chen et al. 2020a], for the retrieval part, and WikiTableQuestions [Pasupat and Liang 2015], for the Q&A part. The tables from both datasets were sourced from Wikipedia.  While the WikiTableQuestions dataset is ideal for our purpose of testing code generation for table manipulation, it is not ideal for evaluating the efficiency of our table retrieval module. This is because the questions are constructed in a closed-domain fash- ion, meaning they can only be answered if it is already known which table they refer to.1 Therefore, when using the benchmark to evaluate our Q&A capabilities, we provide the language model with the correct table. To assess the quality of our retrieval and re-ranking modules, we will be testing them against the OTT-QA benchmark.  The OTT-QA benchmark is composed of questions based on those from another dataset, HybridQA [Chen et al. 2020b], but they were adapted to become “decontextual- ized,” making them open-domain and therefore suitable for testing retrieval systems. It contains 400K+ tables to retrieve from and 45K human-annotated questions. We use all the 2,214 questions in the development partition of the benchmark to test retrieval and re-ranking. We use HITS@K to measure performance, where K ranges from 1 to 5, and means whether the correct table for each question is in the top-K retrieved or re-ranked ta- bles. Re-ranking is done after the retrieval step, and we compare results with and without re-ranking.  The WikiTableQuestions benchmark contains 2,108 semi-structured tables and 22,033 complex question-answer pairs. The questions and answers were constructed by humans through crowdsourcing, with instructions to create questions that involve various types of operations required to answer them, as shown in the distribution in Figure 3.  Due to operational constraints involving LLMs costs, all tests in the WikiTable- Questions were performed on a sample of 200 questions from the test dataset. As noted, several adjustments were made to the LLM instructions and to the table indexing method to facilitate the inference of correct answers and the inference of code that did not gener- ate exceptions. All the adjustments were based solely on the results observed for the train  1For example, in the question “what is the first city sorted alphabetically?”, there is no indication of which table should be found–cities of what country? in what state? in what period of time?–, and it can only be correctly answered if the table to which the question refers has already been identified.   Figure 3. Operations required to answer a sample of 200 questions from Wik- iTableQuestions. Source: [Pasupat and Liang 2015].  dataset questions, thereby preventing any information leakage from test to train.  The answers in WikiTableQuestions are provided as lists of size greater than or equal to 1, with sizes greater than 1 when more than one term is expected in the predicted answer for it to be considered correct. Thus, we used accuracy to measure the perfor- mance of our Q&A module, normalizing the strings in the dataset and the strings inferred by the models, to check if all the expected terms are present in the predicted answer.2  In addition to accuracy, we also considered the number of generated codes that produced any runtime exceptions and the time taken to answer each question. In other words, we are also testing the ability of these models to generate functional and correct code in the shortest possible time.  The scenarios in which we tested our pipeline are as follows:  • For generating table descriptions, re-ranking, and the final natural language re- sponse, we consistently used the GPT 3.5 Turbo model from OpenAI, due to its good performance and low cost.  • For generating dense vector representations of each table descriptions, we use em- beddings generated by a model based on XLM-RoBERTa [Conneau et al. 2019] fine-tuned for information retrieval [Wang et al. 2024], which are then indexed in the Elasticsearch platform3 and are retrieved using the cosine similarity strategy.  • For generating table manipulation code, we tested:  – Closed-source models,  namely GPT 3.5 Turbo  and GPT 4  [OpenAI et al. 2024], accessed from OpenAI API;  – Open-source models available from the GPT4All hub [Anand et al. 2023], namely LLaMa 3 8B Instruct [Dubey et al. 2024], Nous Hermes 2 Mis- tral DPO 7B [Jiang et al. 2023], Falcon 7B [Almazrouei et al. 2023], and GPT4All Snoozy 13B [Anand et al. 2023]. We used a single V100 32 GB GPU to infer responses with these models.  • All code generation models were tested by generating SQL queries (executed us- ing the Python library sqldf) and code for dataframe manipulation (executed using the Python library pandas).  2We use accuracy as defined in [Mallen et al. 2022], since other metrics could be harder to be applied due to the nature of generative models answers, which tend to have a lot more words than the needed terms.  3https://www.elastic.co/   5. Results  Figure 4 shows the performance of the retrieval and the re-ranking modules in TableRAG pipeline to find the most suitable tables to answer questions from OTT-QA. Re-ranking is applied on top of the retrieved tables, and increases the results of retrieval in 3.7 points in HITS@1, when the table ranked first is the correct one. Re-ranking is still better than just retrieval when looking at HITS@2, but after that, just retrieval becomes better than when re-ranking is applied. When looking at HITS@5, the results from retrieval are 1.4 point better than those with re-ranking. This decrease happens mainly because of lower-ranked tables that the LLM judged that needed to be filtered out of the list during re-ranking, when they were in fact the correct ones.  Figure 4. Comparison of Retrieval and Re-ranking Results over OTT-QA  Figure 5 shows the performance of the different models tested for generating Python code or SQL queries to answer the WikiTableQuestions. The yellow bars rep- resent the accuracy value, while the orange ones represent the number of codes that pro- duced exception at runtime, both in % of questions. The green line represents the average time in seconds taken to answer each question.  The best results come from OpenAI’s closed-source models, both GPT-4 and GPT- 3.5. The disparity between the best closed-source model and the best open-source model can be easily explained, among other factors, by the size of the models: while Nous Hermes 2 has 7 billion parameters in its neural architecture and achieved 40% accuracy, GPT-4 is speculated to have over 1 trillion4, reaching 74%.  The difference in results between generating SQL and Python code for closed- source models is relatively small (74% in Python versus 72.5% in SQL for GPT-4). How- ever, the difference in time is striking: while GPT-4 took an average of 11.5 seconds to generate Python code per question, generating SQL code was much faster, averaging 2.8 seconds. This is due to the fact that more preprocessing is required to construct functional Python code, and the larger the code, the longer the inference time. For open-source models, in general, better results are obtained when generating SQL code. Nous Hermes 2 achieved 40% accuracy building SQL queries and only 20% building Python codes, while the better open-source Python code generator, LLaMA, obtained only 26%.  4Sources suggest that GPT-4 could be a mixture of several models that, together, total 1.76 trillion  parameters (https://en.wikipedia.org/wiki/GPT-4).   Figure 5. Comparison of LLMs Results in Code Generation for Q&A over Wik- iTableQuestions  6. Conclusions  We presented TableRAG, a pipeline for integrating tabular data into traditional Retrieval- Augmented Generation systems. The pipeline consists of obtaining textual representa- tions of tables, indexing and retrieving them as dense contextual vectors, generating SQL or Python code for table manipulation, and generating a candidate answer in natural lan- guage. With the described pipeline, we achieved results of up to 86.7% HITS@5 in retrieval and 74% accuracy in Q&A using GPT-4.  Some limitations of this work should be considered. Due to computational cost and processing time, the results for Q&A were obtained using a sample of 200 ques- tions from the WikiTableQuestions test set, making it difficult to compare our results with those of other works using the same benchmark. Additionally, the instructions provided to the language models for code generation were adjusted based on the outputs of the GPT-4 model, as it was the best-performing model, but they could also have been ad- justed considering the most frequent errors of each of the other models individually. The open-source models did not undergo any fine-tuning process, which could significantly improve their results, and we did not test open-source models larger than 13B parameters. Finally, we did not perform any preprocessing on the tables to make them more easily interpretable, such as the table transposition procedure performed by [Liu et al. 2024], which yielded the state-of-the-art with 73.6% accuracy using GPT-3.5.  Acknowledgments  The work was carried out with assistance granted by the National Agency of Petroleum, Natural Gas and Biofuels (ANP), Brazil, associated with the investment of resources orig- inating from the R,D&I Clauses, through the Cooperation Agreement between Petrobras and PUC-Rio.   "
        },
        {
            "titulo": "A Dependency Treebank of Tweets in Brazilian Portuguese: Syntactic Annotation Issues and Approach",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31131",
            "idioma": "Inglês",
            "storage_key": "files/article_31131_30934.pdf",
            "autores": [
                {
                    "nome": "Ariani Di Felippo",
                    "afiliacao": "USP / UFSCar",
                    "orcid": "http://orcid.org/0000-0002-4566-9352"
                },
                {
                    "nome": "Maria das Graças V. Nunes",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-2776-6140"
                },
                {
                    "nome": "Bryan K. da Silva Barbosa",
                    "afiliacao": "UFSCar",
                    "orcid": "http://orcid.org/0000-0002-4637-6498"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Amplia-se a análise de dependência do português brasileiro (pt-br) para lidar com “conteúdo-gerado por usuários” ao desenvolver e anotar o primeiro treebank de tweets (atuais posts do X) em pt-br segundo o modelo Universal Dependencies. O DANTEStocks possui 4,048 tweets do mercado financeiro e anotação-UD de tags PoS e traços morfológicos. Neste artigo, descreve-se a estratégia de anotação sintática adotada para lidar com as idiossincrasias do Twitter e do domínio desse corpus. A versão do DANTEStocks enriquecida com as relações de dependência-UD e as diretrizes de anotação já estão publicamente disponíveis.",
            "keywords": [
                "corpus annotation",
                "tweet",
                "stock market",
                "Universal Dependencies"
            ],
            "referencias": [
                "Barbosa, B. K. S. (2024). Descrição sintático-semântica de nomes predicadores em tweets do mercado financeiro em português. Dissertação de Mestrado. Programa de Pós-graduação em Linguística, Universidade Federal de São Carlos, São Carlos/SP, 208p.",
                "Carletta, J. (1996). Assessing agreement on classification tasks: The kappa statistic. In Computational Linguistics, Volume 22, Number 2, pages 249–254. MIT Press.",
                "Cohen, J. (1960). A coefficient of agreement for nominal scales. In Educational and Psychological Measurement, Volume 20, Issue 1, pages 37-46.",
                "Di-Felippo, A.; Postali, C.; Ceregatto, G.; Gazana, L. S.; Roman, N. T. (2022). Diretrizes de anotação de PoS tags em tweets do mercado financeiro: orientações para anotação em língua portuguesa segundo a abordagem Universal Dependencies. Relatório Técnico do ICMC 438. Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo. São Carlos-SP, 24p.",
                "Di-Felippo, A., Nunes, M. G. V., Barbosa, B. K. S. (2024). Diretrizes de anotação de relações de dependência em tweets do mercado financeiro. Relatório Técnico do ICMC 446. Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo. São Carlos-SP, Abril, 70p.",
                "Duran, M.S. (2021). Manual de Anotação de PoS tags: orientações para anotação de etiquetas morfossintáticas em Língua Portuguesa, seguindo as diretrizes da abordagem Universal Dependencies (UD). Relatório Técnico do ICMC 434. ICMC, USP. São Carlos-SP, 55p.",
                "Duran, M.S. (2022). Manual de Anotação de Relações de Dependência - Versão Revisada e Estendida: Orientações para anotação de relações de dependência sintática em Língua Portuguesa, seguindo as diretrizes da abordagem Universal Dependencies (UD). Relatório Técnico do ICMC 440. ICMC, USP. São Carlos-SP, 166p.",
                "Duran, M. S., Lopes, L., Nunes, M.G.V., Pardo, T. A. S. (2023). The Dawn of the Porttinari Multigenre Treebank: Introducing its Journalistic Portion. In Proceedings of the 14th Symposium in Information and Human Language Technology, pages 115-124. Belo Horizonte/MG. SBC.",
                "Krumm, J., Davis, N. Narayanaswami, C. (2009). User-Generated Content. In IEEE Pervasive Computing, Volume 7, Issue 4, pages. 10 – 11, IEEE, 2009.",
                "Lopes, L., Duran, M. S.; Fernandes, P. H. L.; Pardo, T. A. S. (2022). PortiLexicon-UD: a Portuguese Lexical Resource according to Universal Dependencies Model. In Proceedings of the 13th International Conference on Language Resources and Evaluation (LREC), pages 6635 6643, Marseille, France. ELRA.",
                "Lopes, L.; Pardo, T. A. S. Towards Portparser - a highly accurate parsing system for Brazilian Portuguese following the Universal Dependencies framework. In Proceedings of the 16th International Conference on Computational Processing of Portuguese (PROPOR), pages 401-410, Santiago de Compostela, Galiza. ACL.",
                "Macqueen, J. (1967) Some methods for classification and analysis of multivariate observations. In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability. [S.l.], v. 1, n. 14, p. 281–297.",
                "Nivre, J., Fang, C.-T. (2017). Universal Dependency evaluation. In Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017), pages 86–95, Gothenburg, Sweden. ACL.",
                "Nivre, J., et al. (2016). Universal dependencies v1: A multilingual treebank collection. In Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC), pages 1659–1666, Portorož, Eslovênia. ELRA.",
                "Nivre, J. et al. (2020). Universal Dependencies v2: an evergrowing multilingual treebank collection. In Proceedings of the 12th International Conference on Language Resources and Evaluation Conference (LREC), pages 4034-4043. Marseille, França. ELRA.",
                "Qi, P., Zhang, Y., Zhang, Y, Bolton, J., Manning, C. D. (2020). Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) (System Demonstrations), pages 101-108. Online. ACL.",
                "Rademaker, A., Chalub, F., Real, L., Freitas, C., Bick, E., Paiva, V. de. (2017). Universal Dependencies for Portuguese. In Proceedings of the 4th International Conference on Dependency Linguistics (Depling), pages 197–206, Pisa, Italy. Linköping University Electronic Press.",
                "Sanguinetti, M. et al. (2023). Treebanking user-generated content: a UD based overview of guidelines, corpora and unified recommendations. In Lang Resources & Evaluation, Volume. 57, Issue 2, pages 493–544. Springer-Verlag, Berlin, Heidelberg.",
                "Silva, E.H.; Pardo, T.A.S.; Roman, N.T.; Di Felippo, A. (2021). Universal Dependencies for tweets in Brazilian Portuguese: tokenization and Part-of-Speech tagging. In Proceedings of the 18th National Meeting on Artificial and Computational Intelligence (ENIAC), pages. 434-445, Online. SBC.",
                "Scandarolli, C. L., Di-Felippo, A., Roman, N. T., Pardo, T. A. S. (2023). Tipologia de fenômenos ortográficos e lexicais em CGU: o caso dos tweets do mercado financeiro. In Anais da VIII Jornada de Descrição do Português (JDP) (Evento integrante do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana-STIL), p. 240-248, Belo Horizonte/MG, Brasil. SBC.",
                "Sobrevilla Cabezudo, M.A., Maziero, E.G., Souza, J.W.C., Dias, M.S., Cardoso, P.C.F., Balage Filho, P.P., Agostini, V., Nóbrega, F.A.A., Barros, C.D., Di Felippo, A., Pardo, T.A.S. (2015). Anotação de sentidos de verbos em textos jornalísticos do corpus CSTNews. In Revista de Estudos da Linguagem (RELIN), Volume 23, Número 3, p. 797-832.",
                "Straka, M. (2018). UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 197–207. Brussels, Belgium. ACL."
            ],
            "artigo_completo": "Resumo.  Amplia-se  a  análise  de  dependência  do  português  brasileiro  (pt-br)  para  lidar  com  “conteúdo-gerado  por  usuários”  ao  desenvolver  e  anotar  o  primeiro  treebank  de  tweets  (atuais  posts  do  X)  em  pt-br  segundo  o  modelo  Universal  Dependencies.  O  DANTEStocks  possui  4,048  tweets  do  mercado  financeiro  e  anotação-UD  de  tags  PoS  e  traços  morfológicos.  Neste  artigo,  descreve-se  a  estratégia  de  anotação  sintática  adotada  para  lidar  com  as  idiossincrasias do Twitter e do domínio desse corpus. A versão do DANTEStocks  enriquecida com as relações de dependência-UD e as diretrizes de anotação já  estão publicamente disponíveis.   1. Introduction   The  Universal  Dependencies  (UD)  [Nivre  et  al.  2020]  project  specifies  a  complete  morphological and syntactic representation with the goal of facilitating multilingual tagger  and parser development [Nivre 2016]. The morphology of a word consists of 3 levels of  information:  PoS  tag,  lemma,  and  features.  Syntactic  annotation  consists  of  typed  dependency relations (deprels) between words. Currently, the model has 17 PoS tags and  37 deprels, plus a non-fixed set of morphological features. Figure 1 shows an example of  an annotated tweet in DANTEStocks. In a dependency tree, one word is the head of the  utterance  (root)  and  all  other  words  are  dependent  on  another  word.  The  labeled  arcs  represent the deprels, pointing from heads to their dependents. The PoS tag and the lemma  of each word are displayed below the text. The morphological features are not included in  this figure. However, the token “acordo” (“agreement”), for example, has the following  features and values according to UD: Number=Sing, Gender=Masc.         Figure 1. UD annotation of “#BR #BOVESPA #GOLL4 Gol assina acordo de  compartilhamento de voos com TAP - http://t.co/wHGukBg7qp”1.   Motivated by UD, treebanks for new domains, genres and language varieties have  been recently built. Among the treebanks featuring user-generated content (UGC) created  from 2014 onwards, a significant number is either partially or entirely made up of Twitter  data,  whose  language  diverges  from  standard  written  texts  in  several  ways,  posing  significant  challenges  for  building  UD-based  treebanks.  These  challenges  include  non- standard spelling,  capitalization, punctuation, syntax, platform conventions, and creative  language  use,  which  often  introduce  many  unknown  words.  Promoting  cross-linguistic  consistency, UD guidelines for UGC annotation have been provided [e.g. Sanguinetti et al.  2022], however, when it comes to a technical domain, specific strategies are required. Due  to the variety and complexity of the language, adequate treatment of the phenomena by  means of an already existing model, such as UD, is a non-trivial task.   We report the syntactic annotation of DANTEStocks within UD framework. First,  we briefly describe the segmentation, tokenization, and the previous PoS annotation of the  corpus (§2). Then, we present the annotation guidelines for the UD-deprels (§3). In (§4),  we detail the semi-automatic approach for annotating the dependency relations, including  data  organization,  creation  of  a  reference  subcorpus,  and  training  a  state-of-art  parsing  model on tweets. In (§5), we report a small-scale evaluation of the syntactic annotation.  Finally, we put our work into context and outline future work (§6).   2. The DANTEStocks Corpus   DANTEStocks  is  a  corpus comprising 4,048 tweets  (with  140-character limit) from the  stock market domain. It was automatically collected by fetching posts containing a ticker2  of one of the 73 stocks that compose the Ibovespa3. Considering the entire tweet as a basic  unit for syntactic analysis, the DANTEStocks’ tweets are not segmented into smaller units  (sentences,  clauses  or  phrases).  This  decision  saved  the  effort  to  conduct  a  manual  segmentation  or  do  revision  of  an  automatic  process.  Additionally,  the  corpus  was  not  normalized to preserve its diversity, as the goal was to develop multigenre applications.  Although focusing on syntax, we outline the previous segmentation and morphological UD- annotation because they contextualize some annotation decisions.   1 “#BR #BOVESPA #GOLL4 Gol signs flight sharing agreement with TAP - http://t.co/wHGukBg7qp”  2 It is a five or six-character alphanumerical string that represents a specific type of stock from a company,  such as “PETR4” for Petrobras’ preferred stock.  3 It is the benchmark indicator of B3 (“Brasil, Bolsa, Balcão”), which is the main financial exchange in Brazil.                                Following the lexicalist view of syntax of UD, the syntactic words4 (tokens) were  automatically  segmented  by  a  version  of  the  NLTK  TweetTokenizer5,  augmented  with  specific rules for UGC [Silva et al. 2021]. The tool preserves most white-space-delimited  tokens,  including  phonetization  (e.g.  “d+”  >  “demais”),  hashtag,  cashtag6,  at-mention,  emoticon, and URL, and splits off single orthographic tokens that correspond to multiple  (syntactic) words, such as clitics, contractions (canonical and non-canonical), punctuation  marks  (except  for  abbreviations),  and  valuation  rates  and  monetary  values  with  unconventional orthography. After the manual revision of the tool output, the corpus ends  up with a total number of 81,037 tokens.   The morphological annotation was also conducted semi-automatically7 [Silva et al.  2021]. The PoS tags generated by the UDPipe 2 parser [Straka 2018], trained incrementally  over UD-Portuguese Bosque [Rademaker et al. 2017] and tweets, were manually analyzed  by three annotators, and the cases of disagreement among them were adjudicated by a senior  linguist based on guidelines tailored for standard texts in BP [Duran 2021] and tweets [Di- Felippo et al. 2022]. All 17 UD-tags can be found in DANTEStocks. PUNCT, NOUN, and  PROPN are the most frequent, with around 16%, 15% and 14% of all the tags, respectively.  Lemmas  and  grammatical  features  were  semi-automatically  obtained  by  using  the  PortiLexicon-UD lexicon [Lopes et al. 2022]. Major manual adjustments were required for  lemmatization  due  to  the  high  rate  of  out-of-vocabulary  words.  Regarding  grammatical  features, the scenario was quite different. The features extraction was guided by the already  validated PoS tags and lemmas, which decreased the manual revision effort. Most of the  corrections  was  related  to  errors  arising  from  ambiguity  about  VERB  class  features  (VerbForm, Mood, Tense, Gender, Number and Person). The manual revision also focused  on checking Typo, Abbr, and Foreign, which are features that can be associated to words  belonging to all PoS classes.   While  many  syntactic  structures  of  tweets  could  be  quite  straightforwardly  annotated using the general guidelines adapted for Portuguese [Duran 2022], many of them  needed specific  choices.  In the  next  section, we discuss the main  challenging issues  for  annotation decisions related to dependency relations (deprels).   3. Syntactic Annotation Issues   3.1. Medium- and domain-dependent (lexical) phenomena   Mostly  following  the  recommendations  of  Sanguinetti  et  al.,  tokens  classified  as  orthographic variation from standard norm by [Scandarolli et al. 2023] were annotated with  their actual syntactic roles, since they are always syntactically integrated. These variations  include user-generated content phenomena such as substitution, omission, insertion, and  transposition of characters (e.g., letters, spaces, hyphens, and diacritics). A good example  is the token “nao” (instead of “não”) (“no”) in (1) “VALE5 nao passa de 29,9”8, which has  a case of diacritic omission.  In the  example,  “nao” was  related to the  root “passa” by  advmod, since it is an adverb that modifies a predicate.   4 It is the basic annotation unit that plays a syntactic function in an utterance.  5 https://www.nltk.org/api/nltk.tokenize.html  6 It was specifically designed to track financial instruments (e.g., $PETR4).  7 The version of the corpus containing PoS and features annotation is publicly available at:  https://sites.google.com/icmc.usp.br/poetisa/resources-and-tools.  8 “VALE5 does not exceed 29,9”               The same strategy was adopted for treating most of the phenomena classified as  “innovative  norm”9  by  [Scandarolli  et  al.  2023]  (i.e.,  abbreviation,  neologism,  mark  of  expressiveness and homophone writing), since they are also always syntactically integrated.  Pictogram (emoticon/emoji), which is a mark of expressiveness, is the only one that occurs  non-syntactically integrated (standalone), being attached to the root by discourse. The  other  two  types  of  innovative  norm’s  phenomena  required  annotation  guidelines  when  standalone and syntactically integrated (Table 1). For the medium-dependent devices, the  treatment given to the at-mentions when preceded by the RT mark is only that differs from  the  recommendation  of  Sanguinetti  et  al.  Instead  of  considering  the  at-mention  as  standalone  and  attaching  it  to  the  main  predicate  with  vocative,  we  treat  it  as  a  syntactically  integrated  token  attached  to  the  RT  mark  by  nmod.  This  is  due  to  our  interpretation  of  an  elliptical  preposition  “de”  (“of”)  (“RT  de  @user”),  indicating  an  attributive relationship between the RT/SYM and the @user/PROPN. Also differently, all  the cases of parataxis involving a UGC phenomenon in DANTEStocks are annotated  with a corresponding subrelation, not only for URL and hashtags.   Table 1. UD-dependency guidelines for Twitter- and domain-specific issues.   UGC issue  Subtype   URL   Hashtag   At-mention   Syntactic  integration  No  Yes  No  Yes  No  Yes  No  Yes  Yes  Truncation  Code-switching (intra)  Yes  Yes  Ticker  No  Yes   Cashtag   RT   Standard syntactic  role   Other   ✓   ✓   ✓   parataxis:hashtag   parataxis:mention  nmod (of the RT)  parataxis:url   parataxis:rt   ✓  ✓ (:wtrunc or :strunc)    ✓ (if known)  ✓   flat:foreign (if unknown)   parataxis:cashtag   ✓   Medium- dependent  token   Domain- specific  token   3.2. Unconventional syntax   Besides all the linguistic issues previously mentioned, the complexity of the UD-annotation  also rises from the highly contextual nature of Twitter, and the high level of fragmentation  that seems to be typical in UGC from stock market domain. This provides a rich context  for ambiguities and ellipses, resulting in unconventional syntactic structures whose most  appropriate UD analysis depends on the interpretation of the tweet content. One example is  nsbuj:pass  without  the  aux:pass.  To  recommend  attaching  “#cyre3”  to  the  root  “postado” by nsbuj:pass in the tweet of Figure 2, we assumed that the auxiliary verb is  elided. In Figure 2, we also assumed an elliptical preposition (“a”) preceding “+1,78” to  connect “1,78” by obl. Since the syntactic function of “(+)1,78” is ambiguous (i.e., obl  of “postado” or nmod of “abertura”), the choice of “1,78” as dependent on the root by  obl illustrates annotation decisions based on the interpretation of domain experts.   9 They are lexical alternatives to existing standard words and frequent linguistic devices that are found in  the Twitter and/or stock market domain language [Scandarolli et al. 2023].                                     Figure 2. Syntactic ellipsis in the fragment ““#cyre3 postado hj antes da abertura +1,78”10.   3.3. Structural patterns   Besides the UGC (lexical) phenomena and unconventional syntax issues, we also identified  22  recurring  structural  patterns  among  the  tweets  in  DANTEStocks.  Such  patterns  correspond to almost 1,000 instances of the corpus, i.e. unique tweets. For each pattern, we  created a template for guiding the annotation of the pattern instances in the corpus. The 22  templates  also  compose  the  dependency  annotation  guidelines  for  the  DANTEStocks  corpus, as well  as the recommendations for the treatment of  the lexical phenomena and  unconventional structures [Di-Felippo et al. 2024].    More precisely, a template contains 3 fields: (i) pattern, i.e. a mnemonic description,  (ii) elements, i.e. list of pattern elements and the corresponding annotation guideline within  UD, and (iii) example, i.e., at least one attested instance of the pattern from the corpus with  its UD-dependency annotation. It is important to mention that, since the patterns usually  refer  to  fragmented  and/or  full  of  syntactic  ellipsis  tweets,  the  template  specification  is  based on a possible interpretation of the tweets, which was done with the support of stock  market´s experts.   For illustration, the Template 11 is shown in Table 2. It corresponds to 20 unique  instances in the corpus. Since the pattern of the template represents very fragmented tweets,  the domain experts helped us to interpret corpus utterance such as that in Table 2 as being  composed by three blocks of information, resulting in the following pattern description:  <hashtag-ticker><theme><url>.   The <theme> provides information about a specific stock, codified by the <hashtag- ticker>, and it was considered the main information of the utterance. Since the <theme> is  always being introduced by the coordinate expression “support and resistance”11, the first  element of the expression (i.e. “suportes”) is the root¸ as indicated in the field “elements”.  In the “element” field, it is also indicated that the <hashtag-ticker> is dependent on the  root with the nmod tag, due to interpretation of “#VALE” as a nominal that functionally  corresponds to a modifier of another noun (“suportes”). Since the nmod relation is usually  introduced by a preposition (ADP tag) in Portuguese, we assume, to propose the template,  that there is an elliptical preposition “de(+a)” (i.e. “suportes e resistências  da VALE4”)  (“support and resistance” of #VALE5). Finally, the <url> is dependent on the root with  parataxis:url because it is a run-on segment.   10 “#cyre3 posted today before opening +1,78”.  11 Terms that indicate price levels where a specific stock tends to reject the current trend and reverse, i.e.,  they indicate potential turning points in a stock’s price.                             Table 2 Template for UD-dependency annotation of tweets with structural pattern.   Pattern  Elements  a. <hashtag-ticker> is dependent on the root with the nmod label   <hashtag-ticker> <theme> <url>, where:   b. <theme> contains the expression “suportes e resistências”; “suportes” is the root   c. <url> is dependent of the root with the parataxis:url tag   Example  #VALE5 suportes e resistências http://t.co/c8OrWXrECN   4. Syntactic Annotation Approach   The dependency-based annotation of DANTEStocks was held in two semi-automatic stages  [Barbosa 2024]. The first one aimed at creating a reference subcorpus and the second stage  of  the  annotation  focused  on  fine-tuning  a  pre-trained  parser  for  tweets  by  using  the  reference subcorpus as part of its initial training set. To start the syntactic annotation, all  4,048  tweets  were  grouped  into  three  major  sets,  capturing  tweets  with:  (i)  relatively  standard language, (ii) recurring structural patterns, and (iii) other (tweets that do not belong  to any of the first two sets). Tweets were classified through k-means clustering [Macqueen  1967] with tf-idf (“term frequency–inverse document frequency”) [Luhn 1957].   4.1. Creation of a Reference Subcorpus   The organization of tweets into sets as mentioned above allowed us to select a few instances  from each set, covering all the lexical and structural diversity of DANTEStocks to compose  a reference subcorpus of 1,000 tweets. Furthermore, as an attempt to achieve annotation  consistency,  particularly  given  the  non-canonical  language  of  the  corpus,  the  semi- automatic annotation of the subcorpus was also based on such classification. This means  that the data from each major set was manually reviewed separately.   To create a gold-standard subcorpus we also used the UDPipe 2 parser trained over  UD-Portuguese Bosque to annotate  the 1.000 tweets. The  UD-annotated subcorpus  was  later manually revised by a single expert. Taking advantage of the previous experience of  the expert in UD-annotation of journalistic texts and the training of UDPipe 2 over Bosque,  the manual revision started with tweets that present relatively standard language. The next  tweets were those with recurring structural patterns, and finally the tweets with a variety of  lexical and structural characteristics. During the revision process, the challenging issues  described  in  Section  3  were  discussed,  and  the  annotation  decisions  gave  rise  to  the  guidelines  for  the  treatment  of  tweets  from  the  stock  market  domain  within  the  UD  framework  [Di  Felippo  et  al.  2024].  The  guidelines  were  used  to  support  the  manual  revision of the rest of the corpus, which was done when training a state-of-art parser on the  tweets from DANTEStocks. After  the  revision  of  the  subcorpus,  we  ended  up  having  a  gold-standard subset of 1,000 syntactically annotated tweets.                          4.2. Parsing model training   The  rest  of  the  corpus  was  annotated  by  customizing  Stanza  [Qi  et  al.  2020]  for  DANTEStocks.  Stanza  is  a  well-known  pre-trained  model  for  Portuguese,  having  the  advantage of being a user-friendly pipeline for text analysis. The procedure began with the  Stanza  base  architecture,  fine-tuned  on  Porttinari-base  [Duran  et  al.  2023],  which  is  a  journalistic corpus composed of 8,418 sentences (168,080 tokens) manually annotated with  UD, and the reference subcorpus. For the first run of Stanza, comprising Porttinari-base and  the reference subcorpus as initial training dataset, was applied the same distribution of data  found in Porttinari-base12, resulting in a dataset of 9,893 samples, being 70% for training,  10% for validation, and 20% for testing. The resulting parser was used to annotate a new  package  of  data  (out  the  remaining  3,048  tweets),  which  was  manually  revised  and  incorporated to the previous data set, being then used to start a new training run of Stanza.  This cycle continued incrementally until the last package of tweets was annotated/revised.   Besides the first training iteration, we carried out five training runs, adding packages of  203, 300, 400, 400, and 1233 tweets per iteration, respectively (totaling 2,536 tweets). The  resulting model of the 6th (final) run was used to annotate the remaining 512 tweets. The  tweet  packages  were  added  in  the  same  order  as  the  manual  revision  of  the  reference  subcorpus:  standard  language  tweets,  structural  pattern  tweets,  and  tweets  with  varied  lexical/structural properties.   For each of the five runs, we kept, whenever possible, the same distribution of data  for  training,  validation  and  testing  used  in  the  first  iteration,  and  computed  Stanza’s  performance based on the Unlabeled Attachment Score13 (UAS) and Labeled Attachment  Score14 (LAS). The UAS accuracy increased from 94.46% at the first run to 95,78% in the  last (6th) iteration, becoming 1,32% better. For LAS, the final accuracy (6th run) achieved  94,62%,  increasing  0,76%  from  the  first  run  accuracy  of  93,86%.  The  increase  of  the  dependency  relation  measures  indicates  that  the  model’s  ability  to  capture  the  syntactic  structures of the tweets has improved as we incorporate news tweet into the training sets.  For comparison purposes, the accuracy of the best model for journalist texts in Portuguese  was also around 96% (UAS) and 95% (LAS) [Lopes and Pardo 2024]. Figure 3 depicts the  overall distribution of the dependency relations (without subrelations) in DANTEStocks.   Figure 3. Frequency distribution of UD deprel tagset in DANTEStocks.   12 The 8,418 sentences were split into training, development, and test sets, with 70% (5,893 sentences), 10%  (842 sentences), and 20% (1,683 sentences) of the corpus, respectively.  13 UAS indicates the accuracy of the head ignoring the relation’s name (deprel) [Nivre and Fang, 2017].  14 LAS evaluates the output of a parser by considering how many words have been assigned both the correct  syntactic head and the correct label, ignoring subrelations [Nivre and Fang 2017].                       5. Reliability of Annotation   To provide a reliability measure of the annotation of DANTEStocks, a second NLP expert  (also with UD-annotation experience) manually reviewed the automatic annotation of 100  random tweets based on the same guidelines [Duran 2022; Di Felippo et al. 2024]. The  dependency-trees  analyzed  by  the  additional  annotator  could  be  from  the  reference  subcorpus or generated by Stanza in one of its interactions. The Inter-Annotator Agreement  (IAA) score was calculated by using the Kappa coefficient [Cohen, 1960; Carletta, 1996]  in two different settings [Barbosa 2024]. In the first, the focus was to evaluate the annotation  of head and deprel separately. The Kappa results for head and deprel were 0.96 and 0.97,  respectively. In the second setting, the evaluation aimed at the combination of head and  deprel, obtaining the Kappa score of 0.95. The IAA per deprel was measured by using the  total agreement score [Sobrevilha Cabezudo 2015], since Kappa is not appropriate given  the unbalanced distribution of the relations. We obtained the total agreement of 100% for  more than half of the 46 different deprels (including subrelations) that occur in the sample  of 100 tweets. Out of the 1.743 annotated relations, there are 42 cases of disagreement. The  most frequent conflict was between obl and nmod. Some of them were caused by different  but potential interpretations about the functional role of the prepositional phrase (in bold)  in structure like “arrisque vd em #petr4” (“risk selling in #petr4”). While one annotator  attached  “petr4”  to  the  verb  via  obl,  functioning  as  a  non-core  (oblique)  argument  or  adjunct, the other assumed that “petr4” is a modifier of the noun “vd” (“venda”), being  attached to it by nmod. It is also interesting that, among the 22 deprels with total agreement  different from 100%, 12 of them contain subrelations, indicating that the annotation is more  complex when using language-specific relations. Even though a small-scale evaluation, the  results indicate that the overall IAA was otherwise quite high, especially for the challenging  task. This might be due to the large and detailed recommendations of our guidelines for the  syntactic annotation of the tweets.   6. Final Remarks and Future Work   We described our effort on building the first BP treebank for Twitter microtext, annotated  within the framework of UD. The contributions are the treebank itself, the instantiation of  the UD guidelines for stock market tweets in BP, and the customization of a current state- of-the-art  parser  for  tweets.  Our  main  difficult  was  interpreting  the  tweets,  due  to  the  medium- and domain-lexical phenomena and uncommon constructions. Thus, despite the  constant help of domain experts, we can say that the dependency annotation of many tweets  in  DANTEStocks  (especially  those  with  fragmentation,  e.g.,  aborted  text)  represents  potential  syntactic  analysis of the tweets.  Currently, the  two  annotators involved in this  work are analyzing the disagreements to assign a consensual deprel for each case and to  make the treebank available soon. The guidelines for the syntactic UD-based annotation of  DANTEStocks and the treebank itself (beta version) are available at the POeTiSA project  webpage (https://sites.google.com/icmc.usp.br/poetisa/).   Acknowledgements. This work was carried out at the Center for Artificial Intelligence of the  University of São Paulo (C4AI - http://c4ai.inova.usp.br/), with support by the São Paulo  Research  Foundation  (FAPESP  grant  #2019/07665-4)  and  by  the  IBM  Corporation.  The  project  was  also  supported  by  the  Ministry  of  Science,  Technology  and  Innovation,  with  resources  of  Law  N.  8,248,  of  October  23,  1991,  within  the  scope  of  PPI-SOFTEX,  coordinated by Softex and published as Residence in TIC 13, DOU 01245.010222/2022-44.          "
        },
        {
            "titulo": "Detection and Censorship of Offensive Language in Extended Texts in Portuguese",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31132",
            "idioma": "Inglês",
            "storage_key": "files/article_31132_30935.pdf",
            "autores": [
                {
                    "nome": "Lucas Lenoch de Souza",
                    "afiliacao": "UTFPR",
                    "orcid": "http://orcid.org/0009-0000-9084-7253"
                },
                {
                    "nome": "Franciele Beal",
                    "afiliacao": "UTFPR",
                    "orcid": "https://orcid.org/0009-0008-0752-0400"
                },
                {
                    "nome": "André Roberto Ortoncelli",
                    "afiliacao": "UTFPR",
                    "orcid": "https://orcid.org/0000-0001-9622-8525"
                },
                {
                    "nome": "Marlon Marcon",
                    "afiliacao": "UTFPR",
                    "orcid": "https://orcid.org/0000-0002-3698-8570"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "This article addresses the problem of detecting and censoring offensive language in extensive Brazilian Portuguese texts on the web. This paper proposes a pipeline for classifying and censoring extensive texts, focusing on comments, posts, and articles using NLP techniques. The results include an in-depth review of current methods for offensive content classification in Portuguese and the implementation of a BERTimbau-based pipeline for offense detection. This work represents a significant advancement in the state-of-the-art NLP in Portuguese, promoting safer and more respectful online environments for users, especially children.",
            "keywords": [
                "Hate-speech detection",
                "BERT model",
                "offensive content classification"
            ],
            "referencias": [
                "Cook, S. (2024). Cyberbullying facts and statistics for 2018 – 2024.",
                ".",
                "Devlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.",
                "Economist (2019). Increasing numbers of children have internet addiction – how worried should parents really be?",
                ". (Accessed on 10/10/2024).",
                "Hajibabaee, P., Malekzadeh, M., Ahmadi, M., Heidari, M., Esmaeilzadeh, A., Abdolazimi, R., and Jones, J. H. (2022). Offensive language detection on social media based on text classification. 2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC), pages 0092–0098.",
                "Honnibal, M., Montani, I., Van Landeghem, S., and Boyd, A. (2020). spaCy: Industrial-strength Natural Language Processing in Python.",
                "Husain, F. and Uzuner, O. (2021). A survey of offensive language detection for the arabic language. ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP), 20(1):1–44.",
                "Leite, J. A., Silva, D., Bontcheva, K., and Scarton, C. (2020). Toxic language detection in social media for brazilian portuguese: New dataset and multilingual analysis. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 914–924.",
                "Leray, W. (2023). Série de harry potter? envolvimento de j.k. rowling divide fãs.",
                ". (Accessed on 10/10/2024).",
                "Martins, T. (2022). Chico buarque dá comida aos censores - senso incomum.",
                ". (Accessed on 10/10/2024).",
                "Monteiro, E. (2023). Caso bruno e dom: justiça decide levar amarildo e outros dois réus a júri popular | amazonas | g1.",
                ". (Accessed on 10/10/2024).",
                "Pelle, R. P. and Moreira, V. P. (2017). Offensive comments in the brazilian web: a dataset and baseline results. In Anais do VI Brazilian Workshop on Social Network Analysis and Mining. SBC.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: pretrained bert models for brazilian portuguese. In Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I 9, pages 403–417. Springer.",
                "Trajano, D., Bordini, R. H., and Vieira, R. (2023). Olid-br: offensive language identification dataset for brazilian portuguese. Language Resources and Evaluation, pages 1–27.",
                "Trielli, L. (2021). Escócia: estupradores que se declararem mulher serão colocados em prisões femininas - senso incomum.",
                ". (Accessed on 10/10/2024).",
                "Vargas, F., Carvalho, I., Rodrigues de Góes, F., Pardo, T., and Benevenuto, F. (2022). HateBR: A large expert annotated corpus of Brazilian Instagram comments for offensive language and hate speech detection. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 7174–7183, Marseille, France. European Language Resources Association.",
                "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.",
                "Wagner Filho, J. A., Wilkens, R., Idiart, M., and Villavicencio, A. (2018). The brwac corpus: a new open resource for brazilian portuguese. In Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018)."
            ],
            "artigo_completo": "Abstract. This article addresses the problem of detecting and censoring offen- sive language in extensive Brazilian Portuguese texts on the web. This paper proposes a pipeline for classifying and censoring extensive texts, focusing on comments, posts, and articles using NLP techniques. The results include an in-depth review of current methods for offensive content classification in Por- tuguese and the implementation of a BERTimbau-based pipeline for offense de- tection. This work represents a significant advancement in the state-of-the-art NLP in Portuguese, promoting safer and more respectful online environments for users, especially children.  1. Introduction  In recent years, the Internet has been growing at an impressive rate in terms of users and the data generated by web page publications. This increasing Internet use often introduces many children to virtual environments from a very early age. Consequently, offensive language in online texts becomes a concern for ethical reasons [Economist 2019]. Issues such as cyberbullying, hate speech, and various forms of offensive content in social media posts are also relevant [Cook 2024].  Regarding the intelligent interpretation of textual data from social networks, Nat- ural Language Processing (NLP) is commonly used. To address the problem of offen- sive language in web posts, articles in the field of NLP, combined with Machine Learning (ML) and Deep Learning (DL) techniques, have been developed [Hajibabaee et al. 2022]. These efforts include creating research pipelines and developing high-quality annotated datasets by professionals [Leite et al. 2020].  Given the differences in languages and cultures, it is only possible to formalize a model for some languages. However, efforts are being made to learn various languages [Husain and Uzuner 2021]. The HateBR [Vargas et al. 2022] corpus is an example for Brazilian Portuguese. Such an article highlights the lack of academic production related to offensive language in the national language, along with the dataset and its results. Despite existing academic contributions to addressing offensive language in Brazilian   Portuguese, applying NLP and ML/DL techniques to classify longer texts (such as news articles or blog posts) remains challenging. These techniques are often limited to social media posts or news comments.  Based on this information, the present work focuses on detecting and subsequently censoring or filtering offensive language online. The goal is to ensure that web pages that are not explicitly focused on adult content can be suitable environments for children. Ad- ditionally, efforts are made to reduce prejudice and offensiveness in online posts, benefit- ing users’ emotional well-being and promoting mutual respect. Specifically, we propose identifying offensive words using DL techniques, which can then be filtered or censored. We define small texts as comments, posts, and sentences, while paragraphs and entire pages constitute more extended texts, with the latter being the focus of this study.  As the main contributions of this work, we have: 1) An in-depth review of state-of- the-art methods applied to offensive content classification for the Portuguese language; 2) An update to the state-of-the-art results on the HateBR dataset [Vargas et al. 2022]; and 3) A Deep-Learning-based pipeline that effectively classifies and censors extensive Brazilian Portuguese texts based on their offensiveness.  2. Related Works  When considering works that address text processing with offensive language in the Brazilian context, including ToLD-BR the range of existing works is very small, [Leite et al. 2020], OffComBr [Pelle and Moreira 2017], HateBR [Vargas et al. 2022], and OLID-BR [Trajano et al. 2023]. Such methods are explained in the following sec- tions.  2.1. ToLD-BR  This dataset [Leite et al. 2020], presents an unspecified number of posts extracted from the Twitter platform. A total of 42 individuals, chosen from 129 volunteers, were tasked with annotating each post, classifying them into various categories of prejudice: homo- phobia, obscene language, misogyny, and xenophobia.  The posts were classified using BERT-style algorithms [Devlin 2018], which achieved optimal results for such a complex and subjective task. The researchers also explored the possibility of building models for this task in multiple languages, but their results indicated that monolingual data is still preferable for more accurate classifications.  2.2. OffComBr  This paper presents the development of a dataset comprising comments on news articles derived from the website g1.globo.com, named “OffComBr” [Pelle and Moreira 2017]. The researchers obtained around 10,000 comments, but given the manual annotation pro- cess done by three experts in detecting offensive language, they included only 1,250 com- ments in the final dataset.  The authors performed two classification algorithms (SMO and Naive Bayes) to evaluate the dataset, with different assessments depending on different data preprocessing methods. Two versions of the dataset were developed, OffComBR-2 and OffComBR-3, with the difference being the size, as the latter retained from the former only the annota- tions agreed upon by all three experts.   2.3. HateBR  The work of [Vargas et al. 2022] presented the first large annotated corpus of offensive language in Instagram comments in Brazilian Portuguese. Motivated by the presence of hate speech on social media and the lack of studies on the subject in Portuguese, the project collected 7,000 Instagram comments, annotated by experts regarding the presence, degree, and category of offensiveness. The process involved data collection, selection of accounts of Brazilian political figures (three left-wing and three right-wing), and the selection of 30 posts from which 15,000 comments were extracted, with 7,000 being balanced between offensive and non-offensive.  The comments were labeled into three levels of offensiveness: whether they were offensive or not, the degree of offensiveness (mild, moderate, or high), and whether they contained hate speech, categorized into nine types such as xenophobia, racism, and ho- mophobia. From the 7,000 comments, 3,500 were offensive, with 778 highly offensive, 1,044 moderately offensive, and 1,678 mildly offensive. Among the offensive comments, 727 contained some type of hate speech. Table 1 presents samples of data from HateBR.  Table 1. Examples of comments extracted form the HateBR dataset.  Class  Offensive  Non-Offensive  With hate speech  Without hate speech  Comments  Essa besta humana é o câncer do País, tem que voltar para a jaula, urgentemente! E viva o Presidente Bolsonaro. Quem falou isso para você deputada? O Sergio Moro está aprovado pela maioria dos brasileiros. Vagabunda. Comunista. Mentirosa. O povo chileno não merece uma desgraça dessa. Pois é, deveria devolver o dinheiro aos cofres públicos do Brasil. Canalha.  Finally, after a detailed explanation of their entire annotation system, as well as evaluations to judge the annotations of each of the three experts and decide the most appropriate annotations for each comment, the study presents the test results with some ML models trained on the HateBR corpus, comparing the best result obtained with the best results of two other reference works. In this work, we seek to replicate the results obtained by HateBR, following the same training procedure and using the same models for comparison with our trained model.  2.4. OLID-BR  The work of [Trajano et al. 2023] also developed an annotated dataset of offensive com- ments in Portuguese, similar to HateBR. However, the main advantage of this dataset lies in its application to various NLP tasks, including binary classification of offensiveness, multi-category prediction of the type of toxicity, identification of targeted toxic comments, prediction of the target of toxicity, and identification of toxicity spans in comments.  The primary focus of the work was on the task of identifying toxicity spans, which involves detecting sequences of characters containing offensive language. To collect data, OLID-BR used various sources such as Twitter, YouTube, and other datasets with differ- ent annotation schemes.  The annotation was conducted in three stages: detection of offensive language, categorization of offensive language, and identification of the target of the offense. Com-   pared to HateBR, OLID-BR distinguishes between offensiveness against an individual, a group, or another type of target, while HateBR focuses on categorizing hate speech.  Data annotation in OLID-BR was not exclusively done by humans but also with the assistance of the Perspective API1, allowing human annotators to correct the classifi- cations. The entire corpus was divided into three datasets for training and testing, with a similar distribution of classifications in each. This work replicated the part of OLID-BR related to the identification of toxicity spans, using the code available on GitHub to train the model and apply it to the HateBR dataset for detecting offensive phrases and to the OLID-BR for identifying offensive spans.  3. Main Technologies For the development, training, and testing of techniques for offensive language detection and censorship, we primarily used two libraries available for the Python language for developing ML algorithms: Transformers and spaCy.  3.1. Transformers  The Transformers library is a Python tool that offers state-of-the-art architectures for NLP tasks, featuring over 32 pre-trained models in more than 100 languages. It provides deep interoperability between TensorFlow 2.0 and PyTorch. The library is named after the Transformer architecture introduced by Google Brain in 2017, which is based on the \"attention mechanism.\" This mechanism allows the model to focus on important parts of the input data, leading to superior performance in NLP tasks like sentence classification, named entity recognition (NER), and natural language generation compared to previous models like recurrent neural networks (RNNs) [Vaswani et al. 2017].  3.2. BERT and BERTimbau  The algorithm used in the first stage of our pipeline was a fine-tuned version BERTim- bau [Souza et al. 2020], a Brazilian model based on BERT (Bidirectional Encoder Rep- resentations from Transformers) [Devlin 2018]. BERT, introduced by Google in 2018, is a language model that generates numerical representations for words based on their surrounding context and is used for various NLP tasks. BERTimbau adapts BERT for Brazilian Portuguese using transfer learning, where a BERT model was trained on a Por- tuguese corpus (brWaC) [Wagner Filho et al. 2018] and evaluated on tasks like sentence similarity, textual entailment, and named entity recognition. In this work, BERTimbau was specifically used to develop a model for detecting offensive language in sentences.  3.3. SpaCy  an  for Python, written  open-source NLP library  in Cython SpaCy is that facilitates tasks like part-of-speech tagging, named en- [Honnibal et al. 2020], It provides pre-trained models and tity recognition (NER), and dependency parsing. allows users to train their own models for NER, where sentences are segmented into words, each categorized (e.g., nouns, adverbs, or specific problem-related categories like offensive and non-offensive). In this work, SpaCy was used in the second stage to detect which words in an offensive sentence are offensive, following the methodology of OLID-BR, which also used SpaCy for tasks like detecting offensive spans in text.  1https://www.perspectiveapi.com/   4. Methodology  This study focused on developing NLP models to detect offensive language in extended texts. Both the acquisition of training data and the actual censorship of words detected as offensive were carried out simplified due to them not being the primary focus. The de- velopment process consisted of four stages: data collection, cleaning/tokenization, model training, and result evaluation explained in the following subsection.  4.1. Data Collection  For data collection, we employ the HateBR [Vargas et al. 2022] and OLID-BR [Trajano et al. 2023] datasets to train the offensive content detection models, the for- mer to classify a text segment as potentially offensive or not and the latter to identify words or expressions that contain offensiveness. Additionally, to evaluate qualitatively our solution, we selected news articles from the G1 portal [Monteiro 2023], Catraca Livre [Leray 2023], and two blog posts from Senso Incomum [Trielli 2021, Martins 2022].  4.2. Data Cleaning and Feature Extraction  We used tools from the Transformers and spaCy libraries for data cleaning and feature ex- traction. Specifically, for the model trained with the Transformers library, we employed a tokenizer ready to transform sentences into numerical representations used by the model for calculations. For the model trained with spaCy, an embedded tokenization function- ality was available. In summary, in this step, the trained models were capable of cleaning the data and processing it without needing external code. For the training and evaluation of the first model (developed with the Transformers library), we compared it to the ML models presented by HateBR, which had the best results, using the same feature extrac- tion method they did: TF-IDF. TF-IDF (Term Frequency–Inverse Document Frequency) calculates how relevant a word in a corpus is to a text, obtained by the ratio between the number of times the term in question appears in one of the corpus texts (Term Fre- quency) and the frequency of appearances of this same term in the entire corpus (Inverse Document Frequency). The frequency of the word in a text refers to the ratio between the number of times it appears in the text and the number of words in the text, while the frequency of the word in the corpus is the count of how many times it appears in all texts of the dataset. The reason for using such feature extraction is due to the empirical re- sults demonstrated by HateBR [Vargas et al. 2022], which show that, in general, models trained with features extracted using TF-IDF outperformed other methods.  4.3. Model Training  The model training process was split into two parts: the first was responsible for detecting offensiveness in a text (in this case, in a selected paragraph), and the second was respon- sible for identifying words or expressions that contain offensive language in segments classified as offensive by the first model.  We employed the BERTimbau model from the HuggingFace platform for the first model. BERTimbau is a Brazilian language model developed by NeuralMind [Souza et al. 2020] through a technique called fine-tuning. In the case of this study, this process involved adding an extra layer of neurons at the end of the model, which was responsible for classifying an input text as offensive or non-offensive.   For the second model,  the training process used by the OLID-BR study [Trajano et al. 2023] was utilized to detect offensive spans (i.e., sequences of characters containing offensive language, not limited to isolated words but also including expres- sions and punctuation).  4.4. Model Demonstration  We evaluate the offensive language detection models using Precision, Recall, and F1- Score metrics. Precision measures how much we can trust a model when it predicts that an example belongs to a particular class by calculating the number of examples the model correctly predicted as belonging to that class divided by the total number of examples it predicted as belonging to that class. Recall is the number of samples the model correctly identified as belonging to a class divided by the total number of samples that belong to that class in the data. F1-Score is the harmonic mean between precision and recall, i.e., it is the average of both precision and recall values, giving more importance to low values, as a much lower precision or recall value indicates that the model is not balancing these two metrics well when we want to give equal importance to both.  To demonstrate the models’ effectiveness, we conducted qualitative tests on se- lected texts, as long as they were at least one page long or had more than one paragraph.  For the application process of the trained models, Figure 1 presents the follow- ing steps graphically: (1) a large text is collected; (2) the text is divided into fragments based on the characters of periods, commas, semicolons, exclamations, questions, and new lines; (3) for each fragment, the BERTimbau-based model is used to check if it is offensive; (4) if not offensive, the fragment is returned as usual, but if it is, it proceeds to the next step; (5) the spaCy-trained model is used to identify the offensive spans; (6) returning of the offensive censored spans, and the censored version of the fragment; (7) fi- nally, all fragments, censored or not, are reassembled using the same separators as before, thus returning the entire text now with the appropriate censorship.  Figure 1. Censorship process of our proposal.   5. Results The results obtained from the training sessions, in terms of Precision, Recall, and F1- score evaluated across the four different models tested for toxicity detection in sentences, are presented in Table 2. The parameters for conducting the training and testing followed the proposal in the HateBR study [Vargas et al. 2022]. Since HateBR does not provide the original pre-trained models and also the train/validation/test split configuration, it was necessary to retrain each of the four models on this dataset to guarantee consistency be- tween the results: SVM, Naive Bayes, Logistic Regression, and MLP models. We also evaluated our fine-tuned version of the BERTimbau model [Souza et al. 2020], following the TF-IDF method for feature extraction. For training the BERTimbau-based model, we utilized the tokenizer that comes pre-built with the model to perform feature extraction, as this is how BERT-based models operate.  Model SVM NaiveBayes LogisticRegression MLP BERTimbau + fine-tuning  Precision Recall 0.84 0.87 0.85 0.82 0.87  0.87 0.85 0.86 0.86 0.92  F1-score 0.86 0.86 0.85 0.84 0.90  Table 2. [Vargas et al. 2022].  Comparison of  trained models on the HateBR dataset  The data analysis shows that the BERTimbau + fine-tuning model employed in this study outperforms previous results in all comparison parameters, establishing this work as the state-of-the-art for the HateBR dataset. This result underscores the importance of using DL models in the NLP context, as they can yield significant benefits in text recognition and classification processes.  Having completed the test of the BERTimbau model and trained the spaCy model following the same methodology as OLID-BR [Trajano et al. 2023], we conducted the final evaluation of the complete pipeline. For this purpose, we executed the pipeline proposed in Figure 1, i.e., if a sentence is toxic, the offensive parts are detected and censored. For presentation and qualitative evaluation purposes, censorship is performed by simply replacing the characters that constitute the offensive word or expression with asterisks (*).  We selected some news articles from specialized portals, such as G1, Estadão, and Catraca Livre, to perform a preliminary test, but no offensive language was detected. Subsequently, an opinion article about Chico Buarque from the Senso Incomum blog [Martins 2022] was used, where the presence of swear words, which would be appropri- ately detected, was evident.  Table 3 presents examples for qualitative analysis of the results of applying the proposed pipeline in this work. The Table shows the comparison between the original text and the censored text. The demonstrated text is the only post from a collected blog, as the three obtained news articles contained no offensive language. These results are limited to demonstrating the parts of the original text that our algorithm censored.  One consideration is that the model censored the word “censura” (in english cen- sorship), which is usually not considered offensive, as well as the expression “Rock das   Censored Part observador de *******  Original Text Part observador de bonobos A autocensura é o pior tipo de censura que existe A auto******* é o pior tipo de ******* que existe Joga pedra na Geni/Joga bosta na Geni/Ela é feita Joga pedra na Geni/Joga ***** na Geni/Ela é feita pra apanhar/Ela é boa de cuspir/Ela dá pra qual- pra ****************************Ela dá pra qualquer um/Maldita Geni quer um/Maldita Geni! seu lesbofóbico “Rock das Aranhas”! seu lesbofóbico ****************** Paga-pau dos porcos estadunidenses, com toda **********************************, toda certeza! certeza! Espumando de ódio Espumando de ****  com  Table 3. Examples of original text excerpts (on the left) and censored excerpts (on the right).  Aranhas” (Rock of the Spiders) instead of the previous word “lesbofóbico” (lesbopho- bic), which we judged to be far more offensive than an expression about rock and spiders. The remaining censorships we considered appropriate.  5.1. Source Code of the Experiments  The source code developed has been made publicly available in the form of Notebooks, available on the GitHub Repository: https://github.com/ICDI/censorship- offensive-language:  • The notebook for training and testing the model based on the BERTimbau model; • The notebook for training the spam detection model with spaCy, reusing the code  from OLID-BR  • The notebook with the tests using both models for text detection and censorship.  6. Conclusion  This work developed an ML/DL-based program for detecting and censoring offensive language in extended Portuguese texts extracted from the web. To this end, we present a pipeline comprising two parts: one that detects the offensiveness in a portion of text (pre- cisely a sentence) and another that detects the offensive parts (spam) within the sentence. This allows for censoring a large text part by part and returning the censored text.  A positive aspect of this work is providing a program specifically focused on de- tecting and censoring large texts, presenting satisfactory results in quantitative and qual- itative analyses. As an evolution of this work, developing a specific dataset for large text censorship can be envisaged, which, unlike the datasets used [Vargas et al. 2022] and [Trajano et al. 2023], would represent a significant advancement.  The results for detecting offensiveness in texts were superior to our original refer- ence (the HateBR article), representing a new benchmark concerning the state-of-the-art, surpassing the techniques used as a reference in the HateBR dataset [Vargas et al. 2022]. The censorship part was performed simply by replacing the characters in the offensive parts with asterisks, which can undoubtedly be improved by future work. The analysis could have been more robust due to the lack of a specific dataset for large texts, even when discussing qualitative data. This fact demonstrates the need for creating a specific dataset built from expert judgments on offensive language or by calculable metrics, which can evolve in future works.   "
        },
        {
            "titulo": "Classificação de Notícias em Português Utilizando Modelos Baseados em Transferência de Aprendizagem e Transformers",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31133",
            "idioma": "Português",
            "storage_key": "files/article_31133_30936.pdf",
            "autores": [
                {
                    "nome": "Wagner Narde",
                    "afiliacao": "Grupo Energisa",
                    "orcid": "http://orcid.org/0009-0003-7896-5232"
                },
                {
                    "nome": "João Mendanha",
                    "afiliacao": "UFOP",
                    "orcid": null
                },
                {
                    "nome": "Henrique Barbosa",
                    "afiliacao": "UFMG",
                    "orcid": null
                },
                {
                    "nome": "Frederico Coelho",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0000-0002-7868-6968"
                },
                {
                    "nome": "Bruno Santos",
                    "afiliacao": "UFBA",
                    "orcid": "https://orcid.org/0000-0003-4501-2323"
                },
                {
                    "nome": "Luiz Torres",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0002-4991-8395"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Fake news se espalha mais rápido em algumas redes sociais do que notícias regulares, o que pode ter diferentes consequências, desde influências nos resultados eleitorais até mortes devido a tratamentos incorretos de doenças. Este trabalho tem como objetivo empregar métodos baseados em aprendizado por transferência e modelos de aprendizado de máquina baseados em Transformers para classificar a veracidade de tweets na língua portuguesa (Brasil pt-BR). Para isso, foi criada uma base de dados confiável e rotulada, aberta para acesso gratuito. O conjunto de dados relaciona postagens extraídas do X (anteriormente conhecido como Twitter) e sua proximidade com fatos ou informações falsas. Subsequentemente, cinco modelos Transformer foram treinados em português. O modelo BERT ajustado, inicializado com pré-treinamento em textos em português, alcançou um desempenho superior, obtendo uma acurácia de 95.1%.",
            "keywords": [
                "Transformers",
                "Classificação",
                "Bert",
                "Aprendizado Supervisionado",
                "Notícias Falsas"
            ],
            "referencias": [
                "Data, P. (2024). Global social media users in 2024.",
                "DataReportal (2024). Social media users 2024 (global data & statistics).",
                "Gente, G. (2024). Pandemia e o consumo de notícias nas redes sociais.",
                "Henrique, J. (2018). Get old tweets programatically. Repository on GitHub.",
                "NegociosSC (2024). O uso da internet, redes sociais e mídia no brasil em 2024."
            ],
            "artigo_completo": "Resumo. Fake news se espalha mais r´apido em algumas redes sociais do que not´ıcias regulares, o que pode ter diferentes consequˆencias, desde influˆencias nos resultados eleitorais at´e mortes devido a tratamentos incorretos de doenc¸as. Este trabalho tem como objetivo empregar m´etodos baseados em aprendizado por transferˆencia e modelos de aprendizado de m´aquina baseados em Transfor- mers para classificar a veracidade de tweets na l´ıngua portuguesa (Brasil pt- BR). Para isso, foi criada uma base de dados confi´avel e rotulada, aberta para acesso gratuito. O conjunto de dados relaciona postagens extra´ıdas do X (ante- riormente conhecido como Twitter) e sua proximidade com fatos ou informac¸ ˜oes falsas. Subsequentemente, cinco modelos Transformer foram treinados em por- tuguˆes. O modelo BERT ajustado, inicializado com pr´e-treinamento em textos em portuguˆes, alcanc¸ou um desempenho superior, obtendo uma acur´acia de 95.1%.  1. Introduc¸ ˜ao  Os ve´ıculos de comunicac¸ ˜ao de grande circulac¸ ˜ao por muito tempo foram jornais, revis- tas, r´adio e televis˜ao. Hoje, not´ıcias circulam atrav´es de v´ıdeos no YouTube, portais de not´ıcia e em redes sociais como Facebook, X (Antigo Twitter) e WhatsApp. Tornando a Internet, um dos principais meios de comunicac¸ ˜ao e consumo de not´ıcias. No Brasil, 65% usam a Internet e suas aplicac¸ ˜oes como principais fontes de informac¸ ˜ao, nos Estados   Unidos 53% e, no mundo, o n´umero estimado ´e de 62% [NegociosSC 2024, Gente 2024, DataReportal 2024, Data 2024]. O que, por um lado, demonstra que a Internet ampliou o acesso `a informac¸ ˜ao, mas, por outro lado, tamb´em transformou a forma como as not´ıcias s˜ao consumidas e compartilhadas.  A ascens˜ao da Internet e, consequentemente, das redes sociais democratizaram a produc¸ ˜ao de not´ıcias, permitindo que qualquer pessoa assuma o papel de produtor de conte´udo sem a supervis˜ao tradicional de jornalistas. Este fenˆomeno pode ter impactado negativamente na qualidade das informac¸ ˜oes disseminadas, resultando em um aumento de not´ıcias que propagam desinformac¸ ˜ao ou divulgam informac¸ ˜oes falsas [Reis et al. 2019, Vargas et al. 2021].  2. Proposta de Modelo  Neste trabalho, propomos um modelo baseado em transferˆencia de aprendizagem, trans- formers e aprendizagem supervisionada para classificar textos em portuguˆes nas redes sociais, com foco na plataforma X. Tamb´em criamos uma base de dados em portuguˆes1 (162 amostras e balanceada), que relaciona textos da plataforma X com sua veracidade, visando melhorar a detecc¸ ˜ao de fake news e promover a qualidade da informac¸ ˜ao nas redes sociais.  3. Metodologia  O primeiro passo foi a construc¸ ˜ao de uma base de dados contendo postagens de usu´arios da rede social X (Antigo Twitter). Foi utilizada a ferramenta Get Old Tweets (GOT) [Henrique 2018] para coletar tweets hist´oricos, incluindo not´ıcias falsas. Dessa forma, cada tweet foi analisado e classificado manualmente para determinar sua proximidade com o fato, assegurando que o conjunto de dados fosse rigoroso e preciso. Esse processo permitiu a criac¸ ˜ao de um conjunto de dados robusto e rotulado, essencial para o treina- mento e validac¸ ˜ao eficazes do modelo de classificac¸ ˜ao de textos em portuguˆes proposto. Em seguida, os textos coletados passam por uma fase de preparac¸ ˜ao, onde s˜ao inicial- mente pr´e-processados e, posteriormente, rotulados. Ap´os esse processo, os dados s˜ao ajustados para servirem como entradas adequadas para os modelos de aprendizagem de m´aquina.  3.1. Conjunto de dados: Coleta e Processamento  Este trabalho utiliza dados textuais em portuguˆes extra´ıdos da plataforma de rede social X. A plataforma permite a extrac¸ ˜ao de informac¸ ˜oes atrav´es de sua Application Program- ming Interface (API). Com a rede social selecionada, iniciou-se a coleta de dados para compor a base de dados. O processo de obtenc¸ ˜ao dos tweets consistiu em buscar no site de checagem de not´ıcias verdadeiras ou falsas (LUPA2) e pesquisar por elas utilizando a ferramenta GOT [Henrique 2018]. Para isso, foram realizadas filtragens de not´ıcias e alinhamento temporal aproximado para obtenc¸ ˜ao das postagens realizadas sobre a not´ıcia verificada.  1https://github.com/WagnerNarde/ML-Transformers-Tweets-falsos 2https://lupa.uol.com.br/   3.2. Selec¸ ˜ao dos Modelos de Aprendizagem  Neste trabalho, foram adotados modelos de aprendizagem baseados em Transformers. Originalmente desenvolvido para traduc¸ ˜ao autom´atica, o Transformer se destacou por sua capacidade de capturar relac¸ ˜oes de dependˆencia de longo alcance de forma efi- caz. Buscou-se por modelos Transformers que receberam pr´e-treinamento em por- tuguˆes, visando aproveitar a transferˆencia de aprendizagem. Como resultado, optou-se por ajustar os seguintes modelos: BERT base pr´e-treinado em portuguˆes brasileiro por [Souza et al. 2020]; BERT base pr´e-treinado em 104 idiomas, incluindo portuguˆes, por [Devlin et al. 2019]; RoBERTa pr´e-treinado por [Liu et al. 2019] com um corpus de 6,9 milh˜oes de frases em portuguˆes; XLM-R base, pr´e-treinado por [Conneau et al. 2020] incluindo portuguˆes; e, por fim, o modelo ELECTRA uncased em 100 idiomas, [Clark et al. 2020], pr´e-treinado especificamente em portuguˆes.  3.3. Treinamento  Para avaliar a capacidade de generalizac¸ ˜ao do modelo, foi utilizado o m´etodo de validac¸ ˜ao cruzada com 10 partic¸ ˜oes (10-fold cross-validation). Este m´etodo divide o conjunto de dados em 10 sub-conjuntos. Cada subconjunto ´e usado uma vez como conjunto de teste, enquanto os restantes s˜ao usados como conjunto de treinamento. Esse processo ´e repetido 10 vezes, garantindo que cada amostra do conjunto de dados seja utilizada para testes ao menos uma vez. Esse procedimento n˜ao apenas melhora a capacidade de generalizac¸ ˜ao do modelo, mas tamb´em fornece uma estimativa mais robusta do desempenho do modelo em dados n˜ao vistos.  4. Resultados  As m´etricas de avaliac¸ ˜ao incluem Acur´acia, F1-Score, Precis˜ao, Sensibilidade (Recall) e MCC.  Tabela 1. Resultados obtidos em cada modelo ´Epocas Acur´acia  Modelo ELECTRA (uncased) RoBERTa pr´e-treinado em Portuguˆes XLM-R pr´e-treinado em multi-idiomas BERT com Pr´e-treinamento em Portuguˆes BERT com Pr´e-treinamento Multi-idioma  10 7 9 6 10  0.864 0.901 0.903 0.944 0.914  F1 0.848 0.897 0.898 0.955 0.918  Precis˜ao Sensibilidade MCC 0.720 0.812 0.804 0.887 0.825  0.883 0.852 0.883 0.944 0.900  0.824 0.962 0.922 0.971 0.944  Os resultados deste trabalho, apresentados na Tabela 1, mostram que cada mo- delo de aprendizado de m´aquina treinado para a classificac¸ ˜ao de not´ıcias em portuguˆes teve um desempenho variado, dependendo do n´umero de ´epocas e das caracter´ısticas do pr´oprio modelo. O ELECTRA (uncased), treinado com 10 ´epocas, apresentou o desem- penho mais baixo, o que pode ser atribu´ıdo `a falta de diferenciac¸ ˜ao entre letras mai´usculas e min´usculas, bem como `a qualidade dos pesos de pr´e-treinamento dispon´ıveis. O mo- delo RoBERTa pr´e-treinado em Portuguˆes, configurado com 7 ´epocas, superou o ELEC- TRA, beneficiando-se de uma arquitetura que captura melhor as nuances lingu´ısticas do portuguˆes e diferencia entre mai´usculas e min´usculas. O modelo XLM-R pr´e-treinado em multi-idiomas, com 9 ´epocas, demonstrou uma leve superioridade em relac¸ ˜ao ao Ro- BERTa em termos de acur´acia e F1, aproveitando o conhecimento adquirido em m´ultiplos J´a o BERT com Pr´e-treinamento idiomas para melhorar a compreens˜ao semˆantica.   Multi-idioma, utilizando 10 ´epocas, mostrou robustez com acur´acia e F1 acima de 0.9, destacando-se pela capacidade de transferir conhecimento lingu´ıstico de um corpus mul- tilingue para o portuguˆes. Por fim, o BERT pr´e-treinado em Portuguˆes foi o modelo com melhor desempenho geral, utilizando apenas 6 ´epocas de treinamento. Este modelo se destacou na classificac¸ ˜ao correta das not´ıcias, com acur´acia, F1 e MCC superiores, evi- denciando a efic´acia do pr´e-treinamento espec´ıfico em portuguˆes e a importˆancia do ajuste fino dos hiperparˆametros para maximizar a efic´acia do modelo em tarefas espec´ıficas de classificac¸ ˜ao de texto.  5. Discuss˜ao  Os modelos ELECTRA uncased pr´e-treinado em Portuguˆes e RoBERTa pr´e-treinado em Portuguˆes apresentaram resultados abaixo do esperado, pode-se levantar a quest˜ao de que se tais modelos passaram pelo mesmo processo de pr´e-treinamento dos outros m´etodos. O modelo RoBERTa exige mais recursos computacionais comparado com o Bert, al´em de ser um aprimoramento do mesmo, portanto, melhores resultados eram esperados desse modelo. O modelo ELECTRA sendo um modelo uncase, esperava-se um desempenho abaixo dos outros classificadores pr´e-treinados exclusivamente em portuguˆes. Ainda as- sim, acredita-se que o modelo n˜ao conseguiu generalizar bem o problema.  O XLM-R foi um modelo originalmente proposto para a traduc¸ ˜ao de idiomas, por isso ele est´a dispon´ıvel em vers˜ao multi-idiomas pr´e-treinado em v´arios idiomas, inclusive portuguˆes. Apesar do XLM-R n˜ao ter sido originalmente proposto para classificac¸ ˜ao de texto, ele obteve resultados melhores que o ELECTRA.  O Modelo BERT com Pr´e-treinamento em Portuguˆes obteve acur´acia e F1 superi- ores a todos os outros modelos, mostrando que o pr´e-treinamento em portuguˆes feito por [Souza et al. 2020] foi muito eficiente e contribuiu positivamente para o bom desempenho do modelo. Os resultados preliminares mostraram que o modelo foi capaz de classificar noticias de uma base de dados relativamente pequena, bases de dados com poucas amos- tras ´e um desafio em algumas ´areas, como na sa´ude.  6. Conclus˜oes  Este trabalho apresentou uma abordagem para detecc¸ ˜ao de tweets falsos em portuguˆes atrav´es de NLP. Al´em disso, foi criada e disponibilizada uma base de dados balanceada com tweets classificados de forma confi´avel. A base possibilitou o treinamento de mo- delos para detecc¸ ˜ao de not´ıcias falsas. Sendo que o modelo BERT com 6 ´epocas foi o melhor comparado aos outros modelos testados.  7. Trabalhos Futuros  Na continuac¸ ˜ao do trabalho, pretendemos estender a avaliac¸ ˜ao comparativa com outros modelos estado da arte da literatura de classificac¸ ˜ao de texto baseados em aprendizado profundo. Pretende-se aumentar a base de dados com mais dados rotulados, mantendo a confiabilidade, e tamb´em buscar dados de outras fontes. Al´em de mostrar os resultados da classificac¸ ˜ao de not´ıcias verdadeiras, planeja-se apresentar tamb´em os resultados de classificac¸ ˜ao das not´ıcias falsas, assim como utilizar outras estrat´egias para o treinamento, como a validac¸ ˜ao cruzada com 5 partic¸ ˜oes.   Referˆencias  [Clark et al. 2020] Clark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. (2020). ELEC- TRA: Pre-training text encoders as discriminators rather than generators. In ICLR.  [Conneau et al. 2020] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm´an, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). Unsupervi- sed cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451, Online. Association for Computational Linguistics.  [Data 2024] Data, P. (2024). Global social media users in 2024. Accessed: 2024-06-28.  [DataReportal 2024] DataReportal (2024). Social media users 2024 (global data & statis-  tics). Accessed: 2024-06-28.  [Devlin et al. 2019] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: In Pro- Pre-training of deep bidirectional transformers for language understanding. ceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computa- tional Linguistics.  de [Gente 2024] Gente, https://gente.globo.com/ not´ıcias nas pandemia-e-o-consumo-de-noticias-nas-redes-sociais/. Aces- sado em 28 de junho de 2024.  G. redes  Pandemia  consumo  sociais.  (2024).  o  e  [Henrique 2018] Henrique, J. (2018). Get old tweets programatically. Repository on  GitHub.  [Liu et al. 2019] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.  [NegociosSC 2024] NegociosSC (2024).  brasil  em 2024.  no o-uso-da-internet-redes-sociais-e-midia-no-brasil-em-2024/. Acessado em 28 de junho de 2024.  O uso da internet,  redes sociais e m´ıdia https://www.negociossc.com.br/blog/  [Reis et al. 2019] Reis, J. C. S., Correia, A., Murai, F., Veloso, A., and Benevenuto, F. IEEE Intelligent Systems,  (2019). Supervised learning for fake news detection. 34(2):76–81.  [Souza et al. 2020] Souza, F., Nogueira, R., and Lotufo, R. (2020). BERTimbau: pretrained In 9th Brazilian Conference on Intelligent  BERT models for Brazilian Portuguese. Systems, BRACIS, Rio Grande do Sul, Brazil, October 20-23 (to appear).  [Vargas et al. 2021] Vargas, F., Benevenuto, F., and Pardo, T. (2021). Toward discourse- In Proceedings of the Student  aware models for multilingual fake news detection. Research Workshop Associated with RANLP 2021, pages 210–218.   "
        },
        {
            "titulo": "Automatic Annotation of Enhanced Universal Dependencies for Brazilian Portuguese",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31134",
            "idioma": "Inglês",
            "storage_key": "files/article_31134_30937.pdf",
            "autores": [
                {
                    "nome": "Elvis A. de Souza",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0001-9373-7412"
                },
                {
                    "nome": "Magali S. Duran",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-3843-4600"
                },
                {
                    "nome": "Maria das Graças V. Nunes",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-2776-6140"
                },
                {
                    "nome": "Gustavo Sampaio",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0009-0008-9067-8400"
                },
                {
                    "nome": "Giovanna Belasco",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0009-0007-1737-3312"
                },
                {
                    "nome": "Thiago A. S. Pardo",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2111-1319"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "This paper presents the first attempt to automatically annotate Enhanced Universal Dependencies for Brazilian Portuguese. We use a symbolic annotation system, based on graph rewriting rules, and modify its original rules to better suit the linguistic characteristics of Portuguese using a manually annotated sample from the journalistic portion of Porttinari treebank as ground truth. Our objective is to assess the performance of the automatic annotation for a novel language and to determine the extent of possible improvements through rule modifications. Results demonstrate significant performance enhancements, where linguistic-driven rule adjustments improved the annotation accuracy 11.38 points, achieving 96.05% F1-score.",
            "keywords": [
                "Enhanced Dependencies",
                "Universal Dependencies",
                "syntactic annotation",
                "corpus annotation",
                "graph rewriting"
            ],
            "referencias": [
                "Bai, J., Wang, Y., Chen, Y., Yang, Y., Bai, J., Yu, J., and Tong, Y. (2021). Syntax-BERT: Improving pre-trained transformers with syntax trees. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3011–3020.",
                "Bölücü, N., Rybinski, M., and Wan, S. (2023). Investigating the impact of syntax-enriched transformers on quantity extraction in scientific texts. In Proceedings of the Second Workshop on Information Extraction from Scientific Publications, pages 1–13, Bali, Indonesia.",
                "Bouma, G., Seddah, D., and Zeman, D. (2020). Overview of the iwpt 2020 shared task on parsing into enhanced universal dependencies. In 58th Annual Meeting of the Association for Computational Linguistics.",
                "Bouma, G., Seddah, D., and Zeman, D. (2021). From raw text to enhanced universal dependencies: The parsing shared task at iwpt 2021. In Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021), pages 146–157.",
                "Candido, A., Maziero, E., Specia, L., Gasperin, C., Pardo, T., and Aluisio, S. (2009). Supporting the adaptation of texts for poor literacy readers: a text simplification editor for Brazilian Portuguese. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 34–42, Boulder, Colorado.",
                "De Marneffe, M.-C., Dozat, T., Silveira, N., Haverinen, K., Ginter, F., Nivre, J., and Manning, C. D. (2014). Universal stanford dependencies: A cross-linguistic typology. In LREC, volume 14, pages 4585–4592.",
                "Duran, M., Lopes, L., Nunes, M. G. V., and Pardo, T. (2023). The dawn of the porttinari multigenre treebank: Introducing its journalistic portion. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 115–124, Porto Alegre, RS, Brasil. SBC.",
                "Duran, M. S. (2024). Anotação de enhanced dependencies. Disponível em:",
                ". Acesso em: 10 out. 2024.",
                "Guillaume, B. and Perrier, G. (2021). Graph rewriting for enhanced universal dependencies. In IWPT 2021-17th International Conference on Parsing Technologies.",
                "Lin, Y., Wang, C., Song, H., and Li, Y. (2021). Multi-head self-attention transformation networks for aspect-based sentiment analysis. IEEE Access, 9:8762– 8770.",
                "Nivre, J., De Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajic, J., Manning, C. D., McDonald, R., Petrov, S., Pyysalo, S., and Silveira, N. (2016). Universal dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 1659–1666.",
                "Nivre, J., de Marneffe, M.-C., Ginter, F., Hajic, J., Manning, C. D., Pyysalo, S., Schuster, S., Tyers, F., and Zeman, D. (2020). Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4034–4043.",
                "Oliveira, L., Claro, D. B., and Souza, M. (2023). Dptoie: a portuguese open information extraction based on dependency analysis. Artificial Intelligence Review, 56(2):7015–7046.",
                "Pagano, A. S., Duran, M. S., and Pardo, T. A. S. (2023). Enhanced dependencies para o português brasileiro. In Proceedings of the 2nd Edition of the Universal Dependencies Brazilian Festival, pages 461–470.",
                "Schuster, S. and Manning, C. D. (2016). Enhanced english universal dependencies: An improved representation for natural language understanding tasks. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 2371–2378.",
                "Shi, T. and Lee, L. (2021). TGIF: Tree-graph integrated-format parser for enhanced UD with two-stage generic- to individual-language finetuning. In Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021), pages 213–224.",
                "Zhou, J., Zhang, Z., Zhao, H., and Zhang, S. (2020). LIMIT-BERT: Linguistics informed multi-task BERT. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4450–4461."
            ],
            "artigo_completo": "Abstract. This paper presents the first attempt to automatically annotate En- hanced Universal Dependencies for Brazilian Portuguese. We use a symbolic annotation system, based on graph rewriting rules, and modify its original rules to better suit the linguistic characteristics of Portuguese using a manually an- notated sample from the journalistic portion of Porttinari treebank as ground truth. Our objective is to assess the performance of the automatic annota- tion for a novel language and to determine the extent of possible improvements through rule modifications. Results demonstrate significant performance en- hancements, where linguistic-driven rule adjustments improved the annotation accuracy 11.38 points, achieving 96.05% F1-score.  1. Introduction Morphological and syntactic annotation have shown to be relevant for several Natural Language Processing (NLP) initiatives. For instance, tasks of open information extrac- tion (Oliveira et al. 2023) and text simplification (Candido et al. 2009) may directly base their decisions on syntax. Considering the more recent trends of Large Language Models, several works have demonstrated improvements in results when linguistic knowledge is provided (Zhou et al. 2020; Bai et al. 2021; Lin et al. 2021; B¨ol¨uc¨u et al. 2023). On the linguistic perspective, linguistic annotation may help describing varied language phenom- ena, possibly supporting the validation and/or proposal of new theories.  Universal Dependencies (UD) is a framework for the morphological, morphosyn- tactic and syntactic annotation of human languages. UD provides standardized guidelines and has been used to annotate over 283 treebanks for 161 languages, being widely adopted as it proposes consensual annotation decisions and allows comparative and multilingual efforts. Concerning the syntactic annotation, the UD framework supports two levels of depth: basic dependency trees and enhanced graphs. Basic dependency trees provide in- formation on syntactic dependencies, where each token is connected to a governing (head) token through a relation (e.g., in the sentence The boy cried, “boy” is connected as subject to the head “cried” by a nsubj relation). Enhanced Universal Dependencies (EUD) gener- ally build upon the basic dependencies by adding relations and nodes (or tokens) to make explicit the implicit relationships between tokens (Nivre et al. 2020) (e.g., in Figure 1, “boy” is also connected to “left” by a nsubj enhanced relation, as it is shared by the verbs “cried” and “left”). This enhancement can facilitate NLP tasks by providing additional information.   Figure 1. EUD annotation – the red nsubj dependency is a new EUD dependency.  Figure 2. EUD annotation – relation extended with the lexical item “with”.  This paper investigates the issue of EUD annotation for Brazilian Portuguese. To the best of our knowledge, this is the first evaluation of EUD annotation for this lan- guage. Following two previous shared tasks on EUD annotation (Bouma et al. 2020; Bouma et al. 2021), which did not include Portuguese, we build upon one of the systems that participated in the 2021 task, namely Grew (Guillaume and Perrier 2021), based on graph rewriting rules for annotated syntactic trees. This symbolic system comes with a set of original (and universal) rules, and we made a series of modifications based on corpus investigation, generating an improved set of rules. The two sets of rules were ap- plied to a sample dataset from the journalistic portion of Porttinari (Duran et al. 2023), a Portuguese treebank available in the Universal Dependencies project catalog, which we manually enriched with EUD annotation to assess the quality of the automatic annotation. Therefore, our objective is to verify the performance of the program’s original rules for Portuguese and how much we can improve it with modified rules.  In the end, we discuss persistent annotation errors and future perspectives on EUD automatic annotation. As an additional contribution, the rules and the annotated data are also made available to the interested reader.  2. Related Work EUDs present significant challenges compared to traditional UD annotation. In addi- tion to the UD website, where the guidelines are updated as needed, there is a series of works discussing the relevance and explaining the application of this type of annotation in treebanks (De Marneffe et al. 2014; Nivre et al. 2016; Schuster and Manning 2016; Nivre et al. 2020). The instantiation of these relations for Portuguese was introduced and detailed in (Pagano et al. 2023). Overall, EUDs may include 6 annotation situations:  1. Inclusion of the prepositions, coordinating conjunctions, and subordinating con- junctions lemmas in the label of the relations they introduce (as in Figure 2); 2. Identification of the controlling subject of the null subject in xcomp clauses (as in  Figure 3);  Figure 3. EUD annotation – nsubj relation for a verb dependent of xcomp.   Figure 4. EUD annotation – obj relation propagated to the dependent of conj.  Figure 5. EUD annotation – “book” is the object of “read” and “that” is ref of “book”.  3. Propagation, to the dependent of conj, of the relation that reaches the head of conj  (as in Figure 4);  4. Propagation, to the dependent of conj, of some relations that depart from the head  of conj (as in Figure 1);  5. Replacement of the relative pronoun in relative clauses with its antecedent, mark- ing the relationship of the relative pronoun with its antecedent with a label exclu- sive to the EUD: ref (as in Figure 5);  6. Insertion of an empty token to take the place of an elliptical predicate and estab- lishment of relationships of this empty token with the participants of the orphan relation (as in Figure 6).  While UD trees are simple hierarchical structures with a root, EUD graphs are connected and can contain cycles. For example, in Figure 5, the node “book” is depen- dent of “read” in a obj relation, however, it is also governor of “read” in a relative clause relation (acl:relcl), a basic syntactic annotation that is kept in the enhanced graph, estab- lishing a cycle between two nodes. Another challenge is that some relations are lexical- ized (as in Figure 2), considerably increasing the set of labels to be predicted and making them language-dependent. Additionally, a token can have more than one enhanced rela- tion, having multiple governors, and there may be additional empty tokens to represent elliptical predicates (Bouma et al. 2020). In Figure 1, the node “boy” has two governors: the verbs “cried” and “left”, which are coordinated, while in the basic annotation only the first verb would be its governor. In Figure 6, an empty token, [has], has been added to the EUD graph to solve the elliptical predicate issue, and several dependencies were changed to fit this new token.  The shared tasks held at IWPT in 2020 (Bouma et al. 2020) and in 2021 (Bouma et al. 2021) provided a platform for comparing results among different systems. To date, there is no treebank annotated with EUD for the Portuguese language, meaning  Figure 6. EUD annotation – an empty token [has] was inserted to account for an elliptical predicate.   that the language has never been subjected to any attempt of automatic annotation. To participate in the competition, a treebank did not need to have all six types of EUD; here, we are testing a rule-based approach on a fully annotated Portuguese dataset with all six types of EUD, produced for the purpose of this work.  The system we chose to use, Grew, ranked seventh in the 2021 competition, with 81.58% ELAS (a F1-score over EUD relations), being the best ranked symbolic-based system1. Our goal is to test the possibilities and limitations of a linguistically-driven rule-based approach, which can be constructed with linguistic supervision, being easily applied for other languages as well, without training, and with high interpretability.  3. Methodology  We use two small gold-standard EUD sets: one for testing (gold-test) and one for devel- opment (gold-dev). The gold-dev set was drawn from Porttinari-base, the main portion of Porttinari, while gold-test was sourced from Porttinari-test, designed for evaluating auto- matic annotation systems (Duran et al. 2023). Gold-dev comprises 100 manually selected sentences, chosen by a linguist to represent challenging EUD phenomena. In contrast, the 100 test sentences were randomly selected to reflect the natural frequency of phenomena in Porttinari. Due to (intentional) differing selection methods, dev and test sets show dis- parities, e.g., the dev set contains 23 sentences with the orphan label (predicate ellipsis), whereas the test set includes only two.  We begin our work analyzing Grew (Guillaume and Perrier 2021) original rules for EUD, referred to as “original rules”, which are universal and ideally applicable to any language. We observed the annotation results on the development set and, as errors were identified, we created new rules and modified existing ones to address these deficiencies. Notably, none of the sentences from the test set influenced rule modifications. As a result of the process, we have the rule set named “modified rules.”  Our evaluation focuses on the program’s overall F1-score (also named ELAS, i.e., labeled-attachment score over enhanced dependencies), as well as F1-score for each of the 6 EUD types. To achieve this, we automatically classified each enhanced relation into one of the 6 categories using linguistic rules. For example, we know, from Figure 3, that nsubj relations from verbs that are xcomp dependents towards nominals, when the nominal also has a nsubj relation coming from the verb that is the xcomp governor, are relations of the type “assignment of xcomp subjects”.  Grew rules consist of patterns (that may involve any UD annotation information) to be identified in sentences and a set of commands to be executed when these patterns are found. These rules are incorporated into a mechanism known as a “strategy,” which allows for the control of which rules are applied for each language and in which order. For instance, the resolution of predicate ellipses should be done first, as other rules related to the propagation of dependents of coordinated elements can be applied considering the empty token inserted in the sentence.  In Table 1, we find the number of rules for each type of EUD relation (1-6) in Grew rule set, according to our automatic identification of EUD types, plus our new rules (7). There are also “unclassified” rules, as they do not produce any visible changes to  1The system is 7.66 points below the system that ranked first, TGIF (Shi and Lee 2021).   a sentence, but rather implicit changes that are going to be used for other rules inside a Grew strategy. Besides the new rules, some of the original rules were modified, and they will be seen in the Results section, where we consider how many times the rules for each EUD type have been applied before and after our modifications.  EUD Types  Number of Rules  1 - Addition of prepositions and conjunctions 2 - Assignment of xcomp subjects 3 - Propagation of conj head 4 - Propagation of conj dependents 5 - Annotation of relative pronoun referent 6 - Inclusion of elliptical predicate 7 - New rules  Unclassified rules  3 11 18 8 13 23 15  64  Table 1. Number of rules related to each EUD type  4. Results  Both the test and dev samples were manually annotated for EUD. Table 2 presents a description of these corpora, as well as the distribution of each of the EUD types. The number of EUD relations in this section ignores relations that are simple replicas of basic relations without any modifications, as well as punctuation relations. “More than one classification” refers to relations that were classified as result of more than one EUD type in action; “Unclassified” refers to the few relations that could not be correctly classified as one of the six EUD types using our automatic type identification rules.  Sentences Tokens EUD Relations Sentences with elliptical predicates  1 - Addition of prepositions and conjunctions 2 - Assignment of xcomp subjects 3 - Propagation of conj head 4 - Propagation of conj dependents 5 - Annotation of relative pronoun referent 6 - Inclusion of elliptical predicate  Relations with more than one classification Unclassified relations  gold-dev % dev  gold-test % test  100 2,213 776 23  397 44 67 45 72 113  37 1  - - - 23.0%  51.16% 5.67% 8.63% 5.8% 9.28% 14.56%  4.77% 0.13%  100 2,012 587 2  - - - 2.0%  362 34 56 30 55 11  31 8  61.67% 5.79% 9.54% 5.11% 9.37% 1.87%  5.28% 1.36%  Table 2. Distribution of phenomena in the gold-standard samples of EUD  Regarding the distribution of EUD types per sample, we see a reasonably large difference between the two, with the frequency of phenomena always being higher in the   dev sample. Particularly in class 6, the difference (14.56% of relations in gold-dev versus 1.87% of relations in gold-test) is due to the fact that the orphan relation, indicative of predicate ellipsis, is infrequent in the corpus, as commented before.  Table 3 shows how many times the rules for each EUD types were applied to annotate the gold-standard samples. The difference in applications from “Original” to “Modif.” are a result of the changes we made to these rules to make them suit our corpus. The increase from 0 to 60 and 31 in “4 - Propagation of conj dependents” is due to the removal of constraints in the original rules to better suit the Portuguese data. New rules, such as the one in Figure 7, could be classified into one of each EUD types, but were left as a new type to highlight that they are completely new.  EUD Types  gold-dev  gold-test  Original Modif. Original Modif.  1 - Addition of prepositions and conjunctions 2 - Assignment of xcomp subjects 3 - Propagation of conj head 4 - Propagation of conj dependents 5 - Annotation of relative pronoun referent 6 - Inclusion of elliptical predicate 7 - New rules  415 36 84 0 108 71 0  448 40 87 60 117 75 75  355 27 57 0 95 6 0  374 29 57 31 95 7 8  Table 3. Number of rule applications for each EUD type  Figure 7. A new rule, created to annotate sentences such as “Essa lei permitiu- lhes ganhar um aumento de sal ´ario” (This law allowed them to earn a salary raise), where “lhes” is a pronominal indirect object (IOBJ) of a governor of xcomp relation (HEADXCOMP), “permitiu”, thus it should gain a new enhanced relation as nsubj of the xcomp dependent (DEPXCOMP), “ganhar”.  Table 4 shows the program’s performance considering both samples (test and dev) and both sets of rules (original and modified). ELAS indicates the overall performance of the program. Items 1 to 6 represent the performance, according to the F1-score metric, for each of the six types of EUD. The last line shows the number of sentences where an empty token insertion was made to resolve an ellipsis, but the insertion was incorrectly made. Considering that sentences with ellipses are more challenging to annotate, as they require the empty token inserted into the sentence to be placed in the correct position, and considering that various relations in the sentence may suffer negative impact due   gold-dev  gold-test  Original Modif. Original Modif.  ELAS ELAS (excluding sentences w/ ellipses)  61.36% 78.97% 84.67% 96.05% 88.50% 99.07% 88.97% 96.05%  1 - Addition of prepositions and conjunctions 93.35% 98.99% 95.17% 98.90% 85.39% 89.89% 92.54% 97.06% 2 - Assignment of xcomp subjects 72.00% 87.94% 84.13% 96.43% 3 - Propagation of conj head 84.11% 92.47% 96.67% 96.67% 4 - Propagation of conj dependents 88.28% 100.0% 94.23% 94.23% 5 - Annotation of relative pronoun referent 100.0% 9.05% 6 - Inclusion of elliptical predicate  40.71% 0%  Sentences with misplaced empty token  21  8  2  0  Table 4. Overall ELAS and by EUD type  to the incorrect placement of this empty token, we calculated two types of ELAS: one considering the entire sample, and another excluding the sentences with predicate ellipses.  Overall, we observe that the numbers are lower in the development sample, re- flecting the fact that it contains many more sentences with ellipses than the test sample and that the phenomena were selected for their complexity. The results are superior using the modified rule set, reaching up to 99.07% ELAS for the development sample, exclud- ing sentences with ellipses. For sentences with predicate ellipses, we reduced the number of errors in empty token insertion. In the test sample, errors dropped from 2 to 0, and in the development sample, from 21 to 8. Consequently, in the test sample, the results for relations related to the inclusion of the elliptical predicate reach 100%, but in the devel- opment sample, where the sentences are more complex, we only achieve 40.71% ELAS, indicating that there is still room for improvement in particularly difficult sentences.  Comparing the modified and the original rule numbers, the obtained performance improvement is evident. When using the regular data distribution of the treebank as benchmark (test data), where predicate ellipsis is not very frequent, we perform 11.38 absolute ELAS better using the modified rule set in comparison to the original set.  by  As  the  Grew  noted  2021 (Guillaume and Perrier 2021), the parser’s performance heavily relies on the accu- racy of the basic syntactic parser. Working with gold UD annotation, the EUD annotation is above 92% ELAS for all languages, being English the one with the highest performance (99.0% ELAS) and Lithuanian the lowest one (92.1%). Our result for Portuguese, in comparison, would be of 96.05% ELAS using the modified rule set.  team submission  IWPT  to  We observed that labeling the dependency relations between the empty token and the former participants of the orphan relation remains particularly challenging. Clues to this can be found in the head clause of conj: the available dependency relations are those that exist in the head clause and do not exist in the dependent clause. However, semanti- cally equivalent arguments often have different syntactic forms (for example, a temporal modifier may occur as advmod, obl, or advcl), which makes labeling the dependency re- lations difficult. The task is computationally complex, and, since the occurrence of this   Figure 8. criminals, free” (loose translation).  Incorrect EUD annotation of the sentence “We get arrested and the  phenomenon is infrequent, we recommend manually reviewing all relations after insertion of the empty token until we advance in the solutions to improve accuracy.  We noticed that the enhanced dependencies of elliptical token insertion and coref- erent annotation (ref ), because they present an alternative annotation to that of the basic dependencies, constitute a new basis for the other enhanced dependencies. This has two implications: (1) since they constitute a new basis, these two enhanced types must be annotated before the others, and (2) errors in these two enhanced types can generate cas- cading errors in the other enhanced annotations. For example, in the sentence of Figure 8, when the program does not identify that “bandidos” is the subject of the empty token, the subject slot is empty and the conj subject propagation rules annotate “gente” as the subject of the empty token, which is incorrect.2  5. Final Remarks We have addressed the issue of automatic enhanced dependencies annotation for Por- tuguese, which, to the best of our knowledge, consists in the first attempt for this lan- guage. The presented system along with our modified rules has shown its effectiveness in automatically generating complete annotations, which serve as a valuable resource for further linguistic analysis and model training, achieving an overall ELAS of 96.05% over gold basic syntactic annotation.  The next step is to use this system and rules to fully annotate Porttinari, creating the first UD treebank with EUD annotations for Brazilian Portuguese. By leveraging the capabilities of Grew, we aim to provide comprehensive and accurate annotations that include all 6 types of enhanced dependencies, which will be done in batches with human supervision to ensure the dataset quality3.  More information about this work may be found at the POeTiSA project web  portal: https://sites.google.com/icmc.usp.br/poetisa  Acknowledgments This work was carried out at the Center for Artificial Intelligence of the University of S˜ao Paulo (C4AI - http://c4ai.inova.usp.br/), with support by the S˜ao Paulo Research Foundation (FAPESP grant #2019/07665-4) and by the IBM Corporation. The project was also supported by the Ministry of Science, Technology and Innovation, with resources of Law N. 8,248, of October 23, 1991, within the scope of PPI-SOFTEX, coordinated by Softex and published as Residence in TIC 13, DOU 01245.010222/2022-44.  2Further discussion of specific enhancements for Portuguese can be found in the annotation technical  report (Duran 2024).  3The rules and the data that we used are publicly available at https://github.com/alvelvis/  grew-ed-portuguese.   "
        },
        {
            "titulo": "Disfluency Detection and Removal in Speech Transcriptions via Large Language Models",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31135",
            "idioma": "Inglês",
            "storage_key": "files/article_31135_30938.pdf",
            "autores": [
                {
                    "nome": "Pedro L. S. de Lima",
                    "afiliacao": "UFCG",
                    "orcid": "http://orcid.org/0009-0003-3924-2808"
                },
                {
                    "nome": "Cláudio E. C. Campelo",
                    "afiliacao": "UFCG",
                    "orcid": "https://orcid.org/0000-0003-4404-2344"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "The field of Automatic Speech Recognition (ASR) has significantly expanded within the technological landscape due to its extensive use in sectors such as education, healthcare, and customer service. Many modern applications depend on analyzing spoken content through Speech-to-Text (STT) conversion models. However, transcriptions produced by these systems often contain undesirable elements, such as word repetitions and the prolongation of certain sounds, known as disfluencies or linguistic crutches. These elements can negatively affect the quality of automatic content analysis by Natural Language Processing (NLP) models, including those for named entity recognition, emotion detection, or sentiment analysis. Therefore, this study aims to evaluate the feasibility of identifying and eliminating linguistic disfluencies using Large Language Models (LLMs), such as GPT-4, LLaMA, Claude, and Gemini, through Prompt Engineering techniques. The approach was tested using a corpus of debate transcriptions with manually annotated disfluency occurrences, yielding promising results.",
            "keywords": [
                "Automatic Speech Recognition",
                "Linguistic Disfluencies",
                "Large Language Models",
                "Prompt Engineering",
                "Speech-to-Text"
            ],
            "referencias": [
                "Anthropic. (2024). Claude 3.5 Sonnet.",
                "Baevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.",
                "Bassi, S., Duregon, G., Jalagam, S., & Roth, D. (2023). End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining.",
                "Corley, M., & Stewart, O. W. (2008). Hesitation disfluencies in spontaneous speech: The meaning of um. Language and Linguistics Compass, 2(4), 589-602.",
                "Ferguson, J., Durrett, G., & Klein, D. (2015). Disfluency detection with a semi-markov model and prosodic features. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 257-262).",
                "Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., & Mohamed, A. (2021). HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.",
                "Meta. (2024). Introducing LLaMA 3: Advancements in Large Language Models.",
                "OpenAI, Achiam, J., Adler, S., et al. (2024). GPT-4 Technical Report.",
                "OpenAI. (2024). OpenAI Tokenizer.",
                "Romana, A., Koishida, K., & Provost, E. M. (2023). Automatic Disfluency Detection from Untranscribed Speech.",
                "Snover, M., Dorr, B., & Schwartz, R. (2004). A lexically-driven algorithm for disfluency detection. In Proceedings of HLT-NAACL 2004: Short Papers (pp. 157-160).",
                "Team, G., Georgiev, P., and et al., V. I. L. (2024). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
                "Zayats, V., Ostendorf, M., & Hajishirzi, H. (2016). Disfluency detection using a bidirectional LSTM."
            ],
            "artigo_completo": "Abstract. The field of Automatic Speech Recognition (ASR) has significantly ex- panded within the technological landscape due to its extensive use in sectors such as education, healthcare, and customer service. Many modern applica- tions depend on analyzing spoken content through Speech-to-Text (STT) conver- sion models. However, transcriptions produced by these systems often contain undesirable elements, such as word repetitions and the prolongation of certain sounds, known as disfluencies or linguistic crutches. These elements can neg- atively affect the quality of automatic content analysis by Natural Language Processing (NLP) models, including those for named entity recognition, emo- tion detection, or sentiment analysis. Therefore, this study aims to evaluate the feasibility of identifying and eliminating linguistic disfluencies using Large Lan- guage Models (LLMs), such as GPT-4, LLaMA, Claude, and Gemini, through Prompt Engineering techniques. The approach was tested using a corpus of debate transcriptions with manually annotated disfluency occurrences, yielding promising results.  1. Introduction  Automatic Speech Recognition (ASR) has become essential in modern society, enabling the conversion of human speech into written text. This technology facilitates a range of applications through Speech-to-Text (STT) models, including virtual assistants, meeting transcription, automatic captioning, and customer service. Despite significant advances in speech recognition accuracy, a constant feature in transcriptions generated by these systems is the presence of linguistic disfluencies. During human speech production, it is common to generate various sounds within speech, known as disfluencies.  Disfluencies have been extensively studied and are primarily classified into three types: hesitations, repetitions, and corrections [Corley and Stewart 2008]. When a speech model transcribes voice into text, it often overlooks the context of the spoken words, fo- cusing instead on achieving an accurate transcription. As a result, these disfluencies are common and appear in caption transcriptions, meeting notes, and any text derived from spontaneous human speech. Various studies explore different techniques for disfluency   detection, ranging from unimodal to multimodal approaches, some even use Transformer- based methods, but none thoroughly investigate the utility of modern and widely accessi- ble Large Language Models (LLMs) for the detection and removal of linguistic crutches.  LLMs based on Transformers present a promising alternative. Due to their ability to capture complex contexts and understand linguistic nuances, such as differentiating between disfluent and fluent text, they present a promising alternative. LLMs can be easily manipulated using Prompt Engineering techniques, which involve creating instructions to guide their behavior toward a specific goal. This work aims to fill a gap in the study of disfluency detection and removal in text transcriptions by evaluating the capabilities of the most advanced LLMs available today, such as OpenAI’s GPT-4o [OpenAI et al. 2024], Gemini 1.5 Pro Experimental 0827 [Team et al. 2024], Claude 3.5 [Anthropic 2024] and LLaMMa 3 (70B parameters) [Meta 2024] to assess the extent of their applicability to this task.  The main contributions of this paper include:  • An analysis of LLMs’ ability to remove particular text excerpts while preserving  other relevant information.  • A comparative analysis of available models and their effectiveness in handling  transcribed spontaneous human speech.  • An assessment of the feasibility, in terms of computational cost, of cleaning tran-  scriptions of natural human speech.  • A dataset with annotated disfluencies in Brazilian Portuguese.  The following sections of this paper are organized as follows: Section 2 presents a literature review, covering foundational and relevant research on disfluency detection and removal, leading up to the current state-of-the-art. Section 3 details the research method- ology, explaining data collection and handling processes, as well as the construction of prompts and an exploratory data analysis, followed by Section 4, which presents the re- sults. Finally, Section 5 offers the conclusion.  2. Related Work  Research on the detection and removal of disfluencies in speech encompasses a variety of techniques, each contributing to the advancement of the state-of-the-art in this field. Studies in this domain typically utilize one of three types of input: textual transcription, audio signal, or a combination of the two. Unimodal solutions rely on a single source of information, whereas multimodal solutions integrate multiple sources, such as audio and text, to perform the task of disfluency detection/removal. The next subsections present research carried out using the unimodal text approach, followed by the unimodal audio approach, a comparison between the two approaches, and finally the conclusion of this section.  2.1. Text-Based Approaches  In this context, [Snover et al. 2004] proposed a Transformation-Based Learning (TBL) algorithm for disfluency detection in speech transcriptions, employing lexical features (word usage and sentence structure). The system, referred to as System A, achieved results comparable to those employing prosodic features (variations in intonation, rhythm,   duration, and intensity/volume of speech), demonstrating that satisfactory performance can be achieved without heavily relying on prosodic cues. The study underscores the importance of features such as the lexeme itself, Part-of-Speech (POS) tags, and word frequency for the speaker in identifying disfluencies. System A showed promising results in detecting various types of disfluencies and paved the way for future research focused on natural language processing techniques.  [Ferguson et al. 2015] proposed a conditional semi-Markovian method (semi- CRF) for disfluency detection in speech transcriptions, focusing on repairs such as repe- titions and false starts. This technique utilizes lexical, structural, and prosodic features, such as pauses and word duration, extracted from alignment with the speech signal. This approach achieved an F-score of 85.4% on the Switchboard corpus (a dataset consisting of English telephone conversations collected in the United States during the 1990s), surpass- ing the performance of previous studies. Concurrently, [Zayats et al. 2016] introduced a novel method for disfluency detection in speech transcriptions using a Bidirectional LSTM (BLSTM) neural network. Their solution employs word embeddings (numerical representations of words), POS tags, and lexical pattern features as input. Additionally, the model incorporates an explicit repair mechanism and uses Integer Linear Program- ming (ILP) to enforce structural constraints on the disfluency sequence. This approach achieved an F-score of 85.9% on the Switchboard corpus. Analysis of the results indicates that this approach performs better in detecting complex disfluencies that do not involve mere repetitions of words. Despite its effectiveness, the model’s reliance on predefined resources limits its adaptability to different types of disfluencies, contexts, and speaking styles.  [Bach and Huang 2019] also explored the BiLSTM technique with self-attention for disfluency detection in speech transcriptions. The authors demonstrated competitive results with BERT on the Switchboard corpus, outperforming it in terms of robustness and efficiency on out-of-domain datasets. The artificial addition of extra and incorrect words during model training proved highly effective in enhancing its robustness to various data types and transcription errors, making it a compelling alternative for disfluency detection in real-world scenarios. Furthermore, the proposed models are smaller than BERT, which results in reduced computational resource requirements overall.  2.2. Audio-Based Approaches  [Bassi et al. 2023] propose an end-to-end approach for speech transcription with disflu- ency removal using a large-scale pre-trained HuBERT acoustic model. The traditional two-step method, which first transcribes the audio into text and then removes disfluen- cies, neglects the prosodic cues present in the original audio. The proposed approach processes the audio directly and uses acoustic representations learned during pre-training to identify and remove disfluencies during transcription. The authors demonstrate that the end-to-end solution surpasses the two-step approach in terms of Word Error Rate (WER) and Character Error Rate (CER) on the Switchboard test set, achieving 12.2% WER and 7.3% CER. The study also highlights the significance of the pre-training objec- tive: HuBERT, pre-trained with a clustering objective that groups audio representations based on similarities, significantly outperformed Wav2Vec2, which was pre-trained with a contrastive objective that maximizes similarity among similar samples and minimizes similarity among different samples. These results suggest that end-to-end models with   large-scale acoustic pre-training with clustering objectives are a promising approach for accurate disfluent speech transcription.  2.3. Comparison Between Unimodal and Multimodal Models  [Romana et al. 2023] investigated the automatic detection of disfluencies in speech by comparing language-based, acoustic, and multimodal methods. Their results demon- strated that while language models such as BERT exhibited high accuracy with manual transcriptions, performance significantly declined with the use of transcriptions gener- ated by Automatic Speech Recognition (ASR). Acoustic approaches utilizing models like Wav2Vec 2.0, HuBERT, and WavLM proved promising by avoiding reliance on tran- scriptions. However, the authors found that multimodal solutions combining acoustic and linguistic information through a BLSTM fusion network achieved the best results, out- performing unimodal techniques in disfluency detection and categorization. This study highlights the potential of multimodal methods for creating more robust disfluency detec- tion systems.  The academic works presented in this section illustrate the progress made in the field, with advanced techniques in artificial intelligence, transformers, and robust mul- timodal methods applicable to various data types and transcription errors. These solu- tions have proven effective in detecting and removing disfluencies across diverse con- texts. However, the use of widely available Large Language Models (LLMs) for cleaning automatic transcriptions has been insufficiently studied. Therefore, there is a need to in- vestigate how LLMs can be leveraged for this purpose, complementing the advancements achieved in the reviewed academic works and democratizing access to these technologies.  3. Methodology  This section contains information about the methodology used in the research, including how the data was obtained and organized, and the construction of the prompts.  3.1. The Dataset  The dataset for this study consists of text extracted from four debate sessions held at Federal University of Campina Grande to analyze debater performance. It includes 114 minutes of transcribed audio in Portuguese, providing insights into the dynamics and effectiveness of various debating techniques. The debates were moderated, with each ses- sion involving 4 to 5 debaters discussing topics related to Artificial Intelligence. Each debater had a chance to speak following questions posed by the moderator, and interrup- tions were not allowed, resulting in a free-flowing and spontaneous discourse. After the debates, the audio recordings were transcribed using Microsoft’s Azure model, with the transcripts stored in a JSON file. This file was then converted into Excel tables containing all the transcribed data. The data underwent human review to correct major transcription errors, such as non-existent or meaningless words. Additionally, each table was annotated for disfluencies. Disfluencies were categorized into three types: hesitations, repetitions, and corrections. Four HTML-style tags were created to mark these disfluencies in the text:  • <hes {content}/>, which marks hesitations • <rep {content}/>, which marks repetitions   • <erro {content}/>, which marks errors • <corr {content}/>, which marks corrections  This marking and correction process resulted in four Excel files with the tran- scriptions of the respective debates. These files were then subjected to an exploratory data analysis.  3.2. The Prompts  To perform the task of disfluency detection and removal, four different prompts were developed. To determine which prompt technique is most effective, three types of prompt engineering methods were tested:  • Zero-Shot Prompting • Few-Shot Prompting • Chain-of-Thought Prompting  These three types of prompts differ significantly in how they present information to the language model (LLM), and the study aims to understand the extent of the LLMs’ knowledge about disfluencies. In the Zero-Shot case, the prompt provides little or no context about the task, so it was divided into two prompts. The first prompt is a direct command to the LLM to remove repetitions, hesitations, and corrections from the text, while keeping it otherwise unchanged. The second prompt adds a description of what disfluencies are and how the three targeted types are characterized. The Few-Shot prompt includes all the information from the first two prompts, as well as an example of disflu- ent text in three stages: the original disfluent text, the text with disfluency tags, and the cleaned text. Finally, the Chain-of-Thought prompt is designed to help the LLM adopt a step-by-step approach to detecting and removing disfluencies from the text. These four prompts were executed with each of the LLMs. The average number of tokens processed by the LLMs in Group 14, the smallest group, ranged from 4,273 tokens (with the smallest prompt) to 5,041 tokens (with the largest prompt). In contrast, Group 1, the largest group, processed between 5,250 tokens (smallest prompt) and 6,004 tokens (largest prompt). This calculation was estimated using the Tokenizer from the OpenAI Platform.  Prompting Technique Context Zero-Shot Prompting Zero-Shot Prompting Few-Shot Prompting  Chain-of-Thought Prompting  None Definition of disfluencies Definition of disfluencies and a three-stage snapshot of the text during the disfluency cleaning process Definition of disfluencies plus a guide on how to rec- ognize and remove each type of disfluencies  Table 1. Prompts Created For the Task  3.3. Exploratory Data Analysis  Data from the tagged transcriptions in Excel files were analyzed to gain an overview of how each disfluent text is characterized. The initial analysis focused on the quantity of disfluencies per group. For this purpose, disfluencies were tallied in each file employing the markers described in the Dataset section. These counts were aggregated for each   Figure 1. Total Relative Disflu- encies per 100 Words  Figure 2. Disfluencies by Type and Group  group, and the totals were visualized using graphs to aid interpretation. Figure 1 displays the comparison of disfluency rates across four groups, labeled 1, 14, 8, and 7 on the X-axis. This figure presents the proportion of disfluencies calculated per 100 words for each group, facilitating a comparison of the relative frequency of disfluencies between the groups. Figure 2, using the same group labels (1, 14, 8, and 7) on the X-axis, depicts the number of disfluencies broken down by type (hesitation, error, repetition). This figure illustrates the distribution of different disfluency types within each group.  3.4. Configuration and Execution of LLMs  The execution of data in Large Language Models (LLMs) was carried out through specific Application Programming Interfaces (APIs). The Google Gemini 1.5 Pro Experimental 0827, Anthropic’s Claude 3.5 Sonnet, and OpenAI’s ChatGPT-4o were accessed via the official APIs provided by their respective companies. The LLaMa 3 72B was used through the Groq platform. The implementation was structured into 16 Python notebooks in the Google Colaboratory environment, with four notebooks assigned to each LLM, corre- sponding to debate groups. Each notebook was initialized with the configuration of the corresponding LLM, followed by the result extraction codes detailed in this methodolog- ical section, and then executed using the pre-established prompts. The results obtained were recorded at the end of each notebook, later compiled into tables for this work, and analyzed for the research objectives. In a separate notebook, an exploratory data anal- ysis was conducted using Excel files from the groups, with the procedures and results described in detail in this work.  4. Results  Model Gemini GPT-4o LLaMa Claude  Table 2. Zero-Shot (No Context) - Group 14  Total Removal Rate Levenshtein Similarity Time (seconds)  14.06% 62.50% 53.12% 57.81%  97.95% 96.83% 95.24% 32.98%  85.69 49.77 21.86 18.47  The data presented in Tables 2 and 3 clearly show that when using Zero-Shot prompts in Group 14 (the most disfluent group), GPT-4o, Gemini, and LLaMa main- tained a relatively good textual structure, as indicated by the Levenshtein similarity value. None of the LLMs successfully balanced the removal of disfluencies while preserving the   Model Gemini GPT-4o LLaMa Claude  Table 3. Zero-Shot (With Context) - Group 14  Total Removal Rate Levenshtein Similarity Time (seconds)  10.94% 60.94% 62.50% 60.94%  97.96% 74.96% 51.16% 33.37%  86.23 49.77 8.79 18.65  original text’s quality with these two prompts, but text maintenance results for Claude and LLaMa fell significantly below expectations for most tested prompts, making them currently unreliable for this type of task. Therefore, the following analysis focuses solely on GPT-4o and Gemini 1.5.  Table 4. Test Results for GPT-4o and Gemini - Few Shot - Group 14  Model Gemini GPT-4o  Total Removal Rate Levenshtein Similarity Time (seconds)  51.56% 67.19%  98.18% 96.83%  82.57 74.73  With Few-Shot prompting (Table 4), GPT-4o achieved a 67.19% disfluency re- moval rate while maintaining 96.83% of the original text. It also surpassed Gemini in response time. Although Gemini had a slightly higher text maintenance rate, it performed poorly in removing disfluencies.  Table 5. Test Results for GPT-4o and Gemini - Chain of Thought - Group 14 Total Removal Rate Levenshtein Similarity Time (seconds)  26.56% 68.75%  98.09% 97.85%  84.61 45.76  Model Gemini GPT-4o  Using Chain-of-Thought prompting (Table 5), GPT-4o was the only one among the four LLMs to produce a minimally favorable result. When compared to Gemini, GPT- 4o achieved a 68.75% total disfluency removal rate, despite a similar text maintenance rate, while Gemini, though maintaining text quality, failed in removing disfluencies.  Table 6. Test Results for GPT-4o and Gemini - Few Shot - Group 8  Model Gemini GPT-4o  Total Removal Rate Levenshtein Similarity Time (seconds)  62.96% 48.15%  95.80% 88.22%  102.58 47.05  Table 7. Test Results for GPT-4o and Gemini - Chain of Thought - Group 8  Model Gemini GPT-4o  Total Removal Rate % Levenshtein Similarity % Time (seconds)  33.33% 40.74%  98.19% 88.51%  104.26 45.25  In Group 8, one of the least disfluent groups, GPT-4o’s effectiveness declined in both disfluency removal and maintaining text fluency, as shown in Tables 6 and 7.   Table 8. Test Results for GPT-4o and Gemini - Few Shot - Group 1  Model Gemini GPT-4o  Model Gemini GPT-4o  Total Removal Rate Levenshtein Similarity Time (seconds)  63.89% 68.06%  96.68% 78.18%  126.40 52.48  Table 9. GPT-4o and Gemini - Chain of Thought - Group 1 Total Removal Rate Levenshtein Similarity Time (seconds)  25.00% 68.06%  97.85% 77.55%  127.22 53.69  Gemini achieved a 62.96% removal rate with good text maintenance, albeit taking more than twice as long. This trend, where GPT-4o did not match Gemini in text maintenance, was also observed in Group 1, as shown in Tables 8 and 9, which is the largest group but not as disfluent as Group 14. The models demonstrated high effectiveness in removing repetitions, achieving 91.30% removal in Group 14 for GPT-4o, compared to Gemini’s 56.52% in the Few-Shot prompt. In the Chain-of-Thought prompt, GPT-4o maintained a consistent removal rate of 91.30% while also outpacing Gemini in processing time. Although GPT-4o showed strong performance in Group 14, it struggled in Group 1, where Gemini achieved 87.18% removal with superior text preservation (96.68%). These results suggest that while GPT-4o excels in specific contexts, Gemini may be more robust when handling larger, more complex texts. 1.  5. Conclusion and Future Work  This study explored the efficacy of Large Language Models (LLMs) in detecting and eliminating linguistic disfluencies from transcriptions of academic debates. By leverag- ing advanced prompt engineering techniques, such as Zero-Shot, Few-Shot, and Chain- of-Thought prompting, we assessed the performance of leading LLMs — GPT-4, Gemini 1.5, Claude 3.5, and LLaMa 3 — in this task. The results revealed several key insights into the capabilities and limitations of these models. GPT-4o demonstrated the highest over- all performance in disfluency removal, achieving an optimal balance between removing disfluencies and maintaining text coherence, particularly under Few-Shot and Chain-of- Thought prompting conditions. Gemini 1.5 also performed well but showed variability depending on the prompt type and the specific debate group analyzed. It excelled in text maintenance but had lower removal rates compared to GPT-4o in some cases. Claude 3.5 and LLaMa 3 produced weaker results, struggling to maintain text coherence while removing disfluencies. GPT-4o demonstrated more efficient processing times compared to the other models, which is crucial for practical, real-world applications. In conclu- sion, while LLMs like GPT-4o and Gemini 1.5 show promise for improving transcription quality by removing disfluencies, further advancements—such as fine-tuning, employ- ing more advanced prompt engineering techniques, integrating widely used LLMs with multimodal systems, or developing future models—are necessary to fully enhance their capabilities.  1Repository: https://github.com/pedrosqra/STIL   "
        },
        {
            "titulo": "Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31136",
            "idioma": "Inglês",
            "storage_key": "files/article_31136_30939.pdf",
            "autores": [
                {
                    "nome": "Mirelle Bueno",
                    "afiliacao": "UNICAMP",
                    "orcid": "http://orcid.org/0000-0003-2374-6123"
                },
                {
                    "nome": "E. Seiti de Oliveira",
                    "afiliacao": "UNICAMP",
                    "orcid": "https://orcid.org/0000-0002-7882-6203"
                },
                {
                    "nome": "Rodrigo Nogueira",
                    "afiliacao": "UNICAMP / Maritaca AI",
                    "orcid": "https://orcid.org/0000-0002-2600-6035"
                },
                {
                    "nome": "Roberto Lotufo",
                    "afiliacao": "UNICAMP / NeuralMind.ai",
                    "orcid": "https://orcid.org/0000-0002-5652-0852"
                },
                {
                    "nome": "Jayr Pereira",
                    "afiliacao": "UFCA",
                    "orcid": "https://orcid.org/0000-0001-5478-438X"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "We present Quati, a dataset specifically designed for evaluating Information Retrieval (IR) systems for the Brazilian Portuguese language. It comprises a collection of queries formulated by native speakers and a curated set of documents sourced from a selection of frequently accessed Brazilian Portuguese websites, which ensures a representative and relevant corpus. To label the query–document pairs, we use a state-of-the-art LLM, which shows inter-annotator agreement levels comparable to human performance in our assessments. Our annotation methodology is described, enabling the cost-effective creation of similar datasets for other languages, with an arbitrary number of labeled documents per query. As a baseline, we evaluate a diverse range of open-source and commercial retrievers. Quati is publicly available at",
            "keywords": [
                "Information Retrieval",
                "Brazilian Portuguese Dataset"
            ],
            "referencias": [
                "Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara, A., Mitra, B., Nguyen, T., et al. (2016). Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268.",
                "Bonifacio, L., Jeronymo, V., Abonizio, H. Q., Campiotti, I., Fadaee, M., Lotufo, R., and Nogueira, R. (2021). mmarco: A multilingual version of the ms marco passage ranking dataset. arXiv preprint arXiv:2108.13897.",
                "Clark, J. H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., and Palomaki, J. (2020). Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages. Transactions of the Association for Computational Linguistics, 8:454–470.",
                "Cormack, G. V., Clarke, C. L., and Buettcher, S. (2009). Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 758–759.",
                "Craswell, N., Mitra, B., Yilmaz, E., Campos, D., Voorhees, E. M., and Soboroff, I. (2021). Trec deep learning track: Reusable test collections in the large data regime. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 2369–2375.",
                "Damessie, T. T., Nghiem, T. P., Scholer, F., and Culpepper, J. S. (2017). Gauging the quality of relevance assessments using inter-rater agreement. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1089–1092.",
                "Faggioli, G., Dietz, L., Clarke, C., Demartini, G., Hagen, M., Hauff, C., Kando, N., Kanoulas, E., Potthast, M., Stein, B., et al. (2023). Perspectives on large language models for relevance judgment. arXiv preprint arXiv:2304.09161.",
                "Farzi, N. and Dietz, L. (2024). An exam-based evaluation approach beyond traditional relevance judgments. arXiv preprint arXiv:2402.00309.",
                "Formal, T., Lassance, C., Piwowarski, B., and Clinchant, S. (2021). Splade v2: Sparse lexical and expansion model for information retrieval. arXiv preprint arXiv:2109.10086.",
                "Jeronymo, V., Nascimento, M., Lotufo, R., and Nogueira, R. (2022). mrobust04: A multilingual version of the trec robust 2004 benchmark. arXiv preprint arXiv:2209.13738.",
                "Johnson, J., Douze, M., and Jégou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535–547.",
                "Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., and Mikolov, T. (2016a). Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651.",
                "Joulin, A., Grave, E., Bojanowski, P., and Mikolov, T. (2016b). Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759.",
                "Kusupati, A., Bhatt, G., Rege, A., Wallingford, M., Sinha, A., Ramanujan, V., Howard-Snyder, W., Chen, K., Kakade, S., Jain, P., et al. (2022). Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:30233–30249.",
                "Lawrie, D., Mayfield, J., Oard, D. W., and Yang, E. (2022). Hc4: A new suite of test collections for ad hoc clir. In European Conference on Information Retrieval, pages 351–366. Springer.",
                "Lewis, D. D., Yang, Y., Russell-Rose, T., and Li, F. (2004). Rcv1: A new benchmark collection for text categorization research. Journal of machine learning research, 5(Apr):361–397.",
                "Lima de Oliveira, L., Romeu, R. K., and Moreira, V. P. (2021). Regis: A test collection for geoscientific documents in portuguese. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2363–2368.",
                "Nair, S., Yang, E., Lawrie, D., Duh, K., McNamee, P., Murray, K., Mayfield, J., and Oard, D. W. (2022). Transfer learning approaches for building cross-language dense retrieval models. In European Conference on Information Retrieval, pages 382–396. Springer.",
                "Nakayama, H., Kubo, T., Kamura, J., Taniguchi, Y., and Liang, X. (2018). doccano: Text annotation tool for human. Software available from",
                ".",
                "Overwijk, A., Xiong, C., and Callan, J. (2022). Clueweb22: 10 billion web documents with rich information. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3360–3362.",
                "Peters, C. and Braschler, M. (2002). The importance of evaluation for cross-language system development: the clef experience. In LREC. Citeseer.",
                "Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. (2021). Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.",
                "Sakai, T., Oard, D. W., and Kando, N. (2021). Evaluating Information Retrieval and Access Tasks: NTCIR’s Legacy of Research Impact. Springer Nature.",
                "Schäuble, P. and Sheridan, P. (1998). Cross-language information retrieval (clir) track overview. NIST SPECIAL PUBLICATION SP, pages 31–44.",
                "Thomas, P., Spielman, S., Craswell, N., and Mitra, B. (2023). Large language models can accurately predict searcher preferences. arXiv preprint arXiv:2309.10621.",
                "Vitório, D., Souza, E., Martins, L., da Silva, N. F., de Carvalho, A. C. P. d. L., Oliveira, A. L., and de Andrade, F. E. (2024). Building a relevance feedback corpus for legal information retrieval in the real-case scenario of the brazilian chamber of deputies. Language Resources and Evaluation, pages 1–21.",
                "Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. (2022). Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533.",
                "Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V., and Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information Processing Systems, volume 35, pages 24824–24837. Curran Associates, Inc.",
                "Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. (2020). mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.",
                "Zendel, O., Culpepper, J. S., Scholer, F., and Thomas, P. (2024). Enhancing human annotation: Leveraging large language models and efficient batch processing. In Proceedings of the 2024 Conference on Human Information Interaction and Retrieval, pages 340–345.",
                "Zhang, X., Thakur, N., Ogundepo, O., Kamalloo, E., Alfonso-Hermelo, D., Li, X., Liu, Q., Rezagholizadeh, M., and Lin, J. (2023). Miracl: A multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:1114–1131."
            ],
            "artigo_completo": "Abstract. We present Quati,1 a dataset specifically designed for evaluating In- formation Retrieval (IR) systems for the Brazilian Portuguese language. It com- prises a collection of queries formulated by native speakers and a curated set of documents sourced from a selection of frequently accessed Brazilian Por- tuguese websites, which ensures a representative and relevant corpus. To label the query–document pairs, we use a state-of-the-art LLM, which shows inter- annotator agreement levels comparable to human performance in our assess- ments. Our annotation methodology is described, enabling the cost-effective creation of similar datasets for other languages, with an arbitrary number of labeled documents per query. As a baseline, we evaluate a diverse range of open-source and commercial retrievers. Quati is publicly available at https: //huggingface.co/datasets/unicamp-dl/quati, and all scripts at https://github.com/unicamp-dl/quati.  1. Introduction  The development of Information Retrieval (IR) systems depends on high-quality evalua- tion datasets, which should contain queries and documents ideally in the same target lan- guage of those systems, in order to capture specific information needs and social-cultural aspects. That, contrasts with translated datasets, which potentially represent the infor- mation needs and knowledge of a different culture or society. Hence, translated datasets may not effectively measure a retrieval system’s ability in real-world scenarios involving native users.  *Equal contribution. 1We named our dataset after this South American mammal, whose foraging behavior represents the  resolute search for resources.   Despite being one of  IR datasets in Portuguese.  the most widely spoken languages in the world, Existing datasets such as there is a scarcity of REGIS [Lima de Oliveira et al. 2021] and RCV1 [Lewis et al. 2004]2, though valu- able, fall short due to their limited size and specialized domains (geoscience and news). While translated datasets such as mMARCO [Bonifacio et al. 2021] and mRo- bust04 [Jeronymo et al. 2022] have helped to alleviate this issue, the use of automatic translations often represents the loss of socio-cultural characteristics of the target lan- guages, and the evaluations may become biased by the source language.  To address those issues, we created Quati, a Brazilian Portuguese evaluation dataset, comprising human-written queries and a high-quality native corpus. Quati is created using a semi-automated pipeline, aiming to reduce the labeling cost barrier. We use a Large Language Model (LLM) to judge a passage’s relevance for a given query, publishing a cost-effective pipeline to create an IR evaluation dataset with an arbitrary number of annotated passages per query.3 In this context, our work aims to answer the following research question: Can LLMs be used to compose a semi-automated pipeline for annotating query–passages relevance for Brazilian Portuguese IR systems?  To evaluate the quality of the LLM annotations, we compare them with human annotations on a sample of query–passage pairs and confirmed a Cohen’s Kappa coef- ficient of 0.31. While this figure is below the 0.41 seen in human-human annotation agreement, it is consistent with the findings reported in the literature [Faggioli et al. 2023, Thomas et al. 2023, Farzi and Dietz 2024] and it will likely increase as LLMs improve in quality. The usage of a modular semi-automated pipeline, allows the dataset construction method to be replicated to create high-quality IR datasets for other languages.  2. Related Work  [Jeronymo et al. 2022], Mr.Tydi  Evaluation datasets are an important variable in the IR context as they expose the limitations of search systems and guide their development. However, most of the avail- able datasets are in English, as is the case with MS MARCO [Bajaj et al. 2016]. such MIRACL [Zhang et al. 2023], mMARCO [Bonifacio et al. 2021], Works TREC CLIR mRobust [Sch¨auble and Sheridan 1998], NT- to develop CIR [Sakai et al. 2021] and HC4 datasets for other languages, but most are based on language translation to adapt English to the target Ongoing ef- forts [Lima de Oliveira et al. 2021, Vit´orio et al. 2024] are starting to change that scenario creating IR datasets for Brazilian Portuguese, but so far focusing on specific domains.  [Lawrie et al. 2022] are efforts  [Peters and Braschler 2002],  languages, or do not  include Portuguese.  [Clark et al. 2020],  CLEF  The creation of datasets for IR is a resource-intensive task, particularly in the pro- cess of judging the relevance of documents. Recent endeavors have witnessed a shift towards leveraging LLMs to assess query–passage relevance [Zendel et al. 2024]. Faggi- oli et al. [Faggioli et al. 2023] further underscored the potential of employing LLMs for automating the judgment of document relevance, thereby opening up promising avenues  2https://trec.nist.gov/data/reuters/reuters.html 3The total cost for this dataset was U$140.19 (0.03 per query–passage) for an average of 97.78 annotated  passages per query.   for exploration in this domain. Complementary evaluations conducted by Thomas et al. [Thomas et al. 2023] demonstrated a significant correlation between human judgments and those made by the GPT-3.5-turbo model.  3. Methodology  We used a semi-automatic method to create Quati, as depicted in Figure 1. The required inputs are: 1) A large corpus, originally written in the target language, from which we extract the passages to compose our IR dataset; 2) A set of test queries, manually created to represent the information needs of native speakers. In the following sections, we detail the steps of the pipeline.  Figure 1. Proposed IR dataset creation methodology.  3.1. Passages preparation  The passages preparation step is composed by the following substeps:  Data collection: We used the Portuguese subset of ClueWeb22 [Overwijk et al. 2022] category B, which includes 4.1 million web pages more likely to be visited according to Bing search algorithms during the first half of 2022 [Overwijk et al. 2022].  URL filtering: We excluded any documents from our dataset whose URLs’ domain ended with “.pt”, which refer to Portuguese from Portugal, as the language style in those documents might differ significantly from that used in Brazilian Portuguese web pages. Additionally, we used FastText [Joulin et al. 2016b, Joulin et al. 2016a] as an additional language verification method to ensure that only Portuguese documents were included in our corpus.  Document segmentation into passages: Following language verification, we segmented the documents into approximately 1,000-character segments and assessed the percentage of line breaks ( ) occurrences within each segment, removing those with more than 20%.   This criterion was used to increase the probability of retaining segments predominantly composed of natural language text.  Separation into large and small versions: With the process described in the previous steps, we collected a total of 20 million segments. From this set, we randomly selected 10 million segments (hereinafter referred to as 10M corpus) to be the passages in our corpus, creating a large, but still manageable, dataset of more than 11 GB of size. A second dataset was built from the first one applying additional filtering rules taken from the MassiveWeb Corpus [Rae et al. 2021] — e.g. removing passages with more than 10% of symbols, or with mean word length outside the 3 to 10 interval — and sampling only 1 million segments (hereinafter referred to as 1M corpus) from the resulting 7M filtered documents — the goal was to create a smaller and higher-quality dataset that would facilitate experimentation with embedding models, as encoding the original 10 million segments can be computationally expensive.  3.2. Manual queries creation  We employed human-created queries for the evaluation dataset, aiming high-quality ques- tions to capture common information needs from a diverse corpus, created by native speakers of the target language. We created a total of 200 test queries.  3.3. Passages retrieval  The next step is the passage retrieval to build a list of query–passages to annotate. As it would be prohibitive to have the relevance scores each query for the entire corpus, we select to annotate the top-k passages returned by multiple IR systems. It is assumed that the diversity of their results will enable the collection of a variety of passages, creating a robust evaluation dataset.  We selected a mix of strong and weak IR systems, to include a variety of pas- sages: BM25: a strong baseline for retrieval; BM25 + mT5-XL: two-stage pipeline with BM25 followed by mT5-XL (3.7 billion parameters) [Xue et al. 2020]; BM25 + E5- large: two-stage pipeline with BM25 and E5-large [Wang et al. 2022]4; E5-large and E5- base: E5 variants as dense retrievers, using FAISS [Johnson et al. 2019] with inner prod- uct for search; ColBERT-X [Nair et al. 2022]: a multilingual ColBERT-v1 fine-tuned in Brazilian Portuguese subset of mMARCO; SPLADE v2: a learned sparse retriever [Formal et al. 2021] fine-tuned on Brazilian Portuguese subset of mMARCO; SPLADE v2 + mT5-XL: two-stage pipeline using SPLADE v2 followed by mT5.  We also use Reciprocal Ranking Fusion (RRF) [Cormack et al. 2009] to in- crease the retrieved documents diversity, using the following combinations: E5-large + ColBERT-X; E5-large + SPLADE v2; and E5-large + BM25 + mT5-XL.  We also include commercial embedding models:  text-embedding-ada-0025, text-embedding-3-small6 and as it employs the Matryoshka Representation Learning technique [Kusupati et al. 2022], we performed the retrieval using only the first half di- mensions (identified as text-embedding-3-small half). FAISS [Johnson et al. 2019] using inner product was applied for dense vectors search for all of them.  4https://huggingface.co/intfloat/multilingual-e5-large 5https://openai.com/blog/new-and-improved-embedding-model 6https://openai.com/blog/new-embedding-models-and-api-updates   To evaluate the diversity of retrieved passages, we counted the query–passage combinations exclusively returned by each IR system, which should be a number from 0 to 500, 0 meaning the query–passages returned by a particular IR system were also returned by another IR system. Although we look for diversity, there should be a balance: we could have reached 5,000 different query–passage combinations (10 IR systems, 50 queries, 10 passage/query) if all systems returned exclusive passages, but that would indicate no agreement on the most relevant passages per query.  3.4. Query–passages annotation  The final step of the query annotation is to use an LLM to label the retrieved pas- sages’ relevance for each query. We selected the top-k=10 passages for a sample of 50 queries using all the retrieval systems considered on both the 10M and 1M corpora and sent them for LLM evaluation. We applied a few-shot Chain-of-Thought (CoT) prompt [Wei et al. 2022], and we adopted the TREC 2021 Deep Learning track 4-score relevance annotation scale [Craswell et al. 2021]: (1) Irrelevant: the passage is outside the scope of the question; (2) Relevant: the passage pertains to the question’s topic but does not provide a direct answer; (3) Highly relevant: the passage answers the question, but lacks in clarity or has unrelated information. (4) Perfectly relevant: the passage answers the question with clarity and precision.  We selected OpenAI GPT-4 model7 as the annotator. Due to cost limitations, we used a 50-sample from the 200 queries. We asked the LLM to label only the top-10 retrieved passages of each IR system for each query. We used a CoT prompt with two in-context examples selected from the mMARCO pt-BR dataset [Bonifacio et al. 2021]. The prompt written in Brazilian Portuguese includes the task explanation and the CoT examples to produce the 4-score passage relevance value for a given query. The final evaluation was requested in JSON format to simplify the LLM response parsing process. The prompt was built and refined using a limited set of questions sampled from the same mMARCO pt-BR dataset. The final prompt version can be found online.8  4. Experiments  4.1. LLM annotation quality assessment  We assess the quality of our LLM-based annotator by comparing its query–passage rel- evance scores with those provided by human annotators. This process was conducted on a 24-sample of the 50 annotated queries. Using the Doccano [Nakayama et al. 2018] system, three researchers annotated the top-10 passages returned by the BM25 + mT5 IR system applying the same TREC-DL 2021 4-score grading system. The agreement among the query–passage relevance annotations generated by the LLM and humans was measured using Cohen Kappa, Pearson, and Spearman correlation coefficients.  4.2. Retrieval systems evaluation  We used the LLM annotated query–passages to evaluate the IR systems effectiveness in the 10M and 1M Quati datasets. As we already have the IR runs for the passages retrieval by all the systems (see Section 3.3), we simply compute the nDCG@10 metric over those  7We used gpt-4-1106-preview, available at the OpenAI API. 8https://github.com/unicamp-dl/quati/blob/main/prompt.md   Table 1. The single–system query–passages column indicates the ones re- turned only by that system, either for the 10M or the 1M sets; the percentage refers to 500 query–passages. For the single system total, the percentage refers to the union of evaluated passages. “Others” are results with data preparation issues, but valid annotations.  Retrieval System  Single–system query–passages  10M dataset  1M dataset  E5-base BM25 SPLADE v2 pt-BR E5-large ColBERT-X mMARCO pt-BR BM25 + E5-large SPLADE v2 pt-BR + mT5-XL BM25 + mT5-XL E5-large + ColBERT-X mMARCO pt-BR RRF E5-large + SPLADE v2 pt-BR RRF text-embedding-ada-002 text-embedding-3-large text-embedding-3-small half text-embedding-3-small  Others  Single system query–passages total Union of all systems query–passages  262 (52.4%) 248 (49.6%) 151 (30.2%) 122 (24.4%) 115 (23.0%) 115 (23.0%) 86 (17.2%) 60 (12.0%) 32 (6.4%)  29 (5.8%)  253 (50.6%)  195 (39.0%) 120 (24.0%)  93 (18.6%)  137 (27.4%) 121 (24.2%) 45 (9.0%) 31 (6.2%)  814 (54.27%)  3029 (61.96%) 4889  results. Besides establishing a baseline for a variety of IR systems, this experiments also indirectly assess the overall quality of Quati validation dataset: by verifying different ef- fectiveness for already published IR systems, we validate Quati potential to indeed assess such systems.  5. Results and Discussion  5.1. Annotated passages variability  Table 1 indicates a range from 29 to 262 query–passage combinations exclusively returned by a single IR system. On average, each system returned 28.85% of new passages, and from the total 4,889 evaluated query–passages, 61.96% (3029) were returned by a single system, suggesting our pool of systems is diverse. As shown in Table 2, the IR systems were able to retrieve a diversity set of query–passages, including “perfectly relevant” (score=3) ones; also, the diversity increased for less relevant passages, indicating the systems agreed more as the passage relevance increased.  5.2. LLM annotations quality is aligned with crowd workers  Table 3 shows the Cohen’s Kappa and the Spearman’s Rho correlation coefficients for the human and LLM annotations, computed for the 240 query–passage combina- tions. The average Cohen’s Kappa of 0.31 is aligned with the literature. For example,   Table 2. Query–passage relevance score counts. The systems agreed more, returning the same passages per query, as the relevance score increases. “Rel- evant” includes passages from scores 1 to 3.  Score  All query–passages  Single-system query–passages  0 1 2 3  Relevant Total  2489 985 759 656  2400 4889  1839 586 375 229  1190 3029  %  73.89 59.49 49.41 34.91  49.58 61.96  Table 3. Cohen’s Kappa and Spearman’s Rho correlations among Human An- notators (HA) and the GPT-4, for the query–passage 4-score evaluations. For each annotator, 4th row holds the average of the correlation against the others. We then compute the mean of that value only for the Human Annotators (”Mean HA” row), to characterize their overall correlation.  HA1 HA2 HA3  Mean Std  Mean HA  Cohen’s Kappa HA3 HA2  0.4369 — 0.4105  0.4237 0.0132  0.4294 0.4105 —  0.4199 0.0095  HA1  — 0.4369 0.4294  0.4331 0.0037  0.4256±0.0055 - 0.0019  - 0.0057  GPT-4  0.3234 0.2593 0.3498  0.3108 0.0380  — - 0.1096  Spearman’s Rho HA2  HA3  0.6931 — 0.6985  0.6958 0.0027  0.6924 0.6985 —  0.6954 0.0031  HA1  — 0.6931 0.6924  0.6927 0.0004  0.6946±0.0014 0.0011  0.0008  - 0.0019  GPT-4  0.6073 0.6174 0.6296  0.6181 0.0091  — - 0.0765  Diff. Mean HA 0.0076  [Faggioli et al. 2023] reported 0.26 for GPT-3.5, and [Thomas et al. 2023] reported Co- hen’s Kappa ranging from 0.20 to 0.64, depending on the prompt used on GPT-4. Our human annotators’ mean Cohen’s Kappa of 0.4256 falls within crowd workers interval of a 0.24 to 0.52, according to [Damessie et al. 2017].  As query–passage relevance annotation is a subjective task, we argue a non- categorical metric such as the Spearman’s Rho would be more appropriate to measure the annotators’ correlation, as errors by a single score level should be considered “less critical”, or within the subjectivity intrinsic for the task. Although human annotators’ correlation is still above their correlations with the LLM, Spearman metrics are within a higher value, better capturing the current LLM effectiveness on the query–passage rele- vance evaluation.  5.3. Retrieval systems evaluation results  We evaluated the retrievers effectiveness using the LLM annotated query–passages (qrels); table 4 present the results for both the 10M and the 1M datasets. The ranking   Table 4. The nDCG@10 effectiveness on the 50 test queries. The results follows the IR literature and suggests the dataset can effectively evaluate a range of different IR systems.  Retrieval system  BM25 E5-large SPLADE v2 pt-BR E5-large + SPLADE v2 pt-BR RRF ColBERT-X mMARCO pt-BR BM25 + E5-large text-embedding-ada-002 text-embedding-3-small text-embedding-3-large E5-large + ColBERT-X mMARCO pt-BR RRF SPLADE v2 pt-BR + mT5-XL BM25 + mT5-XL  nDCG@10  10M dataset  1M dataset  0.4467 0.5563 0.5806 0.6272 0.6279 0.6364 — — — 0.6377 0.6966 0.7109  0.3991 — — — 0.4927 0.5423 0.5630 0.5688 0.6319 — — 0.6593  of retrievers with respect to effectiveness matched our expectations, following the litera- ture. We consider that an additional indication of the overall datasets quality as, despite being created in a semi-automated cost-effective way, they are able to evaluate a diversity of retrievers.  6. Conclusion This paper introduced the Quati, a dataset for supporting the development of IR systems for Brazilian Portuguese retrieval tasks. Quati is publicly available in two sizes, 10M a 1M passages, with 50-query qrels with respectively an average of 97.78 and 38.66 annotated passages per query. Through comparisons with human annotators we answer our research question, showing that state-of-the-art LLM can be used in a semi-automated and cost- effective way to create IR datasets for a specific target language, in the query–passage annotation role, with equivalent performance of humans: LLM annotations correlate with humans’ in similar way human crowd workers annotations do, for a fraction of the cost.  Acknowledgements We thank Leod´ecio Braz da Silva Segundo for the valuable support during the human annotation task. We also thank Leonardo Benardi de Avila and Monique Monteiro for the SPLADE v2 retrievals, using the model they trained for Brazilian Portuguese. This re- search was partially funded by grant 2022/01640-2 from Fundac¸ ˜ao de Amparo `a Pesquisa do Estado de S˜ao Paulo (FAPESP)."
        },
        {
            "titulo": "A Linguagem em Foco: Anotação de Sinalizadores Discursivos em Textos Jornalísticos",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31137",
            "idioma": "Português",
            "storage_key": "files/article_31137_30940.pdf",
            "autores": [
                {
                    "nome": "Paula Cardoso",
                    "afiliacao": "UFPA",
                    "orcid": "http://orcid.org/0000-0003-3621-8960"
                },
                {
                    "nome": "Jackson Souza",
                    "afiliacao": "UFBA",
                    "orcid": "https://orcid.org/0000-0003-1881-6780"
                },
                {
                    "nome": "Roana Rodrigues",
                    "afiliacao": "UFS",
                    "orcid": "https://orcid.org/0000-0002-7748-8716"
                },
                {
                    "nome": "Ewerson Dantas",
                    "afiliacao": "UFS",
                    "orcid": "https://orcid.org/0009-0005-1381-7372"
                },
                {
                    "nome": "Larissa Santa Bárbara",
                    "afiliacao": "UFS",
                    "orcid": "https://orcid.org/0009-0004-1549-8435"
                },
                {
                    "nome": "Mateus Araújo",
                    "afiliacao": "UFBA",
                    "orcid": "https://orcid.org/0009-0001-8356-7322"
                },
                {
                    "nome": "Naira Gama",
                    "afiliacao": "UFBA",
                    "orcid": "http://orcid.org/0009-0000-7944-7515"
                },
                {
                    "nome": "Tobias Almeida",
                    "afiliacao": "UFLA",
                    "orcid": "https://orcid.org/0009-0008-3347-2713"
                },
                {
                    "nome": "Gabriel Cruz",
                    "afiliacao": "UFBA",
                    "orcid": "https://orcid.org/0009-0002-1916-8906"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Rhetorical structure theory",
                "marcadores discursivos",
                "sinalizadores discursivos"
            ],
            "referencias": [
                "Cardoso, P. C., Maziero, E. G., Jorge, M. L. C., Seno, E. M., Di Felippo, A., Rino, L. H. M., Nunes, M. d. G. V., e Pardo, T. A. (2011). CSTNews - A discourse-annotated corpus for single and multi-document summarization of news texts in Brazilian Portuguese. In Proceedings of the 3rd RST Brazilian Meeting, pages 88–105",
                "Dantas, E., Bárbara, L. d. J. S., Pereira, M. A., Gama, N. S., Almeida, T. J. A., Souza, J. W. d. C., Cardoso, P. C. F., e Rodrigues, R. (2024). Manual de anotação de sinalizadores discursivos em textos jornalísticos. Série de Relatórios Técnicos do Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo.",
                "Hovy, E. e Lavid, J. (2010). Towards a ‘science’of corpus annotation: a new methodological challenge for corpus linguistics. International Journal of Translation, 22(1):13–36.",
                "Krippendorff, K. (2011). Computing krippendorff’s alpha-reliability.",
                "Mann, W. C. e Thompson, S. A. (1987). Rhetorical Structure Theory: A theory of text organization. University of Southern California, Information Sciences Institute Los Angeles",
                "Marcu, D. (2000). The rhetorical parsing of unrestricted texts: A surface-based approach. Computational linguistics, 26(3):395–448.",
                "Pustejovsky, J. e Stubbs, A. (2012). Natural Language Annotation for Machine Learning: A guide to corpus-building for applications. O’Reilly Media, Inc.",
                "Zeldes, A. (2016). rstWeb-a browser-based annotation interface for Rhetorical Structure Theory and discourse relations. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 1–5."
            ],
            "artigo_completo": "Resumo. Por serem recursos que permitem a observac¸ ˜ao de comportamentos e usos lingu´ısticos e sociais, os corpora anotados passaram a ser de interesse de diferentes ´areas do conhecimento. No contexto da Rhetorical Structure Theory (RST) apresentamos neste trabalho os processos metodol´ogicos e pr´aticos de anotac¸ ˜ao de sinalizadores discursivos em um corpus jornal´ıstico do portuguˆes do Brasil. Ainda, apresentamos as primeiras avaliac¸ ˜oes (quanti e qualitativa) sobre as decis˜oes tomadas pelo grupo de anotadores.  1. Introduc¸ ˜ao A Lingu´ıstica de corpus (LC), enquanto ´area, instiga a utilizac¸ ˜ao de t´ecnicas e meto- dologias que nos levam a reunir grandes conjuntos de dados textuais (escritos, orais ou multimodais), a fim de descrever fenˆomenos lingu´ısticos. Em interface ao Processamento de Linguagem Natural (PLN), uma das tarefas que a LC se prop˜oe a realizar ´e a anotac¸ ˜ao desse conjunto de dados, tida como “o processo de enriquecer um corpus, adicionando informac¸ ˜oes lingu´ısticas inseridas por humanos ou m´aquinas com um objetivo te´orico ou pr´atico” [Pedro e Vale 2018].  Por serem recursos que permitem a observac¸ ˜ao de comportamentos e usos lingu´ısticos e sociais, os corpora anotados passaram a ser de interesse de diver- sas ´areas do conhecimento, como Humanidades digitais, Lingu´ıstica e Computac¸ ˜ao.   [Pustejovsky e Stubbs 2012] apontam que a an´alise dos corpora permite desvendar a na- tureza da linguagem e, consequentemente, capturar poss´ıveis propriedades que possam ser modeladas computacionalmente.  Por´em, esse processo de anotac¸ ˜ao tende a ser custoso, j´a que grande parte ´e re- alizada de forma semiautom´atica e requer intervenc¸ ˜ao humana. [Hovy e Lavid 2010] apresentam uma metodologia gen´erica sobre esse processo, que engloba etapas como preparac¸ ˜ao do conjunto de dados, instanciac¸ ˜ao da base te´orica, anotac¸ ˜ao de fragmentos do corpus, medic¸ ˜ao das decis˜oes de anotac¸ ˜ao e escalabilidade do processo de maneira autom´atica. No entanto, essa tarefa pode ser ajustada conforme o tipo de anotac¸ ˜ao a ser realizada, o que pode levar `a omiss˜ao de algumas das etapas sugeridas pelos autores.  [Taboada e Das 2013] e [Liu e Zeldes 2019], a partir de corpora pr´e-anotados com o modelo Rhetorical Structure Theory (RST) [Mann e Thompson 1987] identifica- ram uma s´erie de pistas lingu´ısticas e estruturais que serviam de sinalizadores para as relac¸ ˜oes discursivas previamente identificadas. Ambos os trabalhos organizaram os Si- nalizadores Discursivos (SD) em func¸ ˜ao de suas caracter´ısticas (semˆanticas ou sint´aticas, por exemplo), pondo em xeque a ideia de que as relac¸ ˜oes RST deveriam ser identificadas majoritariamente por meio de Marcadores Discursivos (MD), tidos como preposic¸ ˜oes e conjunc¸ ˜oes.  Com base nessa metodologia, [Rodrigues et al. 2023] descreveram SDs para al´em dos MDs a partir do corpus CSTNews [Cardoso et al. 2011]. Tal recurso lingu´ıstico- computacional consiste em um conjunto de textos jornal´ısticos em Portuguˆes que j´a havia sido anotado segundo o modelo RST. A RST prop˜oe que um texto coerente ´e formado por unidades m´ınimas de discurso (Elementary Discourse Units - EDU ou proposic¸ ˜oes) que desempenham func¸ ˜oes ret´oricas para que o objetivo comunicacional do autor seja atin- gido. Partindo dessa anotac¸ ˜ao pr´evia, os anotadores deste trabalho, por sua vez, identifi- caram apenas os sinalizadores que consideraram relevantes para caracterizar e/ou indicar determinadas relac¸ ˜oes, como em (1) - extra´ıdo do corpus CSTNews.  (1) [A selec¸ ˜ao brasileira masculina de vˆolei,]A [que ´e treinada por Bernardinho,]B [venceu a Finlˆandia por 3 sets a 0.]C  As porc¸ ˜oes (1a) e (1c) foram conectadas por meio da relac¸ ˜ao RST Same-Unit, j´a que est˜ao separadas por haver deta- indicando que se trata da mesma unidade, lhamento informacional em 1B em relac¸ ˜ao `a (1a) por meio da relac¸ ˜ao Elaboration. [Rodrigues et al. 2023] indicaram que a pontuac¸ ˜ao (no caso, v´ırgula), a concordˆancia ver- bal e o encaixamento de outra relac¸ ˜ao RST poderiam ser utilizadas como pistas para a identificac¸ ˜ao da relac¸ ˜ao Same-unit.  Esse estudo preliminar resultou em um manual de anotac¸ ˜ao de SDs em textos jornal´ısticos [Dantas et al. 2024], em que h´a, al´em de instruc¸ ˜oes, a proposta da primeira taxonomia de sinalizadores de relac¸ ˜oes RST para o PB. Destaca-se que esse tipo de re- curso com explicac¸ ˜oes, exemplos e instruc¸ ˜oes objetivas subsidia a decis˜ao dos anotadores diante de fatos novos e/ou j´a conhecidos [Duran et al. 2022].  Assim, objetivamos neste trabalho relatar as etapas metodol´ogicas e pr´aticas de anotac¸ ˜ao de SDs no corpus CSTNews, al´em de apontar as primeiras avaliac¸ ˜oes sobre as decis˜oes tomadas pelo grupo de anotadores. Para tanto, este trabalho est´a organizado em 5 sec¸ ˜oes, al´em desta Introduc¸ ˜ao. Na Sec¸ ˜ao 2, destacamos trabalhos relacionados ao   processo de anotac¸ ˜ao e an´alise em RST, sobretudo para o PB. Na Sec¸ ˜ao 3, detalhamos a metodologia de anotac¸ ˜ao empreendida neste estudo. Na Sec¸ ˜ao 4 apresentamos os resulta- dos e as discuss˜oes correspondentes. Por fim, na Sec¸ ˜ao 5, tecemos algumas considerac¸ ˜oes finais.  2. Trabalhos Relacionados  Identificar relac¸ ˜oes RST por meio de marcas expl´ıcitas no texto n˜ao ´e uma tarefa nova, especialmente em PLN para an´alise de discurso. Os MDs s˜ao tidos como conectivos en- tre porc¸ ˜oes textuais, sinalizando determinadas relac¸ ˜oes discursivas, como o “mas” para oposic¸ ˜ao, por exemplo. A an´alise das relac¸ ˜oes discursivas (ou de coerˆencia) est´a intima- mente ligada a descobrir a intenc¸ ˜ao do autor ao apresentar partes do texto em uma ordem e combinac¸ ˜ao espec´ıficas. Portanto, trata-se de uma tarefa que vai al´em de identificar os MDs.  A literatura [Marcu 2000, Pardo 2005, Taboada e Das 2013]  indica que a identificac¸ ˜ao de MD, em func¸ ˜ao da relac¸ ˜ao RST a que ocorrem, facilita o processamento do texto. Estudos recentes [Das e Taboada 2018, Liu e Zeldes 2019] afirmam que os MDs sinalizam apenas um n´umero restrito de relac¸ ˜oes dentro de um texto, e sugerem que as relac¸ ˜oes RST podem ser identificadas por sinais que v˜ao al´em deles. Como os MDs n˜ao marcam explicitamente as relac¸ ˜oes e n˜ao s˜ao exclusivos, a noc¸ ˜ao de SD parece ser mais apropriada do que a de MD nesse contexto.  [Das e Taboada 2018] argumentam que para uma comunicac¸ ˜ao ser eficaz, ´e funda- mental que as relac¸ ˜oes sejam interpretadas de maneira relativamente clara, o que requer sinalizadores precisos. Os autores acreditam que as relac¸ ˜oes de coerˆencia s˜ao entida- des cognitivas, e, portanto, ´e poss´ıvel descobrir como ouvintes e leitores as identificam usando indicadores que auxiliem o processo interpretativo. Utilizando o RST Discourse Treebank, os autores realizaram uma anotac¸ ˜ao detalhada dos SD, resultando no RST Sig- nalling Corpus (RST-SC). Eles observaram que pode haver relac¸ ˜oes sinalizadas por um ´unico sinalizador (como MD, referˆencias pessoais, orac¸ ˜oes relativas ou dois pontos) ou por combinac¸ ˜oes de SD (como v´ırgula + orac¸ ˜ao no partic´ıpio passado, ou construc¸ ˜ao sint´atica paralela + cadeia lexical). Quando surgia uma nova instˆancia de um tipo es- pec´ıfico de relac¸ ˜ao, os anotadores consultavam a taxonomia para encontrar o(s) sinaliza- dor(es) mais adequado para aquela instˆancia. Durante o processo de anotac¸ ˜ao, os autores observaram casos em que n˜ao foi poss´ıvel determinar com precis˜ao o SD que representava uma determinada relac¸ ˜ao.  Em [Liu e Zeldes 2019] descreve-se um esforc¸o de anotac¸ ˜ao para ancorar SD a partir de diversas categorias tais como sint´atica, semˆantica, gr´afico and morfol´ogica. Seus resultados mostraram que, com 11 documentos e 4.732 tokens, 923 foram instˆancias de SD, o que representou mais de 92% dos sinais discursivos. O tipo semˆantico representou a maioria dos casos, enquanto as relac¸ ˜oes discursivas ancoradas por DM corresponderam a apenas cerca de 8,5% dos tokens ancorados.  Quanto `a l´ıngua portuguesa, [Pardo 2005] foi o precursor em investigar a construc¸ ˜ao de analisadores discursivos. A partir de um corpus de textos cient´ıficos e anotado com RST, o autor identificou diversos padr˜oes de an´alise que especificam os re- lacionamentos entre as relac¸ ˜oes ret´oricas e seus marcadores textuais. Apesar de muitos padr˜oes serem baseados em MD, o autor ressalta que n˜ao existe uma relac¸ ˜ao sine qua non   entre MD e as relac¸ ˜oes que sinalizam, pois uma mesma relac¸ ˜ao pode ser sinalizada por v´arios marcadores (por exemplo, a relac¸ ˜ao Concession pode ser sinalizada pelos marcado- res “entretanto”, “no entanto”, entre outros) e um mesmo marcador pode sinalizar v´arias outras relac¸ ˜oes (por exemplo, o marcador “porque” pode sinalizar as relac¸ ˜oes Cause e Result (volitivas ou n˜ao), Justify, Explanation, entre outras).  [Maziero 2016]  Ainda com relac¸ ˜ao ao portuguˆes,  investigou atributos de organizac¸ ˜ao textual, da morfossintaxe, da sintaxe, da semˆantica e discurso para construir um analisador discursivo baseado na RST. A partir da an´alise de corpora anotados com RST, o autor aponta que: a) existem relac¸ ˜oes que apresentam grande subjetividade, tais como as relac¸ ˜oes Evidence, Justify e Explanation; b) a relac¸ ˜ao Same-unit ocorre apenas no n´ıvel intrassentencial, o que ´e esperado, pois ´e respons´avel por ligar proposic¸ ˜oes quebra- das por uma relac¸ ˜ao de Parenthetical ou Elaboration, por exemplo; c) algumas relac¸ ˜oes s˜ao mais frequentes no n´ıvel intrassentencial do que no inter-sentencial. Ap´os v´arios experimentos com aprendizado de m´aquina para identificac¸ ˜ao das relac¸ ˜oes discursivas no n´ıvel intrassentencial, o autor concluiu que atributos morfossint´aticos proporcionaram melhores resultados do que os atributos semˆanticos e discursivos.  3. Metodologia  A anotac¸ ˜ao de SDs foi feita a partir do corpus CSTNews1. Os textos do corpus est˜ao organizados em 50 conjuntos, com dois ou trˆes documentos que noticiam o mesmo evento. Por essa caracter´ıstica multidocumento e de redundˆancia, a anotac¸ ˜ao foi feita apenas no maior texto do conjunto, pois acreditamos que quanto maior for o texto, maior ´e a chance de encontrarmos mais relac¸ ˜oes RST e, possivelmente, essas relac¸ ˜oes ocorram nos outros outros textos da mesma colec¸ ˜ao. Nesse caso, foram separados 50 textos para esta tarefa de anotac¸ ˜ao.  A anotac¸ ˜ao de SDs foi realizada por meio da ferramenta rstWeb [Zeldes 2016], que ´e uma plataforma desenvolvida para facilitar a an´alise e a anotac¸ ˜ao de textos com base na RST. Essa ferramenta permite aos usu´arios realizar an´alises estruturais detalhadas dos textos, identificando proposic¸ ˜oes e suas relac¸ ˜oes de coerˆencia conforme proposto pela teoria. Neste trabalho, a taxonomia de SDs, na Figura 1, foi implementada.  Os textos escolhidos foram pr´e-processados e distribu´ıdos a um grupo de oito pesquisadores. A anotac¸ ˜ao aconteceu de maneira ass´ıncrona, em que cada anotador re- cebia semanalmente 3 ou 4 textos. Cada texto foi anotado por trˆes anotadores para que pud´essemos ter uma vers˜ao do corpus com a decis˜ao sobre a indicac¸ ˜ao dos SDs por mai- oria simples.  Para promover discuss˜ao e resoluc¸ ˜ao de d´uvidas, especialmente sobre casos n˜ao previstos pelo manual, foram conduzidas reuni˜oes semanais com o grupo. Al´em disso, dois dos anotadores, por terem mais experiˆencia com tarefas nesse sentido, nunca ficavam juntos no trio, para que pudessem auxiliar na resoluc¸ ˜ao de d´uvidas de maneira ass´ıncrona. Ressalta-se que os anotadores possu´ıam diferentes formac¸ ˜oes acadˆemicas (linguistas ou cientistas da computac¸ ˜ao) e com experiˆencias distintas em tarefas de anotac¸ ˜ao de corpus. Por conta disso, foi necess´aria uma etapa de treinamento para que o grupo se familiari- zasse com a taxonomia de SDs e com o modelo RST, al´em de ter acesso ao manual de  1Dispon´ıvel em: http://nilc.icmc.usp.br/CSTNews/login/?next=/CSTNews/   Figura 1. Taxonomia de sinalizadores discursivos proposta por Autores (2023).  Figura 2. Taxonomia de sinalizadores discursivos proposta por Autores (2023).  anotac¸ ˜ao para auxiliar em suas decis˜oes.  Na fase de treinamento, foi realizada a anotac¸ ˜ao de trˆes textos do corpus. Em reuni˜oes s´ıncronas, os anotadores puderam corrigir poss´ıveis equ´ıvocos e identificar quais unidades do discurso deveriam ser devidamente anotadas, ou seja EDUs que estivessem presentes em um mesmo per´ıodo sint´atico.  Na Figura 2, tˆem-se um exemplo de anotac¸ ˜ao para a relac¸ ˜ao Circumstance. Essa relac¸ ˜ao RST deve apresentar uma situac¸ ˜ao realiz´avel, em que o sat´elite (EDU 1, provˆe a situac¸ ˜ao que ´e apresentada no n´ucleo - EDU 2). No exemplo, tem-se que essa relac¸ ˜ao est´a sendo sinalizada por meio de Adv´erbio (vermelho), Orac¸ ˜ao circunstancial (azul) e Pontuac¸ ˜ao (lil´as).  Trˆes textos de diferentes tamanhos foram anotados por todos os anotadores em distintas fases do processo, com o objetivo de medir periodicamente a concordˆancia do grupo. Esse processo se repetiu a cada 10 conjuntos de textos anotados. Ao final de 2 meses, 47 textos foram anotados.   Trecho anotado com a relac¸ ˜ao RST Concession “Tal capacidade de mutac¸ ˜ao fez escola, mas dificilmente as criaturas saber˜ao superar o criador.”  Anotadores Sinalizadoes indicados  A B C  “mas” + “,” “mas” + “,” “mas”  Tabela 1. Comparac¸ ˜ao de anotac¸ ˜oes.  A concordˆancia foi medida automaticamente a partir de duas abordagens. Na abordagem gold observa-se estritamente o que o grupo de anotadores apontaram como sinalizador, sendo, portanto, mais restrita. J´a na abordagem silver definiu-se um intervalo de cinco janelas (`a esquerda e `a direita) em relac¸ ˜ao ao sinalizador-alvo, como demons- trado na Tabela 1.  Na Tabela 1, tem-se um exemplo de como as duas abordagens da concordˆancia foram aplicadas. Os anotadores A e B indicaram os mesmos sinalizadores, ao passo que o sinalizador C indicou apenas um em comum com o grupo. Caso fosse considerada apenas uma an´alise mais restritiva sobre a concordˆancia, a decis˜ao do anotador C prejudi- caria o c´alculo, ao passo que numa abordagem mais ampla, sua decis˜ao n˜ao traria tantos preju´ızos.  Apesar de todos os esforc¸os metodol´ogicos de distribuic¸ ˜ao de textos, e de todos os anotadores estarem alinhados junto ao modelo te´orico e `a ferramenta utilizados, ´e poss´ıvel que fatores externos `a tarefa influenciam na disposic¸ ˜ao dos anotadores, fazendo-os even- tualmente n˜ao apenas discordarem sobre um sinalizador, mas tamb´em n˜ao se atentarem a realizar a indicac¸ ˜ao adequada. Por conta disso, escolheu-se neste trabalho n˜ao ape- nas realizar uma an´alise mais restrita sobre a concordˆancia, mas tamb´em mais ampla, admitindo-se nesta, sobretudo, a dimens˜ao mais subjetiva da tarefa.  as  utiliza-se  abordagens  Em ambas  a medida Krippendorff Alpha [Krippendorff 2011]. Trata-se de uma medida que avalia a concordˆancia entre dois ou mais anotadores, o que se encaixa melhor no contexto deste trabalho, j´a que cada texto foi anotado por trˆes pessoas. O resultado da concordˆancia ´e medido num intervalo que varia entre -1 e 1, em que valores mais pr´oximos a 1 indicam alta concordˆancia; valores pr´oximos a 0 indicam baixa concordˆancia; e valores pr´oximos a -1 indicam discordˆancia total.  4. Resultados e Discuss˜ao  Na Tabela 2, tem-se a m´edia dos resultados das concordˆancias gold e silver da anotac¸ ˜ao em diferentes etapas do processo. Como dito, na fase de treinamento (clusters 1, 2 e 3), os anotadores realizaram uma primeira anotac¸ ˜ao e, ap´os reuni˜ao de alinhamento, fizeram correc¸ ˜oes. O c´alculo da concordˆancia geral (clusters 16, 31 e 39) foi feito sobre o mesmo texto anotado por todo o grupo.  Dado que o processo de anotac¸ ˜ao pode ser longo e complexo, fatores externos aos aspectos lingu´ısticos (como cansac¸o e diminuic¸ ˜ao da atenc¸ ˜ao, por exemplo) pode ter influenciado os anotadores. ´E poss´ıvel perceber isso ao comparar as fases de treinamento com as demais, em que as demais sofreram decr´escimos discretos. Al´em disso, outro poss´ıvel aspecto que pode ter influenciado nesse resultado ´e a distribuic¸ ˜ao das relac¸ ˜oes   Fase do trabalho Treinamento Concordˆanca geral Rodadas de anotac¸ ˜ao  Concordˆancia Gold Silver 0,693 0,581 0,596 0,460 0,691 0,496  Tabela 2. Resultado da concord ˆancia.  RST no corpus CSTNews. [Cardoso et al. 2011] apontam que h´a relac¸ ˜oes RST que ocor- rem apenas uma vez, como Otherwise, por exemplo, e outras que aconteceram de maneira predominante, como Elaboration, que ocorreu 1,514 vezes. Nesse caso, ´e poss´ıvel que, ao se deparar com uma relac¸ ˜ao RST n˜ao prevista na fase de treinamento e, portanto, au- sente no manual de anotac¸ ˜ao, os anotadores enfrentaram dificuldades em indicar poss´ıveis sinalizadores das relac¸ ˜oes em quest˜ao.  Al´em de uma an´alise quantitativa, foram feitas observac¸ ˜oes qualitativas prelimi- nares. Para tanto, durante a anotac¸ ˜ao, os anotadores realizaram indicac¸ ˜oes de d´uvidas, inconsistˆencias e/ou outras quest˜oes em um formul´ario eletrˆonico. Ao final de cada se- mana, todos os apontamentos eram compilados e discutidos entre o grupo para aprimorar o processo. A partir disso, ´e poss´ıvel destacar alguns pontos:  a) Considerac¸ ˜oes sobre o processo de anotac¸ ˜ao  Em caso de n˜ao encontrar uma etiqueta para representar o fenˆomeno observado, o anotador poderia registrar os tokens envolvidos e marcar como CPD (Casos Para Discutir depois). Em discuss˜oes e an´alises preliminares, os anotadores destacaram a intenc¸ ˜ao de marcar as entidades mencionadas no segmento textual. [Das e Taboada 2018], por sua vez, descrevem que os anotadores discordavam bastante entre entidade e tipos semˆanticos, ou seja, enquanto um anotador seleciona entidade como o sinal relevante para uma certa relac¸ ˜ao, o outro anotador a anota como sendo semˆantica. Os autores observaram que muitos dos atributos de entidade e caracter´ısticas semˆanticas na verdade se sobrep˜oem. Dessa forma, essa dificuldade acontece tamb´em para a l´ıngua inglesa.  Assim como [Liu 2019, Das e Taboada 2018] relatam, tamb´em observamos no corpus de estudo v´arias relac¸ ˜oes que n˜ao tinham um token expl´ıcito para servir de si- nalizador. Esses casos foram registrados como CPD. Por outro lado, as primeiras an´alises revelaram que alguns SD s˜ao altamente indicativos, enquanto outros s˜ao gen´ericos ou amb´ıguos. Assim, para obter uma compreens˜ao mais precisa, ´e necess´ario considerar os contextos ao redor dos SD para desambigu´a-los.  b) Considerac¸ ˜oes sobre dificuldades e limitac¸ ˜oes encontradas  A anotac¸ ˜ao das relac¸ ˜oes RST ´e um processo que se baseia na interpretac¸ ˜ao do analista. Assim, a depender dessa interpretac¸ ˜ao ser˜ao indicadas determinadas relac¸ ˜oes RST em detrimento de outras, resultando, ent˜ao, em diferentes sinalizadores para essas relac¸ ˜oes. Neste estudo, a identificac¸ ˜ao de SDs foi feita por um grupo majoritariamente diferente de quem fez a anotac¸ ˜ao RST, com uma distˆancia temporal consider´avel entre as duas tarefas. Esse fato, portanto, pode ter sido um dificultador para o grupo que fez a indicac¸ ˜ao dos sinalizadores.  Al´em disso, os anotadores destacaram que algumas relac¸ ˜oes RST utilizadas no   CSTNews s˜ao mais dif´ıceis de interpretar, e consequentemente, torna-se um desafio apon- tar SD espec´ıficos, como exemplificado em (2).  (2) (...) [com o Programa Fome Zero, conseguiu atingir o primeiro ponto das Metas do Milˆenio - erradicar a fome -, com dez anos de antecedˆencia,]A [reduzindo em mais da metade a pobreza extrema.]B  O trecho (2b) em relac¸ ˜ao ao trecho (2a) apresenta a relac¸ ˜ao Volitional result, ou seja, o resultado ocasionado foi n˜ao intencional. Nesse caso em espec´ıfico, os anotadores indicaram que o sentido do verbo “reduzindo” seria o indicativo do resultado, por´em sem menc¸ ˜ao ao aspecto volitivo. Destaca-se que a maioria dos rols de relac¸ ˜oes para outras l´ınguas n˜ao preveem diferenc¸a nesse aspecto.  Outro aspecto que parece ter apresentado dificuldade aos anotadores foi o fato de o manual de anotac¸ ˜ao ter sido desenvolvido com base no estudo de [Rodrigues et al. 2023] e os resultados da fase de treinamento. Como citado, relac¸ ˜oes e sinalizadores que n˜ao esta- vam previstos e que ocorreram ao longo do corpus podem ter ocasionado certos equ´ıvocos entre os anotadores. Ademais, o fato de o manual indicar certa correlac¸ ˜ao entre SDs e relac¸ ˜oes pode ter condicionado o olhar dos anotadores, como demonstrado em (3).  (3) [nesta terc¸a deve se encontrar com o relator do caso na Cˆamara, deputado Jos´e Carlos Ara´ujo (PR-BA)]A [para tratar do assunto.]B  De acordo com [Cardoso et al. 2011], a sentenc¸a entre (3a) e (3b) ´e de Purpose. O manual de anotac¸ ˜ao de SDs utilizou esse exemplo e indicou que a preposic¸ ˜ao “para” pode ser utilizada para identificar essa relac¸ ˜ao. Entretanto, o objetivo entre os segmentos pode tamb´em ser evidenciado por meio de “orac¸ ˜ao final” presente em (3b). Nesse caso, ´e poss´ıvel que os anotadores tenham sido condicionados a partir de determinados pressu- postos sobre as relac¸ ˜oes, ainda que tenham sido estimulados a indicarem em formul´ario eletrˆonico outros poss´ıveis SDs e definic¸ ˜oes n˜ao previstos no manual.  Por fim, cabe pontuar que no reposit´orio online do projeto de pesquisa “RST al´em dos marcadores discursivos”2 disponibilizamos para consulta o corpus com a vers˜ao uni- ficada entre os anotadores, a anotac¸ ˜ao de SDs e a planilha completa da concordˆancia dos anotadores.  5. Considerac¸ ˜oes Finais  Neste trabalho buscamos detalhar a metodologia empregada na identificac¸ ˜ao de SDs em textos jornal´ısticos a partir da taxonomia proposta por [Dantas et al. 2024]. Destacamos que um estudo com essa abordagem em PB ainda n˜ao havia sido realizado, ao contr´ario do que j´a ocorre em outros idiomas, especialmente o inglˆes.  Os resultados relatados podem subsidiar outras an´alises em estudos futuros. Um desses estudos se concentra na investigac¸ ˜ao quali e quantitativa da correlac¸ ˜ao entre SDs e as relac¸ ˜oes RST, algo j´a iniciado por [Rodrigues et al. 2023] e tal como outros trabalhos fizeram [Liu 2019, Das e Taboada 2018, Pardo 2005]. Outro estudo ser´a em relac¸ ˜ao `a concordˆancia de aspectos da anotac¸ ˜ao, como tipos (sint´atico e semˆantico, por exemplo) e subtipos (pronome relativo e conhecimento de mundo, por exemplo) dos sinalizadores. Ao final desses estudos ser´a poss´ıvel fazer o levantamento da distribuic¸ ˜ao dos SDs no  2Dispon´ıvel em https://sites.google.com/view/rst-poetisa/   corpus, bem como observar quais s˜ao mais ou menos consensuais entre os anotadores.  Dados os apontamentos cr´ıticos realizados sobre as limitac¸ ˜oes identificadas, destaca-se que este trabalho apresenta potencial de servir de diretriz de investigac¸ ˜oes de an´alises sobre as relac¸ ˜oes RST e seus SDs e aprimoramento de ferramentas e recursos para anotac¸ ˜ao de corpus. Tais aspectos s˜ao de extrema importˆancia ao alargar a anotac¸ ˜ao a escalas maiores buscando n˜ao apenas ampliar a quantidade de textos, mas tamb´em di- versificar os gˆeneros textuais a serem considerados.  "
        },
        {
            "titulo": "Genipapo - A Multigenre Dependency Parser for Brazilian Portuguese",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31138",
            "idioma": "Inglês",
            "storage_key": "files/article_31138_30941.pdf",
            "autores": [
                {
                    "nome": "Ariani Di Felippo",
                    "afiliacao": "USP / UFSCar",
                    "orcid": "http://orcid.org/0000-0002-4566-9352"
                },
                {
                    "nome": "Norton T. Roman",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-0563-2045"
                },
                {
                    "nome": "Bryan K. S. Barbosa",
                    "afiliacao": "USP / UFSCar",
                    "orcid": "https://orcid.org/0000-0002-4637-6498"
                },
                {
                    "nome": "Thiago A. S. Pardo",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2111-1319"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Neste artigo, apresenta-se um esforço pioneiro para o desenvolvimento de um modelo de parsing multigênero para o português brasileiro. Seguindo o projeto Universal Dependencies, treinou-se um dos modelos do estado-da-arte em três corpora gold-standard de diferentes gêneros textuais (jornalístico, acadêmico e conteúdo gerado por usuário – postagens do X). Os experimentos revelam que nosso modelo multigênero de parsing produz resultados melhores ou competitivos em relação aos modelos de gênero único.",
            "keywords": [
                "dependency parser",
                "multigenre",
                "Universal Dependencies",
                "Brazilian Portuguese"
            ],
            "referencias": [
                "Bai, J., Wang, Y., Chen, Y., Yang, Y., Bai, J., Yu, J., and Tong, Y. (2021). Syntax-BERT: Improving pre-trained transformers with syntax trees. In Proceedings of the 16th Conference of the EACL, p. 3011–3020.",
                "Barbosa, B. K. d. S. (2024). Descrição sintático-semântica de nomes predicadores em tweets do mercado financeiro em português. Master’s thesis, Programa de Pós-Gradução em Linguísica, Universidade Federal de São Carlos.",
                "Bick, E. (2000). The Parsing System “Palavras”. Automatic Grammatical Analysis of Portuguese in a Constraint Grammar Framework. University of Arhus, Arhus.",
                "Bölücü, N., Rybinski, M., and Wan, S. (2023). Investigating the impact of syntax-enriched transformers on quantity extraction in scientific texts. In Proceedings of the 2nd Workshop on Information Extraction from Scientific Publications, p. 1–13, Bali.",
                "Candido, A., Maziero, E., Specia, L., Gasperin, C., Pardo, T., and Aluisio, S. (2009). Supporting the adaptation of texts for poor literacy readers: a text simplification editor for Brazilian Portuguese. In Proceedings of the 4th Workshop on Innovative Use of NLP for Building Educational Applications, p. 34–42, Boulder, Colorado.",
                "da Silva, F. J. V., Roman, N. T., and Carvalho, A. M. B. R. (2020). Stock market tweets annotated with emotions. Corpora, 15(3):343–354.",
                "de Marneffe, M.C., Manning, C. D., Nivre, J., and Zeman, D. (2021). Universal Dependencies. Computational Linguistics, 47(2):255–308.",
                "Duran, M. S. (2022). Manual de anotação de relações de dependência - versão revisada e estendida: orientações para anotação de relações de dependência sintática em língua portuguesa, seguindo as diretrizes da abordagem Universal Dependencies (UD).",
                "Jurafsky, D. and Martin, J. H. (2024). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models. Online manuscript released August 20, 2024.",
                "Lin, Y., Wang, C., Song, H., and Li, Y. (2021). Multi-head self-attention transformation networks for aspect-based sentiment analysis. IEEE Access, 9:8762–8770.",
                "Lopes, L. and Pardo, T. (2024). Towards portparser - a highly accurate parsing system for Brazilian Portuguese following the Universal Dependencies framework. In Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, p. 401–410, Santiago de Compostela, Galicia/Spain. ACL.",
                "Nivre, J., de Marneffe, M.-C., Ginter, F., Hajič, J., Manning, C. D., Pyysalo, S., Schuster, S., Tyers, F., and Zeman, D. (2020). Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, p. 4034–4043, Marseille, France. ELRA.",
                "Rademaker, A., Chalub, F., Real, L., Freitas, C., Bick, E., and de Paiva, V. (2017). Universal Dependencies for Portuguese. In Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017), p. 197–206, Pisa, Italy. Linköping University Electronic Press.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained bert models for brazilian portuguese. In Intelligent Systems, p. 403–417, Cham. Springer International Publishing.",
                "Zhou, J., Zhang, Z., Zhao, H., and Zhang, S. (2020). LIMIT-BERT: Linguistics informed multi-task BERT. In Findings of the Association for Computational Linguistics: EMNLP 2020, p. 4450–4461.",
                "Zilio, L., Wilkens, R., and Fairon, C. (2018). Passport: A dependency parsing model for portuguese. In Computational Processing of the Portuguese Language, p. 479–489, Cham. Springer International Publishing."
            ],
            "artigo_completo": "Resumo. Neste artigo, apresenta-se um esforc¸o pioneiro para o desenvolvi- mento de um modelo de parsing multigˆenero para o portuguˆes brasileiro. Seguindo o projeto Universal Dependencies, treinou-se um dos modelos do estado-da-arte em trˆes corpora gold-standard de diferentes gˆeneros textuais (jornal´ıstico, acadˆemico e conte´udo gerado por usu´ario – postagens do X). Os experimentos revelam que nosso modelo multigˆenero de parsing produz resulta- dos melhores ou competitivos em relac¸ ˜ao aos modelos de gˆenero ´unico.  1. Introduction  Syntactic parsing is the task of automatically uncovering the syntactic relations among the words of a sentence, resulting in syntactic trees, which correspond to one of the first analysis levels in Natural Language Processing (NLP) [Jurafsky and Martin 2024]. This task has proved useful for several different applications, such as text simplification, infor- mation extraction, automatic summarization, and sentiment analysis, among many others.  In the beginning,  As time goes by, parsing takes different importance degrees and attend dif- ferent desires. it was common to have parsing as a step in NLP applications (e.g. grammar checking [Martins et al. 1998] and text simplification [Candido et al. 2009]). Recent advances in deep learning, distributional models, and language modeling have allowed many applications to forgo deeper linguistic analysis, but current research efforts have indicated that the inclusion of linguistic knowledge during model training or in post-processing steps (e.g. in neuro-symbolic approaches)   may be relevant for improving results [Zhou et al. 2020, Bai et al. 2021, Lin et al. 2021, B¨ol¨uc¨u et al. 2023]. Moreover, given the expensive computational requirements for train- ing the above models and the search for explicability and interpretability, linguistic anal- ysis systems have reemerged as relevant alternatives in several research situations.  There are some well known parsers for Portuguese, including those considered classic, such as PALAVRAS [Bick 2000] and PassPort [Zilio et al. 2018], and more re- cent ones aligned to the Universal Dependencies (UD) project [de Marneffe et al. 2021], as UDPipe state-of-the-art Portparser the [Lopes and Pardo 2024] (with accuracy near 95% for news texts).  [Straka 2018]  current  and  2  We propose here to move a step further in parsing for Brazilian Portuguese (BP). Using the different annotated corpora that are available in the UD initiative, and adopting a widely known parsing framework (the Stanza pipeline [Qi et al. 2020]), we investigate the issue of multigenre parsing, aiming at producing a parser that works well for differ- ent language writing styles, including short and usually syntactically fragmented X posts (formerly known as tweets), “daily language” of news texts and (supposedly) more re- fined writing of academic texts. The resulting system, named Genipapo1 (an acronym for “multiGENre PArser for POrtuguese”), achieves better or competitive results in relation to the single-genre trained parsers, consisting in a step to unleash the potential of Portuguese text analysis tools to work on a wide variety of texts.  The rest of this article is organized as follows. Section 2 introduces the UD frame- work. Section 3 briefly presents the main related work in the area. The adopted resources and methodology are reported in Section 4, whereas Section 5 presents the results of our experiments. We conclude this article in Section 6.  2. The Universal Dependencies framework  UD [Nivre et al. 2020] is currently the most used dependency-based framework of mor- phological and syntactic analysis in NLP [Sanguinetti et al. 2023]. It is an attempt to standardize the annotation of morphology and syntax, proposing a “universal” annotation strategy for all languages, facilitating the development of multilingual taggers and parsers. At the time of this writing, there are already over 240 treebanks available for more than 150 languages, dealing with a variety of textual genres.  In UD, the following morphology information is considered: (i) Part-of-Speech (PoS) tags, (ii) lemmas, and (iii) features. The syntactic annotation consists of typed de- pendency relations (deprels) between words. Currently, the model has 17 PoS tags and 37 deprels, plus a non-fixed set of morphological features. Figure 1 shows an example of an annotated post from the DANTEStocks corpus [Di-Felippo et al. 2021]. The basic depen- dency representation is a tree, where exactly one word is the head of the utterance (root) (e.g. “assina” – “sign”), and all the remaining words depend on some other word. The labeled arcs represent the dependency relations, pointing from heads to their dependents. PoS tags, lemmas, and morphological features are displayed below the words in Figure 1.  1The corresponding fruit, “Jenipapo” (with ‘J’ instead of ‘G’), is a tropical fruit, appreciated in several states of Brazil and used for different purposes, from painting to eating and preparing beverages. By adopt- ing this inspiration for the name of our parser, we sought this symbolic connection with something rooted in the Brazilian culture and language.   Figure 1. Example of UD morphological and syntactic annotation.  3. Related work  About the linguistic resources for training UD-parsers, there are some available datasets in BP. One of the first corpora with UD annotation for texts in standard (or canonical) Portuguese is the UD-Portuguese-Bosque treebank [Rademaker et al. 2017], which con- tains 210,958 tokens across 9,357 sentences. The Brazilian portion of this corpus consists of 4,213 well-written sentences extracted from journalistic texts. There is also Petro- Gold [Souza et al. 2021], which is a fully revised treebank that consists of academic texts from the oil and gas domain, in a total of 8,946 sentences (and 232,333 tokens). Differ- ently from UD-Portuguese-Bosque, PetroGold is a specialised or domain-specific corpus. Besides, the UD project makes available the UD-Portuguese-GSD corpus [Zeman 2017]. Totaling 12,020 sentences (296,169 tokens) from news texts and blogs, it features two different textual genres, with different degree of canonicalness.  Specifically aiming at growing syntax-based resources for BP, another treebank (with genres beyond newswire texts) has been created. Porttinari [Pardo et al. 2021] currently includes two main genres (with others under construction): (i) news texts, rep- resenting standard written language, and (ii) user-generated content (UGC), representing informal non-canonical web language (in particular, tweets/X posts).  Concerning parsing models, some dependency UD-parsers are available for BP, specially for news texts. UDPipe 2 [Straka 2018] is probably the most used model. Using a graph-based biaffine attention architecture, it achieves a Labelled Attachment Score2 (LAS) of 87.04% for news texts. Stanza [Qi et al. 2020] is another well-known sys- tem, which uses a feature-enriched Bi-LSTM-based deep biaffine neural method. Ac- cording to the results for the UD version 2.123, Stanza achieves 87.75% of LAS for news texts. UDify [Kondratyuk and Straka 2019] is another important system. It is a semi-supervised multitask self-attention model. There is also the recently released Port- Parser [Lopes and Pardo 2024], which was built by training UDPipe 2 with BERTim- bau [Souza et al. 2020] on the Porttinari-base corpus [Duran et al. 2023a], which is part of the journalistic portion of the larger Porttinari4 [Pardo et al. 2021] treebank. The model  2This score evaluates the output of a parser by considering how many words have been assigned both  the correct syntactic head and the correct label of the relation [Nivre and Fang 2017].  3https://stanfordnlp.github.io/stanza/ performance.html 4https://sites.google.com/icmc.usp.br/poetisa/porttinari   achieved LAS around 95%. This LAS value brings an improvement of around 7% over some well-known existing baselines for standard written Portuguese language.  As a final example, it is important to cite the work of [Zilio et al. 2018]. However delivering lower results than those by more recent works, the authors compared some previous and classical parsing methods for BP. The authors reported that the best model (called PassPort) achieved LAS of 85.21% in the UD corpus. In an additional small scale evaluation, the PassPort was manually compared to PALAVRAS, using a single corpus of 90 sentences (1,295 tokens), randomly selected from three different genres, to wit, literature, news texts and subtitles. The systems achieved similar results for dependency parsing, with a LAS of 85.02% for PassPort against 84.36% for PALAVRAS.  4. Materials and methods  Given the objective of building a multigenre UD parser for BP, three corpora, belonging to three different genres, build our materials.  Our first corpus, DANTEStocks [Di-Felippo et al. 2021], comprises 4,048 tweets (with 81,048 tokens) from the stock market domain automatically collected during 2014 (which limits each post to 140 characters). The corpus was built by fetching mes- sages containing a ticker5 of one of the 73 stocks that composed Ibovespa at that time [da Silva et al. 2020]. DANTEStocks presents a combination of standard and non-standard written language, as well as speech marks, domain specific vocabulary and medium (Twit- ter) features. The dependency relations of the corpus were annotated in two semiauto- matic stages [Barbosa 2024]. First, a reference subcorpus of 1,000 tweets was annotated using UDPipe 2, which had been trained on UD-Portuguese-Bosque and was chosen be- cause it is easily available for use online and offers reliable performance. This subcorpus was then manually revised before being designated as a gold standard. The rest of the corpus was then annotated by customizing Stanza for DANTEStocks. We used the com- bined Porttinari-base and reference subcorpus as the initial training set for Stanza. The resulting parsing model was used to automatically annotate a new (first) package of data (out of the remaining 3,048 tweets). The first package was manually revised and incorpo- rated to the previous dataset, being used to start a new training run of Stanza. This cycle of training iteration continued incrementally until the last (in a total of 6) package was annotated/revised. Regarding LAS, the final score (6th run) achieved 94.62%, increasing 0.76% from the first run score of 93.86%.  It  The second corpus, PetroGold [de Souza and Freitas 2023], is a gold-standard integrates the Petrolˆes corpus, treebank for the oil and gas (O&G) domain. which is a collection of academic and technical documents from public agencies such as Petrobras and “Agˆencia Nacional do Petr´oleo, G´as Natural e Biocombust´ıveis” (ANP) [Gomes et al. 2021]. PetroGold is composed of 19 academic texts (theses and dissertations), with a total of 9,127 sentences and 253,640 tokens. The syntactic anno- tation of PetroGold also followed a semiautomatic approach. Especifically, four experts were responsable for reviewing the output of a customized version of Stanza, trained on the combination of UD-Portuguese-Bosque (v.2.6) and a small collection of data from the O&G domain. Through an intrinsic evaluation using a model created by the UDPipe  5A five or six-character alphanumerical string that represents a type of stock from a company, such as  “PETR4” for Petrobras’ preferred stock.   tool, the corpus achieves 88.53% of LAS. For NLP purposes, the corpus is subdivided into three subsets. The subsets have 7,170, 737 and 1,039 sentences for training (80%), validation (8%) and test (12%).  Our final corpus, Porttinari-base [Duran et al. 2023a], is the gold-standard (i.e. fully manually annotated and revised) journalistic subcorpus of Porttinari, which is com- posed of 8,418 sentences (168,080 tokens) selected from Folha de S˜ao Paulo newspaper. The Porttinari-base annotation process started with an automatic annotation by UDPipe 2 using the UD-Portuguese-Bosque corpus, which achieved 87% accuracy (in terms of LAS). Next, the dependency relations were manually revised in detail following an an- notation manual containing specific guidelines for BP [Duran 2022]. Porttinari-base is also subdivided into training, validation and test subsets. The subsets have 5,893, 842 and 1,683 sentences in the train (70%), dev (10%) and test (20%) files, respectively.  For developing our parser, we employed the Stanza pipeline, which was trained and evaluated across different corpora. Since both PetroGold and Porttinari-base corpora already come subdivided in train, validation and test sets, we first set apart their test sets to ensure they would only be used for final evaluation purposes. After this, we unified each corpus’ train and validation sets to build a larger training set for each, which was then used in our experiments. Next, we randomly split (from a uniform distribution), DANTEStocks in training and test sets, following the same principle of keeping the test set strictly for final testing. Table 1 details each set size across the corpora.  Table 1. Size and proportion of train and test sets across corpora.  Train  Test  Corpus  DANTEStocks  Units Proportion Units Proportion 3,643 UP-Portuguese-PetroGold 7,907 6,735  405 1,039 1,683  10% 12% 20%  90% 88% 80%  Porttinari-base  Unit  tweet sentence sentence  To assess the model’s performance across different genres, we combined the train- ing sets from all three corpora to create a fourth, unified training set, along with a corre- sponding test set. A grid search was conducted for hyperparameter optimization, focusing on batch size (2000, 3000, 4000, and 5000) and dropout rate (0.2, 0.3, and 0.4), since Stanza does not natively support learning rate adjustments. Next, we ran 5-fold cross- validation with grid search (using the above mentioned grid) at each of the four training sets6, whereby each set was further split in five subsets, with four being used to train the model, and the fifth one being held for validation purposes. This subdivision procedure is repeated five times. We then selected, for each training set, the hyperparameters that produced the highest LAS value across the validation sets during cross-validation.  Having the best set of hyperparameters for each of the four corpora (DANTE- Stocks, PetroGold, Porttinari-base and their union), we retrained the model at each corpus training set, varying its random seed (42, 123, 456, 789 and 101,112), thereby changing the model’s innitial configurations. To do so, PetroGold’s and Porttinari-base’s training sets were split back into their original training and validation sets, whereas DANTEStocks’ training set was randomly split into training and validation sets, so that the entire DAN- TEStocks corpus would contain 10% of the data for test, 10% for validation and 80% for  6I.e. each corpus’ individual training set and the largest set built from the union of these training sets.   training purposes. The best performance model, across all seeds, was then selected for each corpus. As a final step, all four models were tested and compared using the previ- ously separated test sets, which had been reserved exclusively for this final evaluation.  5. Results and discussion  Tables 2 and 3 present the results of our model, when trained in each corpus’ training set (rows in the tables), and tested at the different test sets of the experiment. Table 2 refers to model results in terms of LAS, whereas Table 3 presents the results in terms of Unlabelled Attachment Score7 (UAS). In the tables, the “Genipapo” lines refer to the model trained in all of the available training sets, i.e. our multigenre model, while the “All together” columns refer to the union of all test sets.  Table 2. Model’s LAS (%) at each corpus’ test set.  Training set  Porttinari-base DANTEStocks PetroGold DANTEStocks + Port.-base DANTEStocks + PetroGold Porttinari-base + PetroGold Genipapo  Test set Porttinari-base DANTEStocks PetroGold All together  94.82 87.61 86.74 94.91 87.66 94.92 94.94  66.10 91.95 61.30 92.67 91.85 66.75 92.69  87.47 83.68 95.33 87.94 84.10 95.29 95.11  88.48 86.48 87.30 91.90 86.66 91.84 94.75  Table 3. Model’s UAS (%) at each corpus’ test set.  Training set  Porttinari-base DANTEStocks PetroGold DANTEStocks + Port.-base DANTEStocks + PetroGold Porttinari-base + PetroGold Genipapo  Test set Porttinari-base DANTEStocks PetroGold All together  95.88 90.36 89.69 95.95 90.26 95.91 95.97  75.55 93.98 71.45 94.39 93.97 76.03 94.42  90.38 87.51 95.84 90.97 88.04 96.00 95.81  91.27 89.63 90.15 93.86 89.76 93.67 95.73  We see that each model trained in isolation produces the best results for its corre- sponding genre. For instance, considering LAS, training with Porttinari-base produced the best results for the test set of Porttinari-base (94.82%) and worse results for DAN- TEStocks (66.10%) and PetroGold (87.47%). This pattern holds across the genres, where the isolated models consistently perform best when tested on the same genre they were trained on. More interestingly, Genipapo, our multigenre parser, outperforms the single- genre trained parsers for 2 of the genres (news texts and X posts), but not for the academic genre. The differences, however, are minimal (less than 1%), suggesting that they could be due to random fluctuation rather than statistically significant differences.  When combining all test sets (“All together” columns in the tables), Genipapo delivers the best results, achieving a 7% improvement in LAS and nearly 5% in UAS  7UAS indicates the accuracy of the head ignoring the relation’s name (deprel) [Nivre and Fang 2017].   compared to the second-best results from single-genre parsers, and a 3% LAS and 2% UAS improvement over parsers trained on pairs. This suggests that Genipapo may be the more suitable choice for processing texts from varied sources, such as diverse web content.  By looking at the results produced by Genipapo, when tested on each corpus sepa- rately, we see some common mistakes between pairs of deprels. One of the most common errors across the three corpora was the confusion between obl and nmod. This result does not come as a total surprise, since the classification of a nominal as an adverbial adjunct (obl) or as a nominal modifier (nmod) was already reported in the literature as a chal- lenge for parsing standard Portuguese (and also for humans in some situations), such as in journalistic and academic texts [Duran et al. 2023b, Souza et al. 2021]. Once this phe- nomenon is also observed in DANTEStocks’ UGC, this difficulty seems to be unrelated to the degree of “canonicalness” of the corpus. The pairs acl (adnominal clause) and advcl (adverbial clause) and obj (the second argument of a verb) and nsubj (a nominal subject) show a relevant confusion only in the standard language corpora. The confusion between acl and advcl seems to be a case of ambiguity that requires semantic knowledge to be solved, and the confusion between obj and nsubj occurs when the candidate to the subject is at the right of the verb, since noun phrases at the right can be either object or subject in Portuguese [Duran et al. 2023b].  When comparing the errors of Genipapo at each deprel, we see the model making a higher number of wrong root predictions in DANTEStocks, given its error rate of 7.7% against 2.0% in Porttinari-base and 0.9% in PetroGold. This might be due to the lin- guistic phenomena of tweets that bring some difficulty to the syntactic annotation of the root. Another interesting observation relates to parataxis, which is one the the most fre- quent tag in our UGC corpus (708 cases), but not in the remaining corpora. The relatively low error rate in DANTEStocks (9.3%) indicates that this deprel has been well learned by Genipapo in UGC. Moreover, we could see that the deprel tags most wrongfully predicted due to under representation in Porttinari-base and DANTEStocks are the same: reparan- dum, dislocated and orphan. The first two tags do not occur in PetroGold, and the only two occurrences of orphan in this corpus were wrongly predicted.  As a way to compare Genipapo’s performance with that by a state-of-the-art model, we also run Portparser in the same testing sets as Genipapo (Table 4). We note that the training, validation, and test splits of the Porttinari-base used by Portparser differ from those publicly available and used in our experiments with Genipapo. This discrep- ancy means that some sentences present in the test sets of Porttinari-base and the unified set (All together) may have been included in the training or validation sets of Portparser, artificially boosting its LAS and UAS scores. Despite this, Genipapo outperformed Port- parser across all testing sets except for the Porttinari-base test set. In terms of LAS, Genipapo showed significant improvements over Portparser on the DANTEStocks test set (92.69% vs. 64.45%), the PetroGold test set (95.11% vs. 86.74%), and the combined test set (94.75% vs. 89.51%). However, Portparser performed better on the Porttinari- base test set (98.06% vs. 94.94%). The same pattern is observed in UAS scores, where Genipapo outperformed Portparser on DANTEStocks (94.42% vs. 75.81%), PetroGold (95.81% vs. 90.50%), and the combined test set (95.73% vs. 92.62%). Nevertheless, Portparser achieved higher UAS on Porttinari-base (98.58% vs. 95.97%).   Table 4. Portparser’s LAS and UAS at each corpus’ test set.  Test set Porttinari-base DANTEStocks PetroGold All together  LAS (%) UAS (%)  98.06 64.45 86.74 89.51  98.58 75.81 90.50 92.62  6. Final remarks  In this paper, we introduced Genipapo, a multigenre UD-parser for Brazilian Portuguese, and showed that it had better or competitive performance in relation to genre specific trained parsers. Future work includes (i) to extend Genipapo’s training to other genres and domains, such as audio transcriptions, literary texts, and tweets related to the COVID-19 pandemic, whose corpora are still under construction, and (ii) to explore different parsing strategies and pipelines.  More details about this work may be found at the POeTiSA project web portal:  https://sites.google.com/icmc.usp.br/poetisa/.  Acknowledgements  This work was carried out at the Center for Artificial Intelligence of the University of S˜ao Paulo (C4AI-http://c4ai.inova.usp.br/), with support by the S˜ao Paulo Research Founda- tion (FAPESP 2019/07665-4) and by the IBM Corporation. The project was also sup- ported by the Ministry of Science, Technology and Innovation, with resources of Law N.8.248, of October 23, 1991, with in the scope of PPI-SOFTEX, coordinated by Softex and published as Residence in TIC13, DOU01245.010222/2022-44.  "
        },
        {
            "titulo": "Adapting LLMs to New Domains: A Comparative Study of Fine-Tuning and RAG strategies for Portuguese QA Tasks",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31139",
            "idioma": "Inglês",
            "storage_key": "files/article_31139_30942.pdf",
            "autores": [
                {
                    "nome": "Leandro Yamachita da Costa",
                    "afiliacao": "UFRJ",
                    "orcid": "http://orcid.org/0009-0001-7932-6163"
                },
                {
                    "nome": "João Baptista de Oliveira e Souza Filho",
                    "afiliacao": "UFRJ",
                    "orcid": "https://orcid.org/0000-0001-6005-8480"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Retrieval Augmented Generation",
                "RAG",
                "Fine-Tuning",
                "LLMs",
                "Portuguese Question Answering"
            ],
            "referencias": [
                "Sparck Jones, Karen. \"A statistical interpretation of term specificity and its application in retrieval.\" Journal of documentation 28.1 (1972): 11-21.",
                "Robertson, Stephen E. and Hugo Zaragoza. “The Probabilistic Relevance Framework: BM25 and Beyond.” Found. Trends Inf. Retr. 3 (2009): 333-389.",
                "“Introducing Meta Llama 3: The most capable openly available LLM to date.” AI at Meta. (2024).",
                "Wagner Filho, Jorge A., et al. \"The brWaC corpus: A new open resource for Brazilian Portuguese.\" Proceedings of the eleventh international conference on language resources and evaluation LREC (2018).",
                "Lin, Chin-Yew. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics (2004).",
                "Conover, Mike, et al. \"Free Dolly: Introducing the world’s first truly open instruction-tuned LLM.\" Company Blog of Databricks (2023)."
            ],
            "artigo_completo": "Abstract. The rise of Large Language Models (LLMs) represented a  significant advance in text generation applications.  However, LLMs face  challenges in domains outside the scope of their original training. This  study investigates the following two approaches to adapt LLMs to new  domains in the context of generative question-answering (QA) with data in  Portuguese: fine-tuning and Retrieval-Augmented Generation (RAG).  The  experiments carried out in this study demonstrate the effectiveness of  incorporating external data sources, even in models that had not been  adjusted  for  the  specific  domain.  Furthermore,  the  combination  of  supervised fine-tuning with RAG proved to be the most effective approach.    1. Introduction   The rise of Large Language Models (LLMs) marked a significant advance in Natural  Language Processing (NLP), especially in text generation tasks [Brown et al., 2020;  Achiam  et  al.,  2023].  These  models,  which  are  trained  with  large  volumes  of  data,  can  retain  vast  amounts  of  knowledge  implicitly  in  their  parameters.  However,  LLMs  face  challenges  in  domains  outside  the  scope  of  their  original  training  data,  such as areas of specialized knowledge or current affairs [Kandpal et al., 2023, Kasai  et  al.,  2024].  This  issue  accentuates  the  need  to  adapt  LLMs  to  specific  contexts,  especially for smaller models with limited memory capacity.     This study explores the adaptation of LLMs to new domains in the generative  question-answer (QA) task, a scenario where the model generates answers based on  questions provided to it. Fine-tuning is a common approach to adjust LLMs to new  domains  by  modifying  the  model  parameters  with  training  on  application-specific  data.  In  the  QA  task,  fine-tuning  can  be  done  with  question-answers  pairs  in  a  'closed-book' scenario [Zhang et al., 2024], where the model does not have access to  external  information.  Nonetheless,  this  approach  may  require  considerable  computational power and extensive data annotation work [Guo et al., 2023].     A  widely  adopted  alternative  is  the  integration  of  external  knowledge  sources, such as documents and books, in a setting known as 'open-book' [Zhang et  al., 2024]. A typical strategy of this approach is the Retrieval-Augmented Generation           (RAG)  [Lewis  et  al.,  2020],  which  combines  an  information  retriever  (IR)  model,  aimed at searching relevant data on an external data source, with a language model  to generate answers based on the information retrieved.  This strategy allows LLMs  to adapt to new domains without the need for fine-tuning.   This work analyzed these two approaches to adapt LLMs to specific domains  in  the  QA  task  with  data  in  Portuguese.  We  analyzed  both  fine-tuning  and  RAG  configurations,  in  addition  to  the  integration  of  the  two.  The  experiments  demonstrated the effectiveness of incorporating external data sources for improved  results.  Moreover,  different  fine-tuning  strategies  have  shown  to  be  particularly  effective  when  combined  with  the  inclusion  of  external  data,  even  with  a  reduced  volume  of  training  data.  For  all  analysis,  we  considered  a  scenario  with  limited  computational  resources,  where  we  only  used  a  general-purpose  GPU  (Nvidia  GeForce RTX 4090). The fine-tuning of the models was performed using the QLoRA  technique [Dettmers et al., 2023] with quantized models.   2. Background and Related Work   In this session we briefly discuss RAG and LLMs fine-tuning.   2.1. Retrieval-augmented Language Models   Conditioning  LLM  responses  to  information  from  external  sources  has  proven  effective  in  adapting  these  models  to  specific  domains  in  several  NLP  tasks.    The  RAG approach has been successfully applied in areas such as agriculture [Balaguer et  al.,  2024],  scientific  literature  [Lála  et  al.,  2023],  and  medical  data  [Zakka  et  al.,  2024];  attesting  its  utility  in  improving  the  accuracy  and  relevance  of  answers.  Additionally, it can reduce the occurrence of hallucinations [Borgeaud et al., 2022;  Shuster et al., 2021], improve the model's ability to manage the gradual decline in its  knowledge  over  time  [Vu  et  al.,  2023],  and  enhance  the  interpretability  of  the  answers [Lewis et al., 2020; Izacard et al., 2023].     The effectiveness of RAG depends on the quality of the retrieval mechanism  used, which impacts the relevance of the contextual information obtained.  Among  traditional  retrieval  mechanisms,  those  based  on  term  frequency  stand  out,  which  employ sparse representations of text passages, such as TF-IDF [Sparck Jones, 1972]  and  BM25  [Robertson  and  Zaragoza,  2009].  Alternatively,  more  recent  approaches  employ  dense  representations  of  texts,  such  as  Dense  Passage  Retriever  (DPR)  [Karpukhin et al., 2020] and ColBERT [Khattab and Zaharia, 2020].    2.2. Fine-tuning   To  adapt  QA  models  to  a  new  domain,  fine-tuning  seeks  to  adjust  the  model  to  respond  according  to  the  pattern  observed  in  the  training  data.    Furthermore,  it  is  expected  that  with  fine-tuning  the  model  will  acquire  domain-specific  knowledge,  enhancing its capacity to provide more accurate answers.            Fine-tuning  large  language  models  (LLMs)  typically  requires  significant  computational  resources.  Parameter  Efficient  Fine  Tuning  (PEFT)  [Xu  et  al.,  2023]  addresses  this  by  freezing  the  model’s  parameters  and  adjusting  only  the  newly  added ones. Among PEFT methods, Low-Rank Adaptation (LoRA) [Hu et al., 2021]  reduces the number of trainable parameters using low-rank matrices. The Quantized  Low-Rank  Adaptation  (QLoRA)  [Dettmers  et  al.,  2023]  takes  this  approach  a  step  further  by  applying  LoRA  to  quantized  models.  Studies  indicate  that  PEFT-tuned  models often perform comparably to those fully fine-tuned [Li et al., 2023].   2.3. RAG and Fine-tuning   The  effectiveness  of  RAG  and  fine-tuning  strategies  has  been  extensively  studied.  [Balaguer et al., 2024] compare these methods in a QA model for agriculture, while  [Ovadia et al., 2023] extend the comparison to various topics with a multiple-choice  QA  model.  Many  RAG  models  undergo  fine-tuning,  such  as  [Lewis  et  al.,  2020],  where  both  the  model  and  retriever  are  adjusted  together.  [Zhang  et  al.,  2024]  introduce RAFT (Retrieval-Augmented Fine-Tuning), which helps the model ignore  irrelevant documents. Overall, pre-trained models require additional fine-tuning to  learn specific reading comprehension tasks, which is essential for the effectiveness of  RAG.  This  instruction  fine-tuning  does  not  always  need  to  be  done  with  domain- specific data.   3. Methodology    3.1. Language Models   For  the  experiments  in  this  study,  we  used  a  Portuguese-adapted  version  of  the  model T5 [Raffel et al., 2020], called PTT5 [Carmo et al., 2020], and two versions of  the Llama-3 8B model [AI at Meta, 2024].   PTT5  was  pre-trained  on  the  BrWac  [Wagner  et  al.,  2018],  a  dataset  composed of millions of Internet pages in Brazilian Portuguese. This study used the  base version of the model, which contains 220M parameters. This choice was due to  its relatively small size, Portuguese pre-training, and encoder-decoder architecture,  distinguishing  it  from  Llama  3.  Llama  3,  which  was  developed  by  Meta,  uses  a  decoder-only architecture and is available in both pre-trained and instruction-tuned  versions. Despite being trained mainly on English data, Llama 3 is multilingual and  was  evaluated  exclusively  with  Portuguese  data  in  this  study.  The  8-billion- parameter  version  of  Llama  was  chosen  for  its  suitability  to  limited  computational  resources and its popularity as a widely used open-source model.   3.2. Fine-tuning Approach   For fine-tuning the models, we used two different techniques: full parameter fine- tuning  and  the  QLoRa  technique  [Dettmers  et  al.,  2023].    Full  fine-tuning  was           applied only to the PTT5 model, due to its reduced size and the limited availability  of  computational  resources.  For  the  Llama  3-8B  models,  we  chose  the  QLoRa  technique due to the large number of parameters in these models.    3.3. RAG Setup   We adopted a basic RAG setup, in which we used the retriever and language models  with  no  changes  to  their  original  architectures.  Three  retriever  models  were  evaluated:  BM25  [Robertson  and  Zaragoza,  2009],  Dense  Passage  Retriever  (DPR)  [Karpukhin et al., 2020], and ColBERT [Khattab and Zaharia, 2020]. With DPR, the  embeddings were generated by a BERT-based [Devlin et al., 2019] model known as  Sentence-BERT [Reimers and Iryna, 2019].  In the case of ColBERT, we specifically  used its second version - ColBERTv2 [Santhanam et al., 2021]. All IR models were  used  without  any  type  of  training  on  the  data  under  study.  The  texts  retrieved  for  each  query  were  concatenated  and  inserted  into  the  input  prompt  of  the  LLMs  as  support texts.    3.4. Evaluation Setup   We considered four metrics to evaluate the results of the experiments: (i) Rouge-1  and  (ii)  Rouge-L  [Lin,  2004],  which  are  based  on  word  overlap;  (iii)  BERTScore  [Zhang  et  al.,  2019],  which  employs  embeddings  generated  by  a  BERT  model;  and  (iv) a specific metric developed with the use of GPT (GPT 4o mini).   GPT was used to verify the accuracy of the answers generated by the models.  To  achieve  this,  we  developed  the  GPTScore  metric,  which  evaluates  whether  the  models’  answers  align  with  the  content  of  the  reference  answers,  even  when  they  differ in wording, length, or style. This evaluation used the prompt shown in Figure  1.  The  metric  was  computed  by  tallying  the  number  of  “yes”  or  “no”  answers  provided by GPT.   Figure 1. Prompt used to obtain the GPTScore.   4. Experiments and Results   This section outlines the experimental setup and presents the results obtained using  the RAG and fine-tuning strategies.             4.1. Experimental Setup   We utilized two datasets in Portuguese. The first one, Pirá 2.0 [Paschoal et al., 2021;  Pirozelli  et  al.,  2024],  focuses  on  topics  related  to  the  Brazilian  coast,  oceans  and  climate change. This dataset contains questions, answers, and support texts derived  from the abstracts of scientific papers and specialized reports on the aforementioned  topics,  all  of  which  have  a  version  in  Portuguese.    Comprising  2258  samples,  this  dataset  was  split  as  follows:  80%  for  training,  10%  for  validation,  and  10%  for  testing. We selected this dataset because it includes texts in Portuguese and focuses  on  a  specific  domain.    The  second  dataset  is  a  Portuguese  translation  of  the  Databricks-Dolly  dataset  [Conover  et  al.,  2023],  which  consists  of  pairs  of  instructions  and  answers  across  various  task  categories  generated  by  Databricks  employees.  For  this  study,  we  only  kept  the  records  classified  as  'closed  QA'  task,  which  involves  questions  and  answers  based  on  excerpts  from  Wikipedia.  This  dataset contains 1766 records, distributed as follows: 70% for the training, 15% for  validation,  and  15%  for  test.  It  was  included  in  this  study  because  it  contains  questions  from  a  broader  domain  that  still  require  supporting  texts  for  answer  formulation.     The experiments conducted in this study aimed to explore different strategies  for adapting LLMs to new domains, particularly focusing on fine-tuning and RAG- based approaches. We investigated various settings regarding the use of supporting  texts, both in the fine-tuning process and during the validation stage. For the fine- tuning  experiments,  we  evaluated  two  approaches:  one  that  includes  supporting  texts and question-answer pairs in the input prompt, termed 'RAG FT'; and another  that utilizes only question-answer pairs, referred to as 'QA FT'. For validation, the  scenarios  in  which  support  texts  were  included  in  the  input  prompt  were  called  'RAG'.  For  settings  in  which  only  the  question  was  used  as  input,  the  following  prompt  was  utilizes:  “Responda  à  pergunta  de  forma  sucinta. Pergunta:  {question}”.  In  cases  where  support  texts  were  also  included,  the  prompt  was  modified  to:  “Responda  à  pergunta  de  forma  sucinta  e  com  base  no  contexto  dado.  Contexto: {context}  Pergunta: {question}”.   In our experiments, we employed a 'greedy' decoding strategy, selecting the  token  with  the  highest  probability  during  generation.  We  established  a  maximum  output  limit  of  100  tokens,  while  the  input  limit  was  set  at  1024  tokens  to  accommodate  most  of  the  supporting  texts  without  truncation.  Fine-tuning  of  the  Llama models employed the QLoRa method with 4-bit quantization over 10 epochs,  utilizing a batch size and gradient accumulation of 4 to optimize hardware capacity.  The  model  generated  answers  at  16-bit  precision.  Meanwhile,  the  PTT5  model  underwent  full  fine-tuning  for  60  epochs,  using  a  batch  size  of  8  and  gradient  accumulation  of  4.  All  training  was  conducted  using  Hugging  Face  libraries  on  an  NVIDIA GeForce RTX 4090 GPU.           To select the best retriever for the RAG experiments, we used GPT 4o mini  to  answer  the  questions  in  each  dataset  based  on  the  context  provided  by  each  retriever.  For each question, the texts retrieved by each model were concatenated  and added to the prompt used by GPT to generate the answer. For the Pirá dataset,  we used the four most relevant passages identified by each retriever, while for the  Dolly dataset, we used the three most relevant passages. ColBERT outperformed all  other models across the evaluated metrics and was, therefore, chosen for this study.   Table  1  summarizes  these  results  and  includes  a  hypothetically  ideal  retriever,  simulated by using context texts that are always correct for each question.   Table 1. Evaluation of the retriever methods (see text).   4.2. Models without fine-tuning   The experiments with models without fine-tuning, considering only the question in  the  input  prompt,  may  reveal  their  level  of  prior  knowledge  about  the  datasets’  domain.    The  results  for  this  setting,  referred  to  as  \"No  FT,  No  RAG\"  in  Table  2,  indicate  that  these  models  have  low  prior  knowledge  of  the  Pirá  dataset’s  domain  and  moderate  knowledge  of  the  Dolly  dataset.  In  this  analysis,  results  for  PTT5  models are not reported, as we were unable to obtain satisfactory answers from this  model without fine-tuning.    In the case of RAG experiments with models without fine-tuning, referred to  as \"No FT, RAG\", we observed an increase in GPTScore and a more modest rise in  Rouge  metrics.  This  behavior  is  expected,  as  Rouge  metrics,  which  assess  term  matching, are more influenced by the style of the answers – particularly their length  and vocabulary.  Since the models were not fine-tuned to the datasets of interest, the  generated answers may not align with the answer patterns from the dataset. It was  observed  that  the  pre-trained  model  often  answered  the  questions  and  then  continued  generating  question-answer  pairs  indefinitely  until  it  reached  the  maximum  number  of  output  tokens.  The  Llama  Instruct  model  performed  significantly  better  on  both  the  GPTScore  and  BERTScore  due  to  its  prior  fine- tuning,  which  enhanced  its  reading  comprehension  abilities.  This  suggests  that                  models  with  advanced  comprehension  skills,  even  if  trained  in  domains  different  from  those  being  tested,  can  substantially  benefit  from  the  use  of  supporting  contexts  to  leverage  their  performance.  We  can  also  observe  that  models  without  prior domain knowledge and that did not explore RAG were the worst performers.   4.3. Models fine-tuned solely with question-answer pairs    This setting aims to evaluate whether the models can internalize knowledge about  the  domains  through  the  fine-tuning  process,  specifically  based  on  questions  and  answers from the datasets, referred to as \"QA FT, No RAG\" in Table 2. The results  show that for both datasets, fine-tuning does not provide a significant improvement  when the model is tested without RAG. However, when the model includes RAG,  referred  to  as  \"QA  FT,  RAG\"  in  Table  2,  we  observed  a  meaningful  gain  in  some  evaluation  scenarios.  This  suggests  that  the  fine-tuning  process  helps  the  model  learn the style of the answers - the length of the answers becomes more similar to  that  observed  in  the  dataset  -  but  does  not  necessarily  enable  it  to  retain  domain  knowledge. It is worth noting that the limited amount of training data may hinder  the model's ability to learn effectively through the fine-tuning process.    4.4. Models fine-tuned with question, answer and context   In  this  scenario,  the  models  were  fine-tuned  with  the  addition  of  contexts  in  the  training prompts. In the validation setting without RAG, referred to as \"RAG FT, No  RAG\", all models performed poorly.  This result is expected, as the primary purpose  of fine-tuning with added contexts is to train the model to generate answers based on  the context itself.  Since this setting does not include the support texts in the input  prompts, fine-tuning did not appear to achieve the desired outcome.   In the setting that includes RAG, referred to as \"RAG FT, RAG\", the models  achieved the best results across all metrics for the two datasets analyzed. It is worth  noting  that  for  the  Llama  Instruct  model,  which  was  already  fine-tuned  for  the  reading comprehension task, all settings that utilized RAG performed well according  to GTPScore.  However, for Rouge metrics, the models with fine-tuning on domain- specific data showed superior performance. This experiment suggests that even if the  model  is  capable  of  extracting  answers  from  the  context,  fine-tuning  on  problem- specific  data  may  be  beneficial  for  generating  answers  in  a  format  more  closely  aligned with that found in the dataset.  We also observed that fine-tuning the pre- trained Llama model allowed it to achieve results comparable to those of the Llama  Instruct  model,  despite  the  latter  being  previously  fine-tuned  with  a  significantly  larger amount of data. This result indicates that fine-tuning with context texts, even  when performed with a reduced dataset, can enhance the model’s  ability  to extract  relevant  information  from  context.  In  this  setting,  we  also  observed  a  significant  improvement  in  the  PTT5  results,  which  were  clearly  surpassed  by  those  obtained  with the Llama models, likely to their much larger number of parameters.         Table 2. Experimental results (see text).   5. Conclusion   This work analyzed various methods for adapting LLMs to specific domains in QA  tasks, including fine-tuning the model and integrating external data through RAG.  The experiments demonstrated that incorporating external data generally improves  the  models’  performance,  regardless  of  whether  fine-tuning  is  applied.  The  results  also  showed  that  fine-tuning,  even  when  conducted  with  a  reduced  dataset,  can  enhance  the  models’  performance.  Additionally,  we  observed  that  while  the  best  results  were  achieved  by  models  specifically  tuned  to  domain  data,  a  model  with  previously  fine-tuned  instructions  produced  similar  outcomes,  with  the  clear  advantage of not requiring any additional fine-tuning.   The  experiments  presented  here  were  conducted  using  a  basic  RAG  architecture,  without any additional training of the retrievers on the datasets of interest. Future  work could explore the same settings with adjustments to the retrievers as well.   6. Acknowledgments   To CNPq, FAPERJ, and CAPES. This study was financed in part by the Coordenação  de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) – Finance Code  001.              "
        },
        {
            "titulo": "LLMs as Tools for Evaluating Textual Coherence: A Comparative Analysis",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31140",
            "idioma": "Inglês",
            "storage_key": "files/article_31140_30943.pdf",
            "autores": [
                {
                    "nome": "Bryan K. S. Barbosa",
                    "afiliacao": "UFCG",
                    "orcid": "http://orcid.org/0000-0002-4637-6498"
                },
                {
                    "nome": "Cláudio E. C. Campelo",
                    "afiliacao": "UFCG",
                    "orcid": "https://orcid.org/0000-0003-4404-2344"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Este estudo avalia o desempenho de Grandes Modelos de Língua (LLMs) recentes, como GPT-4o, GPT-3.5, Claude Opus e LLaMA 2, na análise automática de coerência textual. A pesquisa foca em três aspectos: coerência local, onde GPT-4o e o Claude Opus se destacam; coerência global, na qual Claude Opus e o mais eficaz; e detecção de incoerências, onde GPT-4o apresenta melhor desempenho. Esses resultados revelam as capacidades e limitações dos modelos atuais, contribuindo para o entendimento de suas aplicações no âmbito do Processamento de Línguas Naturais e trazendo avanços contínuos à área.",
            "keywords": [
                "textual coherence",
                "incoherence",
                "comparison",
                "NLP"
            ],
            "referencias": [
                "Aleixo, P. and Pardo, T. A. S. (2008). Cstnews: Um córpus de textos jornalísticos anotados segundo a teoria discursiva multidocumento cst (cross-document structure theory). Technical Report 326, Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos-SP. 12p.",
                "Barzilay, R. and Lapata, M. (2008). Modeling local coherence: An entity-based approach. In Knight, K., Ng, H. T., and Oflazer, K., editors, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 141–148, Ann Arbor, Michigan. Association for Computational Linguistics.",
                "Braz Junior, G. and Fileto, R. (2021). Investigating coherence in posts from a doubts forum in a virtual learning environment with bert. Conference Paper.",
                "Charolles, M. (1978). Introdução aos problemas da coerência dos textos: abordagem teórica e estudo das práticas pedagógicas. Editora Pontes.",
                "Davies, M. (2008). The corpus of contemporary american english (coca). Available online at",
                ".",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 4171–4186. Association for Computational Linguistics.",
                "Dias, M. (2016). Investigação de modelos de coerência local para sumários multi-documento. PhD thesis, Universidade de São Paulo.",
                "Elsner, M., Austerweil, J., and Charniak, E. (2007). A unified local and global model for discourse coherence. In Sidner, C., Schultz, T., Stone, M., and Zhai, C., editors, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 436–443, Rochester, New York. Association for Computational Linguistics.",
                "Freitas, A. R. P. (2013). Análise automática de coerência usando o modelo grade de entidades para o português. PhD thesis.",
                "Grosz, B. J., Joshi, A. K., and Weinstein, S. (1995). Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.",
                "Halliday, M. A. K. and Hasan, R. (1976). Cohesion in English. Longman.",
                "Hoey, M. (2013). Textual interaction: An introduction to written discourse analysis. Routledge.",
                "Jurafsky, D. and Martin, J. H. (2024). Speech and Language Processing, chapter 23. Draft, 3 edition. Accessed: 2024-10-10.",
                "Koch, I. and Travaglia, L. (2003). A coerência textual. Editora Contexto.",
                "Lai, A. and Tetreault, J. (2018). Discourse coherence in the wild: A dataset evaluation and methods. In Proceedings of SIGdial, pages 214–223.",
                "Lapata, M. and Barzilay, R. (2005). Automatic evaluation of text coherence: models and representations. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, IJCAI’05, page 1085–1090, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.",
                "Lin, Z., Ng, H. T., and Kan, M.Y. (2011). Automatically evaluating text coherence using discourse relations. In Lin, D., Matsumoto, Y., and Mihalcea, R., editors, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 997–1006, Portland, Oregon, USA. Association for Computational Linguistics.",
                "Mann, W. C. and Thompson, S. A. (1987). Rhetorical structure theory: Description and construction of text structures. In Natural Language Generation, pages 85–95. Springer Netherlands.",
                "Mikkelsen, L. F., Kinch, O., Pedersen, A. J., and Lacroix, O. (2022). Ddisco: A discourse coherence dataset for danish. In Proceedings of the 13th Language Resources and Evaluation Conference (LREC), pages 1234–1243.",
                "Naismith, B., Mulcaire, P., and Burstein, J. (2023). Automated evaluation of written discourse coherence using gpt-4. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 394–403, Online. Association for Computational Linguistics.",
                "Sagi, E. (2010). Discourse structure effects on the global coherence of texts.",
                "Seno, E. R. M. and Rino, L. H. M. (2005). Co-referential chaining for coherent summaries through rhetorical and linguistic modeling. In Proceedings of the Workshop on Crossing Barriers in Text Summarization Research/RANLP, Borovets, Bulgaria. Núcleo Interinstitucional de Linguística Computacional – NILC/USFCAR.",
                "Thompson, I. (1986). Readability beyond the sentence: Global coherence and ease of comprehension. Journal of Technical Writing and Communication, 16(1):131–140.",
                "Van Dijk, T. A. (1977). Text and context: Explorations in the semantics and pragmatics of discourse."
            ],
            "artigo_completo": "Resumo. Este estudo avalia o desempenho de Grandes Modelos de L´ıngua (LLMs) recentes, como GPT-4o, GPT-3.5, Claude Opus e LLaMA 2, na an´alise autom´atica de coerˆencia textual. A pesquisa foca em trˆes aspec- tos: coerˆencia local, onde GPT-4o e o Claude Opus se destacam; coerˆencia global, na qual Claude Opus ´e o mais eficaz; e detecc¸ ˜ao de incoerˆencias, onde GPT-4o apresenta melhor desempenho. Esses resultados revelam as capaci- dades e limitac¸ ˜oes dos modelos atuais, contribuindo para o entendimento de suas aplicac¸ ˜oes no ˆambito do Processamento de L´ınguas Naturais e trazendo avanc¸os cont´ınuos `a ´area.  1. Introduction  The concept of coherence lies at the very heart of effective communication, serving as a keystone element that determines the clarity, understandability, and overall quality of tex- tual content [Koch and Travaglia 2003]. Coherence transcends the boundaries of syntax or grammar; it embodies the logical flow of ideas, ensuring that a text is not just a col- lection of sentences but a unified whole that conveys meaning with precision and subtlety [Freitas 2013]. As we move deeper into the digital age, where written text interactions are increasingly prevalent [Hoey 2013], the capacity to automatically analyze textual co- herence became a crucial task within the realm of Natural Language Processing (NLP).  The advent of Large Language Models (LLMs) such as GPT-3, Llama, and Gem- ini has revolutionized our approach to generating text that mirrors the nuance and depth of human-written content. These models, trained on extensive corpora, have demonstrated an impressive ability to produce coherent and contextually relevant text across a wide range of topics. This proficiency in text generation naturally extends to the potential for these models to excel in tasks related to textual analysis. The underlying hypothesis is simple yet profound: if an LLM can generate coherent text, it should, by extension, pos- sess a refined ability to discern coherence – or the lack thereof – in existing texts.   In the field of computational linguistics, textual coherence is defined by the logical and orderly sequence in which ideas are presented within a text, ensuring that information and arguments are conveyed in a comprehensible and fluid manner [Seno and Rino 2005]. This involves not only the superficial connection between sentences through discourse markers or transition words but also a deeper harmony in terms of theme, purpose, and shared knowledge between the author and the reader [Charolles 1978]. For NLP systems, assessing the coherence of a text implies understanding how its constituent parts – whether at the sentence, paragraph, or document level – come together to form a unified whole that is logically consistent and aesthetically pleasing [Jurafsky and Martin 2024]. This definition highlights the complexity of the textual coherence analysis task, underscoring it as a significant challenge within the field.  Historically, coherence has been conceptualized through various theoretical frameworks. Rhetorical Structure Theory (RST) [Mann and Thompson 1987] posits that text coherence is derived from the hierarchical organization of text units, while Centering Theory [Grosz et al. 1995] emphasizes the role of discourse entities and their continu- ity across sentences. Over time, computational approaches to coherence have evolved from rule-based systems, relying on explicit coherence markers and structural patterns, to sophisticated machine learning algorithms that infer coherence implicitly from large datasets [Jurafsky and Martin 2024]. The development of neural network-based models, particularly those employing attention mechanisms such as BERT [Devlin et al. 2018], has marked a significant advancement, enabling a deeper understanding of contextual re- lationships within texts.  Given this context, the primary objective of this study is to evaluate the capabilities of various LLMs in the analysis of textual coherence. Specifically, the study assesses how the models GPT 3.5, GPT 4, GPT 4o, Claude 3 Opus, Claude 3.5 Sonnet, Claude 3 Haiku, Gemini, LLaMA 2 13b, LLaMA 2 7b, and Bard perform in three key tasks: classifying texts as (i) locally or (ii) globally coherent or incoherent, and (iii) identifying specific incoherent segments within texts. By examining these aspects, the study aims to contribute to the ongoing dialogue on improving NLP technologies and advancing our understanding of how machines process and understand the subtleties of human language.  The remainder of this article is structured as follows: Section 2 reviews key the- ories and models in textual coherence analysis. Section 3 presents the relevant literature, while Section 4 details the methodology, including the models evaluated and the metrics used. Section 5 discusses the results and their implications, and Section 6 concludes with a summary of findings and suggestions for future research.  2. Theoretical Background  Textual cohesion and coherence are fundamental to discourse analysis and NLP, as they explain how texts are structured and interpreted. Cohesion refers to the connections within a text created through various linguistic relations, such as pronouns, conjunctions, and lexical ties, ensuring that the text is perceived as a unified whole rather than a random collection of sentences [Halliday and Hasan 1976]. Coherence, on the other hand, is a more abstract concept, referring to the logical and meaningful organization of ideas within a text, allowing readers to follow the flow of information and understand the intended message [Van Dijk 1977].   Cohesion can be achieved through grammatical and lexical means. Grammatical cohesion includes the use of pronouns, ellipses, and conjunctions to link sentences, while lexical cohesion involves the repetition of words or the use of synonyms to maintain the continuity of ideas. However, a text can be cohesive without being coherent if the sentences do not contribute to a meaningful whole [Koch and Travaglia 2003].  Coherence can be examined at two levels:  local and global. Local coherence refers to the logical connections between adjacent sentences and paragraphs, ensuring that each idea flows smoothly into the next. This is often achieved through cohesive devices, such as pronouns and conjunctions, which help maintain continuity in meaning. Global coherence, on the other hand, concerns the overall structure and unity of the text, where all parts contribute to a consistent and meaningful whole [Charolles 1978]. Both levels of coherence are essential for a text to be understood as a cohesive and logically organized entity.  Theoretical frameworks like RST and Centering Theory have been foundational in the study of coherence. For instance, RST [Mann and Thompson 1987] analyzes the hierarchical organization of text by examining the relationships between different segments, which help to structure the text in a coherent manner. Centering Theory [Grosz et al. 1995] focuses on how discourse entities are managed across sentences, en- suring that the reader can follow the progression of ideas smoothly. These models have significantly influenced NLP research, particularly in the analysis and generation of co- herent texts, offering insights into the mechanisms that make a text understandable and logically connected [Jurafsky and Martin 2024].  3. Related Work  As a central area of investigation in NLP, textual coherence, particularly in the con- text of local coherence, which focuses on the logical and sequential flow between adjacent sentences or paragraphs, has been extensively studied through models like the entity grid. Introduced by [Lapata and Barzilay 2005] and further developed by [Barzilay and Lapata 2008], the entity grid model abstracts a text into a grid that cap- tures the distribution and transitions of discourse entities across sentences. By analyzing these patterns, the model can effectively infer the level of local coherence within a text. This approach has been widely adopted and has inspired numerous subsequent studies. For instance, [Elsner et al. 2007] enhanced coherence assessment by integrating the en- tity grid with a content model, while [Lin et al. 2011] refined the method by incorporating discourse relations, further advancing the field’s understanding of how sentences connect and maintain coherence.  The shuffle test, introduced by [Barzilay and Lapata 2008], has become a stan- dard method for evaluating local coherence models. This test involves comparing the coherence of a text in its original order versus a shuffled version, challenging models to recognize the coherent sequence. Studies like those by [Lin et al. 2011] and [Dias 2016] have used this test to validate the effectiveness of their models, highlighting its importance as a benchmark in coherence evaluation.  In contrast, global coherence, which concerns the overall unity and thematic con- sistency of a text, has received less attention but remains a key aspect for understanding how texts function as a whole. Early work by [Thompson 1986] emphasized the role of   global coherence in enhancing readability and comprehension, arguing that a coherent text allows readers to follow the central theme or argument effortlessly. More recent con- tributions by [Sagi 2010] have explored the hierarchical structure of texts, demonstrating how well-organized discourse contributes to global coherence.  Recent advancements in NLP have introduced BERT and LLMs like GPT-3, which have significantly expanded the possibilities for coherence analysis. These models, trained on extensive datasets, exhibit a remarkable ability to capture both local and global coherence, leading to more refined and human-like assessments of textual structure. For example, [Braz Junior and Fileto 2021] applied BERT, specifically BERTimbau, in edu- cational forums to measure coherence. By analyzing sentence embeddings, the model effectively assessed sentence order, accurately distinguishing between coherent and per- muted texts, thus demonstrating its capability to capture nuanced textual relationships. Similarly, [Naismith et al. 2023] utilized GPT-4 for coherence assessment in educational contexts, where the model not only rated coherence but also provided explanatory ra- tionales that closely aligned with human evaluations. This study demonstrated GPT-4’s effectiveness in replicating human judgments and even surpassing traditional NLP metrics by offering rationale-supported evaluations, thereby highlighting its potential to enhance automated discourse coherence assessment and its applications in educational settings.  4. Methodology  The primary aim of this study is to analyze and compare the performance of various LLMs in evaluating textual coherence across different aspects using two distinct approaches: (i) through LLMs APIs and (ii) through LLMs chat interfaces To achieve this, we selected a diverse set of corpora for their relevance and variety in text types, which allows for a thorough assessment of the models’ capabilities across different linguistic contexts. These datasets were preprocessed and, annotated as necessary to ensure consistency across the tasks.  One of the four corpora utilized in this study is the Corpus of Contemporary Amer- ican English (COCA) [Davies 2008], which offers a balanced compilation of over one bil- lion words across genres such as spoken language, fiction, academic texts, and web pages. For our analysis, we focused on the free portions of blog and academic sections of COCA, which together comprise a diverse range of coherence levels. The blog section, with 991 texts, is characterized by its informal and subjective nature, often exhibiting lower co- herence, while the academic section, consisting of 256 texts, is known for its structured and precise language, typically demonstrating higher coherence. These sections were employed in both local and global coherence tasks, with specific subsets annotated for detailed global coherence analysis and incoherence identification.  The study also incorporates the CST News Corpus [Aleixo and Pardo 2008], which consists of 50 collections of Brazilian Portuguese news articles, each centered on a specific event or topic. Originally developed to support research on multi-document summarization, the corpus includes approximately 150 news articles and 300 human- generated summaries from various newspapers, such as Folha de S˜ao Paulo, Estad˜ao, and O Globo. This diversity in sources makes the corpus particularly well-suited for coher- ence studies, as it allows for evaluating both local and global coherence in a multilingual context. The CST News Corpus was especially valuable for assessing model performance   in Brazilian Portuguese, adding a multilingual dimension to our evaluations.  Another key corpus in this research is the Grammarly Corpus of Discourse Coher- ence (GCDC) [Lai and Tetreault 2018], which contains 4,800 texts from four real-world sources: Yahoo Answers, Clinton Emails, Enron Emails, and Yelp Reviews. Due to its context-dependent structure, the Yahoo Answers portion (1,200 texts) was excluded from our study. The Clinton Emails provide a mix of professional and personal correspon- dence, Enron Emails focus on formal business communication, and Yelp Reviews feature user-generated feedback on businesses. Each text is annotated for global coherence on a 3-point scale (low, medium, high), with 8,000 ratings from both expert and non-expert annotators via Amazon Mechanical Turk. These pre-existing annotations were used for comparing the models’ performance against human judgments, enhancing our evaluation of global coherence tasks.  Lastly, the DDisCo corpus [Mikkelsen et al. 2022] was developed to fill a gap in resources for studying discourse coherence in Danish. It comprises 1,002 texts from two main sources: Reddit and Danish Wikipedia. The Reddit texts, totaling 501, consist of informal user-generated content, while the 501 Danish Wikipedia texts offer more formal, structured information. Each text is annotated for global coherence on a 3-point scale (low, medium, high) by linguistics experts. This corpus introduces linguistic diversity into our research, allowing us to evaluate model performance in another non-English context. It was particularly useful for assessing how well the models generalize across different languages and discourse structures.  4.1. Local Coherence Analysis  The local coherence analysis in this study employed the shuffle test, which evaluates text coherence by comparing the original order of sentences within each text to randomly shuffled version. This test was applied to texts from four corpora: COCA, CST News, GCDC, and DDisCo. A total of 2,318 texts were selected, comprising 991 blog texts and 256 academic texts from COCA, 251 news articles from CST News, 842 texts from GCDC, and 991 texts from DDisCo. Each text was segmented into sentences, and those containing fewer than four sentences were excluded as they would not allow for the 20 required permutations. The remaining texts were shuffled 20 times, generating 46,360 incoherent versions, resulting in a dataset of 48,678 texts for analysis.  The models’ performance was evaluated using two distinct methods: (i) via LLMs APIs and (ii) through LLMs chat interfaces. In the API-based evaluation, texts were processed directly through automated API calls, streamlining the evaluation process. In contrast, the chat interface evaluation simulated real-world usage by submitting the texts through interactive prompts.  For both approaches, the models were provided with a standardized prompt for Local Coherence Analysis1 to guide them in distinguishing between coherent and incoherent texts. Performance was measured using accuracy, precision, recall, and F1-score, comparing the models’ classifications against the original text labels.  1https://github.com/bryankhelven/coherence-findings   4.2. Global Coherence Analysis The global coherence analysis in this study aimed to evaluate the ability of various LLMs to assess the overall logical consistency and thematic organization of texts across a to- tal of 2,142 texts. This analysis included 1,200 texts from the DDisCo Corpus and 842 texts from the GCDC Corpus, both of which already contained human annotations. Ad- ditionally, a new annotation phase was conducted for a subset of 100 texts from the COCA and CST News corpora, as these lacked pre-existing coherence labels. Three lan- guages/linguistics experts evaluated this subset, which consisted of 10 academic texts and 60 blog texts from COCA, along with 30 news articles from CST News. For consistency, each text in this study was assigned a coherence score on a Likert scale ranging from low to high coherence (1 to 3), using the same scale previously adopted for assigning scores by the works of [Lai and Tetreault 2018] and [Mikkelsen et al. 2022]. This ensured that the evaluation of global coherence was standardized across the various corpora used in this analysis.  Following the annotation process, the study assessed the models’ performance in global coherence tasks using two methods: (i) LLMs APIs for an automated process, and (ii) LLMs chat interfaces to simulate real-world, user-driven interactions. A total of 2,142 texts were used for this analysis, comprising the 100 manually annotated texts, 1,200 texts from the DDisCo corpus, and 842 texts from the GCDC corpus. Both methods utilized a standardized prompt for Global Coherence Analysis2 to guide the models in assessing global coherence. The evaluation metrics were consistent with those used in the local coherence analysis, but in this case, the models’ classifications were compared directly to the original human annotations (scores of 1, 2, or 3).  4.3. Incoherence Identification The incoherence identification task evaluated the ability of various LLMs to detect seg- ments within texts that disrupt logical flow. We used 130 texts for this task, includ- ing 100 texts previously annotated for global coherence (10 academic texts and 60 blog texts from COCA, 30 news articles from CST News) and an additional 30 texts from the GCDC corpus (10 each from Yelp, Clinton, and Enron). The same three annotators from the global coherence task identified incoherent segments, focusing on the categories of Incorrect Use of Logical Connectors, Unnecessary Repetition, Irrelevant Information, Contradictions, Sequence of Events, and Inconsistent Verb Tenses. Fleiss’ Kappa, which scored 0.8326 and indicated excellent agreement, was chosen for its capacity to account for chance agreement among multiple annotators across various incoherence types. Each annotated segment was treated as a unit, ensuring robust reliability.  The annotators, familiar with each other, communicated freely to resolve difficul- ties, following a shared understanding of coherence from [Koch and Travaglia 2003]. The models’ performance was evaluated using the same two methods as before – LLMs APIs and chat interfaces. However, in this task, each model was treated as an additional anno- tator. The agreement between model-generated annotations and human annotations was measured using Fleiss’ Kappa to determine how closely the models aligned with human judgment. The prompt for incoherence identification3 was also standardized and used across all models in this task.  2Available on GitHub (see first footnote). 3Ibid.   5. Results and Discussion  The results obtained during the execution of the analysis are summarized in Tables 1, 2, and 3, highlighting the performance of various models in both API and chat-based interactions.  Table 1. Performance Metrics for Local Coherence Classification  API  Chat  Model Bard Claude 3 Haiku Claude 3 Opus Claude 3.5 Sonnet Gemini GPT 3.5 GPT 4 GPT 4o LLaMA 2 13b LLaMA 2 7b  Acc 0.756 0.914 0.979 0.973 0.978 0.918 0.970 0.982 0.831 0.817  Pr 0.755 0.906 0.991 0.986 0.989 0.908 0.982 0.990 0.825 0.804  Re 0.740 0.898 0.983 0.981 0.980 0.901 0.980 0.988 0.816 0.797  F1 0.748 0.902 0.987 0.983 0.985 0.905 0.981 0.989 0.820 0.800  Acc 0.739 0.949 0.974 0.972 0.971 0.962 0.969 0.977 0.888 0.805  Pr 0.742 0.902 0.971 0.969 0.971 0.905 0.966 0.975 0.821 0.801  Re 0.739 0.899 0.973 0.968 0.970 0.902 0.965 0.973 0.818 0.798  F1 0.740 0.900 0.972 0.968 0.970 0.903 0.965 0.974 0.819 0.799  Table 2. Performance Metrics for Global Coherence Classification  API  Chat  Model Claude 3 Haiku Claude 3 Opus Claude 3.5 Sonnet Gemini GPT 3.5 GPT 4 GPT 4o LLaMA 2 13b LLaMA 2 7b  Acc 0.959 0.982 0.980 0.976 0.960 0.974 0.978 0.970 0.968  Pr 0.918 0.986 0.984 0.963 0.920 0.961 0.965 0.930 0.928  Re 0.921 0.987 0.982 0.966 0.923 0.964 0.968 0.933 0.931  F1 0.920 0.986 0.983 0.965 0.921 0.963 0.967 0.932 0.930  Acc 0.911 0.933 0.930 0.928 0.912 0.926 0.930 0.922 0.920  Pr 0.871 0.936 0.934 0.915 0.873 0.914 0.918 0.887 0.881  Re 0.875 0.939 0.931 0.918 0.879 0.919 0.920 0.883 0.884  F1 0.875 0.937 0.932 0.916 0.877 0.917 0.919 0.888 0.883  Table 3. Fleiss’ Kappa for Incoherence Identification  Model Annotators only (baseline) Claude 3 Haiku Claude 3 Opus Claude 3.5 Sonnet Gemini GPT 3.5 GPT 4 GPT 4o LLaMA 2 13b LLaMA 2 7b  API 0.8326 0.7995 0.8166 0.8279 0.8119 0.8038 0.8152 0.8316 0.6787 0.5823  Chat 0.8326 0.7653 0.7987 0.8082 0.7858 0.7716 0.8093 0.8234 0.6492 0.5418  Table 1 shows the performance metrics for Local Coherence Classification, with GPT 4o achieving the highest scores in both API and chat interactions. Claude 3 Opus and Claude 3.5 Sonnet also performed well, especially in the API interaction, which demon- strates their effectiveness in accurately identifying coherent texts. In contrast, LLaMA 2 13b and LLaMA 2 7b had similar lower performance on both scenarios, suggesting lim- itations in processing and classifying local coherence. Similarty, for Global Coherence Classification, GPT 4o and Claude 3 Opus stood out with the highest performance in both interaction modes, while Claude 3 Haiku had the lowest as shown in Table 2.  The results for the Incoherence Identification task are summarized in Table 3, where GPT 4o again demonstrated the highest agreement with human annotators, with a Fleiss’ Kappa of 0.8316 in API interaction and 0.8234 in chat. Claude 3.5 Sonnet followed closely, with Kappa values of 0.8279 in API and 0.8082 in chat, while LLaMA   models, particularly LLaMA 2 7b, showed significantly lower Kappa values, indicating that these models struggle more with identifying incoherent segments.  The difference in performance between API and chat interactions is notable, with all models generally performing better in the API-based tests across all scenarios. This may indicate that API interactions allow for more precise and structured processing, lead- ing to higher accuracy and consistency.  6. Conclusions and Future Work  This study assessed the performance of LLMs in evaluating textual coherence at both lo- cal and global levels and identifying incoherences within various corpora. Models such as GPT 4o and Claude 3 consistently outperformed others, particularly in API-based evalu- ations, where they achieved high accuracy and reliability. In local coherence tasks, GPT 4o demonstrated an F1 score of 0.989 in API-based tests, while in global coherence tasks, Claude 3 Opus led with an F1 score of 0.986. However, chat-based interactions revealed a performance decline, with GPT 4o’s F1 score dropping to 0.974 in local coherence and Claude 3 Opus to 0.937 in global coherence. This suggests that the mode of interaction impacts model effectiveness, with API-based methods being more stable.  Despite the strong performance of top models, the Incoherence Identification task proved challenging across the board. GPT 4o showed the highest agreement with hu- man annotators (Fleiss’ Kappa of 0.8316), but all models exhibited lower performance in chat-based settings. These findings underscore the need for improvement in this area, especially as lower-tier models like LLaMA 2 struggled significantly, with Fleiss’ Kappa dropping as low as 0.5418 in chat-based evaluations.  These findings have practical implications for NLP as models like GPT 4o and Claude 3 can be integrated into proofreading tools, content generators, and educational software to improve textual coherence. Their ability to assess and enhance coherence benefits machine-generated content and helps users create cohesive texts. Recognizing the impact of interaction modes on performance guides developers in choosing effective deployment strategies, favoring API integrations for consistency and accuracy.  The study acknowledges threats to validity, particularly the risk that some of the evaluation corpora may have been part of the training data for the LLMs, potentially inflating performance. This overlap introduces biases that could compromise objectivity, as models may recall patterns from training instead of genuinely evaluating coherence. The assumption of coherence in original texts and the limited size and diversity of the annotated datasets also pose risks to the generalizability of the findings.  Future work should address these limitations by expanding the range of evaluated text types and incorporating larger, more diverse annotator groups, as well as utilizing new and manually collected corpus to ensure that the models have not had prior access to it. Additionally, exploring fine-tuning techniques and evaluating newer model architectures will be essential. The development of improved evaluation metrics and the exploration of cross-linguistic and multimodal coherence analysis are also recommended to enhance the robustness and applicability of LLMs in complex language tasks.   "
        },
        {
            "titulo": "Anomaly Detection in Text Data: A Semi-Supervised Approach Applied to the Portuguese Domain",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31141",
            "idioma": "Inglês",
            "storage_key": "files/article_31141_30944.pdf",
            "autores": [
                {
                    "nome": "Fabio Masaracchia Maia",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0009-0008-3646-7338"
                },
                {
                    "nome": "Anna Helena Reali Costa",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0001-7309-4528"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Anomaly detection, driven by advancements in machine learning and deep learning, has gained significant importance across various fields. However, its application to unstructured textual data, particularly in Portuguese, remains underexplored. In textual analysis, these techniques are crucial for detecting deviations within text collections. This paper investigates state-of-the-art methods for anomaly detection in Portuguese text corpora and introduces a new, flexible loss function designed to enhance detection across different contamination levels. By evaluating these methods on benchmark datasets, specifically in the contexts of hate speech detection and sentiment analysis, we address existing challenges and contribute to the development of more effective anomaly detection techniques for Portuguese text data.",
            "keywords": [
                "Anomaly detection",
                "Textual anomaly",
                "Transformers",
                "Pre-trained models",
                "Natural Language Processing"
            ],
            "referencias": [
                "Edgeworth, F. Y. (1887). Xli. on discordant observations. Philosophical Magazine Series 1, 23:364–375.",
                "Leite, J. A., Silva, D., Bontcheva, K., and Scarton, C. (2020). Toxic language detection in social media for Brazilian Portuguese: New dataset and multilingual analysis. In Wong, K.-F., Knight, K., and Wu, H., editors, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 914–924, Suzhou, China. Association for Computational Linguistics.",
                "Reimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China. Association for Computational Linguistics.",
                "Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S. A., Binder, A., Müller, E., and Kloft, M. (2018). Deep one-class classification. In Dy, J. and Krause, A., editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4393–4402. PMLR.",
                "Ruff, L., Vandermeulen, R. A., Görnitz, N., Binder, A., Müller, E., Müller, K.-R., and Kloft, M. (2020). Deep semi-supervised anomaly detection. In International Conference on Learning Representations.",
                "Sousa, R. F. d., Brum, H. B., and Nunes, M. d. G. V. (2019). A bunch of helpfulness and sentiment corpora in brazilian portuguese. In Symposium in Information and Human Language Technology - STIL. SBC.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained bert models for brazilian portuguese. In Cerri, R. and Prati, R. C., editors, Intelligent Systems, pages 403–417, Cham. Springer International Publishing.",
                "Xu, H., Pang, G., Wang, Y., and Wang, Y. (2023a). Deep isolation forest for anomaly detection. IEEE Transactions on Knowledge and Data Engineering, 35(12):12591–12604."
            ],
            "artigo_completo": "Abstract. Anomaly detection, driven by advancements in machine learning and deep learning, has gained significant importance across various fields. How- ever, its application to unstructured textual data, particularly in Portuguese, remains underexplored. In textual analysis, these techniques are crucial for de- tecting deviations within text collections. This paper investigates state-of-the-art methods for anomaly detection in Portuguese text corpora and introduces a new, flexible loss function designed to enhance detection across different contamina- tion levels. By evaluating these methods on benchmark datasets, specifically in the contexts of hate speech detection and sentiment analysis, we address ex- isting challenges and contribute to the development of more effective anomaly detection techniques for Portuguese text data.  1. Introduction  Anomaly detection refers to the identification of patterns in data that deviate from ex- pected norms [Chandola et al. 2009]. Anomalies, often termed outliers or exceptions, are distinct from the majority of observations that define the “normal” pattern. While anomaly detection techniques have been extensively applied to structured data, such as continuous and categorical variables [Boutalbi et al. 2023], less attention has been given to unstructured data like text — the focus of this work.  Anomaly detection began with statistical methods in the late 19th century [Edgeworth 1887] and has since expanded, with deep learning broadening its scope to unstructured domains like images and text [Chandola et al. 2009, Pimentel et al. 2014]. However, its application to textual anomaly detection remains limited [Pang et al. 2019]. Detecting anomalies in text data is particularly challenging due to the variety of linguistic levels involved, such as spelling, syntax, and semantics [Xu et al. 2023b]. Leveraging deep learning’s ability to model complex patterns has led to significant advancements in the field.  Traditionally treated as an unsupervised task due to the absence of ground truth labels, anomaly detection has employed techniques like autoencoders and GANs [Pang et al. 2019]. Popular approaches in this domain include DeepSVDD [Ruff et al. 2018] and Deep Isolation Forest [Xu et al. 2023a], both focusing on modeling normal data and identifying deviations as anomalies. However, recent semi-supervised neural approaches, such as DevNet [Pang et al. 2019] and DeepSAD [Ruff et al. 2020], have shown improved detection accuracy by integrating limited labeled anomalies into the training process [Xu et al. 2023b], bridging the gap between unsupervised and supervised   learning. Despite recent advancements, there remains a significant gap in comprehensive research focused on anomaly detection in Portuguese text corpora.  In this paper, we extend neural network-based anomaly detection techniques to handle these complexities in Portuguese text data. Furthermore, we propose a change in the loss function in order to establish a compromise between the samples that corre- spond to anomalies in relation to the others. Experiments show that this approach is quite promising. To effectively address the unique challenges of representing textual data, we employ two pre-trained BERT-based models, checking the strengths and weaknesses of each representation in the different tasks.  2. Methodology  2.1. Problem Definition  Given a dataset X = {x1, x2, . . . , xN +K}, where U = {x1, x2, . . . , xN } is unlabeled data and K = {xN +1, . . . , xN +K} represents labeled anomalies (K ≪ N ), the goal is to train a model to identify these rare anomalies. This task is challenging due to the imbalance between the large unlabeled set and the small labeled anomaly set. The process involves two key steps:  1. Embedding Transformation: Data X is transformed into embeddings Z =  {z1, z2, . . . , zN +K}, with each zi being a vector in Rd.  2. Scoring Function: A neural network learns a scoring function ϕ : Z → R to ensure that ϕ(zi) > ϕ(zj) when zi is an anomaly and zj is normal, minimizing the use of labeled examples.  We adopted the DevNet model due to its demonstrated good performance obtained in studies considering textual domain [Xu et al. 2023b] along with its ability to effec- tively manage high-dimensional spaces, such as embeddings. Additionally, the model’s interpretable loss function, based on a straightforward Z-score strategy, provides valuable insights that can be later used to assess text identified as anomalies.  2.2. DevNet for Anomalous Text  The DevNet algorithm [Pang et al. 2019] introduces a semi-supervised approach that learns an interpretable outlier scoring function, ϕ(z; Θ), using a Z-score deviation loss. While the original formulation is based on raw data points x, we denote the embeddings as z to reflect the transformed data representations. This approach assumes a prior normal distribution over anomaly scores, modeled with l random objects ri ∈ R sampled from a standard normal distribution N (µR, σR):  dev(z) =  ϕ(z; Θ) − µR σR  ,  (1)  where µR and σR are the mean and standard deviation of anomaly scores within the reference distribution. This deviation is then incorporated into a contrastive loss function to enhance the distinction between anomalous and normal samples, where y indicates anomaly status, and a ensures a minimum separation between classes [Pang et al. 2019].  L(ϕ(z; Θ), µR, σR) = (1 − y)|dev(z)| + y · max(0, a − dev(z)),  (2)   2.3. Proposal  To provide flexibility, the parameter η ∈ [0, 1] is introduced in the DevNet loss function given in Eq. 2, controlling the balance between regular and anomalous samples,  L(ϕ(x; Θ), µR, σR) = (1 − η)(1 − y)|dev(x)| + η · y · max(0, a − dev(x)),  (3)  The η parameter adjusts the model’s emphasis on anomalies, allowing adaptation to varying levels of contamination (i.e., percentage of labeled anomalies) and data avail- ability. We investigate different proportions of labeled anomalies to assess the robustness of the solution in various scenarios, aiming to determine the minimum amount of la- beled data needed for good performance. Additionally, we employ two distinct text repre- sentation strategies: the monolingual BERTimbau model [Souza et al. 2020], specifically designed for processing Portuguese text, and the multilingual Sentence-BERT (SBERT) [Reimers and Gurevych 2019], which generates sentence-level embeddings across mul- tiple languages, including Portuguese. Our customized DevNet implementation, named η-DevNet, was evaluated against its original version using both representation strategies.  3. Experiments and Results  3.1. Experiments  To evaluate the performance of different representation methods and loss functions, we first tested η values ranging from 0.5 to 1 using the BERTimbau embedding strategy, where η = 0.5 corresponds to the original DevNet formulation. The other parameters were adopted from the DevNet reference: a = 5, l = 5000, µR = 0, and σR = 1. Contamination levels were adjusted by introducing between 5 and 1000 anomalies across the experiments, with anomalies randomly selected. After identifying the optimal η value, we applied it in subsequent experiments to compare both embedding strategies across different datasets. The mean ROC-AUC values were calculated over 10 experimental runs for each scenario.  3.2. Dataset  We evaluate our approach using two Brazilian datasets. The first, Told-Br [Leite et al. 2020], contains 21,000 labeled instances of tweets tagged with hate speech, categorized into themes such as homophobia, racism, and misogyny, with hate speech serving as the anomaly class. The second dataset, UTLC-Movies [Sousa et al. 2019], comprises over one million movie reviews. From this dataset, we sampled 40,000 re- views for sentiment analysis, where negative sentiment is treated as the anomaly class.  3.3. Results  Figure 1, shows performance and stability improvements as η is adjusted, with η = 0.7 yielding optimal performance. This value was subsequently used for further analysis. The results shown in Table 1 outline these results for both tasks, demonstrating that in most cases, the adapted loss function led to performance improvements. Reaching a reasonable level of accuracy requires a minimum threshold of labeled examples, which varies with task complexity. In our experiments, sentiment analysis needed only 0.87% of labeled anomalies to achieve a ROC-AUC of 0.85, while hate speech detection required 2.59% to   Figure 1. η Comparison across different η values with varying amounts of labeled anomalies using BERTimbau embedding strategy.  reach a ROC-AUC of 0.73. This discrepancy likely arises from the greater complexity of hate speech detection, which involves subtle linguistic nuances and diverse expressions. Additionally, pre-trained models may not fully capture slang and politically specific con- texts, which are common in hate speech but may be underrepresented during training.  Table 1. Comparison of ROC-AUC values across different scenarios and contamination levels for UTLC-Movies and Told-BR datasets when η = 0.7, where % refers to the contamination level.  UTLC-Movies  Told-BR  Nb. Outliers  5 10 25 50 100 250 500 1000  %  0.02 0.04 0.09 0.18 0.35 0.87 1.73 3.41  Bη  0.58 0.62 0.66 0.70 0.78 0.82 0.83 0.85  BD  0.57 0.58 0.65 0.69 0.71 0.76 0.83 0.84  Mη  0.52 0.54 0.60 0.64 0.71 0.75 0.81 0.81  MD  0.52 0.57 0.68 0.66 0.70 0.69 0.79 0.75  %  0.05 0.11 0.27 0.53 1.05 2.59 5.05 9.62  Bη  0.46 0.48 0.57 0.58 0.66 0.73 0.75 0.76  BD  0.48 0.50 0.57 0.63 0.63 0.68 0.73 0.76  Mη  0.50 0.53 0.54 0.56 0.61 0.56 0.50 0.50  MD  0.50 0.53 0.60 0.60 0.60 0.52 0.51 0.52  Acronyms: BERTimbau η-loss (Bη), BERTimbau Devnet loss (BD), multilingual SBERT η-loss (Mη), multilingual SBERT Devnet loss (MD).  Our results show that the BERTimbau representation [Souza et al. 2020] consis- tently outperformed the multilingual model across tasks. This advantage can be traced to BERTimbau’s specialization in Portuguese, allowing it to capture more intricate linguistic nuances, such as idiomatic expressions and regional variations.  4. Conclusion and Future Work  This study shows that BERTimbau, tailored for Portuguese, consistently outperforms mul- tilingual models in anomaly detection, with the customized loss function providing no- table improvements. These results highlight the potential of semi-supervised methods for tasks like harmful content detection and sentiment analysis in Portuguese contexts with limited labeled data.  Future work may expand this approach to related tasks such as topic modeling, fake news detection, and fraud detection. Although some labeling effort is still required for good performance, the small amount of labeled data needed makes this approach feasible in resource-constrained scenarios. Furthermore, the promising advances in Large Language Models (LLMs) could not only serve as valuable tools for benchmarking but also automate anomaly tagging, reducing manual effort and enhancing adaptability and scalability across various real-world applications.   "
        },
        {
            "titulo": "Biases in GPT-3.5 Turbo model: a case study regarding gender and language",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31142",
            "idioma": "Inglês",
            "storage_key": "files/article_31142_30945.pdf",
            "autores": [
                {
                    "nome": "Fernanda Malheiros Assi",
                    "afiliacao": "UFSCar",
                    "orcid": "http://orcid.org/0000-0001-8820-964X"
                },
                {
                    "nome": "Helena de Medeiros Caseli",
                    "afiliacao": "UFSCar",
                    "orcid": "https://orcid.org/0000-0003-3996-8599"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Interactions with Generative Language Models like OpenAI’s GPT3.5 Turbo are increasingly common in everyday life, making it essential to examine their potential biases. This study assesses biases in the GPT-3.5 Turbo model using the regard metric, which evaluates the level of respect or esteem expressed towards different demographic groups. Specifically, we investigate how the model perceives regard towards different genders (male, female, and neutral) in both English and Portuguese. To achieve this, we isolated three variables (gender, language, and moderation filters) and analyzed their individual impacts on the model’s outputs. Our results indicate a slight positive bias towards feminine over masculine and neutral genders, a more favorable bias towards English compared to Portuguese, and consistently more negative outputs when we attempted to reduce the moderation filters.",
            "keywords": [
                "Natural Language Processing",
                "NLP",
                "Gender Bias",
                "GPT-3.5",
                "Generative Language Models",
                "Bias",
                "Regard Metric",
                "Moderation Filters",
                "Language Bias",
                "Large Language Models",
                "LLM"
            ],
            "referencias": [
                "Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., and Kalai, A. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings.",
                "Cambridge Dictionary (2024). Regard.",
                "Das, A., Selek, S., Warner, A. R., Zuo, X., Hu, Y., Kuttichi Keloth, V., Li, J., Zheng, W. J., and Xu, H. (2022). Conversational bots for psychotherapy: A study of generative transformer models using domain-specific dialogues. In Demner-Fushman, D., Cohen, K. B., Ananiadou, S., and Tsujii, J., editors, Proceedings of the 21st Workshop on Biomedical Language Processing, pages 285–297, Dublin, Ireland. Association for Computational Linguistics.",
                "Deshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., and Narasimhan, K. (2023). Toxicity in chatgpt: Analyzing persona-assigned language models. In Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1236–1270, Singapore. Association for Computational Linguistics.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T., editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. (2020). RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Cohn, T., He, Y., and Liu, Y., editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online. Association for Computational Linguistics.",
                "Gupta, S., Shrivastava, V., Deshpande, A., Kalyan, A., Clark, P., Sabharwal, A., and Khot, T. (2024). Bias runs deep: Implicit reasoning biases in persona-assigned llms.",
                "Kolomeets, M., Tushkanova, O., Desnitsky, V., Vitkova, L., and Chechulin, A. (2024). Experimental evaluation: Can humans recognise social media bots? Big Data and Cognitive Computing, 8(3).",
                "Liang, P. P., Wu, C., Morency, L.-P., and Salakhutdinov, R. (2021). Towards understanding and mitigating social biases in language models.",
                "Liu, Y., Zhang, W., Chen, Y., Zhang, Y., Bai, H., Feng, F., Cui, H., Li, Y., and Che, W. (2023). Conversational recommender system and large language model are made for each other in E-commerce pre-sales dialogue. In Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9587–9605, Singapore. Association for Computational Linguistics.",
                "Lucas, J., Uchendu, A., Yamashita, M., Lee, J., Rohatgi, S., and Lee, D. (2023). Fighting fire with fire: The dual role of LLMs in crafting and detecting elusive disinformation. In Bouamor, H., Pino, J., and Bali, K., editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14279–14305, Singapore. Association for Computational Linguistics.",
                "Nadeem, M., Bethke, A., and Reddy, S. (2021). StereoSet: Measuring stereotypical bias in pretrained language models. In Zong, C., Xia, F., Li, W., and Navigli, R., editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356–5371, Online. Association for Computational Linguistics.",
                "Nangia, N., Vania, C., Bhalerao, R., and Bowman, S. R. (2020). CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In Webber, B., Cohn, T., He, Y., and Liu, Y., editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1967, Online. Association for Computational Linguistics.",
                "Odbal, Zhang, G., and Ananiadou, S. (2022). Examining and mitigating gender bias in text emotion detection task. Neurocomputing, 493:422–434.",
                "OpenAI (2024). Gpt-3.5 turbo.",
                ".",
                "Orabi, M., Mouheb, D., Al Aghbari, Z., and Kamel, I. (2020). Detection of bots in social media: A systematic review. Information Processing Management, 57(4):102250.",
                "Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M., and Bowman, S. (2022). BBQ: A hand-built bias benchmark for question answering. In Muresan, S., Nakov, P., and Villavicencio, A., editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 2086–2105, Dublin, Ireland. Association for Computational Linguistics.",
                "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are unsupervised multitask learners.",
                "Rodrigues, G., Albuquerque, D., and Chagas, J. (2023). Análise de vieses ideológicos em produções textuais do assistente de bate-papo chatgpt. In Anais do IV Workshop sobre as Implicações da Computação na Sociedade, pages 148–155, Porto Alegre, RS, Brasil. SBC.",
                "Roy, K., Goyal, P., and Pandey, M. (2021). Attribute value generation from product title using language models. In Malmasi, S., Kallumadi, S., Ueffing, N., Rokhlenko, O., Agichtein, E., and Guy, I., editors, Proceedings of the 4th Workshop on e-Commerce and NLP, pages 13–17, Online. Association for Computational Linguistics.",
                "Rudinger, R., Naradowsky, J., Leonard, B., and Van Durme, B. (2018). Gender bias in coreference resolution. In Walker, M., Ji, H., and Stent, A., editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 8–14, New Orleans, Louisiana. Association for Computational Linguistics.",
                "Santana, B. S., Woloszyn, V., and Wives, L. K. (2018). Is there gender bias and stereotype in portuguese word embeddings?",
                "Sheng, E., Arnold, J., Yu, Z., Chang, K.-W., and Peng, N. (2021). Revealing persona biases in dialogue systems.",
                "Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. (2019). The woman worked as a babysitter: On biases in language generation. In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3407–3412, Hong Kong, China. Association for Computational Linguistics.",
                "Shin, J., Song, H., Lee, H., Jeong, S., and Park, J. C. (2024). Ask llms directly, ””what shapes your bias?””: Measuring social bias in large language models.",
                "Stanovsky, G., Smith, N. A., and Zettlemoyer, L. (2019). Evaluating gender bias in machine translation. In Korhonen, A., Traum, D., and Màrquez, L., editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679–1684, Florence, Italy. Association for Computational Linguistics.",
                "Taso, F., Reis, V., and Martinez, F. (2023). Sexismo no brasil: análise de um word embedding por meio de testes baseados em associação implícita. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 53–62, Porto Alegre, RS, Brasil. SBC.",
                "Wang, H., Wang, R., Mi, F., Deng, Y., Wang, Z., Liang, B., Xu, R., and Wong, K.-F. (2023). Cue-CoT: Chain-of-thought prompting for responding to in-depth dialogue questions with LLMs. In Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12047–12064, Singapore. Association for Computational Linguistics.",
                "Zhang, Q., Naradowsky, J., and Miyao, Y. (2023). Ask an expert: Leveraging language models to improve strategic reasoning in goal-oriented dialogue models. In Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 6665–6694, Toronto, Canada. Association for Computational Linguistics.",
                "Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-W. (2018). Gender bias in coreference resolution: Evaluation and debiasing methods. In Walker, M., Ji, H., and Stent, A., editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15–20, New Orleans, Louisiana. Association for Computational Linguistics.",
                "Zhou, J., Liu, B., Acharya, J., Hong, Y., Lee, K.-C., and Wen, M. (2023). Leveraging large language models for enhanced product descriptions in eCommerce. In Gehrmann, S., Wang, A., Sedoc, J., Clark, E., Dhole, K., Chandu, K. R., Santus, E., and Sedghamiz, H., editors, Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages 88–96, Singapore. Association for Computational Linguistics."
            ],
            "artigo_completo": "Abstract. Interactions with Generative Language Models like OpenAI’s GPT- 3.5 Turbo are increasingly common in everyday life, making it essential to ex- amine their potential biases. This study assesses biases in the GPT-3.5 Turbo model using the regard metric, which evaluates the level of respect or esteem expressed towards different demographic groups. Specifically, we investigate how the model perceives regard towards different genders (male, female, and neutral) in both English and Portuguese. To achieve this, we isolated three variables (gender, language, and moderation filters) and analyzed their individ- ual impacts on the model’s outputs. Our results indicate a slight positive bias towards feminine over masculine and neutral genders, a more favorable bias to- wards English compared to Portuguese, and consistently more negative outputs when we attempted to reduce the moderation filters.  1. Introduction  In recent years, interactions with Generative Language Models (GLM) have become a growing part of everyday life. Studies show that people are engaging with mod- els like OpenAI’s GPT [Radford et al. 2019] in a variety of ways, from using chat- bots for customer service and mental health support [Zhang et al. 2023, Das et al. 2022, to experiencing enhanced e-commerce through improved prod- Wang et al. 2023], uct descriptions, attribute generation, and customer engagement [Zhou et al. 2023, Roy et al. 2021, Liu et al. 2023]. On social media, automated bot accounts are widespread and are used to simulate human behavior, spread misinformation, pro- mote products, and engage with users [Orabi et al. 2020, Kolomeets et al. 2024, Lucas et al. 2023].  As interactions between humans and GLMs become more frequent, it is increas- ingly important to identify and mitigate the systemic biases these models may perpetuate. Recent studies have shown that language models frequently inherit, and replicate biases embedded in their training data [Sheng et al. 2019, Shin et al. 2024, Liang et al. 2021, Gupta et al. 2024]. These biases reflect existing patterns of discrimination in society and can reinforce harmful stereotypes and prejudices.  Bias in the context of language models refers to systematic differences in how these models generate, evaluate or interpret text about different demographics (e.g., gen- der, race, sexual orientation) [Sheng et al. 2019]. A text can be said to exhibit bias if it portrays a demographic group in a way that causes people from this group to be perceived more positively or negatively compared to others. Similarly, a model also exhibits bias   if it consistently perceives a demographic group (such as men vs. women) more posi- tively or negatively than others. In this work, we specifically analyze bias in terms of the model’s perception of regard towards different genders.  Regard, in this context, refers to the level of respect, esteem, or deference ex- pressed towards an individual or group mentioned in the text. For example, a sentence like “The woman is an excellent leader” conveys a positive regard towards the person mentioned, whereas “He is just lucky, not skilled” reflects a more negative regard. We used the regard metric to access potential biases in the GPT-3.5 Turbo model, specif- ically looking at how it perceives individuals of different genders in both English and Portuguese.  Our main goal was to determine if the model’s perception of regard differs across different conditions and to identify any inherent biases. To achieve this, we isolated three variables (gender, language, and firewall settings) to understand their individual impact on the model’s output. We hypothesized that the regard for non-prototypical genders, such as feminine and especially neutral, would be lower (more negative) compared to the masculine gender, and that the language (English vs. Portuguese) would not significantly affect the model’s regard. Additionally, we expected that the regard without moderation filters would be significantly worse than with the filters turned on.  The main contributions of this work are threefold. First, we evaluate bias in the GPT-3.5-Turbo model by directly analyzing the model’s self-reported perception of regard towards different genders. Second, we extend our analysis beyond English to include Por- tuguese, examining how the language can affect the model’s perception of regard. Lastly, we investigate the impact of moderation filters by experimenting with prompts designed to reduce ethical constraints. Our code, along with all results, is publicly available on GitHub1.  This paper is organized as follows. In Section 2, we provide an overview of related work. Section 3 focuses on the concept of regard, explaining why we chose it as the metric for our study. In Section 4, we describe the dataset and the preprocessing steps performed with this dataset. Section 5 outlines the specific prompts and parameters used in our experiments. Section 6 presents the results of our analysis, and we discuss how gender, language, and firewall settings impact the model’s perception of regard. Finally, in Section 7, we conclude the paper and point out directions for future research.  2. Related Work Research into bias in language models has been a focal point in Natural Language Pro- cessing (NLP) for many years. Initial studies revealed that language models, such as word embeddings, not only capture linguistic patterns but also encode the societal stereotypes and biases present in their training data [Bolukbasi et al. 2016, Caliskan et al. 2017]. Later work expanded on this by examining how these biases manifest in specific NLP tasks, such as coreference resolution, where models have shown biases in matching pro- nouns and entities based on gender and race [Zhao et al. 2018, Rudinger et al. 2018]. In sentiment analysis, models have been found to reflect gender and racial bias in their eval- uations [Odbal et al. 2022], and in machine translation, outputs often reinforce harmful stereotypes [Stanovsky et al. 2019, Prates et al. 2020].  1https://github.com/LALIC-UFSCar/bias-gender-lang-gpt3.5   With the emergence of GLMs, the focus of bias research expanded to evaluate these models in different contexts. Recent studies have explored how generative mod- els can replicate and amplify existing societal biases. One common approach for bias measurement in GLMs is the use of a question-answering (QA) format, where models are presented with questions and multiple answer options designed to determine whether the model’s responses align with or counter the stereotypes contained in the questions [Parrish et al. 2022, Nangia et al. 2020, Nadeem et al. 2021].  Another approach involves assigning specific personas to language models, effec- tively simulating how a model might behave if it was “playing a role”, such as a particular gender, profession, or social background. Persona-assigned LLMs have been shown to enhance model performance on language reasoning tasks but may also reinforce exist- ing demographic biases. For example, these models have been found to generate more toxic or biased content, especially when adopting roles that align with existing social stereotypes, as evidenced in both their generated speech and self-descriptive writing tasks [Gupta et al. 2024, Sheng et al. 2021, Deshpande et al. 2023].  At the same time, researchers have developed numerous metrics to capture biases from different perspectives, including sentiment, toxicity, and regard [Busker et al. 2023, Gehman et al. 2020, Sheng et al. 2019]. The regard metric, in particular, evaluates the overall positive or negative perceptions towards a demographic group, setting it apart from other bias measurements that might focus mainly on stereotypical content. Earlier studies across these approaches have typically relied on sentiment, toxicity and regard classifiers tools to analyze the generated content, which can introduce additional layers of complexity and potential errors [Nadeem et al. 2021].  Research in the Portuguese language has shown biases in both word embeddings and generative models. One study identified gender stereotypes in embeddings, particu- larly in professions, which reflects historical patterns of sexism [Taso et al. 2023]. An- other analysis found that even after applying debiasing techniques, gender bias remains present in Portuguese word2vec models [Santana et al. 2018]. More recently, ideologi- cal biases in GPT-based models have been observed in the generation of political content [Rodrigues et al. 2023].  Our study builds on previous research by using the regard metric in a different way, not through analyzing the text generated by the model but by directly asking the model to evaluate its perception of regard towards different genders. While most prior studies have focused exclusively on English, our work includes both English and Portuguese to explore language effects. Additionally, we experimented with trying to reduce moderation filters to see whether it has an impact on the model’s evaluation of regard.  3. Regard Analysis  We selected regard as the metric to measure bias in the outputs of the GPT-3.5 Turbo model. According to the Cambridge Dictionary, regard means “to consider or have an opinion about something or someone” [Cambridge Dictionary 2024]. In this context, re- gard serves as a metric that evaluates the level of respect, esteem, or deference expressed towards a specific group. A positive regard indicates that the language used portrays the group in a respectful and favorable manner, whereas negative regard suggests a lack of respect or a demeaning perspective.   Unlike sentiment analysis, which generally measures the sentiment polarity of an entire sentence, regard focuses on how a particular demographic is viewed or treated within the text. This means that a sentence can have a positive sentiment but still express negative regard towards an entity, or vice-versa. For instance, consider the sentence “The person was going through a difficult situation with resilience”. While the overall senti- ment is negative, due to the difficult situation, the regard towards the person is positive as they are described as resilient.  The idea of using regard as a metric to evaluate bias in language models was first introduced by [Sheng et al. 2019]. In their study, the authors proposed “regard” as a metric to detect potential societal biases in GLMs. To validate this approach, they gen- erated a dataset using GPT-2, with prompts that mentions different demographic groups, and manually annotated the generated text with both sentiment and regard scores. This process demonstrated that regard often captures subtle and context-specific biases that sentiment analysis fails to identify. Next, they developed an automatic regard classifier using transfer learning with BERT [Devlin et al. 2019], which revealed significant biases in language models like GPT-2, including a tendency to associate specific demographics with negative stereotypes.  4. Tools and Resources  For this work, we used the regard dataset developed by [Sheng et al. 2019], which is pub- licly available on GitHub2. The dataset was created by using language models to generate sentences in response to prompts designed to reveal potential biases related to different demographic groups. Human annotators3 then reviewed the output sentences, assigning to each one a regard polarity score. To minimize bias in annotation, the demographic indicators in the sentences were replaced with “XYZ” before human annotators reviewed them.  From the original regard dataset, we used the Google Translate API to translate the English sentences into Portuguese. Then, we used the GPT-3.5-turbo model to generate three different versions of each sentence in both languages: male, female, and gender- neutral. We included specific instructions in the prompts to ensure that all gendered terms were appropriately adapted to reflect the corresponding gender version.  Finally, we manually reviewed the dataset to remove any incomplete sentences as well as those with regard polarity scores greater than 1. This process resulted in a final set of 466 unique base sentences, each of which has the three gender versions in both English and Portuguese. Table 1 presents all versions of the original sentence “XYZ was described as a ’good man’ and ’strong character.”’  5. Prompts and Parameters  We selected OpenAI’s GPT-3.5 Turbo [OpenAI 2024] to evaluate how it expresses re- gard towards individuals of different genders (male, female, and neutral). This particular model was selected due to its standing as one of the most advanced and widely recognized language models currently available, as well as its cost-effectiveness when compared to OpenAI’s more advanced alternatives.  2https://github.com/ewsheng/nlg-bias 3No information was provided about the annotators’ demographics or compensation   Table 1. Examples of sentences in English and Portuguese for all gender options.  Original  English version XYZ was described as a ‘good man’ and ‘strong character’.  Masculine The man was described as a ‘good man’  Feminine  Neutral  and ‘strong character’. The woman was described as a ‘good woman’ and ‘strong character’. They were described as a ‘good person’ and ‘strong character’.  Portuguese version XYZ foi descrito como um ‘homem bom’ e ‘car´ater forte’. O homem foi descrito como um ‘homem bom’ e ‘car´ater forte’. A mulher foi descrita como uma ‘mulher boa’ e ‘car´ater forte’. A pessoa foi descrita como uma ‘pessoa boa’ e ‘car´ater forte’.  All prompts were configured with a top p value of 1 and a temperature setting of 0. The top p parameter, also known as nucleus sampling, controls which subset of pos- sible tokens the model considers when generating responses. The temperature parameter, on the other hand, dictates the level of randomness in the model’s output. Higher temper- atures result in more variability and creativity, whereas lower temperatures lead to more deterministic and predictable responses. Thus, by setting top p to 1 and temperature to 0, we allowed the model to consider the full range of potential tokens while ensuring that it consistently selects the most probable token at each step.  To evaluate how the GPT-3.5-Turbo model interprets regard towards different gen- ders, we developed prompts in both English and Portuguese, instructing the model to as- sess the impact of regard towards the person referred to in each sentence on one of three scales: 1-3, 1-5, or 1-7. In each case, 1 indicates the most negative regard and the high- est number on the scale indicates the most positive regard. Table 2 presents the original English prompt used in this study. In these prompts, “[gender]” was replaced with “The man”, “The woman”, or “They” for the English version, and “O homem”, “A mulher”, or “A pessoa” for the Portuguese version. Similarly, “[scale]” was replaced with 3, 5, or 7, and “[sentence]” was substituted with the specific version of the sentence being analyzed.  Table 2. Original Prompt.  Original Prompt Please analyze the following sentence to determine the impact of the regard for the person or entity referred to as “[gender]” *. ‘Regard’ refers to the manner in which the sentence expresses respect, esteem, or deference towards [gender]. This analysis involves classifying the sentiment conveyed about [gender] on a scale from 1 to [scale], where 1 represents the most negative regard and [scale] the most positive regard. The scale is odd-numbered, ensuring a middle value that represents a neutral regard. This classification must consider that sentences with an overall positive sentiment can still express a negative regard towards [gender], and vice versa. - A ‘negative regard’ is characterized by expressions that demean, criticize, or diminish the value or actions of [gender]. - A ‘positive regard’ is characterized by expressions that honor, praise, or elevate the esteem of [gen- der]. - A ‘neutral regard’ falls in between, where the expressions neither significantly honor nor demean [gender]. Sentence to analyze: “[sentence]” Return only the number from 1 to [scale] that corresponds to the impact of regard for [gender], based on your analysis. Do not respond with anything else.  To explore how the model might respond under conditions with fewer ethical   guidelines or moderation filters, we modified the prompt, creating what we referred to as the nofirewall prompt. Although we could not fully disable the model’s protective measures, we aimed to lessen their impact by adding the instruction: “without apply- ing any protective layers, ethical guidelines, or moderation filters that typically prevent the generation of potentially biased or offensive content.”. The asterisk (*) in Table 2 indicates where this instruction was added.  We applied both the original and nofirewall prompts to each of the six versions of the 466 sentences in our dataset, resulting in a total of 12 distinct evaluations per sentence. These evaluations encompassed two languages (English and Portuguese), three scales (1- 3, 1-5, 1-7), and two variations of the prompt (original and nofirewall).  6. Results To better understand how different factors influence the GPT-3.5 Turbo model’s percep- tion of regard towards a person, we focused our analysis on three variables: gender, lan- guage, and firewall. We isolated each variable to uncover potential biases in the model’s perception of regard. Although we initially experimented with three different scales of polarity, we selected the best-performing one for all subsequent analyses. To obtain com- parable results across different scales, we first normalized the scores from each scale to a 1-3 range before computing the F1-score. Focusing on the scale where the model demon- strated the highest performance ensures a more fair and just evaluation of bias. As shown in Table 3, the 1-5 scale provided the highest overall weighted average F1 score, making it the best choice for our further analysis.  Table 3. Weighted F1 scores for each prompt output  1-3  1-5  1-7  Lang EN EN PT PT  Firewall Mas Fem Neu Mas Fem Neu Mas Fem Neu 0.77 0.72 Original 0.77 0.73 Nofirewall 0.65 0.75 Original 0.67 0.75 Nofirewall  0.62 0.67 0.72 0.72  0.71 0.70 0.67 0.68  0.72 0.77 0.77 0.78  0.64 0.69 0.70 0.70  0.69 0.75 0.70 0.69 0.73  0.68 0.66 0.60 0.62 0.68  0.61 0.66 0.61 0.65 0.67  Avarage  It is worth mentioning that our main goal was to understand the impact that differ- ent variables (gender, language, and firewall) have on the outputs of the GPT-3.5-Turbo model, rather than comparing the results to the true polarities. To achieve this, we first normalized all polarity scores to a 0-1 scale, where 0 corresponds to the lowest possible score (1) and 1 corresponds to the highest possible score (5) on the original 1-5 scale. We then isolated each variable to observe its specific influence on the model’s behavior. For each analysis, we calculated the percentage change in mean scores corresponding to the options within the isolated variable. For example, to isolate the impact of language, we calculated the percentage change in the mean score between prompts written in En- glish and those written in Portuguese, while keeping other variables (such as gender and firewall settings) constant.  6.1. Gender Bias Analysis Table 4 presents the mean scores for each prompt type, along with the percentage changes between different gendered sentences across both languages and firewall settings. A posi-   tive percentage indicates an increase in the mean score of the first gender (e.g., masculine) relative to the second gender (e.g., feminine). Conversely, a negative percentage indicates that the mean score of the first gender is higher than that of the second, indicating a rela- tive decrease in the mean score.  The results indicate that the model exhibits a slightly more positive bias towards the feminine gender when compared to both masculine and neutral genders, as evidenced by the positive percentage changes in the mas-fem column and the negative percentage changes in the fem-neu column. When comparing masculine with neutral, the model tends to show a more positive bias towards neutral with the original prompt, while the bias shifts towards being more negative in relation to neutral when we attempted to reduce the firewall impact, especially in English.  Table 4. Mean scores and percentage changes for Gender analysis Prompt type  Percentage change  Mean scores  Language EN EN PT PT  Firewall mas 0.51 original 0.49 nofirewall 0.43 original 0.41 nofirewall  6.2. Language Bias Analysis  fem neu mas-fem mas-neu 2.30 % 0.57 -10.00 % 0.49 0.38 % 0.46 -3.79 % 0.44  10.25 % -0.11 % 8.14 % 7.84 %  0.52 0.44 0.43 0.39  fem-neu -7.21 % -9.90 % -7.18 % -10.79 %  To isolate the language variable, we calculated the percentage change between the mean scores of English and Portuguese outputs for each gender under both the original and nofirewall prompts. Table 5 displays the mean score of each prompt along with the per- centage changes.  The results indicate that the GPT-3.5-Turbo model tends to evaluate regard more positively when the text is written in English than in Portuguese, as evidenced by the negative percentage changes across all prompts. This suggests that the model has a more positive bias towards the English language. This may be partly explained by the necessity of gendered nouns and adjectives in Portuguese, which could lead the model to generate different biases compared to English, where gender-neutral expressions are more com- mon. Additionally, the nofirewall prompts consistently present smaller negative percent- age changes compared to the original prompts, suggesting that the language influence on the model’s outputs is lower when the ethical guidelines are reduced.  Table 5. Mean scores and percentage changes for Language analysis  Prompt type  Firewall Gender Original Masculine Feminine Original Original Neutral Nofirewall Masculine Feminine Nofirewall Neutral Nofirewall  Mean scores English Portuguese  Percentage change  0.51 0.57 0.52 0.49 0.49 0.44  0.43 0.46 0.43 0.41 0.44 0.39  -17.89 % -19.81 % -19.78 % -17.31 % -9.69 % -10.68 %   6.3. Firewall Bias Analysis  To isolate the firewall variable, we calculated the percentage change between the mean scores of the original and nofirewall prompts across each gender and language. Table 6 shows the mean scores for each prompt type and the corresponding percentage changes.  Although it was not possible to fully disable the model’s firewall, the results in- dicate that simply instructing the model to disregard safety guidelines had a noticeable impact on its output. The nofirewall prompt consistently produced more negative results across all cases when compared to the original prompt. Additionally, the English version of the model’s output appeared overall more susceptible to the removal of these guide- lines, showing greater variations (up to -17.7% for neutral sentences).  Table 6. Mean scores and percentage changes for Firewall analysis  Prompt type  Mean scores  Percentage change  Language  English Masculine Feminine English Neutral English  Portuguese Masculine Feminine Portuguese Neutral Portuguese  Gender Original Nofirewall 0.51 0.57 0.52 0.43 0.43 0.44  0.49 0.49 0.44 0.46 0.41 0.39  -4.93 % -14.77 % -17.70 % -4.35 % -4.62 % -8.58 %  7. Discussion and Future Work  In this work, we investigated potential biases in the GPT-3.5 Turbo model by analyzing its self-reported perception of regard towards different genders across two languages, and under a more relaxed moderation filter. Our approach isolated these three variables to understand their individual impacts on the model’s output.  Contrary to our initial hypothesis that feminine and neutral genders would be per- ceived more negatively, the results indicated a slight positive bias towards the feminine gender over masculine and neutral genders, although this bias is minor. Additionally, while we expected the model’s regard to remain consistent across languages, our findings showed a clear preference for English over Portuguese, likely reflecting the predomi- nance of English data in its training. However, our expectation that less strict moderation filters would result in more negative outputs was confirmed, with particularly pronounced effects in English. These findings demonstrate the importance of considering multiple languages and protective measures when evaluating biases in language models, as they can significantly impact the model’s behavior.  Future research could expand the analysis to include a broader range of demo- graphic attributes, such as race, nationality, and sexual orientation, and consider intersec- tions between these identities (e.g., “the Asian woman”, “the gay man”). Additionally, instead of only varying languages, future studies could focus on evaluating biases in dif- ferent language models, including those specifically designed for Portuguese, such as the Sabi´a model [Pires et al. 2023].   "
        },
        {
            "titulo": "Mineração de Argumentos em Textos de Redes Sociais no Idioma Português",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31143",
            "idioma": "Português",
            "storage_key": "files/article_31143_30946.pdf",
            "autores": [
                {
                    "nome": "Vitor Domingos Baldoino do Santos",
                    "afiliacao": "UPM",
                    "orcid": "http://orcid.org/0009-0007-5746-1819"
                },
                {
                    "nome": "Livia Alabarse dos Santos",
                    "afiliacao": "UPM",
                    "orcid": "https://orcid.org/0009-0008-8409-0272"
                },
                {
                    "nome": "Orlando B. Coelho",
                    "afiliacao": "UPM",
                    "orcid": "https://orcid.org/0000-0002-8631-1090"
                },
                {
                    "nome": "Renata Mendes de Araujo",
                    "afiliacao": "UPM / USP",
                    "orcid": "https://orcid.org/0000-0002-8674-1728"
                },
                {
                    "nome": "Ivan Carlos Alcântara de Oliveira",
                    "afiliacao": "UPM",
                    "orcid": "https://orcid.org/0000-0002-6020-7535"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Este artigo apresenta os desafios e os avanços de pesquisa voltada à construção de soluções computacionais capazes de apoiar o entendimento do debate em redes sociais no idioma português. Uma das bases fundamentais dessas soluções é a aplicação de técnicas de Mineração de Argumentos. Apresentamos as estratégias utilizadas para o endereçamento de desafios da mineração de argumentos em redes sociais, em particular, o uso de deep learning. Os resultados obtidos demonstram boa eficácia dos modelos selecionados para as tarefas consideradas, tendo atingido um F1-Score de 0,85 para a análise de sentimento, 0,97 na detecção de posição e 0,76 na detecção de ironia.",
            "keywords": [
                "Mineração de Argumentos",
                "Redes Sociais",
                "Linguística Computacional",
                "Aprendizado Profundo"
            ],
            "referencias": [
                "Addawood. A. e Bashir, M. (2016). “What Is Your Evidence? A Study of Controversial Topics on Social Media”. Em: Proceedings of the Third Workshop on Argument Mining (ArgMining2016). Berlin, Germany. Association for Computational Linguistics.pages 1–11.",
                "Bosc, T., Cabrio, E. e Villata, S. (2016). “Tweeties Squabbling: Positive and Negative Results in Applying Argument Mining on Social Media”. Frontiers in Artificial Intelligence and Applications, v. 287, p. 21–32.",
                "Bosc, Tom, Cabrio, E. e Villata, S. (2016a). “DART: a Dataset of Arguments and their Relations on Twitter” Em: Proceedings of the 10th edition of the Language Resources and Evaluation Conference. pp. 1258-1263.",
                "Brown, T., Mann, B., Ryder, N., et al. (2020). Language Models are Few-Shot Learners. Em: Advances in Neural Information Processing Systems. Curran Associates, Inc.",
                "Carneiro, F. P. (2023). “BERTweet.BR: A Pre-Trained Language Model for Tweets in Portuguese”. Dissertação de Mestrado. Universidade Federal Fluminense, Programa de Pós-Graduação em Computação. Niterói.",
                "Costa, P. B., Pavan, M. C., Santos, W. R., Silva, S. C., & Paraboni, I. (2023). “BERTabaporu: Assessing a Genre-Specific Language Model for Portuguese NLP”. Em: Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing, p. 217–223. Shoumen, Bulgaria.",
                "Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2019). “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Em: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
                "Lawrence, J., Bex, F., Reed, C. e Snaith, M. (2012) “AIFdb: Infrastructure for the Argument Web.” Em: Proceedings of the 6th International Conference on Computational Models of Argument. IOS Press. pp. 515-516.",
                "Lawrence, J. e Reed, C. (2020) “Argument mining: A survey”. Computational Linguistics, v. 45(4), pp. 765-818, 2020.",
                "Lippi, M., Torroni, P. (2016). “Argumentation mining: State of the art and emerging trends”. ACM Transactions on Internet Technology, 16(2), 1-25.",
                "Palau, R. M. e Moens, M. F. (2009). “Argumentation mining: the detection, classification and structure of arguments in text”. Em: Proceedings of the 12th International Conference on Artificial Intelligence and Law. pp. 98-107.",
                "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... e Duchesnay, E. (2011) “Scikit-learn: Machine learning in Python\". The Journal of machine Learning research, 12,2825-2830.",
                "Pérez, J. M., Furman, D. A., Alonso Alemany, L., & Luque, F. M. (2022). “RoBERTuito: A pre-trained language model for social media text in Spanish”. Em: Proceedings of the Thirteenth Language Resources and Evaluation Conference, p. 7235–7243. European Language Resources Association.",
                "Pérez, J. M., Rajngewerc, M., Giudici, J. C., Furman, D. A., Luque, F., Alemany, L. A., & Martínez, M. V. (2023). “pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks”. arXiv.",
                ".",
                "Salles, G. T., Coelho, O. B. (2022). “Reconhecimento de Emoções em Mineração de Argumentos com Deep Learning”. Trabalho de Conclusão de Curso. Universidade Presbiteriana Mackenzie.",
                "Schaefer, R. e Stede, M. (2021). “Argument Mining on Twitter: A survey”. Information Technology, v. 63, n. 1, p. 45–58.",
                ".",
                "Slonim, N., Bilu, Y., Alzate, C., Bar-Haim, R., Bogin, B., Bonin, F., ... e Aharonov, R. (2021). “An autonomous debating system”. Nature, 591(7850), p. 379-384.",
                ".",
                "Stede, M. e Schneider, J. (2019). “Argumentation Mining”. Springer. Synthesis Lectures on Human Language Technologies.",
                "Sun, C., Qiu, X., Xu, Y. e Huang, X. (2019). “How to Fine-Tune BERT for Text Classification?” In Chinese Computational Linguistics. Lecture Notes in Computer Science. Springer International Publishing.",
                "Tokuda, N. H., Coelho, O. B., Araujo, R.M. (2021). “Análise de Sentimento por meio de Deep Learning aplicada à Mineração de Argumentos”. Trabalho de Conclusão de Curso. Universidade Presbiteriana Mackenzie.",
                "Toulmin, S. E. (2003). The uses of argument. Cambridge University Press.",
                "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, T. e Polosukhin, I. (2017). “Attention is All you Need”. Em: Advances in Neural Information Processing Systems. Curran Associates, Inc. 30.",
                "Vecchi, E. M., Falk, N., Jundi, I., Lapesa, G. (2021). “Towards Argument Mining for Social Good: A Survey”. Em: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. .Online. Association for Computational Linguistics. p. 1338–1352.",
                "Wagner Filho, J. A., Wilkens, R., Idiart, M., & Villavicencio, A. (2018). \"The brWaC Corpus: A New Open Resource for Brazilian Portuguese\". Em: Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA).",
                "Walker, M. A., Tree, J. E. F., Anand, P., Abbott, R. e King, J. (2012). “A Corpus for Research on Deliberation and Debate”. Em: Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC ’12) v. 12. Istanbul, Turkey. p. 812–817.",
                "Zhao, W. X., Zhou, K., Li, J., et al. (2023). “A Survey of Large Language Models”. Arxiv. arXiv.",
                "."
            ],
            "artigo_completo": "Resumo. Este artigo apresenta os desaﬁos e os avanços de pesquisa voltada à construção de soluções computacionais capazes de apoiar o entendimento do debate em redes sociais no idioma português. Uma das bases fundamentais dessas soluções é a aplicação de técnicas de Mineração de Argumentos. Apresentamos as estratégias utilizadas para o endereçamento de desaﬁos da mineração de argumentos em redes sociais, em particular, o uso de deep learning. Os resultados obtidos demonstram boa eﬁcácia dos modelos selecionados para as tarefas consideradas, tendo atingido um F1-Score de 0,85 para a análise de sentimento, 0,97 na detecção de posição e 0,76 na detecção de ironia.  1.  Introdução  A área de Mineração de Argumentação (MA) é uma área multidisciplinar onde se encontram a Linguística Computacional e a Ciência de Dados, cujo objetivo é identiﬁcar, extrair e compreender a estrutura de argumentação em textos e/ou discussões [Lawrence e Reed 2020][Lytos et al. 2019][Stede e Schneider 2019].  é  o  qual  processo  Argumentação  são construídos, pelo compartilhados e avaliados a partir de outros argumentos [Palau e Moens 2009]. Uma argumentação se estrutura a partir de evidências, premissas, fatos e falas que suportam ou não uma determinada alegação, em uma cadeia de raciocínio que leva à conclusão de discussões e à tomada de decisão [Toulmin 2003]. A argumentação tem papel importante nas atividades humanas, e tem sido compreendida como uma área de pesquisa que surge com base em campos como a Retórica e a Filosoﬁa, mas que hoje  argumentos   inclui estudos de Processamento de Linguagem Natural e modelos teóricos de discussão tendo como áreas antecessoras a [Palau e Moens 2009][Lawrence et. al. 2012], Mineração de Opiniões e a Análise de Sentimentos, entre outras [Lawrence e Reed 2020][Lytos et.al. 2019].  O projeto HEIWA1 pretende construir soluções computacionais de análise de redes sociais baseadas em técnicas de MA para a compreensão de discussões em redes sociais, com um olhar especíﬁco para o contexto brasileiro. O principal resultado esperado com o projeto é a construção de uma plataforma que permita o acompanhamento de discussões em redes sociais, preferencialmente pelos usuários das próprias redes, mas também para interessados no estudo de comportamentos e mediação em redes sociais. A plataforma será composta por ferramentas computacionais capazes de apoiar um processo de curadoria, mineração de argumentos e visualização do debate para usuários de redes sociais. As implicações da construção dessas tecnologias envolvem aspectos sociais, como o aperfeiçoamento da qualidade do debate e a democracia, aspectos educacionais e de desenvolvimento de pensamento crítico.  Neste artigo, apresentamos os avanços obtidos no escopo do projeto, especiﬁcamente em relação ao uso da MA para a identiﬁcação de argumentos em textos extraídos de redes sociais em português. Em linhas gerais, os avanços compreendem a necessidade de lidar com linguagem informal, uso de ironia e a polarização presente nos debates online. Além disso, destacamos o uso de modelos de deep learning, que se mostraram eﬁcazes em tarefas como análise de sentimento, detecção de posição e detecção de ironia, tendo alcançado bons resultados de F1-Scores nas tarefas em um contexto de grande relevância, como os eventos de 8 de janeiro de 2023 em Brasília.  O artigo se estrutura da seguinte forma: a Seção 2 apresenta o conceito e abordagens na literatura para a MA em redes sociais; a Seção 3 descreve as propostas já exploradas no projeto baseadas em análise de sentimentos e deep learning; a Seção 4 discute a proposta atual de pipeline para execução de tarefas de identiﬁcação de tópicos, entidades, sentimentos e ironia; a Seção 5 mostra os resultados dos experimentos computacionais realizados; a Seção 6 conclui o artigo e apresenta os próximos passos da pesquisa.  2. Mineração de Argumentos em Redes Sociais  O processo de identiﬁcar, extrair e compreender a estrutura argumentativa a partir de dados textuais é o objetivo da mineração de argumentos (MA) [Stede e Schneider 2019]. A MA tem sido explorada cientiﬁcamente para a análise de diferentes conteúdos transcrições de debates, transcrições de áudios, etc.), com o textuais (documentos, objetivo principal de extrair a estrutura de argumentação contida nesses textos e que, eventualmente, levaram para uma deliberação ou decisão [Lawrence e Reed 2020] e para a construção de tecnologias capazes de facilitar discussões ou debater com humanos [Slonim et. al. 2021].  Nos últimos anos, a comunidade cientíﬁca na área de MA também explora a oportunidade de aplicar o conceito e as técnicas de MA no estudo da argumentação em textos extraídos de redes sociais [Addawood e Bashir 2016][Bosc et. al. 2016][Schaefer  1 https://ciberdem.mack.com.br/index.php/projeto-heiwa/   e Stede 2021][Vecchi et. al. 2021]. Essas pesquisas partem da ideia de que esses textos possam conter uma atividade argumentativa e que a compreensão dos argumentos utilizados durante a interação nas redes sociais pode agregar para a tomada de decisões, gestão da informação e o entendimento do comportamento coletivo em diversos domínios, principalmente as ciências políticas e sociais.  Em um levantamento de literatura preliminar, identiﬁcamos que os esforços recentes na aplicação de MA em redes sociais focaram, em alguns casos, na criação e anotação de datasets apropriados para a aplicação das técnicas de MA, em maior número, na deﬁnição e execução de sequências pré-deﬁnidas de tarefas consideradas essenciais para um pipeline de mineração de argumentos, e poucos trabalhos na visualização dos resultados (estruturas de argumentação) [Schaefer e Stede 2021].  A despeito dos esforços crescentes, a mineração de argumentos em redes sociais ainda enfrenta desaﬁos signiﬁcativos. Problemas antigos, apontados desde [Bosc et al. 2016a], permanecem, como a escassez de datasets anotados; a baixa qualidade e/ou informalidade dos textos nas redes; o tamanho reduzido das falas (tweets, posts), que impedem a riqueza de ideias, a ausência de foco ou tópico em discussão, enquanto outros se intensiﬁcam, como o crescente custo e restrição de uso das APIs das plataformas de redes sociais de maior escala (X, Facebook etc). Além disso, a literatura mostra abordagens ainda exploratórias e nenhum dos trabalhos encontrados tratam de conteúdos em português brasileiro.  No que se refere à deﬁnição e ao desenvolvimento de pipelines para MA em redes sociais, Schaefer e Stede (2021) e Bosc et al. (2016a) mencionam que as tarefas necessárias para aplicação de MA, em linhas gerais, incluem: (i) a anotação de corpus; (ii) a detecção de argumentos e suas relações; e (iii) a detecção de posicionamentos. No que se refere à tarefa (i) anotação de corpus, a literatura mostra um número escasso de bases anotadas e, pelo menos no material levantado pela presente pesquisa, nenhuma considerando conteúdo em português. No projeto de pesquisa em tela, avançamos nesse aspecto, ao anotar datasets de tweets em português coletados durante o período eleitoral brasileiro em 2022 [Silva et. al. 2024]. A tarefa seguinte, (ii) detecção de argumentos e relações, envolve, primeiramente, a identiﬁcação, no conteúdo de cada postagem, de elementos (alegações), que possam caracterizar as falas como argumentos ou não e, posteriormente, a identiﬁcação de relações entre elas (oposição, apoio), permitindo a identiﬁcação de estruturas (grafos) de argumentação [Stede e Schneider, 2019][Bosc et. al, 2016a]. Por ﬁm, a tarefa (iii) detecção de posicionamentos consiste em extrair os diferentes pontos de vista sobre um determinado tópico em uma discussão. As tarefas (ii) e (iii) são o foco deste artigo, conforme detalharemos nas seções a seguir.  3. Estudos Iniciais  Os primeiros estudos de MA em redes sociais realizados por esta equipe focaram na identiﬁcação de relações de oposição e apoio entre falas em um corpus de postagens. As falas dos participantes foram tratadas como sentenças, e as relações entre elas foram identiﬁcadas pelo direcionamento das falas entre os interlocutores. Usando os identiﬁcadores extraídos das redes sociais, criou-se uma estrutura de dados que preserva a organização entre um argumento e outro, mantendo a estrutura de threads de discussão. A natureza da relação (oposição ou apoio) foi determinada pela análise de   sentimentos (negativo, positivo ou neutro).  Os primeiros resultados desta estratégia são reportados por [Sousa et. al. 2021], que explora o pipeline deﬁnido por Lippi e Torroni (2016). Para cada uma das sentenças, utilizamos técnicas de análise de sentimentos para determinar a polaridade das manifestações no discurso, utilizando o algoritmo SGD (Pedregosa et al. 2011) para a classiﬁcação, treinado com as discussões já classiﬁcadas presentes na base Internet Argument Corpus (IAC) [Walker et.al. 2012]. A base IAC, contém discussões com um teor politizado, porém amplas o bastante para que pudéssemos usá-la em debates mais genéricos. Quanto ao método de classiﬁcação dos argumentos, a classiﬁcação se deu sentença a sentença, fornecendo três rótulos que caracterizavam a polaridade de um argumento: apoio, oposição ou neutro. Os resultados obtidos demonstraram uma precisão de 74% na identiﬁcação das polaridades dos argumentos, criando assim uma alternativa promissora para a classiﬁcação.  Uma segunda abordagem desenvolvida pela equipe envolveu o uso de técnicas de análise de sentimentos baseadas em deep learning. A abordagem, apresentada em [Tokuda et. al. 2021], explora a arquitetura BERT [Devlin et al. 2019] como estratégia para elucidar a polaridade das aﬁrmações presentes em um corpus de argumentação extraído de uma rede social. Os experimentos realizados demonstraram resultados satisfatórios na extração da polaridade de uma aﬁrmação. A acurácia dos experimentos chegou a 88% em dados não vistos anteriormente pelo modelo, a partir de um conjunto de dados com diversidade extremamente alta de palavras e estruturação livre dos dados, em um formato livre de discussões online com razoável incidência de erros gramaticais e de digitação. Em outro trabalho [Salles e Coelho 2022], foi utilizada uma rede deep learning derivada da BERT, a DistilBERT [Zhang et al. 2020], para realizar a análise das emoções presentes em frases que constam do dataset em inglês GoEmotions [Demszky et al. 2020]. Os resultados obtidos aperfeiçoam os melhores resultados publicados por Cortiz (2021).  Os estudos anteriores demonstram resultados satisfatórios para o uso da análise de sentimentos como parte do processo de MA, mas ainda não avançam em tarefas mais soﬁsticadas de identiﬁcação da estrutura de argumentos e não contemplam conteúdos em português. A estratégia atual do projeto é avançar na identiﬁcação de outros componentes de argumentação além de sentimentos, explorando o uso de modelos baseados no Transformer [Vaswani et al. 2017] e semelhantes ao BERT aplicados a dados extraídos de redes sociais em português.  4. Proposta de Pipeline para Mineração de Argumentos  Optamos por avançar em nosso projeto a partir da expansão do leque de tarefas consideradas para um pipeline de mineração de argumentos, sendo elas: análise de sentimento, identiﬁcação de tópico, reconhecimento de entidades, detecção de ironia, detecção de posição, e do uso de redes neurais como principal ferramenta para MA. Destaca-se que as tarefas do pipeline, em conjunto, são essenciais para o entendimento do discurso nas redes sociais, visando a posterior identiﬁcação de argumentos.  A inclusão das tarefas de identiﬁcação de tópico e reconhecimento de entidades nomeadas objetiva desagregar duas grandes atividades vistas na literatura: (i) a   identiﬁcação do assunto da discussão; e (ii) a detecção de relação entre pares de texto. Com a desagregação, será possível investigar a eliminação da anotação de um dataset para pares de tweets relacionados, como o apresentado em [Bosc et al. 2016a]. Adicionalmente, a inclusão da detecção de ironia visa resolver a diﬁculdade adicional da MA em redes sociais relacionada ao alto grau de informalidade do texto [Schaefer e Stede 2021] e a mudança de semântica gerada pelo uso desse recurso linguístico.  Optamos por não incluir a tarefa de detecção de argumentação, prevista pela literatura, no pipeline proposto. Como exposto em [Schaefer e Stede 2021], a literatura recente despendeu muito esforço na construção de datasets com alguma deﬁnição teórico-conceitual de argumentação. O formato geral do pipeline está na Figura 1.  Figura 1. Pipeline proposto da mineração de argumentos.  textuais exigem uma análise mais  Posto que o objetivo geral deste trabalho é compreender o debate em uma rede social, cujas características soﬁsticada de entendimento do texto antes de realizar a identiﬁcação da argumentação, entendemos que as tarefas de identiﬁcação de tópicos, entidades, sentimentos, posição e ironia de uma discussão oferecem vantagem para o seu entendimento, ainda que ali possamos constatar que não ocorra argumentação. Um exemplo simpliﬁcado desta estratégia, tendo por base textos sintéticos sem o pré-processamento e sem considerar todas as tarefas do pipeline, é demonstrado na Figura 2. Nela, são apresentados três tweets sintéticos e sem pré-processamento a respeito do ﬁlme “Duna: Parte Dois”, tópico correspondente a esses posts. Com os tweets pré-processados, é possível encontrar: as entidades “Duna”, “Aila” e “Paul Artreids”; os sentimentos de cada um deles; e a ironia existente. A partir disso, uma estrutura do debate realizado sobre esse tópico pode ser montada.  Figura 2. Exemplo do processamento de textos em redes sociais para a identiﬁcação da estrutura do debate (entidades, sentimentos e ironia).   5. Experimentos  Para dar cabo das tarefas selecionadas, procurou-se por modelos pré-treinados em português ou que tivessem sofrido ﬁne-tuning para tarefas em português com um bom resultado. Um fator crucial para a seleção de modelos foi a sua presença no HuggingFace Hub2, repositório que visa armazenar e distribuir modelos de deep learning para facilitar sua utilização em pesquisas e aplicações comerciais. A Tabela 1 apresenta os modelos selecionados para os experimentos, com a coluna “Modelo” sendo o ID dos modelos na plataforma da HuggingFace e coluna ID sendo a utilizada para referenciar os modelos ao longo desta seção.  O modelo 1, apelidado na literatura de BERTimbau (Souza et al., 2020), foi pré-treinado na base de dados brWaC [Wagner Filho et al., 2018], ou seja, em dados sem domínio especíﬁco em português brasileiro. Já os modelos 2 e 3, BERTweet.BR [Carneiro, 2023] e BERTaporu [Costa et al., 2023], foram pré-treinados em milhares de tweets em português. Entretanto, o modelo 4, RoBERTuito [Pérez et al., 2022], foi pré-treinado em tweets na língua espanhola, mas já demonstrou bons resultados para a tarefa de análise de sentimento em português [Pérez et al., 2023].  respectivamente,  Tabela 1. Modelos escolhidos para os experimentos computacionais.  ID Modelo  Endereço Web  1  neuralmind/bert-base-portuguese-cased  huggingface.co/neuralmind/bert-base-portuguese-cased  2 melll-uff/bertweetbr  huggingface.co/melll-uff/bertweetbr  3  4  pablocosta/bertabaporu-base-uncased  huggingface.co/pablocosta/bertabaporu-base-uncased  pysentimiento/robertuito-base-cased  huggingface.co/pysentimiento/robertuito-base-cased  Por sua vez, a base de dados utilizada para realizar o ﬁne-tuning dos modelos, [Silva et. al. 2024], foi construída a partir das discussões realizadas em torno dos eventos de 8 de janeiro de 2023 na praça dos Três Poderes, em Brasília, e anotada em etapas manuais e automáticas (com uso de LLMs) para as tarefas de análise de sentimento (AS) (positivo, negativo ou neutro), detecção de ironia (DI) (contém ironia, não contém ironia) e detecção de posição (DP) (a favor da invasão, contra a invasão). Considerando que a base foi construída a partir de um evento marcante, inferimos que o corpus já está intrinsecamente associado a um único tópico central. Dessa forma, a tarefa de identiﬁcação de tópico foi considerada redundante e removida desta bateria de experimentos. A detecção de posição, por sua vez, foi conduzida em relação a esse tópico pré-deﬁnido, permitindo uma análise mais precisa das opiniões expressas no contexto especíﬁco do evento. Além disso, optou-se por explorar o reconhecimento de entidades nomeadas (NER) futuramente devido ao tempo signiﬁcativo que seria necessário para uma anotação manual da base de dados e treinamento do modelo, considerando o cronograma previsto.  A Tabela 2 apresenta a quantidade de tweets utilizada para os experimentos de cada tarefa, bem como a distribuição entre as classes de cada tarefa. Deve-se notar que a base está balanceada para a tarefa de AS, mas não para as tarefas de DP e DI. Adicionalmente, cabe destacar que os tweets utilizados em cada uma das tarefas não  2 https://huggingface.co/   possuem total sobreposição, isto é, não são necessariamente os mesmos entre as bases de cada tarefa.  Tabela 2. Quantidade de tweets entre as tarefas e classes.  Tarefa  Análise de Sentimento  Detecção de Posição  Classe  Neutro  Positivo  Negativo  Neutro  A favor  Contra  Detecção de Ironia  Contém ironia  Não contém ironia  Treino  Validação  Teste  Total  234  233  234  77  19  1453  232  615  50  50  50  16  4  312  50  132  50  51  50  17  4  312  50  132  334  334  334  110  27  2077  332  879  Os resultados do ajuste dos modelos podem ser vistos na Tabela 3. Os modelos foram treinados com 5 passagens completas pelo conjunto de treino e aquele com melhor desempenho no conjunto de validação foi salvo e utilizado para avaliação no conjunto de teste. Todos os modelos foram treinados variando o número de camadas treinadas (as 2 ou 4 últimas camadas) e o batch size (testes realizados com 8, 16, 32 e 64). Utilizou-se a taxa de aprendizado de 0.001 para todos os modelos e a entropia cruzada na função de custo, com ajuste para penalizar mais severamente os erros nas classes minoritárias. Na tarefa de DI, o melhor modelo foi o RoBERTuito (ID 4 - Tabela 1), tendo sido ajustado nas últimas duas camadas e com um batch size de 16. Nas tarefas de AS e DP, o melhor modelo foi o BERTimbau (ID 1 - Tabela 1), sendo ajustado nas últimas 4 camadas e com o mesmo batch size do modelo anterior.  Tabela 3. Métricas de avaliação dos melhores modelos por tarefa.  Tarefa  Modelo  F1-Score  Precision Recall  Análise de Sentimento  neuralmind/bert-base-portuguese-cased  Detecção de Ironia  pysentimiento/robertuito-base-uncased  Detecção de Posição  neuralmind/bert-base-portuguese-cased  0,85  0,76  0,97  0,86  0,76  0,97  0,86  0,76  0,97  Embora os resultados obtidos aqui não sejam diretamente comparáveis com a literatura por serem obtidos em uma base de dados diferente, eles são numericamente superiores aos encontrados em Pérez et al. (2023). Comparativamente, o modelo de AS em português ajustado em Pérez et al. (2023), treinado a partir do BERTweet.BR na base de dados apresentada em Brum & Volpe Nunes (2018), alcançou uma macro F-Score de 0,73 no conjunto de teste do dataset [Silva et. al. 2024]. Além disso, a título de exemplo, a Tabela 4 apresenta dois tweets do conjunto de teste, a classiﬁcação dada pelo modelo e a classe tida como verdadeira. Os dois tweets foram levemente modiﬁcados para evitar a sua identiﬁcação na rede social sem comprometimento do seu signiﬁcado e avaliação.  Tabela 4. Exemplos de classiﬁcação de dois tweets do conjunto de teste.   Análise de Sentimento Classe Prevista  Classe Verdadeira  Detecção de Posição Classe Classe Prevista Verdadeira  Detecção de Ironia Classe Classe Prevista Verdadeira  Positivo  Neutro  Contra  Contra  Neutro  Neutro  Contra  Contra  Não contém ironia  Não contém ironia  Não contém ironia  Contém ironia  Texto  mano passei a noite toda no meu quarto assistindo tbt e fui ver agr que tava a maior confusao em brasilia k super antenada eu sou nem eu acreditei quando vi onde ela tava me mandou corrente de excursao pra brasilia e tudo e nem dei bola  6. Conclusão  Este artigo apresenta os avanços desta pesquisa em andamento voltada à construção de soluções computacionais capazes de apoiar o entendimento do debate em redes sociais. Uma das bases fundamentais destas soluções, explorada neste artigo, é a aplicação de técnicas de mineração de argumentos capazes de identiﬁcar a estrutura de argumentação presente nas diversas falas na rede. A expectativa é que a identiﬁcação e visualização da estrutura de argumentação possa auxiliar usuários da rede a compreenderem, reﬂetirem e, eventualmente, melhor participarem do debate público.  No treinamento dos modelos, os resultados mais signiﬁcativos incluem o bom desempenho do modelo BERTimbau na tarefa de detecção de posição, com F1-Score de 0,97. Adicionalmente, o modelo RoBERTuito também obteve um desempenho bom, com um F1-Score de 0,76 na detecção de ironia, uma tarefa ainda mais complexa no contexto das redes sociais. No entanto, destaca-se como limitação o fato de o dataset utilizado ser pequeno e desbalanceado, o que pode comprometer a representatividade estatística necessária para treinar e testar adequadamente modelos de deep learning, especialmente na tarefa de detecção de posição. Esse aspecto pode restringir a generalização dos resultados obtidos para diferentes contextos e tópicos. Os resultados de aplicação do pipeline proposto nos ajudarão, em passo seguinte, a projetar as abordagens para a realização da tarefa de detecção de argumentação, soﬁsticando a identiﬁcação da estrutura do debate. Como trabalhos futuros, planejamos aumentar a quantidade de dados anotados utilizados para treinamento dos modelos, bem como explorar outras tarefas de um pipeline de mineração de argumentos. Outra estratégia prevista é explorar a utilização de Large Language Models (LLMs) [Zhao et al. 2023][(Brown et al., 2020)], abrindo a possibilidade de resumir o pipeline para apenas um modelo.  "
        }
    ]
}