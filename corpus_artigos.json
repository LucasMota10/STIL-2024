{
    "Conferência Principal": [
        {
            "titulo": "Leveraging Structured Data Input for Effective Chatbot Integration in Enterprises",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31108",
            "idioma": "Inglês",
            "storage_key": "files/article_31108_30911.pdf",
            "autores": [
                {
                    "nome": "Caio Siqueira",
                    "afiliacao": "PUC-Rio",
                    "orcid": "http://orcid.org/0009-0009-2808-3278"
                },
                {
                    "nome": "Orlando Fonseca",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0000-0002-0205-2483"
                },
                {
                    "nome": "Giuliano Ferreira",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0009-0004-7824-0191"
                },
                {
                    "nome": "Omar Leiva",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0000-0001-6067-8372"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "This paper introduces an approach for integrating structured data into chatbot applications. Utilizing our Mindmap tool, which hierarchically organizes data and maps nodes to actions, we developed an augmented JSONschema to improve chatbot contextual understanding and response accuracy. Byapplying the Langchain suite and Retrieval-Augmented Generation techniques,our method enhances data retrieval and processing from a vector store, signifi-cantly improving interaction relevance.",
            "keywords": [
                "Chatbot Integration",
                "Structured Data",
                "RAG",
                "Langchain"
            ],
            "referencias": [
                "LangChain Documentation. Langchain: Building applications with llms through composability. n.d.",
                "Lewis, P., Oguz, B., Rinott, R., Riedel, S., and Stenetorp, P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.",
                "OpenAI (2023). Gpt-4 technical report. Technical report, OpenAI.",
                "Vercel, Inc. (2024). React Foundations: About React and Next.js. Next.js Documentation.",
                "Weiying, K., Pham, D. N., Eftekharypour, Y., and Pheng, A. J. (2019). Benchmarking nlp toolkits for enterprise application. In PRICAI 2019: Trends in Artificial Intelligence: 16th Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yanuca Island, Fiji, August 26-30, 2019, Proceedings, Part III 16, pages 289–294. Springer.",
                "Wen, Y., Wang, Z., and Sun, J. (2023). Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. arXiv preprint arXiv:2308.09729."
            ],
            "artigo_completo": "Leveraging Structured Data Input for Effective Chatbot\nIntegration in Enterprises\n\nCaio Siqueira1, Orlando Guilarte1, Giuliano Ferreira1, Omar Leiva1\n\n1Diretoria de Sistemas de Informac¸ ˜ao - PUC-Rio\nRua Marquˆes de S˜ao Vicente 225 – 22451-900 - G´avea - Rio de Janeiro, Brasil\n\n{csiqueira,ofonsek,giuliano.biblioteca}@puc-rio.br\n\nomarleivac@aluno.puc-rio.br\n\nAbstract. This paper introduces an approach for integrating structured data\ninto chatbot applications. Utilizing our Mindmap tool, which hierarchically\norganizes data and maps nodes to actions, we developed an augmented JSON\nschema to improve chatbot contextual understanding and response accuracy. By\napplying the Langchain suite and Retrieval-Augmented Generation techniques,\nour method enhances data retrieval and processing from a vector store, signifi-\ncantly improving interaction relevance.\nKeywords: Chatbot Integration; Structured Data; RAG; Langchain\n\n1. Introduction\n\nIn recent years, the evolution of consumer-facing software has led to heightened expec-\ntations among corporate users for more intuitive and natural interactions with computer\nsystems. Traditionally, user interfaces in corporate environments have relied on window-\nbased interaction paradigms. However, the advent of sophisticated natural language\nprocessing (NLP) technologies has begun to shift this paradigm, making natural lan-\nguage interfaces increasingly desirable for enterprise applications [Weiying et al. 2019].\nA significant milestone in this transition was the public release of ChatGPT by OpenAI\n[OpenAI 2023]. The underlying technology behind ChatGPT is based on a Large Lan-\nguage Model (LLM), a type of machine learning model trained on extensive datasets con-\ntaining diverse textual content. The scale of these training datasets allows LLMs to effec-\ntively respond to a wide range of user inputs. However, deploying LLMs in corporate en-\nvironments presents unique challenges. A key limitation is that these models are typically\nnot trained on proprietary data from private institutions, which must remain confidential\nto protect organizational integrity and data privacy. To address this issue, researchers\nhave explored the use of knowledge graphs to further enhance LLMs [Wen et al. 2023],\nand techniques such as Retrieval-Augmented Generation (RAG) have been developed to\nprovide LLMs with context derived from private datasets, thereby enhancing their rele-\nvance and accuracy in enterprise settings [Lewis et al. 2020]. Despite the effectiveness\nof these techniques, they often fall short of meeting the demands of corporate software\ndevelopment, where frequent updates and rapid responses to organizational changes are\ncommon. These approaches also face challenges such as difficulty in incorporating new\nknowledge and explaining their reasoning processes [Wen et al. 2023].\n\nThis study proposes a mechanism for structuring data more effectively to feed\nLLM-based chatbots with the contextual information necessary to provide accurate and\n\n\fcontextually relevant responses within corporate environments. Given the stringent re-\nquirements of corporate settings, where control over generated outputs is paramount, the\napproach outlined in this work prioritizes supervised interaction models over autonomous\nagent-based systems. Unlike general public applications, where disclaimers can mitigate\nthe risks of inaccurate or flawed outputs, corporate environments require a higher level of\noversight to prevent potential adverse outcomes.\n\n2. Conception\n\nWith the growing demand from our key clients, particularly those forming the consumer\nbase of our business model, for a more integrated and sophisticated LLM chatbot inter-\nface, we developed an approach that leverages existing structured chatbot systems and the\ndata already mapped within these systems.\n\nThe structured chatbot system organized data into a hierarchical, tree-like struc-\nture, where each parent node represented a topic, and the child nodes indicated possible\nresponses or subtopics associated with that topic. This setup ensured a well-defined and\nnavigable dialogue structure. With the goal of creating an LLM-based chatbot integrated\nwith RAG — which employs a vector store for semantic retrieval — we developed a tool\ncapable of exporting this structured data for integration into the new system.\n\nHowever, we identified significant limitations in our current software, which\nonly supported data generation in RTF (Rich Text Format), a format unsuitable for our\nneeds. To overcome this challenge, we developed a new tool to manage the registra-\ntion of nodes and their associated child nodes. This led to the creation of a web-based\ninterface (Mindmap), built using advanced JavaScript frameworks (React with Next.js\n[Vercel, Inc. 2024]).\n\nThe introduction of this tool significantly expanded the scope of the project, al-\nlowing us to move beyond simple topic and child text nodes, enabling the creation of more\ncomplex data structures tailored to specific objectives. One of the critical features devel-\noped was the mapping of nodes to actions, which dictate the operations to be executed by\nthe chatbot. The text registered within each node is subsequently parsed as parameters for\nthe corresponding action.\n\nWith the development of the Mindmap tool, we now have a robust platform that al-\nlows for the systematic registration of necessary data and facilitates its export into formats\nmore suitable for subsequent processing.\n\n(a) Screenshot of the Mindmap tool\n\n(b) Snippet of the augmented json\n\nFigure 1. Generated augmented json\n\n\f3. Proposed pipeline\n\nThe Mindmap tool exports hierarchical data as a structured JSON file, which is pro-\ncessed into an augmented JSON using Python scripts within the Langchain suite\n[LangChain Documentation ], currently associated with the GPT-4 model from OpenAI.\nThis augmented file incorporates questions and answers, derived from the hierarchy of\neach node, its actions, and related metadata.\n\nThese questions and answers are generated based on the node’s actions and prede-\nfined profiles. The resulting JSON is then used to populate a vector store database, serving\nas the foundation for the chatbot’s RAG functionalities. The design is agnostic concern-\ning the chatbot engine’s method of consuming this data to construct its vector store or run\nthe inference itself. For example, a collaborating research group utilizes the vector store\nwithout incorporating questions and answers, using them later to validate the generated\ndataset. In contrast, internally, we employ the questions and answers to create documents\ndirectly in the vector store. A comparative study on the efficacy of these strategies for\nvarious use cases could be the focus of future research.\n\nTable 1. Sample of node actions\n\nAction description\n\nUse a vector store for\ndocument\n\nAction name\nFetch data on a website The node contains a URL and a CSS selector to fetch\ndata. Additionally, it contains a JUDGE text to check\nif the data is similar to what is expected.\nGiven a document link, build a vector store specific\nto that document. Once an initial query matches that\nthe answer should come from the document, a second\nLLM query is used on that specific vector store.\nTranslate the user input into an API call. Translate the\nreturn into readable output to serve for the user. The\nnode has instructions on how this should be done.\nUse the node text as context to provide an answer to\nthe user input.\n\nServe node text\n\nFetch an API\n\n4. Data structure\n\nThe output augmented JSON schema encapsulates several key elements, including the\nlist of actions within the exported object, the profiles used for generating questions and\nanswers, and the hierarchical structure of nodes and their children. Each node is assigned\na unique identifier and an update timestamp, generated by our Mindmap application, to\nfacilitate efficient updates in downstream consumer applications.\n\nTo enhance the functionality of each node, we introduced a supplementary struc-\nture termed ”helper.” Each action within a node is associated with a helper, which con-\nsists of parameters parsed during the generation of the augmented JSON. Internally, our\nteam loads these helpers into the vector database, enabling their retrieval at runtime via\nLangchain and Python.\n\nLooking forward, a significant improvement on our roadmap involves integrat-\ning the Mindmap frontend tool with the Python backend responsible for generating the\n\n\faugmented JSON structure. We also plan to migrate the generated content, including\nquestions and answers, into our relational database alongside the nodes. This integration\nwill streamline the workflow, allowing for direct export of the augmented JSON from the\nMindmap application, thereby enhancing the user experience and operational efficiency.\n\nTable 2. Sample profiles used when generating questions and answers\n\nQuestion Profile\nComputer science stu-\ndent\n\nLanguage student\n\nInternet user\n\nInstitutional chatbot\n\nProfile description\nYou are a computer science\nstudent who focuses your in-\nput using direct messages\nYou are a language student\nwith rich vocabulary\nYou are an unknown inter-\nnet user with poor gram-\nmar which basically uses key-\nwords when interacting with\nsystems\nYou are an organization chat-\nbot, which needs to answer in\na formal way never betraying\nthe ideals of the organization\n\nUsed in\nQuestions\n\nQuestions\n\nQuestions\n\nAnswers\n\n5. Conclusion\n\nThe work we present proposes an innovative and cohesive approach to integrating the\nentire workflow in creating a chatbot application that is closely coupled with its under-\nlying data. By leveraging structured data input and the advanced functionalities of our\nMindmap tool, we have established a solid foundation that not only supports the contin-\nuous generation of enhanced JSON structures but also facilitates real-time data retrieval\nand processing through advanced technologies such as Langchain and Python.\n\nThe management of the Mindmap software is designed to be in the hands of the\nprocess owners, ensuring that those who understand the intricacies of each process are\ndirectly involved in its configuration and oversight. For example, one of our chatbot\ninstances is currently managed by our Process Management Office (EP) 1.\n\nOur collaboration with the partner research group, the Applied Computational\nIntelligence Laboratory (ICA) 2, has proven invaluable, providing critical insights and\nvaluable feedback for the continuous improvement of our tool.\n\nLooking ahead, we are confident that the continued development and refinement\nof this tool will further enhance our ability to integrate complex data structures with chat-\nbot applications, ultimately contributing to more intelligent and responsive systems across\nthe organization. This work not only demonstrates the potential of structured data integra-\ntion but also lays the groundwork for future innovations in the field of enterprise chatbot\nsolutions.\n\n1Escrit´orio de Processos - https://ep.dsi.puc-rio.br\n2Laborat´orio de Inteligˆencia Computacional Aplicada - https://ica.ele.puc-rio.br\n\n\fReferences\n\nLangChain Documentation. Langchain: Building applications with llms through com-\n\nposability. n.d.\n\nLewis, P., Oguz, B., Rinott, R., Riedel, S., and Stenetorp, P. (2020). Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks. In Proceedings of the 34th Conference\non Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n\nOpenAI (2023). Gpt-4 technical report. Technical report, OpenAI.\n\nVercel, Inc. (2024). React Foundations: About React and Next.js. Next.js Documentation.\n\nWeiying, K., Pham, D. N., Eftekharypour, Y., and Pheng, A. J. (2019). Benchmarking nlp\ntoolkits for enterprise application. In PRICAI 2019: Trends in Artificial Intelligence:\n16th Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yanuca Is-\nland, Fiji, August 26-30, 2019, Proceedings, Part III 16, pages 289–294. Springer.\n\nWen, Y., Wang, Z., and Sun, J. (2023). Mindmap: Knowledge graph prompting sparks\n\ngraph of thoughts in large language models. arXiv preprint arXiv:2308.09729.\n\n\f"
        },
        {
            "titulo": "Avaliação de modelos para detecção de ataques de replay usando diferentes bases de dados",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31109",
            "idioma": "Português",
            "storage_key": "files/article_31109_30912.pdf",
            "autores": [
                {
                    "nome": "Giovana Y. Nakashima",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Higor D. C. Santos",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Jone W. M. Soares",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Mário Uliani Neto",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Fernando O. Runstein",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Ricardo P. V. Violato",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Marcus Lima",
                    "afiliacao": "PUC-Campinas",
                    "orcid": "https://orcid.org/0009-0008-7254-285X"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Ataque de replay é uma falsificação de fala utilizada na tentativa de autenticação de locutor. Redes neurais profundas têm sido propostas como métodos para detecção de áudios fraudulentos. Tendo em vista a utilização desses modelos em aplicações reais, além de bom desempenho na aprendizagem, espera-se que o modelo obtido apresente bons resultados com bases de dados distintas da utilizada no treinamento. Neste trabalho, duas abordagens foram avaliadas com três bases de dados públicas, com resultados que indicam baixa capacidade de generalização dos modelos.",
            "keywords": [
                "biometria de voz",
                "ataques de replay",
                "anti-spoofing"
            ],
            "referencias": [
                "Chettri, B., Mishra, S., Sturm, B. L., and Benetos, E. (2018). A study on convolutional neural network based end-to-end replay anti-spoofing.",
                "Lee, S.-K., Tsao, Y., and Wang, H.-M. (2022). Detecting replay attacks using single-channel audio: The temporal autocorrelation of speech. In Proceedings of 2022 APSIPA Annual Summit and Conference. 2022 APSIPA Annual Summit and Conference."
            ],
            "artigo_completo": "Avaliac¸ ˜ao de modelos para detecc¸ ˜ao de ataques de replay\nusando diferentes bases de dados\n\nGiovana Y. Nakashima1, Higor D. C. Santos1, Jone W. M. Soares1,\nM´ario Uliani Neto1, Fernando O. Runstein1, Ricardo P. V. Violato1, Marcus Lima2\n\n1CPQD - Centro de Pesquisa e Desenvolvimento, Campinas, SP, Brasil\n\n2Pontif´ıcia Universidade Cat´olica de Campinas, SP, Brasil\n\n{giovana.nakashima, higorcea, jonewisney, marcuslima3}@gmail.com\n{uliani, runstein, rviolato}@cpqd.com.br\n\nAbstract. A replay attack is a speech forgery used in an attempt to authenticate\na speaker. Deep neural networks have been proposed as methods for detecting\nfraudulent audio. In view of the use of these models in real applications, in ad-\ndition to good learning performance it is expected that the models show good\nresults with databases other than the one used for training. In this work two ap-\nproaches were evaluated with three public databases, with results that indicate\nlow generalization capacity of the models.\n\nResumo. Ataque de replay ´e uma falsificac¸ ˜ao de fala utilizada na tentativa de\nautenticac¸ ˜ao de locutor. Redes neurais profundas tˆem sido propostas como\nm´etodos para detecc¸ ˜ao de ´audios fraudulentos. Tendo em vista a utilizac¸ ˜ao\ndesses modelos em aplicac¸ ˜oes reais, al´em de bom desempenho na aprendiza-\ngem, espera-se que o modelo obtido apresente bons resultados com bases de\ndados distintas da utilizada no treinamento. Neste trabalho, duas abordagens\nforam avaliadas com trˆes bases de dados p´ublicas, com resultados que indicam\nbaixa capacidade de generalizac¸ ˜ao dos modelos.\n\n1. Introduc¸ ˜ao\n\nSistemas de biometria de voz est˜ao sendo amplamente utilizados nos mais diversos seto-\nres, como ind´ustria automotiva, financeiro, sa´ude e educac¸ ˜ao [Khan et al. 2023]. Nesses\ncasos, a autenticac¸ ˜ao do usu´ario ´e realizada por sistemas de verificac¸ ˜ao autom´atica de\nlocutor (Automated Speaker Verification – ASV), suscet´ıveis a ataques de falsificac¸ ˜ao\n(spoofing).\n\nAtaque de replay consiste na apresentac¸ ˜ao a um ASV da reproduc¸ ˜ao de um ´audio\npreviamente gravado, com o objetivo de validar a fala do locutor como genu´ına. Esse\ntipo de falsificac¸ ˜ao ocorre de forma passiva e ´e dif´ıcil de ser detectado, uma vez que o\nsinal reproduzido apresenta semelhanc¸as f´ısicas (frequˆencias, espectros, formas de on-\ndas) ao original [Khan et al. 2023]. Para aumentar a confiabilidade, os sistemas ASV\ns˜ao combinados com sistemas que identificam a fala falsificada, tamb´em chamados de\nantispoofing [Alzantot et al. 2019].\n\nA s´erie de competic¸ ˜oes Automatic Speaker Verification Spoofing and Counterme-\nasures Challenge (ASVspoof) promove, desde 2015, o desenvolvimento de m´etodos para\n\n\fdetecc¸ ˜ao de falsificac¸ ˜ao. A cada edic¸ ˜ao, uma base de dados ´e disponibilizada para ser uti-\nlizada no treinamento e na validac¸ ˜ao de contramedidas aos ataques [Alzantot et al. 2019,\nLee et al. 2022, Nautsch et al. 2021].\n\nDe forma geral, os m´etodos iniciam com a extrac¸ ˜ao de atributos (features), predo-\nminantemente baseados na an´alise espectral do ´audio, como espectrogramas, cepstrogra-\nmas, coeficientes em escala mel e variac¸ ˜oes da an´alise de Fourier [Khan et al. 2023]. O\nprocesso de aprendizado ocorre utilizando-se esses atributos como entradas para um clas-\nsificador, tipicamente uma rede neural. Frequentemente tem sido utilizadas redes con-\nvolucionais [Chettri et al. 2018, Korshunov et al. 2018] (Convolutional Neural Network\n- CNN) e suas variac¸ ˜oes, como a rede convolucional leve (Light Convolutional Neural\nNetwork - LCNN) [Lavrentyeva et al. 2017, Lavrentyeva et al. 2019] e a rede convolucio-\nnal residual (Residual Neural Network - ResNet) [Alzantot et al. 2019, Zhang et al. 2021].\n\nUsualmente, para utilizac¸ ˜ao em uma aplicac¸ ˜ao real, ´e esperado que o modelo apre-\nsente uma boa capacidade de generalizac¸ ˜ao [Korshunov and Marcel 2016], isto ´e, que seu\ndesempenho n˜ao seja muito diferente quando comparados os resultados obtidos em dados\ndistintos dos usados no treinamento.\n\nO objetivo deste trabalho ´e estudar o desempenho de abordagens propostas `a\ndetecc¸ ˜ao de ataque de replay entre bases de dados distintas da utilizada no aprendizado\ndas redes, de modo a contribuir para a discuss˜ao sobre generalizac¸ ˜ao dos modelos.\n\n2. Metodologia\n\nNeste trabalho, duas abordagens de classificac¸ ˜ao foram avaliadas com trˆes bases de dados\np´ublicas. O desempenho dos m´etodos foi mensurado pelo EER (Equal Error Rate), ponto\nde operac¸ ˜ao em que a taxa de falsa aceitac¸ ˜ao (False Acceptance Rate - FAR) e a taxa de\nfalsa rejeic¸ ˜ao (False Rejection Rate - FRR) s˜ao iguais [Jain et al. 2008].\n\n2.1. Bases de Dados\n\nO estudo foi realizado com trˆes bases de dados p´ublicas: ASVspoof 20191, ASVspoof\n20212 e REMASC3. As trˆes bases foram gravadas em inglˆes e, portanto, a influˆencia da\nvariac¸ ˜ao de idioma n˜ao pode ser explorada nesse caso. O treinamento de todos os modelos\nneste trabalho utilizou o conjunto de treinamento da base de dados ASVspoof 2019.\n\nAs bases ASVspoof 2019 e ASVspoof 2021 s˜ao compostas por arquivos de ´audio\ndo tipo flac, com um canal e taxa de amostragem de 16kHz. Ambas possuem outros tipos\nde ataque al´em do ataque de replay, mas, para os experimentos deste trabalho, foram\nutilizados apenas os conjuntos relativos ao ataque denominado de acesso f´ısico (Physical\nAccess - PA), pois s˜ao os dados com o ataque de replay. Esses conjuntos s˜ao formados\npor 218.430 e 943.110 amostras, respectivamente.\n\nA base de dados REMASC foi concebida visando sistemas controlados por voz\n(Voice Controlled Systems - VCS), em que a coleta do ´audio ocorre a uma distˆancia maior\ndo locutor. Seu conjunto abrange 54.712 amostras, armazenadas em arquivos do tipo\nwav, multicanais, amostrados a 16kHz e 44kHz [Gong et al. 2019]. Os ´audios foram\n\n1https://datashare.ed.ac.uk/handle/10283/3336\n2https://www.asvspoof.org/index2021.html\n3github.com/ndmobilecomplab/replay-attack\n\n\fpadronizados em monocanais a 16kHz, aplicando a m´edia dos canais e reduc¸ ˜ao da taxa\nde amostragem (downsampling) quando necess´ario.\n\nAs trˆes bases de dados disponibilizam arquivos de protocolo, indicando quais da-\ndos devem ser usados para treinamento e teste, bem como a identificac¸ ˜ao do locutor e do\n´audio e sua classificac¸ ˜ao original (genu´ıno ou falso). Al´em disso, fornecem metadados\ncomo tamanho do ambiente e caracter´ısticas dos dispositivos utilizados para coleta.\n\n2.2. Modelos\n\nNeste trabalho, foram avaliadas dois tipos de arquitetura de redes neurais, ResNet e\nLCNN. A ResNet avaliada usa como atributos de entrada a magnitude do espectro em\nescala logar´ıtmica (Log-magnitude STFT - Short-Time Fourier Transform). Foi utilizado\num modelo pr´e-treinado disponibilizado publicamente 4.\n\nQuanto `a rede LCNN, foi utilizada uma implementac¸ ˜ao disponibilizada publi-\ncamente 5 e usada em um estudo que avaliou diversos atributos como entrada para a\nrede [Lee et al. 2022, Lee 2024]. Neste caso, como n˜ao h´a modelo pr´e-treinado dis-\npon´ıvel, o treinamento foi executado utilizando os seguintes atributos: an´alise discreta\narbitr´aria de Fourier (arbitrary discrete Fourier analysis - ADFA), cepstrogramas (CEPS\ne CEPS1724), constant Q analysis (CQA), transformada discreta de cosseno (discrete\ncosine transform - DCT), an´alise discreta de Fourier em escala Mel (Mel-scale discrete\nFourier analysis - MDFA) e espectrogramas (Spec e Spec1724). Os espectrogramas e\ncepstrogramas foram extra´ıdos pela transformada r´apida de Fourier (Fast Fourier Trans-\nform - FFT) utilizando uma janela de Blackman com comprimento 1024 (Spec e Ceps) e\n1724 (Spec1724 e Ceps1724).\n\nAinda, como referˆencia para comparac¸ ˜ao, foi usada a abordagem LFCC-LCNN\ndisponibilizada como baseline para o desafio ASVspoof 2021 [Liu et al. 2023], que inclui\num modelo pr´e-treinado.\n\n3. Resultados e Discuss˜ao\n\nOs treinamentos da abordagem RD-LCNN foram realizados ao longo de cem ´epocas e\no foi escolhido o modelo obtido a partir da ´epoca com menor valor EER no conjunto de\ndesenvolvimento da base ASVspoof 2019.\n\nA Tabela 1 apresenta os resultados de todos os modelos avaliados nas trˆes bases\nde dados, ASVspoof 2019, ASVspoof 2021 e REMASC. As colunas (a), mostram como\nreferˆencia os resultados relatados na literatura para os subconjuntos de desenvolvimento\n(Dev) e de avaliac¸ ˜ao (Eval) da base ASVspoof 2019 [Nautsch et al. 2021] e, nas demais\ncolunas, os resultados obtidos nos experimentos deste trabalho.\n\nO modelo pr´e-treinado da baseline LFCC-LCNN apresentou bom desempenho\ncom o subconjunto eval da base de dados ASVspoof 2019, com EER = 2,43%, fato es-\nperado, uma vez que o treinamento ocorreu com o subconjunto train dessa mesma base\nde dados. ´E importante observar que a mesma superou as baselines propostas em 2019,\nCQCC-GMM e LFCC-GMM, que apresentaram EER = 11,04% e EER = 13,54%, res-\npectivamente [Nautsch et al. 2021]. J´a com a base ASVspoof 2021, o desempenho (EER\n\n4https://github.com/nesl/asvspoof2019\n5https://github.com/shihkuanglee/RD-LCNN/tree/main\n\n\fTabela 1. Resultados do ERR (%) obtidos (ASVspoof 2019 (b), ASVspoof 2021 e\nREMASC) e reportados pela literatura (ASVspoof 2019 (a))\n\n= 45,67%) foi similar ao relatado pela literatura (EER = 44,77%) [Liu et al. 2023] e, por-\ntanto, muito pior.\n\nA abordagem ResNet para a base de dados ASVspoof 2019 do subconjunto eval\napresentou resultado (EER = 7,07%) pr´oximo ao da literatura (EER = 3,81%) e, embora\n86% maior, ainda foi melhor que os das baselines do desafio de 2019: EER = 11,04%\n(B01 - CQCC-GMM) e EER = 13,54% (B02 - LFCC-GMM) [Nautsch et al. 2021]. Para\nas bases ASVspoof 2021 e REMASC observa-se um baixo desempenho, com valores\nEER acima de 40%.\n\nOs modelos treinados da abordagem RD-LCNN apresentaram resultados para o\nsubconjunto dev do ASVspoof 2019 pr´oximos aos relatados na literatura. Para o sub-\nconjunto eval da base ASVspoof 2019, o atributo “DCT” expressou a maior diferenc¸a. A\ninferˆencia nas bases de dados ASVspoof 2021 e REMASC tamb´em resultaram em valores\nEER muito piores, maiores que 35%.\n\n4. Conclus˜ao\n\nOs resultados obtidos para a base de dados ASVspoof 2019 demonstram bom desempenho\nda baseline e das abordagens experimentadas, semelhantes aos relatados pela literatura.\nObserva-se que a baseline obteve ERR = 2,43% no ASVspoof 2019 - eval, e, embora\nn˜ao se tenha encontrado valor na literatura para efeito de comparac¸ ˜ao, esse resultado foi\nmelhor que os de ambas as baselines do desafio de 2019.\n\nOs altos valores EER obtidos com as bases de dados ASVspoof 2021 e REMASC\nratificam a situac¸ ˜ao exposta por [Korshunov and Marcel 2016], apontando para uma baixa\ncapacidade de generalizac¸ ˜ao de todas as abordagens processadas.\n\nAgradecimentos\n\nEste projeto foi apoiado pelo Minist´erio da Ciˆencia, Tecnologia e Inovac¸ ˜oes, com recursos\nda Lei no 8.248, de 23 de outubro de 1991, no ˆambito do PPI-SOFTEX, coordenado pela\nSoftex e publicado PDI 03, DOU 01245.023862/2022-14.\n\n\fReferˆencias\n\nAlzantot, M., Wang, Z., and Srivastava, M. B. (2019). Deep residual neural networks for\naudio spoofing detection. Proceedings of the Annual Conference of the International\nSpeech Communication Association, INTERSPEECH, 2019-September:1078–1082.\n\nChettri, B., Mishra, S., Sturm, B. L., and Benetos, E. (2018). A study on convolutional\n\nneural network based end-to-end replay anti-spoofing. arXiv.\n\nGong, Y., Yang, J., Huber, J., MacKnight, M., and Poellabauer, C. (2019). Remasc: Rea-\nlistic replay attack corpus for voice controlled systems. In Proceedings of the Annual\nConference of the International Speech Communication Association, INTERSPEECH,\nvolume 2019-September, pages 2355–2359. International Speech Communication As-\nsociation.\n\nJain, A. K., Flynn, P., and Ross, A. A. (2008). Handbook of Biometrics. Springer.\n\nKhan, A., Malik, K. M., Ryan, J., and Saravanan, M. (2023). Battling voice spoofing: a\nreview, comparative analysis, and generalizability evaluation of state-of-the-art voice\nspoofing counter measures. Artificial Intelligence Review, 56:513–566. 01.\n\nKorshunov, P., Gonc¸alves, A. R., Violato, R. P. V., Sim˜oes, F. O., and Marcel, S. (2018).\nOn the use of convolutional neural networks for speech presentation attack detection.\nIn IEEE, editor, 2018 IEEE 4th international conference on identity, security, and\nbehavior analysis (ISBA), pages 1–8.\n\nKorshunov, P. and Marcel, S. (2016). Cross-database evaluation of audio-based spoo-\nfing detection systems. In Proceedings of the Annual Conference of the International\nSpeech Communication Association, INTERSPEECH, volume 08-12-September-2016,\npages 1705–1709. International Speech and Communication Association.\n\nLavrentyeva, G., Novoselov, S., Malykh, E., Kozlov, A., Kudashev, O., and Shcheme-\nlinin, V. (2017). Audio replay attack detection with deep learning frameworks.\nIn\nProceedings of the Annual Conference of the International Speech Communication As-\nsociation, INTERSPEECH, volume 2017-August, pages 82–86. International Speech\nCommunication Association.\n\nLavrentyeva, G., Novoselov, S., Tseren, A., Volkova, M., Gorlanov, A., and Kozlov, A.\n\n(2019). Stc antispoofing systems for the asvspoof2019 challenge. arXiv.\n\nLee, S.-K. (2024). Arbitrary discrete fourier analysis and its application in replayed spe-\n\nech detection. arXiv.\n\nLee, S.-K., Tsao, Y., and Wang, H.-M. (2022). Detecting replay attacks using single-\nchannel audio: The temporal autocorrelation of speech. In Proceedings of 2022 AP-\nSIPA Annual Summit and Conference. 2022 APSIPA Annual Summit and Conference.\n\nLiu, X., Wang, X., Sahidullah, M., Patino, J., Delgado, H., Kinnunen, T., Todisco, M.,\nYamagishi, J., Evans, N., Nautsch, A., and Lee, K. A. (2023). Asvspoof 2021: Towards\nspoofed and deepfake speech detection in the wild. IEEE/ACM Transactions on Audio\nSpeech and Language Processing, 31:2507–2522.\n\nNautsch, A., Wang, X., Evans, N., Kinnunen, T., Vestman, V., Todisco, M., Delgado, H.,\nSahidullah, M., Yamagishi, J., and Lee, K. A. (2021). Asvspoof 2019: spoofing coun-\ntermeasures for the detection of synthesized, converted and replayed speech. arXiv.\n\n\fZhang, Z., Yi, X., and Zhao, X. (2021). Fake speech detection using residual network\nwith transformer encoder. In IH and MMSec 2021 - Proceedings of the 2021 ACM\nWorkshop on Information Hiding and Multimedia Security, pages 13–22. Association\nfor Computing Machinery, Inc.\n\n\f"
        },
        {
            "titulo": "Avaliação de arquiteturas de síntese de fala generativa com abordagens de espectrograma e fim-a-fim em cenários low-resource para clonagem de voz",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31110",
            "idioma": "Português",
            "storage_key": "files/article_31110_30913.pdf",
            "autores": [
                {
                    "nome": "Bruno C. dos S. Ribeiro",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Gustavo H. dos S. Figueiredo",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Leonardo H. da S. Correia",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Mário Uliani Neto",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Fernando O. Runstein",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Ricardo P. V. Violato",
                    "afiliacao": "CPQD",
                    "orcid": null
                },
                {
                    "nome": "Marcus Lima",
                    "afiliacao": "PUC-Campinas",
                    "orcid": null
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "síntese de fala",
                "clonagem de voz",
                "low-resource"
            ],
            "referencias": [
                "Casanova, E., Junior, A. C., Shulby, C., Oliveira, F. S. d., Teixeira, J. P., Ponti, M. A., and Aluísio, S. (2022). Tts-portuguese corpus: a corpus for speech synthesis in brazilian portuguese. Language Resources and Evaluation, 56(3):1043–1055.",
                "Kong, J., Kim, J., and Bae, J. (2020). Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis."
            ],
            "artigo_completo": "Avaliac¸ ˜ao de arquiteturas de s´ıntese de fala generativa com\nabordagens de espectrograma e fim-a-fim em cen´arios\nlow-resource para clonagem de voz\n\nBruno C. dos S. Ribeiro1, Gustavo H. dos S. Figueiredo1,\nLeonardo H. da S. Correia1, M´ario Uliani Neto1, Fernando O. Runstein1,\nRicardo P. V. Violato1, Marcus Lima2,\n\n1CPQD - Centro de Pesquisa e Desenvolvimento, Campinas, SP, Brasil\n\n2Pontif´ıcia Universidade Cat´olica de Campinas, SP, Brasil\n\nResumo. O artigo compara modelos de s´ıntese de fala com arquiteturas base-\nadas em espectrograma e fim-a-fim, com o objetivo de determinar a capacidade\nde clonagem de voz em cen´ario low-resource. Foram avaliados conjuntos de\ntreinamento de adaptac¸ ˜ao com diferentes quantidades de fala para clonagem\nde uma voz alvo, e o tempo necess´ario para realizar o treinamento. O modelo\nVITS mostrou-se mais eficiente, alcanc¸ando os melhores resultados no teste de\nqualidade perceptual no cen´ario low-resource com dados no idioma portuguˆes,\ne completou o treinamento em menos tempo, quando comparado com o Taco-\ntron2.\n\n1. Introduc¸ ˜ao\n\nA s´ıntese de fala tem sido um campo de intenso estudo e inovac¸ ˜ao ao longo dos ´ultimos\nanos, com avanc¸os significativos impulsionados pelos r´apidos progressos na ´area de inte-\nligˆencia artificial generativa. Dentro deste contexto, diversas abordagens tˆem sido explo-\nradas, incluindo as arquiteturas baseadas em espectrogramas e as abordagens fim-a-fim.\n\nAs arquiteturas Tacotron [Wang et al. 2017] e Tacotron2 [Shen et al. 2018] tˆem\nsido amplamente estudadas e aplicadas, demonstrando a capacidade de converter texto\nem fala natural por meio da gerac¸ ˜ao de espectrogramas intermedi´arios, que s˜ao posteri-\normente transformados em sinais de fala atrav´es de vocoders, como as arquiteturas Wa-\nveNet [van den Oord et al. 2016] e HiFi-GAN [Kong et al. 2020]. Apesar dos resultados\npromissores, esses modelos frequentemente requerem grandes quantidades de dados e\nlongos per´ıodos de treinamento para atingir um n´ıvel satisfat´orio de qualidade e naturali-\ndade na fala.\n\nAs abordagens mais recentes de s´ıntese de fala, como o modelo VITS (do inglˆes,\nVariational Inference Text-to-Speech) [Kim et al. 2021], prop˜oem uma estrat´egia fim-a-\nfim que elimina a necessidade de um est´agio intermedi´ario expl´ıcito de gerac¸ ˜ao do es-\npectrograma, combinando de forma eficaz a gerac¸ ˜ao e a codificac¸ ˜ao do sinal de fala em\num ´unico fluxo de trabalho. Este m´etodo tem mostrado potencial em reduzir significati-\nvamente a quantidade de dados necess´arios para o treinamento, bem como o tempo total\npara alcanc¸ar resultados de alta qualidade.\n\nA eficiˆencia da s´ıntese de fala em cen´arios com recursos limitados (low-resource)\n´e uma ´area de interesse crescente, especialmente para idiomas com menor disponibili-\ndade de dados anotados, como o caso do portuguˆes. Trabalhos recentes tˆem investigado a\n\n\fefic´acia de diferentes modelos em condic¸ ˜oes low-resource, abordando desafios espec´ıficos\ncomo a qualidade da fala sintetizada, a adaptabilidade de modelos pr´e-treinados para no-\nvos falantes e a eficiˆencia computacional do processo de treinamento [Lux et al. 2022].\n\nO objetivo deste artigo ´e comparar as arquiteturas baseadas em espectrogramas e\nfim-a-fim no contexto de clonagem de voz em portuguˆes, com ˆenfase no desempenho do\nVITS versus o Tacotron2. O objetivo ´e comparar os modelos em cen´arios low-resource\ne quantificar o n´umero m´ınimo de dados e tempo de treinamento necess´arios para atingir\nresultados de alta qualidade. Os resultados baseiam-se em m´etricas de qualidade objetiva\ne subjetiva, e na an´alise do tempo de treinamento. Esperamos fornecer insights pr´aticos\npara a escolha e implementac¸ ˜ao de modelos de s´ıntese de fala com voz personalizada em\ncondic¸ ˜oes de dados restritos, contribuindo para a eficiˆencia e a acessibilidade da tecno-\nlogia de s´ıntese de fala em uma ampla gama de aplicac¸ ˜oes para o idioma portuguˆes do\nBrasil.\n\n2. Metodologia\n\nO treinamento dos modelos foi realizado utilizando duas bases de fala no idioma Por-\ntuguˆes Brasileiro: (i) o TTS-Portuguese Corpus [Casanova et al. 2022], composto por\ntextos de dom´ınio p´ublico provenientes tanto da Wikip´edia quanto do Chatterbot-corpus\n(um corpus criado originalmente para a construc¸ ˜ao de chatbots), contendo aproximada-\nmente 10 horas e 28 minutos de fala de um ´unico locutor masculino, gravada com taxa\nde amostragem de 48 kHz e 16 bits, tendo 3.632 ´audios no formato WAV linear, com um\nrange de durac¸ ˜ao de 0,67 a 50,08 segundos (todos os clipes de ´audio com durac¸ ˜ao supe-\nrior a 20 segundos foram removidos do treinamento); (ii) uma base de fala propriet´aria\ndo CPQD composta por um locutor masculino contendo 20 minutos de fala, gravada com\ntaxa de amostragem de 48kHz, 16 bits e formato PCM linear, contendo os arquivos de\n´audio e as transcric¸ ˜oes ortogr´aficas correspondentes.\n\nO treinamento foi realizado a partir do reposit´orio do VITS1, que foi adaptado\npara a inclus˜ao de fonemas do idioma portuguˆes do Brasil, realizado atrav´es do uso do\nm´odulo Phonemizer2 em conjunto com a pipeline de preparac¸ ˜ao de dados.\n\nO treinamento dos modelos base ocorreram ao longo de 80 horas e 2.000 ´epocas\nno dataset TTS-Portuguese Corpus. A partir do ´ultimo checkpoint gerado pelo modelo\nbase, foram realizados fine-tunings trocando os dados de treinamento pela base pro-\npriet´aria com a voz do locutor masculino, usando conjuntos de treinamento com 20, 15,\n10 e 5 minutos de fala visando avaliar a quantidade m´ınima de dados necess´arios para\nobter s´ıntese de boa qualidade. O objetivo do fine-tuning ´e adaptar o modelo base para\nas caracter´ısticas da voz alvo, ou seja, realizar a clonagem de voz. Ap´os apenas 1 hora\nde treinamento de fine-tuning usando 20 minutos de fala, foram observados resultados\nde alta qualidade tanto no VITS como no Tacotron2. A qualidade melhorou ainda mais\nap´os 20 horas de treinamento. Ambos utilizaram o vocoder HiFi-GAN, sendo que no\ncaso do Tacotron2 o vocoder foi treinado de forma independente. Para os conjuntos de\ntreinamento menores, a sec¸ ˜ao 3 apresenta os resultados obtidos.\n\n1https://github.com/jaywalnut310/vits/\n2https://pypi.org/project/phonemizer/3.0.1/\n\n\f3. Resultados\n\nPara avaliar a qualidade da fala sintetizada resultante foram utilizadas medidas objetivas\ne subjetivas. As m´etricas objetivas foram o MCD (do inglˆes, Mel-Cepstral Distortion)\ne o F0 RMSE (do inglˆes, Log-F0 Root Mean Square Error) [Hayashi et al. 2021]. Para\na avaliac¸ ˜ao subjetiva foi utilizada a m´etrica MOS (Mean Opinion Score), em um experi-\nmento que contou com 15 avaliadores n˜ao especialistas.\n\nA m´etrica MCD, calculada por meio do reposit´orio TTS Objective Metrics3, quan-\ntifica a distˆancia entre dois sinais de fala. Quanto menor o valor MCD, mais semelhantes\ns˜ao as vozes. A qualidade da voz sintetizada foi avaliada com base no conjunto de teste,\ncom frases separadas para validac¸ ˜ao. Ao comparar a voz sintetizada resultante do modelo\nde fine-tuning obtido com 20 minutos, com a voz original gravada, obteve-se valores de\nMCD entre 1.6 e 1.78. A m´etrica MCD mostra valores pr´oximos de 0, indicando que\no modelo ´e capaz de gerar fala sintetizada pr´oxima da fala gravada. Para o F0 RMSE,\naplicada nas mesmas sentenc¸as, foram obtidos valores entre 0.18 e 0.34. Os resultados\nreforc¸am a alta qualidade da fala sintetizada.\n\n3.1. Avaliac¸ ˜ao Subjetiva\n\nPara a avaliac¸ ˜ao subjetiva foi utilizado o servidor webMUSHRA4. Um grupo de 15 avali-\nadores n˜ao especialistas ouviram um conjunto de amostras e atribuiram notas de 0 a 100\ncom base na naturalidade da voz, sendo 0 nada natural e 100 muito natural. Esse processo\npermitiu realizar uma an´alise subjetiva da qualidade do ´audio sintetizado, proporcionando\numa an´alise mais fidedigna da percepc¸ ˜ao humana em relac¸ ˜ao ao desempenho dos mode-\nlos. As avaliac¸ ˜oes mostram uma melhor qualidade do VITS em relac¸ ˜ao ao Tacotron2. A\nFigura 1 mostra o boxplot com os dados do teste subjetivo, utilizando ´audios sintetizados\npor modelos obtidos atrav´es do fine-tuning com diferentes conjuntos de treinamento da\nvoz alvo. Na legenda, 400 representa o conjunto com 20 minutos de fala, 300 indica 15\nminutos, 200 indica 10 minutos e 100 indica o conjunto com 5 minutos de fala.\n\nOs resultados indicam que o VITS (C1) consistentemente recebeu avaliac¸ ˜oes mais\naltas em comparac¸ ˜ao ao Tacotron2 (C2). O desvio padr˜ao menor do VITS em comparac¸ ˜ao\nao Tacotron2 em todos os conjuntos de treinamento indica que as opini˜oes dos usu´arios\nsobre a qualidade do ´audio gerado pelo VITS s˜ao mais consistentes e robustas.\n\nNo teste realizado com o conjunto contendo 5 minutos de fala de treinamento,\no VITS teve uma m´edia de 71,49 enquanto o Tacotron2 teve 67,49. Essa diferenc¸a foi\nconsistente em todos os conjuntos de treinamento (20, 15, 10 e 5 minutos). No entanto,\na diferenc¸a aumenta com um volume maior de dados, sugerindo que o VITS n˜ao apenas\nproduz ´audio de melhor qualidade, mas tamb´em que melhora mais conforme a quantidade\nde dados de treinamento aumenta.\n\n4. Conclus˜ao\n\nO objetivo principal deste trabalho foi comparar as arquiteturas de s´ıntese de fala ge-\nnerativa com abordagens de espectrograma (Tacatron2) e fim-a-fim (VITS) em cen´arios\n\n3https://github.com/AI-Unicamp/TTS-Objective-Metrics\n4https://github.com/audiolabs/webMUSHRA/\n\n\fFigura 1. Boxplot da distribuic¸ ˜ao das pontuac¸ ˜oes por est´ımulo. C1 representa o\nVITS, e C2 representa o Tacotron2.\n\nlow-resource, com uso de at´e 5 minutos de fala no treinamento de fine-tuning, para clo-\nnagem de voz; ou seja, avaliar a capacidade de adaptac¸ ˜ao dos modelos base pr´e-treinados\nfazendo uso de dados limitados de uma nova voz personalizada.\n\nO modelo VITS, quando treinado com 20 minutos, mostrou resultados com alta\nqualidade ap´os apenas 1 hora de treinamento. Por outro lado, o Tacotron2, sob as mesmas\ncondic¸ ˜oes, apresentou maior variabilidade e menor consistˆencia na qualidade do ´audio\nsintetizado. Mesmo quando treinado com 5 minutos o VITS apresentou boa qualidade e\nbaixa variˆancia. Ao comparar o tempo de treinamento, o modelo VITS mostrou-se mais\neficiente, alcanc¸ando bons resultados em menos tempo e com menos dados em relac¸ ˜ao ao\nTacotron2.\n\nOs resultados indicam que o VITS n˜ao s´o oferece uma s´ıntese de fala de melhor\nqualidade, com maior similaridade `a voz original e menor variˆancia entre as amostras sin-\ntetizadas, mas tamb´em ´e mais eficiente em termos de tempo de treinamento em cen´arios\nlow-resource.\n\n5. Agradecimentos\n\nEste projeto foi apoiado pelo Minist´erio da Ciˆencia, Tecnologia e Inovac¸ ˜oes, com recursos\nda Lei no 8.248, de 23 de outubro de 1991, no ˆambito do PPI-SOFTEX, coordenado pela\nSoftex e publicado PDI 03, DOU 01245.023862/2022-14.\n\n\fReferˆencias\n\nCasanova, E., Junior, A. C., Shulby, C., Oliveira, F. S. d., Teixeira, J. P., Ponti, M. A., and\nAlu´ısio, S. (2022). Tts-portuguese corpus: a corpus for speech synthesis in brazilian\nportuguese. Language Resources and Evaluation, 56(3):1043–1055.\n\nHayashi, T., Yamamoto, R., Yoshimura, T., Wu, P., Shi, J., Saeki, T., Ju, Y., Yasuda,\nY., Takamichi, S., and Watanabe, S. (2021). Espnet2-tts: Extending the edge of tts\nresearch.\n\nKim, J., Kong, J., and Son, J. (2021). Conditional variational autoencoder with adversarial\n\nlearning for end-to-end text-to-speech.\n\nKong, J., Kim, J., and Bae, J. (2020). Hifi-gan: Generative adversarial networks for\n\nefficient and high fidelity speech synthesis.\n\nLux, F., Koch, J., and Vu, N. T. (2022). Low-resource multilingual and zero-shot multis-\n\npeaker tts.\n\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y.,\nWang, Y., Skerry-Ryan, R., Saurous, R. A., Agiomyrgiannakis, Y., and Wu, Y. (2018).\nNatural tts synthesis by conditioning wavenet on mel spectrogram predictions.\n\nvan den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalch-\nbrenner, N., Senior, A., and Kavukcuoglu, K. (2016). Wavenet: A generative model\nfor raw audio.\n\nWang, Y., Skerry-Ryan, R., Stanton, D., Wu, Y., Weiss, R. J., Jaitly, N., Yang, Z., Xiao,\nY., Chen, Z., Bengio, S., Le, Q., Agiomyrgiannakis, Y., Clark, R., and Saurous, R. A.\n(2017). Tacotron: Towards end-to-end speech synthesis.\n\n\f"
        },
        {
            "titulo": "Beyond Single Models: Leveraging LLM Ensembles for Human Value Detection in Text",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31111",
            "idioma": "Inglês",
            "storage_key": "files/article_31111_30914.pdf",
            "autores": [
                {
                    "nome": "Diego Dimer Rodrigues",
                    "afiliacao": "UFRGS",
                    "orcid": "http://orcid.org/0000-0002-9544-117X"
                },
                {
                    "nome": "Mariana Recamonde-Mendoza",
                    "afiliacao": "UFRGS",
                    "orcid": "https://orcid.org/0000-0003-2800-1032"
                },
                {
                    "nome": "Viviane P. Moreira",
                    "afiliacao": "UFRGS",
                    "orcid": "https://orcid.org/0000-0003-4400-054X"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Every text may reflect its writer’s opinions, and these opinions, especially in political contexts, are often tied to specific human values that they either attain or constrain. Identifying these values can provide policymakers with deeper insights into the underlying factors that influence public discourse and decision-making. While current large language models (LLMs) have shown promise across various tasks, no single model may generalize sufficiently to excel in tasks like human value detection. In this work, we utilize data from the Human Value Detection task at CLEF 2024 and propose leveraging multiple ensembles of LLMs to enhance the identification of human values in text. Our results found that the ensemble models achieved higher F1 scores than all baseline models, suggesting that combining multiple models can offer performance comparable to very large models but at much lower memory requirements.",
            "keywords": [
                "LLM",
                "LLM Ensembles",
                "Human Value Detection",
                "Text Classification",
                "Natural Language Processing (NLP)",
                "Ensemble Learning",
                "Text Analysis",
                "Multi-Model Approaches"
            ],
            "referencias": [
                "Ammanabrolu, P., Jiang, L., Sap, M., Hajishirzi, H., and Choi, Y. (2022). Aligning to social norms and values in interactive narratives. In Carpuat, M., de Marneffe, M.-C., and Meza Ruiz, I. V., editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5994–6017, Seattle, United States. Association for Computational Linguistics.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding.",
                "He, P., Liu, X., Gao, J., and Chen, W. (2021). DEBERTA: Decodingenhanced BERT with disentangled attention. In International Conference on Learning Representations.",
                "Hoang, M., Bihorac, O. A., and Rouces, J. (2019). Aspect-based sentiment analysis using BERT. In Hartmann, M. and Plank, B., editors, Proceedings of the 22nd Nordic Conference on Computational Linguistics, pages 187–196, Turku, Finland. Linköping University Electronic Press.",
                "Jiang, D., Ren, X., and Lin, B. Y. (2023). Llm-blender: Ensembling large language models with pairwise ranking and generative fusion.",
                "Kiesel, J., Çöltekin, Ç., Heinrich, M., Fröbe, M., Alshomary, M., De Longueville, B., Erjavec, T., Handke, N., Kopp, M., Ljubešić, N., Meden, K., Mirzhakhmedova, N., Morkevičius, V., Reitis-Münstermann, T., Scharfbillig, M., Stefanovitch, N.,Wachsmuth, H., Potthast, M., and Stein, B. (2024a). Overview of touché 2024: Argumentation systems. In Goharian, N., Tonellotto, N., He, Y., Lipani, A., McDonald, G., Macdonald, C., and Ounis, I., editors, Advances in Information Retrieval, pages 466–473, Cham. Springer Nature Switzerland.",
                "Kiesel, J., Çöltekin, Ç., Heinrich, M., Fröbe, M., Alshomary, M., Longueville, B. D., Erjavec, T., Handke, N., Kopp, M., Ljubešić, N., Meden, K., Mirzakhmedova, N., Morkevičius, V., Reitis-Münstermann, T., Scharfbillig, M., Stefanovitch, N.,Wachsmuth, H., Potthast, M., and Stein, B. (2024b). Overview of Touché 2024: Argumentation Systems. In Goeuriot, L., Mulhem, P., Quénot, G., Schwab, D., Nunzio, G. M. D., Soulier, L., Galuscakova, P., Herrera, A. G. S., Faggioli, G., and Ferro, N., editors, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 15th International Conference of the CLEF Association (CLEF 2024), Lecture Notes in Computer Science, Berlin Heidelberg New York. Springer.",
                "Legkas, S., Christodoulou, C., Zidianakis, M., Koutrintzes, D., Petasis, G., and Dagioglou, M. (2024). Hierocles of alexandria at touché: Multi-task & multihead custom architecture with transformer-based models for human value detection. In Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), CEUR Workshop Proceedings,",
                ".",
                "Schwartz, S. H. (1994). Are there universal aspects in the structure and contents of human values? Journal of Social Issues, 50(4):19–45.",
                "Schwartz, S. H., Cieciuch, J., Vecchione, M., Davidov, E., Fischer, R., Beierlein, C., Ramos, A., Verkasalo, M., Lönnqvist, J.-E., Demirutku, K., Dirilen-Gumus, O., and Konty, M. (2012). Refining the theory of basic individual values. Journal of Personality and Social Psychology, 103(4):663–688.",
                "Sobhanam, H. and Prakash, J. (2023). Analysis of fine tuning the hyper parameters in RoBERTa model using genetic algorithm for text classification. International Journal of Information Technology, 15(7):3669–3677.",
                "Yeste, V., Ardanuy, M., and Rosso, P. (2024). Philo of alexandria at touché: A cascade model approach to human value detection. In Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), CEUR Workshop Proceedings,",
                "Yunis, H. (2024). Arthur schopenhauer at touché 2024: Multi-lingual text classification using ensembles of large language models. In Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), CEUR Workshop Proceedings,"
            ],
            "artigo_completo": "Beyond Single Models: Leveraging LLM Ensembles for\nHuman Value Detection in Text\n\nDiego Dimer Rodrigues, Mariana Recamonde-Mendoza, Viviane P. Moreira\n\n1Instituto de Inform´atica – (UFRGS)\n\n{ddrodrigues,mrmendoza,viviane}@inf.ufrgs.br\n\nAbstract. Every text may reflect its writer’s opinions, and these opinions, es-\npecially in political contexts, are often tied to specific human values that they\neither attain or constrain. Identifying these values can provide policymakers\nwith deeper insights into the underlying factors that influence public discourse\nand decision-making. While current large language models (LLMs) have shown\npromise across various tasks, no single model may generalize sufficiently to ex-\ncel in tasks like human value detection. In this work, we utilize data from the\nHuman Value Detection task at CLEF 2024 and propose leveraging multiple\nensembles of LLMs to enhance the identification of human values in text. Our\nresults found that the ensemble models achieved higher F1 scores than all base-\nline models, suggesting that combining multiple models can offer performance\ncomparable to very large models but at much lower memory requirements.\n\n1. Introduction\n\nPeople can agree or disagree on numerous topics even when using the same information.\nThese differences arise largely from their individual beliefs about what is worth striving\nfor, a concept referred to as (human) values. Human values can conflict or align, leading\nto a wide range of opinions on controversial issues. This divergence is one of the reasons\nfor the formation of different political parties, each representing the values of specific\ngroups [Kiesel et al. 2022].\n\nGiven its significance, the study of human values spans multiple disciplines, in-\ncluding social sciences [Schwartz 1994] and formal argumentation [Bench-Capon 2003].\nResearchers focused on various aspects, such as classifying values, detecting them in\ntext, and understanding their societal impact.\nIn computer science, there is a grow-\ning body of work dedicated to value detection and emotion recognition from text\n[Dellaert et al. 1996, Tariq et al. 2019, Ammanabrolu et al. 2022]. These tasks are chal-\nlenging and yet have a broad spectrum of applications, such as aiding policymakers in\ngauging public sentiment, detecting political alignment, and more.\n\nIn this work, we aim to advance the field of human value detection by lever-\naging multiple ensembles of Large Language Models (LLMs) to identify these values\nin text and enhance model performance. We adopt the value taxonomy presented in\n[Schwartz et al. 2012], which categorizes values into two types for each value—attained\nand constrained. However, our task focuses solely on identifying the presence of a value\nin a sentence, so we sum the attained and constrained versions to determine whether a\nsentence contains a particular value. We conduct this study with a dataset from CLEF\n2024. The data is highly imbalanced, making this a challenging classification problem.\n\n\f2. Background and Related Work\nHuman Value Detection has recently gained attention, particularly as the focus of a\nshared task at CLEF 2024. This task aimed to detect human values in speech, attracting\nparticipation from 20 teams. The outcomes of this competition, including the performance\nmetrics of each team, are detailed in [Kiesel et al. 2022]. These efforts underscore the\ncomplexity of detecting nuanced human values in text and highlight the need for advanced\nmodels that can accurately capture such subtleties.\n\nLLMs have revolutionized NLP tasks across various domains. The introduction of\nTransformer architectures [Vaswani et al. 2017] marked a significant leap forward, lead-\ning to the development of powerful pre-trained models like BERT [Devlin et al. 2019],\nRoBERTa [Liu et al. 2019], and DeBERTa [He et al. 2021]. These models have been\nhighly effective in text classification, sentiment analysis, and content generation, sig-\nnificantly reducing the need for training models from scratch. Numerous studies\n[Xian et al. 2023, Hoang et al. 2019, Sun et al. 2019, Sobhanam and Prakash 2023] have\ndemonstrated the efficacy of fine-tuning these models for specific tasks, showcasing their\nversatility and robustness in handling diverse NLP challenges.\n\nEnsemble Learning is a well-established technique in machine learning, often\nemployed to improve predictive performance by combining multiple models. Tradition-\nally associated with decision trees [Quinlan 1986], ensemble learning has evolved to in-\ncorporate various frameworks, including those involving LLMs [Jiang et al. 2023].\n\n3. Methodology\nThe data used in this study comes from the Human Value Detection at CLEF (Conference\nand Labs of the Evaluation Forum) 2024 task (ValueEval’24) [Kiesel et al. 2024a] and\nconsists of approximately 3K human-annotated texts containing over 73K sentences. The\nannotation associated with each sentence indicates whether a specific human value is\n“attained” and “constrained”. A total of 19 human values are analyzed. Each column\nreceives the value 0, 0.5, or 1, indicating whether the sentence does not contain the human\nvalue, partially contains it, or fully contains it, respectively. This study focused on the\nEnglish dataset. All models were optimized for F1-Macro score.\n\nTo approach the task as a multi-label classification problem, we combined the\n“attained” and “constrained” columns in the labels file, summing their values to determine\nwhether a specific human value is present in a sentence (0 for false, 1 for true). The result\nwas an array of 19 boolean values for each sentence, which were then used as inputs for\nmodel fine-tuning. Thus, each human value represents a class and the predictive model\nmay assign more than one class for a given sentence. While the value Humility was\nremoved by many CLEF participants due to its scarcity in the training set (present in only\n0.2% of sentences), we retained it, considering it important to predict even rare values to\nensure comprehensive performance across all values.\n\nUsing the training dataset, we fine-tuned six models: base and large versions of\nBERT [Devlin et al. 2019], RoBERTa [Liu et al. 2019], and DeBERTa [He et al. 2021].\nAfter fine-tuning, we used the validation data to create a new dataset that included the sen-\ntences, prediction probabilities for each class, and binary predictions indicating whether\na value is present in a sentence. The true labels are also carried onto the dataset to enable\nevaluations. Five different ensemble approaches were used to combine model outputs:\n\n\f• prob-equal: Probabilities from each model were summed and then averaged. A\n\nthreshold of 0.2 was applied.\n\n• prob-large-double: Probabilities from base models were summed, and probabil-\nities from large models were doubled before summing. The total was divided by\nthe number of votes (nine), and a threshold of 0.2 was applied.\n\n• preds-majority: Binary predictions from all models were summed, with a thresh-\nold of 2 applied to predict a value as present if at least two models identified it.\n• preds-large-double: Binary predictions were summed, with large models receiv-\ning two votes each. A threshold of 2 was used, meaning a value would be predicted\nas present if one large model or two base models identified it.\n\n• prob-weight-macro-f1: The probabilities predicted by each model were weighted\nby their F1 scores on the validation set. The weighted probabilities were then\nsummed and normalized, followed by applying a threshold of 0.2.\n\nFor reproducibility, all experiments, ensemble diagrams, and scripts used for fine-\ntuning are available on GitHub1, with a fixed random seed for all libraries. Implementa-\ntion details and further results are also in our repository. The models used in this study\nare publicly accessible and can be downloaded from HuggingFace.\n\n4. Results\n\nResults are presented in Table 1. The RoBERTa Large model achieved the highest accu-\nracy among the individual models, which aligns with expectations given the larger model\nsize. However, since the primary metric for model selection during training was the macro\nF1-score rather than accuracy, it is not surprising that larger models and ensemble models\ndo not consistently show higher accuracy.\n\nTable 1. F1 and Accuracy results for our models and baselines. ⋆ means the\nmodel is an ensemble, and † means it used the multilingual dataset version\n\nModel\n\nMacro F1 Accuracy\n\nBase models\n\nEnsembles\n\nBaselines\n\nBERT-base-uncased\nBERT-large\nRoBERTa-base\nRoBERTa-large\nDeBERTa-base\nDeBERTa-large\n\nprob-equal\nprob-large-double\nprob-weight-macro-f1\npreds-majority\npreds-large-double\n\n[Legkas et al. 2024] †\n[Yunis 2024] ⋆ †\n[Yeste et al. 2024]\n\n0.160\n0.263\n0.248\n0.282\n0.274\n0.295\n\n0.330\n0.326\n0.330\n0.318\n0.319\n\n0.390\n0.350\n0.280\n\n0.502\n0.482\n0.485\n0.508\n0.480\n0.507\n\n0.447\n0.438\n0.445\n0.484\n0.418\n\n–\n–\n–\n\nTable 1 also compares our results with the top-3 models from the CLEF 2024\nsubmissions. Notably, our ensemble approaches, specifically prob-weight-macro-f1 and\n\n1https://github.com/diegodimer/valueeval24\n\n\fprob-equal, performed only 0.03 and 0.02 below the top-scoring models from the con-\nference, which utilized XLM models and the multilingual dataset. The approach by\nArthur Schopenhauer [Yunis 2024] leveraged an ensemble of DeBERTa-v2-xxlarge and\nxlmRoBERTa-large models. Similarly, Hierocles of Alexandria [Legkas et al. 2024] em-\nployed both the multilingual and English-translated datasets, incorporating sentence se-\nquence information and fine-tuning an XLM-RoBERTa-xl model. Finally, team Philo of\nAlexandria [Yeste et al. 2024] fine-tuned a DeBERTa model specifically for this task.\n\nLooking into the scores for each of the 19 human values, we see that our ensem-\nbles demonstrated competitive performance, closely matching the results of XLM models\nand outperforming the DeBERTa-base model across nearly all values. This task was par-\nticularly challenging due to the significant class imbalance in the dataset, with nearly 50%\nof test set instances not containing any of the 19 values. This imbalance skews predictions\ntowards false negatives, resulting in lower F1 scores despite high accuracy, as models may\ncorrectly predict the absence of values due to their prevalence.\n\nOverall, the results demonstrate that ensemble models can achieve performance\ncomparable to very large models, even when utilizing models that require less compu-\ntational resources. Although training an XLM-DeBERTa model was not feasible on the\nhardware used for this study due to memory constraints, our ensembles still achieved a\nstrong macro F1-score. Specifically, the best ensemble model improved the macro F1-\nscore from 0.295 (the highest among the base models) to 0.33, highlighting the effective-\nness of ensemble methods in enhancing model performance in this context.\n\n5. Conclusion\n\nIn this study, we tackled the complex task of identifying human values in text, a challenge\ncrucial for understanding the values that shape public discourse and decision-making.\nBy leveraging multiple ensembles of LLMs, we demonstrated that ensemble-based ap-\nproaches could significantly enhance individual model performance in this task. This\nsuggests that instead of relying solely on a single, powerful LLM, ensemble methods\noffer a more robust and effective solution for complex NLP tasks.\n\nDespite the advanced capabilities of models like GPT-4.0, these models still strug-\ngle to consistently deliver satisfactory performance in this domain. For instance, in the\nValueEval’24, a team using GPT-4.0 for zero-shot classification achieved an F1-score\nof 0.25 [Kiesel et al. 2024b], which is lower than the performance of our ensemble ap-\nproaches. This highlights the inherent challenges in human value detection, where the\nnuances of language and context often exceed the capacity of a single model, no matter\nhow sophisticated. Future work will include a qualitative analysis to better understand the\nerrors made by the models and improve the proposed approaches, reinforcing the potential\nof ensemble learning as a key strategy in advancing the field.\n\nAcknowledgments. This work has been partially funded by CNPq-Brazil and Capes\nFinance Code 001.\n\nReferences\n\n[Ammanabrolu et al. 2022] Ammanabrolu, P., Jiang, L., Sap, M., Hajishirzi, H., and Choi,\nY. (2022). Aligning to social norms and values in interactive narratives. In Carpuat,\n\n\fM., de Marneffe, M.-C., and Meza Ruiz, I. V., editors, Proceedings of the 2022 Confer-\nence of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5994–6017, Seattle, United States. Association\nfor Computational Linguistics.\n\n[Bench-Capon 2003] Bench-Capon, T. J. M. (2003). Persuasion in practical argument\nusing value-based argumentation frameworks. Journal of Logic and Computation,\n13(3):429–448.\n\n[Dellaert et al. 1996] Dellaert, F., Polzin, T., and Waibel, A. (1996). Recognizing emotion\nIn Proceeding of Fourth International Conference on Spoken Language\n\nin speech.\nProcessing. ICSLP ’96, volume 3, pages 1970–1973 vol.3.\n\n[Devlin et al. 2019] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT:\n\nPre-training of deep bidirectional transformers for language understanding.\n\n[He et al. 2021] He, P., Liu, X., Gao, J., and Chen, W. (2021). DEBERTA: Decoding-\nenhanced BERT with disentangled attention. In International Conference on Learning\nRepresentations.\n\n[Hoang et al. 2019] Hoang, M., Bihorac, O. A., and Rouces, J. (2019). Aspect-based sen-\nIn Hartmann, M. and Plank, B., editors, Proceedings\ntiment analysis using BERT.\nof the 22nd Nordic Conference on Computational Linguistics, pages 187–196, Turku,\nFinland. Link¨oping University Electronic Press.\n\n[Jiang et al. 2023] Jiang, D., Ren, X., and Lin, B. Y. (2023). Llm-blender: Ensembling large\n\nlanguage models with pairwise ranking and generative fusion.\n\n[Kiesel et al. 2022] Kiesel, J., Alshomary, M., Handke, N., Cai, X., Wachsmuth, H., and\nStein, B. (2022). Identifying the Human Values behind Arguments. In Muresan, S.,\nNakov, P., and Villavicencio, A., editors, 60th Annual Meeting of the Association for\nComputational Linguistics (ACL 2022), pages 4459–4471. Association for Computa-\ntional Linguistics.\n\n[Kiesel et al. 2024a] Kiesel, J., C¸ ¨oltekin, C¸ ., Heinrich, M., Fr¨obe, M., Alshomary, M.,\nDe Longueville, B., Erjavec, T., Handke, N., Kopp, M., Ljubeˇsi´c, N., Meden, K.,\nMirzhakhmedova, N., Morkeviˇcius, V., Reitis-M¨unstermann, T., Scharfbillig, M., Ste-\nfanovitch, N., Wachsmuth, H., Potthast, M., and Stein, B. (2024a). Overview of touch´e\n2024: Argumentation systems. In Goharian, N., Tonellotto, N., He, Y., Lipani, A., Mc-\nDonald, G., Macdonald, C., and Ounis, I., editors, Advances in Information Retrieval,\npages 466–473, Cham. Springer Nature Switzerland.\n\n[Kiesel et al. 2024b] Kiesel, J., C¸ ¨oltekin, C¸ ., Heinrich, M., Fr¨obe, M., Alshomary, M.,\nLongueville, B. D., Erjavec, T., Handke, N., Kopp, M., Ljubeˇsi´c, N., Meden, K.,\nMirzakhmedova, N., Morkeviˇcius, V., Reitis-M¨unstermann, T., Scharfbillig, M., Ste-\nfanovitch, N., Wachsmuth, H., Potthast, M., and Stein, B. (2024b). Overview of Touch´e\n2024: Argumentation Systems. In Goeuriot, L., Mulhem, P., Qu´enot, G., Schwab, D.,\nNunzio, G. M. D., Soulier, L., Galuscakova, P., Herrera, A. G. S., Faggioli, G., and\nFerro, N., editors, Experimental IR Meets Multilinguality, Multimodality, and Interac-\ntion. 15th International Conference of the CLEF Association (CLEF 2024), Lecture\nNotes in Computer Science, Berlin Heidelberg New York. Springer.\n\n\f[Legkas et al. 2024] Legkas, S., Christodoulou, C., Zidianakis, M., Koutrintzes, D., Petasis,\nG., and Dagioglou, M. (2024). Hierocles of alexandria at touch´e: Multi-task & multi-\nhead custom architecture with transformer-based models for human value detection.\nIn Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024),\nCEUR Workshop Proceedings, CEUR-WS. org.\n\n[Liu et al. 2019] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,\nM., Zettlemoyer, L., and Stoyanov, V. (2019). RoBERTa: A robustly optimized BERT\npretraining approach.\n\n[Quinlan 1986] Quinlan, J. R. (1986).\n\nInduction of decision trees. Machine Learning,\n\n1(1):81–106.\n\n[Schwartz 1994] Schwartz, S. H. (1994). Are there universal aspects in the structure and\n\ncontents of human values? Journal of Social Issues, 50(4):19–45.\n\n[Schwartz et al. 2012] Schwartz, S. H., Cieciuch, J., Vecchione, M., Davidov, E., Fischer,\nR., Beierlein, C., Ramos, A., Verkasalo, M., L¨onnqvist, J.-E., Demirutku, K., Dirilen-\nGumus, O., and Konty, M. (2012). Refining the theory of basic individual values.\nJournal of Personality and Social Psychology, 103(4):663–688.\n\n[Sobhanam and Prakash 2023] Sobhanam, H. and Prakash, J. (2023). Analysis of fine tun-\ning the hyper parameters in RoBERTa model using genetic algorithm for text classifi-\ncation. International Journal of Information Technology, 15(7):3669–3677.\n\n[Sun et al. 2019] Sun, C., Qiu, X., Xu, Y., and Huang, X. (2019). How to fine-tune BERT for\ntext classification? In Sun, M., Huang, X., Ji, H., Liu, Z., and Liu, Y., editors, Chinese\nComputational Linguistics, pages 194–206, Cham. Springer International Publishing.\n\n[Tariq et al. 2019] Tariq, Z., Shah, S. K., and Lee, Y. (2019). Speech emotion detection\nusing iot based deep learning for health care. In 2019 IEEE International Conference\non Big Data (Big Data), pages 4191–4196.\n\n[Vaswani et al. 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. CoRR,\nabs/1706.03762.\n\n[Xian et al. 2023] Xian, G., Guo, Q., Zhao, Z., Luo, Y., and Mei, H. (2023). Short text\nclassification model based on DeBERTa-DPCNN. In 2023 4th International Confer-\nence on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE),\npages 56–59.\n\n[Yeste et al. 2024] Yeste, V., Ardanuy, M., and Rosso, P. (2024). Philo of alexandria at\nIn Working Notes of\ntouch´e: A cascade model approach to human value detection.\nthe Conference and Labs of the Evaluation Forum (CLEF 2024), CEUR Workshop\nProceedings, CEUR-WS. org.\n\n[Yunis 2024] Yunis, H. (2024). Arthur schopenhauer at touch´e 2024: Multi-lingual text clas-\nsification using ensembles of large language models. In Working Notes of the Confer-\nence and Labs of the Evaluation Forum (CLEF 2024), CEUR Workshop Proceedings,\nCEUR-WS. org.\n\n\f"
        },
        {
            "titulo": "A Change in Perspective: The Trade-Off Between Perspective API and Custom Models in Classifying Hate Speech in Portuguese",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31112",
            "idioma": "Inglês",
            "storage_key": "files/article_31112_30915.pdf",
            "autores": [
                {
                    "nome": "Arthur Buzelin",
                    "afiliacao": "UFMG",
                    "orcid": "http://orcid.org/0009-0007-0816-7189"
                },
                {
                    "nome": "Yan Aquino",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0009-0009-1647-4298"
                },
                {
                    "nome": "Pedro Bento",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0009-0007-4461-7503"
                },
                {
                    "nome": "Samira Malaquias",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0009-0006-8711-3412"
                },
                {
                    "nome": "Wagner Meira Jr",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0000-0002-2614-2723"
                },
                {
                    "nome": "Gisele L. Pappa",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0000-0002-0349-4494"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "This paper examines the performance of the Perspective API, developed by Jigsaw, in detecting hate speech in Portuguese. Although the Perspective API supports multiple languages, its performance metrics are often aggregated, obscuring specific details. Our study reveals that the API’s AUC-ROC score for Portuguese is significantly lower than for English (0.744 vs. 0.942). To address this, we developed a BERT classifier model trained on a Portuguese Twitter hate speech dataset. Our model, with just 100 messages in it’s training set, outperformed the Perspective API. These findings highlight the need for more granular performance metrics and suggest that custom models may offer better solutions for specific languages.",
            "keywords": [
                "Transformers",
                "BERT",
                "Perspective API",
                "NLP",
                "Hate Speech Detection"
            ],
            "referencias": [
                "Davidson, T., Warmsley, D., Macy, M., and Weber, I. (2017). Automated hate speech detection and the problem of offensive language. In Proceedings of the 11th International AAAI Conference on Web and Social Media (ICWSM). AAAI.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "Fortuna, P., Nunes, S., Soler-Company, J., and Wanner, L. (2019). A hierarchically-labeled portuguese hate speech dataset. In Proceedings of the Third Workshop on Abusive Language Online, pages 94–104. Association for Computational Linguistics.",
                "Kennedy, C., Bacon, G., Sahn, A., and Vacano, C. (2020). Constructing interval variables via faceted rasch measurement and multitask deep learning: a hate speech application.",
                "Kobellarz, J. K. and Silva, T. H. (2022). Should we translate? evaluating toxicity in online comments when translating from portuguese to english. In Anais do Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia), pages 95–104, Porto Alegre, Brazil. Sociedade Brasileira de Computação. In: 28th Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia), 2022, Curitiba.",
                "Lees, A., Tran, V. Q., Tay, Y., Sorensen, J., Gupta, J., Metzler, D., and Vasserman, L. (2022). A new generation of perspective api: Efficient multilingual character-level transformers.",
                "Lima, Q. L. H., Pagano, S. A., and da Silva, A. (2024). Toxic content detection in online social networks: A new dataset from brazilian reddit communities. In 16th International Conference on Computational Processing of Portuguese (PROPOR 2024).",
                "Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.",
                "Nogara, G., Pierri, F., Cresci, S., Luceri, L., Törnberg, P., and Giordano, S. (2024). Toxic bias: Perspective api misreads german as more toxic.",
                "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32.",
                "Roy, S. G., Narayan, U., Raha, T., Abid, Z., and Varma, V. (2021). Leveraging multilingual transformers for hate speech detection. ArXiv, abs/2101.03207.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained bert models for brazilian portuguese. In Proceedings of the 9th Brazilian Conference on Intelligent Systems (BRACIS), pages 403–417. IEEE."
            ],
            "artigo_completo": "A Change in Perspective:\nThe Trade-Off Between Perspective API and Custom Models\nin Classifying Hate Speech in Portuguese\n\nArthur Buzelin1, Yan Aquino1, Pedro Bento1, Samira Malaquias1,\nWagner Meira Jr1, Gisele L. Pappa1\n\n1Departamento de Ciˆencia da Computac¸ ˜ao – Universidade Federal de Minas Gerais\nBelo Horizonte – MG – Brazil\n\n{arthurbuzelin, yanaquino, pedro.bento, samiramalaquias}@dcc.ufmg.br\n\n{meira, glpappa}@dcc.ufmg.br\n\nAbstract. This paper examines the performance of the Perspective API, devel-\noped by Jigsaw, in detecting hate speech in Portuguese. Although the Perspec-\ntive API supports multiple languages, its performance metrics are often aggre-\ngated, obscuring specific details. Our study reveals that the API’s AUC-ROC\nscore for Portuguese is significantly lower than for English (0.744 vs. 0.942).\nTo address this, we developed a BERT classifier model trained on a Portuguese\nTwitter hate speech dataset. Our model, with just 100 messages in it’s train-\ning set, outperformed the Perspective API. These findings highlight the need for\nmore granular performance metrics and suggest that custom models may offer\nbetter solutions for specific languages.\n\n1. Introduction\n\nPerspective API is a tool designed to identify and mitigate toxic language online\n[Lees et al. 2022]. Using advanced machine learning and Natural Language Processing\n(NLP) models, Perspective API analyzes textual content to detect various forms of harm-\nful speech, including threats, insults, and hate speech. It is considered state-of-the-art for\ndetecting toxicity, and used by multiple platforms, such as Reddit, The New York Times,\nThe Wall Street Journal, and EL PA´IS.\n\nDespite its widespread adoption and claimed multilingual support, including Por-\ntuguese, the actual performance of the Perspective API in different languages remains un-\nclear. The official documentation and associated research papers often report performance\nmetrics by aggregating data from multiple languages, within a multilingual dataset. This\naggregation conceal the individual performance metrics for Portuguese, making it diffi-\ncult to evaluate the API’s effectiveness in this specific language. The lack of transparency\nin language-specific performance metrics raises concerns about the API’s reliability when\napplied to non-English texts.\n\nThe widespread acceptance of Perspective API as a leading tool for hate speech\ndetection combined with its claimed support for Portuguese, suggests that professionals\nmay readily adopt it for work in Portuguese-speaking contexts. However, if the API’s\nperformance in Portuguese is not on par with its performance in English or the aggregated\nresults, this could lead to inaccurate analyses and conclusions, particularly in fields like\n\n\fcomputational social sciences, where precise language detection is critical. The potential\nfor misleading results is especially concerning when the tool’s reliability in Portuguese is\ntaken for granted based on its performance in other languages.\n\nThis paper addresses this gap by evaluating the performance of the Perspective\nAPI in detecting hate speech specifically in Portuguese. It also assesses the feasibility\nof developing a custom hate speech detection tool tailored for Portuguese. To guide our\ninvestigation, we formulated the following research questions:\n\nRQ1: How well does the Perspective API perform when detecting hate speech in\n\nPortuguese?\n\nRQ2: For Portuguese, is it more effective and efficient to use a custom-made tool\nrather than relying on existing solutions like the Perspective API? If so, how much effort\nwould it take to build it?\n\nTo address these questions, we evaluated the Perspective API’s metrics using a\nPortuguese Twitter hate speech dataset. We then compared it to metrics obtained in a sim-\nilar English dataset regarding classification, date of collection, and content. Our findings\nrevealed that the Perspective API’s performance in Portuguese was significantly worse\nthan in English. Based on this insight, we developed our version of a BERT classifier to\ndetect hate speech in Portuguese. Remarkably, with just 100 messages, the BERT model\noutperformed the Perspective API in detecting hate speech in Portuguese. In contrast,\nthe BERT model trained with the English dataset did not surpass the Perspective API’s\nperformance.\n\n2. Related Works\n\nThis section reviews studies related to hate speech detection models and language-specific\nperformance comparisons. oportunities\n\n2.1. Model Comparisons\n\nMultilingual transformer models, such as BERT and its variants, have gained sig-\nnificant attention in hate speech detection across various languages. For instance,\n[Roy et al. 2021] demonstrated the superiority of fine-tuned transformer models in han-\ndling multilingual data, showcasing their effectiveness compared to more generalized ap-\nproaches like the Perspective API. This highlights the potential of specialized models to\noutperform broader, one-size-fits-all solutions.\n\nAnother noteworthy contribution is by [Kennedy et al. 2020], who introduced a\nhybrid approach that combines faceted Rasch measurement with multitasking deep learn-\ning. This methodology enhances both the interpretability and precision of hate speech\ndetection by integrating traditional psychometric techniques with advanced deep learning\nmodels. Compared to the Perspective API, which relies on more generalized algorithms,\nKennedy et al.’s approach offers a more nuanced understanding of linguistic variations\nand the intensity of hate speech.\n\n2.2. Language-Specific Comparisons\n\nIn Perspective’s introductory paper [Lees et al. 2022], developers reported AUC-ROC\nscores of 0.98 for English, 0.91 for Russian, and 0.87 for a group of ten other languages.\n\n\fThese results highlight a disparity in the API’s effectiveness across languages, raising\nconcerns about its applicability in non-English contexts.\n\nFurther studies have confirmed these concerns were relevant. For instance,\n[Nogara et al. 2024] analyzed the use of the Perspective API in German and found that the\nAPI tends to classify German texts as significantly more toxic than their English coun-\nterparts. This finding underscores the potential biases and inaccuracies that arise when\napplying the API to languages other than English, highlighting the need for further inves-\ntigation into its multilingual capabilities.\n\nThe seminal study of the use of the Perspective API’s in Portuguese was conducted\nby [Kobellarz and Silva 2022]. They compared identical texts in Portuguese and English\nusing the API and concluded that it performs better when analyzing texts in their origi-\nnal language. This suggests that the Perspective API may be less effective in detecting\nnuances in translated or non-native language content.\n\nBuilding upon this study, [Lima et al. 2024] developed a manually labeled dataset\nof toxic messages in Portuguese and evaluated the API against this dataset. Their findings\nrevealed significant discrepancies, emphasizing the need for the API to undergo more\nfocused training on Portuguese-language content to improve its accuracy and reliability\nin detecting hate speech.\n\nAdditionally, [Silva et al. 2023] proposed standardized datasets and benchmarks\nfor sentiment analysis in English, specifically addressing the challenges of automating\nthe development process. While their focus was on English, the methods and standards\nthey advocate could provide valuable insights for improving the Perspective API’s perfor-\nmance in other languages, including Portuguese.\n\n2.3. Research Gap\n\nDespite the widespread use and validation of the Perspective API for hate speech detection\nin various languages, a significant gap remains in its performance evaluation for less com-\nmonly studied languages like Portuguese. Previous research has shown the API’s strong\nperformance in English and other major languages, demonstrated by high AUC-ROC\nscores and robust metrics. However, detailed assessments for less-represented languages\nin its training datasets are lacking.\n\nTo address these gaps, we conducted focused evaluations of the Perspective API’s\nperformance for individual languages. Our study highlights the advantages of developing\ncustom models tailored to specific languages, such as Portuguese, offering more accurate\nand reliable hate speech detection. This emphasizes the need to consider custom solu-\ntions alongside existing multilingual models to improve the effectiveness of hate speech\ndetection across diverse languages.\n\n3. Methodology\nIn this section, we discuss the dataset selection, Perspective API evaluation, and the\nBERThs models fine-tuning.\n\n3.1. Dataset\n\nOur analysis required a Portuguese hate speech dataset and a similar English dataset,\nInstead of manually labeling messages,\nfor the purpose of an unbiased comparison.\n\n\fwhich can be costly and prone to errors, we opted to use two well-known Twit-\nthe Hierarchically-Labeled Portuguese Hate Speech Dataset\nter hate speech datasets:\n[Fortuna et al. 2019] and the Automated Hate Speech Detection and the Problem of Of-\nfensive Language dataset [Davidson et al. 2017].\n\nBoth datasets were created using the same methodology for classifying messages.\nThis involved identifying and mining accounts likely to post hate speech-related tweets\nin 2017. The tweets were then classified as either containing hate speech or not, which\nmatches the output of the Perspective API.\n\nThe original Portuguese and English datasets vary significantly in size and propor-\ntion of hate speech messages. The Portuguese dataset includes 5,934 non-toxic messages\nand 1,607 toxic messages, resulting in a ratio of approximately 3.7 non-toxic messages\nper toxic message. On the other hand, the English dataset initially consisted of 25,000\nclassified tweets, with 3,280 non-toxic messages and 21,720 toxic messages.\n\nFor a fair comparison of classification scores between the two datasets, we bal-\nanced their proportions by using the Portuguese dataset as the baseline, since this will be\nthe main object of our study. By selecting a random sample of messages from the English\ndataset that reflected the same proportion, we leveraged a final English dataset consisting\nof 3,280 non-toxic messages and 886 toxic messages, with the same ratio of non-toxic to\ntoxic messages of approximately 3.7.\n\n3.2. Comparing Perspective API results\n\nTo compare the models of Perspective for English and Portuguese, we selected random\nsamples of messages and analyzed them for toxicity using the Perspective API. We fo-\ncused on the Toxicity attribute, which is widely used in literature due to its robustness and\ncompatibility with both datasets under examination. The analysis was conducted in June\n2024, and the Perspective API provided toxicity scores for each sample in both datasets.\n\nEach message was assigned a toxicity score ranging from 0 to 1, where 0 repre-\nsents a very low probability of toxicity and 1 indicates a very high probability. To ensure\nthe most precise possible comparison, we optimized the threshold for toxicity classifi-\ncation by maximizing the F1 score for each dataset individually. The optimal threshold\nwas determined to be 0.48 for the Portuguese dataset and 0.59 for the English dataset,\nreflecting the different calibrations needed by the two languages.\n\n3.3. BERThs (BERT hate speech) Model\n\nThis section shows how we fine-tuned our own BERT classifier for hate speech detection,\nnamely BERThs. BERThs was fine-tuned using both a Portuguese and an Englih dataset.\n\nInitially, the goal of the model, particularly the Portuguese one, was not to achieve\nthe highest possible accuracy, but to be easy to replicate. This will help us show whether\na simple fine-tuned model may be more effective than the Perspective API in Portuguese.\n\nFor fine-tuning the BERThs-Pt, we used BERTimbau [Souza et al. 2020] as the\nbase model, as it is pre-trained in Portuguese and better suited for our task. Given the\nsmall size of the annotated corpus, we fine-tuned and evaluated the model 30 times using\ndifferent randomized non-overlapping stratified sets: training, validation, and test sets,\ncomprising 80%, 10%, and 10% of the labeled dataset. Each split maintained the original\n\n\fclass distribution of approximately 21.3% toxic messages and 78.7% non-toxic messages.\nThe same test sets were used to evaluate the Perspective API. This approach ensured\nrobustness and prevented issues such as training on an all-toxic set of messages, which\ncould lead to unreliable results.\n\nTo determine the minimum number of messages needed for our classifier to out-\nperform the Perspective API, initially, only 10 messages from the training set were used\nfor fine-tuning BERTimbau. We incrementally added 10 more messages to the training\nset after each iteration, until BERThs achieved a better AUC score than Perspective. The\nAUC metric was chosen because it was the only metric reported for Portuguese in the\nPerspective API paper. After that, we added 200 new messages to the training set in each\nsubsequent iteration, until all messages were included, highlighting the highest perfor-\nmance our model could achieve.\n\nThe fine-tuning was performed using the PyTorch library [Paszke et al. 2019],\nwith the AdamW optimizer [Loshchilov and Hutter 2017] and a learning rate of 5 × 10−6.\nThe classification thresholds were established based on the output probabilities of the\nmodel, defined as the thresholds that yielded the best mean F1-score on our validation set.\n\nFor BERThs-En, we employed the BERT uncased model [Devlin et al. 2019],\nwhich is optimized for English language processing. The fine-tuning procedure followed\nthe same general approach used for the Portuguese variant, but with specific modifications\nto account for the superior performance of the Perspective API on English texts. Specifi-\ncally, instead of gradually increasing the training set by 10 messages and subsequently by\n200 messages per iteration, we opted to directly increase the training set by 200 messages\nin each iteration.\n\n4. Results\n\nThis section presents the Perspective API prediction metrics for the English and Por-\ntuguese datasets and compares them to BERThs. The models were fine-tuned on an\nNVIDIA RTX 4090 GPU. As the models were trained 30 times with different data sam-\nples, the results in this section present the mean followed by the standard deviation.\n\n4.1. Perspective Performance\n\nTable 1 shows the performance metrics of the Perspective API in the English and Por-\ntuguese datasets. It highlights a significant disparity in the model’s effectiveness between\nthe two languages, with the English dataset consistently achieving higher scores across all\nmetrics. Notably, the accuracy, precision, recall, F1 score, and AUC-ROC are consider-\nably lower for the Portuguese dataset, suggesting that the model’s capability to accurately\nclassify toxic content is compromised in Portuguese.\n\nNote that the F1 score – which serves as a balanced measure of a model’s precision\nand recall in classification tasks – is almost 35 percentage points lower in Portuguese. On\ntop of that, typically, there is a trade-off between the precision and recall metrics; adjust-\ning the threshold to improve one often causes the deterioration of the other. However, in\nthis case, precision and recall are significantly lower for the Portuguese dataset, indicat-\ning an overall performance issue. Low precision usually implies in a high number of false\npositives, while low recall indicates many false negatives.\n\n\fTable 1. Metrics for the Perspective API model in English and Portuguese.\nPerspective API(En) Perspective API(Pt) Difference\nMetric\nAccuracy\nPrecision\nRecall\nF1 Score\nAUC-ROC\n\n0.901\n0.813\n0.744\n0.777\n0.942\n\n0.122\n0.336\n0.340\n0.339\n0.199\n\n0.779\n0.477\n0.404\n0.438\n0.743\n\nFigure 1. Graph displaying the mean AUC across varying training sizes of the\nBERThs-Pt model, with the left panel covering up to 100 Twitter posts to assess\nearly performance, and the right panel extending to 5000 posts to evaluate the\nmodel’s full training potential.\n\nThe most concerning results are in the AUC-ROC score, which measures the clas-\nsification abilities of the Perspective API in its official paper. The Portuguese dataset\nscores 20 percentage points lower than the English dataset in AUC-ROC. This is both\nsurprising and alarming, given that the multilingual Perspective API is reported to have\nan AUC-ROC of 0.877, only slightly lower than the English counterpart in the official\ndocumentation.\n\nThese findings suggest that, while the Perspective API claims to support multiple\nlanguages, its performance in Portuguese is substantially lower than in English. This\nunderscores the importance of evaluating multilingual models on a per-language basis to\nensure their effectiveness and reliability across different linguistic contexts.\n\n4.2. Evaluating BERThs\n\nBERThs-Pt was evaluated by incrementally increasing the training set by 10 messages at\na time. Figure 1 illustrates the model’s performance as the number of training messages\nincreased. The red line represents the average performance of the Perspective API on the\ntest set. Observe that our model surpassed the Perspective API in AUC-ROC score with\nonly 100 training messages.\n\nOn the other hand, the BERThs-En dataset showed a different result. Even with an\nextensive training data, leaving aside a small portion for testing and validation, the fine-\ntuned BERT model still performed worse than the Perspective API, achieving an average\nAUC-ROC of 0.934 compared to Perspective’s 0.942.\n\nThese findings suggest that while the Perspective API is an excellent tool, it is\n\n\fnot suited for Portuguese. This means creating a custom model can easily surpass the\nPerspective API’s performance with a relatively small amount of labeled data. Therefore,\na classifier tailored to the specific linguistic and contextual nuances of Portuguese is better\nsuited for the detection of hate speech.\n\n4.3. Qualitative Analysis\n\nTable 2 shows a comparative analysis of the classification outcomes for the Perspective\nAPI and BERThs-Pt on a Portuguese hate speech dataset. The most notable observation\nfrom this analysis is that BERThs-Pt misclassified significantly fewer messages (14.3%)\nwhen compared to the Perspective API(20.9%), indicating that the BERT model is gen-\nerally more accurate in discerning the nuances of the text. This superiority is particularly\nevident in its handling of subtle and context-dependent instances of hate speech, where\nthe Perspective API often struggles. The analysis further reveals that while both models\nperform well with clear and unambiguous content, they encounter challenges with am-\nbiguous language and contextually rich messages, cases where the BERT model shows a\nbetter overall ability to navigate these complexities.\n\nt\nc\ne\nr\nr\no\nc\n\nBERThs-Pt was correct\n\nTable 2. Comparison of misclassified and correctly classified messages from the\nPerspective API and BERThs-Pt. Four random examples from each quadrant are\nincluded. In quadrants where only one model missed the true class, the message\nlabels refer to the model that made the mistake. “FP” refers to False Positive,\n“FN” to False Negative, “TP” to True Positive, and “TN” to True Negative.\nBERThs-Pt missed\n6.9% of Messages\n(FP)“Nossa, mas feminismo\nnecess´ario hoje em dia?”\n(FN) “Se vc bate nessa mulher, al´em\nde covarde, com certeza vc gosta de ”\n(FP) “quem ´e playboy safado fortalece\nno RT”\n(FP) “Isso sim ´e tratar gay com\nindiferenc¸a...”\n\n72.2% of Messages\n(TP) “Que mulher burra do cacete”\n(TP) “gorda e feia”\n(TN) “Boa semana para todos!”\n(TN) “N˜ao vou orar, sou ateu”\n\ns\na\nw\ne\nv\ni\nt\nc\ne\np\ns\nr\ne\nP\n\nd\ne\ns\ns\ni\n\nm\ne\nv\ni\nt\nc\ne\np\ns\nr\ne\nP\n\n13.5% of Messages\n(FN) “Vai tamb´em ser lanc¸ado um\nmanual de boas maneiras para lidar\ncom fufas, gays e transsexuais, os\nchamados LGTB”\n(FN) “as pessoas n˜ao entendem que no\nmeio dos refugiados tem in´umeros\nterroristas, ´e uma coisa t˜ao ´obvia”\n(FN) “E traveco mesmo , m´o piroc˜ao”\n(FN) “Vocˆe ´e cheinha, N ˜AO ´e\ngostosa.”\n\n7.4% of Messages\n(FP) “Pra mim BBB sempre foi uma\nmerda.”\n(FN) “feliz dia do n˜ao tenho roupa pra\nsair”\n(FN) “Isso ´e injusto!”\n(FN) “meritocracia: existe”\n\nHaving established that BERThs-Pt generally outperforms the Perspective API,\nwe conducted a quadrant-specific analysis to explore these differences further. The first\nquadrant represents messages that both methods classified correctly, accounting for 72.2%\nof the messages. This indicates their effectiveness in handling unambiguous content,\n\n\fas shown in Table 2. The high success rate highlights the capability of both models to\nmanage straightforward cases of hate speech or benign content where linguistic ambiguity\nis low. However, real-world scenarios often involve more nuanced language, where model\ndifferences become more evident.\n\nThe second quadrant covers the 6.9% of messages that Perspective correctly clas-\nsified but BERThs-Pt misclassified. A random sample of four of these messages reveals\nthat they are somewhat ambiguous, making it difficult to determine with certainty whether\nthey were wrongly classified. These cases highlight the challenges of accurately catego-\nrizing nuanced and context-dependent language.\n\nThe third quadrant, which includes 13.5% of the messages that Perspective mis-\nclassified and BERThs-Pt correctly classified. It becomes apparent that Perspective strug-\ngles with more complex contexts, particularly when the hate speech is not explicit. The\nmodel has particular difficulty with slang or coded language, such as derogatory terms\ntargeting LGBTQ+ individuals. Perspective’s limitations in understanding such indirect\ninsults become evident here, suggesting that its generalized training may not sufficiently\ncapture the nuances of the Portuguese language.\n\nFinally, the fourth quadrant, comprising 7.4% of messages, involves cases where\nboth models failed. These messages typically lack sufficient context, making accurate\nclassification challenging. The shared difficulty in this category underscores the chal-\nlenges of detecting hate speech when language is ambiguous or context is missing.\n\n5. Conclusions\n\nThis study assessed the performance of the Perspective API in detecting hate speech in\nPortuguese, comparing it to English and exploring the potential of custom-trained models.\nThe results show a significant performance gap, with the API achieving an AUC-ROC\nscore of 94.2 in English but only 74.4 in Portuguese. This drop illustrates the limitations\nof using a generalized multilingual tool for specific languages.\n\nRelying on a model that supports Portuguese yet delivers subpar results poses\ntwo main issues. First, research conducted using such a tool may produce inaccurate or\nmisleading outcomes, undermining the validity of the study. Second, researchers from\nnon-English-speaking regions, may feel compelled to conduct their research in English\ncontexts to leverage the more reliable performance of tools like the Perspective API, po-\ntentially overlooking important linguistic and cultural nuances.\n\nWhile the Perspective API excels in English, our study shows it may not be the\nbest choice for Portuguese. A custom BERT model we developed using BERTimbau out-\nperformed the API with only 100 training messages, suggesting that fine-tuning models\nfor specific languages can yield better results in hate speech detection.\n\nIn conclusion, while the Perspective API offers robust performance for English,\nits efficacy in Portuguese is limited. Researchers and practitioners should consider devel-\noping custom models tailored to their specific linguistic contexts to achieve more accurate\nand reliable results.\n\nAcknowledgments\n\nThis work was partially funded by CNPq, CAPES, FAPEMIG, and IAIA - INCT on AI.\n\n\fReferences\n\nDavidson, T., Warmsley, D., Macy, M., and Weber, I. (2017). Automated hate speech de-\ntection and the problem of offensive language. In Proceedings of the 11th International\nAAAI Conference on Web and Social Media (ICWSM). AAAI.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Bert: Pre-training of deep\n\nbidirectional transformers for language understanding.\n\nFortuna, P., Nunes, S., Soler-Company, J., and Wanner, L. (2019). A hierarchically-\nIn Proceedings of the Third Workshop on\nlabeled portuguese hate speech dataset.\nAbusive Language Online, pages 94–104. Association for Computational Linguistics.\n\nKennedy, C., Bacon, G., Sahn, A., and Vacano, C. (2020). Constructing interval variables\nvia faceted rasch measurement and multitask deep learning: a hate speech application.\n\nKobellarz, J. K. and Silva, T. H. (2022). Should we translate? evaluating toxicity in\nonline comments when translating from portuguese to english. In Anais do Simp´osio\nBrasileiro de Sistemas Multim´ıdia e Web (WebMedia), pages 95–104, Porto Alegre,\nBrazil. Sociedade Brasileira de Computac¸ ˜ao. In: 28th Simp´osio Brasileiro de Sistemas\nMultim´ıdia e Web (WebMedia), 2022, Curitiba.\n\nLees, A., Tran, V. Q., Tay, Y., Sorensen, J., Gupta, J., Metzler, D., and Vasserman, L.\n(2022). A new generation of perspective api: Efficient multilingual character-level\ntransformers.\n\nLima, Q. L. H., Pagano, S. A., and da Silva, A. (2024). Toxic content detection in online\nsocial networks: A new dataset from brazilian reddit communities. In 16th Interna-\ntional Conference on Computational Processing of Portuguese (PROPOR 2024).\n\nLoshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv\n\npreprint arXiv:1711.05101.\n\nNogara, G., Pierri, F., Cresci, S., Luceri, L., T¨ornberg, P., and Giordano, S. (2024). Toxic\n\nbias: Perspective api misreads german as more toxic.\n\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin,\nZ., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-\nperformance deep learning library. Advances in neural information processing systems,\n32.\n\nRoy, S. G., Narayan, U., Raha, T., Abid, Z., and Varma, V. (2021). Leveraging multilin-\n\ngual transformers for hate speech detection. ArXiv, abs/2101.03207.\n\nSilva, M., de Oliveira, V., and Pardo, T. (2023). A sentiment analysis benchmark for auto-\nmated machine learning applications and a proof of concept in hate speech detection.\nIn Anais do XIV Simp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem\nHumana, pages 199–206, Porto Alegre, RS, Brasil. SBC.\n\nSouza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained bert models for\nIn Proceedings of the 9th Brazilian Conference on Intelligent\n\nbrazilian portuguese.\nSystems (BRACIS), pages 403–417. IEEE.\n\n\f"
        },
        {
            "titulo": "Segmentação Textual Baseada em Tópicos em Português Utilizando BERTimbau",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31113",
            "idioma": "Português",
            "storage_key": "files/article_31113_30916.pdf",
            "autores": [
                {
                    "nome": "Luciano A. C. da Silva",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0009-0002-2061-9903"
                },
                {
                    "nome": "Maiara S. F. Rodrigues",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0009-0006-6138-8258"
                },
                {
                    "nome": "Adriana P. Archanjo",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0000-0001-9503-194X"
                },
                {
                    "nome": "Luis Pessoa",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0009-0000-6290-4476"
                },
                {
                    "nome": "Miguel L. Silva",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0009-0002-9411-4465"
                },
                {
                    "nome": "Thiago F. de Almeida",
                    "afiliacao": "CPQD",
                    "orcid": "https://orcid.org/0009-0004-4528-9351"
                },
                {
                    "nome": "Leonardo Silveira",
                    "afiliacao": "PUC-Campinas",
                    "orcid": "https://orcid.org/0000-0002-4468-6812"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Neste trabalho, exploramos a segmentação textual para o português utilizando o modelo BERTimbau, com bases de dados construídas usando tradução automática e a partir de notícias online. Obtivemos P",
            "keywords": [
                "segmentação textual",
                "processamento de linguagem natural",
                "datasets em português",
                "BERTimbau"
            ],
            "referencias": [
                "Francisco, O. J. (2018). Recuperação de informação em atas de reunião utilizando segmentação textual e extração de tópicos. Dissertação de mestrado, Universidade Federal de São Carlos, Sorocaba.",
                "Hearst, M. A. (1997). Text tiling: Segmenting text into multi-paragraph subtopic passages. Computational linguistics, 23(1):33–64.",
                "Retkowski, F. and Waibel, A. (2024). From text segmentation to smart chaptering: A novel benchmark for structuring video transcriptions. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 406–419."
            ],
            "artigo_completo": "Segmentac¸ ˜ao Textual Baseada em T´opicos em Portuguˆes\nUtilizando BERTimbau\n\nLuciano A. C. da Silva1, Maiara S. F. Rodrigues1, Adriana P. Archanjo1,\nLuis Pessoa1, Miguel L. Silva1, Thiago F. de Almeida1, Leonardo Silveira2,\n\n1CPQD - Centro de Pesquisa e Desenvolvimento, Campinas, SP, Brasil\n\n2Pontif´ıcia Universidade Cat´olica de Campinas, SP, Brasil\n\nluciano.augusto.silva@usp.br, maiara.frodrigues2000@gmail.com,\nprestoarch@hotmail.com, {luisp, mfilho, tfelipea}@cpqd.com.br,\nleonardo.silveira@ga.ita.br\n\nAbstract. In this work, we explore text segmentation for Portuguese using the\nBERTimbau model, with datasets derived from machine translation and online\nnews sources. We obtained Pk = 6.89 for an in-domain evaluation, but worse\nresults in out-of-domain evaluations, highlighting the importance of a diverse\ntraining set to improve generalization across multiple domains.\n\nResumo. Neste trabalho, exploramos a segmentac¸ ˜ao textual para o portuguˆes\nutilizando o modelo BERTimbau, com bases de dados constru´ıdas usando\ntraduc¸ ˜ao autom´atica e a partir de not´ıcias online. Obtivemos Pk = 6, 89 para\numa avaliac¸ ˜ao dentro do dom´ınio, mas resultados piores em avaliac¸ ˜oes fora do\ndom´ınio, destacando a importˆancia de uma base de treinamento diversificada\npara melhorar a generalizac¸ ˜ao em m´ultiplos dom´ınios.\n\n1. Introduc¸ ˜ao\nCom o aumento na gerac¸ ˜ao de conte´udo textual n˜ao estruturado, como transcric¸ ˜oes au-\ntom´aticas de not´ıcias, aulas e reuni˜oes, h´a tamb´em um crescente interesse em extrair\nde forma eficiente informac¸ ˜oes relevantes desse material [Retkowski and Waibel 2024,\nGklezakos et al. 2024]. Por exemplo, pode ser desafiador encontrar o in´ıcio de um\ndeterminado t´opico discutido na transcric¸ ˜ao de uma longa reuni˜ao, a menos que essa\ntranscric¸ ˜ao esteja devidamente estruturada. A segmentac¸ ˜ao textual baseada em t´opicos ´e\numa tarefa de Processamento de Linguagem Natural (PLN) que divide um texto longo em\nsegmentos n˜ao sobrepostos, de acordo com as mudanc¸as de t´opico [Hearst 1997]. Essa\nferramenta permite estruturar e compreender melhor grandes volumes de dados, facili-\ntando a busca e a extrac¸ ˜ao de informac¸ ˜oes.\n\nrecentes\n\ntrabalhos\n\nH´a poucos\n\nsobre segmentac¸ ˜ao textual em portuguˆes\n[Cardoso et al. 2017, Francisco 2018]. Neste artigo, exploramos a segmentac¸ ˜ao tex-\ntual baseada em t´opicos para o portuguˆes, aplicando a abordagem proposta em\n[Yu et al. 2023], utilizando o modelo BERTimbau [Souza et al. 2023]. Constru´ımos os\nconjuntos de dados de treinamento e teste por meio de traduc¸ ˜ao autom´atica para o por-\ntuguˆes, e utilizando not´ıcias extra´ıdas da internet.\n\n2. Metodologia\nNeste trabalho, utilizamos a abordagem proposta por [Yu et al. 2023] que trata a\nsegmentac¸ ˜ao textual como um problema de classificac¸ ˜ao de uma sequˆencia de sentenc¸as,\n\n\fem que se deseja identificar a ´ultima sentenc¸a de cada t´opico, ou seja, identificar as fron-\nteiras dos segmentos. O componente principal ´e um modelo de linguagem pr´e-treinado do\ntipo Transformer encoder [Vaswani et al. 2023], que produz a representac¸ ˜ao contextual\ndas sentenc¸as do texto de entrada. Cada representac¸ ˜ao de sentenc¸a ´e usada na classificac¸ ˜ao\nde fronteira do segmento, conforme mostrado na Figura 1.\n\nFigura 1. Estrutura do modelo de segmentac¸ ˜ao proposto por [Yu et al. 2023]\n\nEm [Yu et al. 2023], al´em da tarefa principal de segmentac¸ ˜ao baseada em t´opicos,\ns˜ao definidas duas tarefas auxiliares adicionais, Topic-aware Sentence Structure Predic-\ntion (TSSP) e Contrastive Semantic Similarity Learning (CSSL), com o objetivo de mode-\nlar a coerˆencia textual e obter melhores resultados na segmentac¸ ˜ao. O modelo ´e treinado\nde forma supervisionada, otimizando a soma das perdas das trˆes tarefas definidas, sobre\num conjunto de treinamento devidamente anotado.\n\nNeste trabalho, utilizamos datasets para o treinamento e a avaliac¸ ˜ao obtidos por\nmeio de traduc¸ ˜ao autom´atica para portuguˆes ou constru´ıdos a partir de not´ıcias em por-\ntuguˆes extra´ıdas da internet. Os datasets WikiSection e WIKI-50 foram usados por\n[Yu et al. 2023] e passaram pelo processo de traduc¸ ˜ao autom´atica usando a API de\ntraduc¸ ˜ao da Google. O dataset WikiSection [Arnold et al. 2019] foi usado para trei-\nnamento e avaliac¸ ˜ao, e consiste num conjunto de 38K artigos em inglˆes e alem˜ao,\nnos dom´ınios de doenc¸as e cidades. Ap´os a traduc¸ ˜ao, restaram 3.590 documentos no\ndom´ınio de doenc¸as e 19.539 documentos no dom´ınio de cidades. O dataset WIKI-50\n[Koshorek et al. 2018] foi usado apenas para avaliac¸ ˜ao, e consiste originalmente em um\nconjunto de 50 amostras em inglˆes, provenientes da Wikipedia.\n\nPara a avaliac¸ ˜ao dos modelos, utilizamos tamb´em datasets em portuguˆes cons-\ntru´ıdos a partir de not´ıcias extra´ıdas com webscrapping do portal G11 (portal de not´ıcias\ndo Grupo Globo de Comunicac¸ ˜ao), e do canal de not´ıcias do IBGE2 (Instituto Brasileiro\nde Geografia e Estat´ıstica). Os documentos de texto foram formados pela concatenac¸ ˜ao\naleat´oria de not´ıcias, sendo cada not´ıcia considerada um segmento de t´opico diferente.\nNo caso do dataset G1, foram gerados 454 documentos a partir de 1.300 not´ıcias. Para o\ndataset IBGE, foram gerados 1.517 documentos a partir de 3.376 not´ıcias.\n\n1https://g1.globo.com/tecnologia/noticia/2012/11/siga-o-g1-por-rss.html\n2https://servicodados.ibge.gov.br/api/docs/noticias?versao=3\n\n\fComo o nosso objetivo ´e aplicar a segmentac¸ ˜ao para o portuguˆes, substitu´ımos\no modelo usado em [Yu et al. 2023] pelo modelo BERTimbau [Souza et al. 2023], pr´e-\ntreinado para o portuguˆes do Brasil. Utilizamos as vers˜oes BERTimbau Base (110M de\nparˆametros) e BERTimbau Large (335M de parˆametros)3.\n\nO treinamento foi realizado em uma GPU NVIDIA T4, usando BERTimbau Base\ne Large, com 70% do dataset WikiSection em portuguˆes, por 5 ´epocas, com learning rate\nde 5 × 10−5, batch size de 2 e gradiente acumulado de 2. Criamos sempre um modelo\ntreinado com WiKiSection/cidades e o outro modelo treinado com WiKiSection/doenc¸as.\nNo caso do BERTimbau Large, o treinamento durou aproximadamente 2 dias e 5 horas\npara o conjunto de cidades e pouco mais de 11 horas para o conjunto de doenc¸as.\n\nA avaliac¸ ˜ao dos modelos seguiu a mesma linha de [Yu et al. 2023]. Usamos trˆes\nm´etricas usuais para avaliac¸ ˜ao de segmentac¸ ˜ao textual: F1, Pk [Beeferman et al. 1999], e\nWindowDiff [Pevzner and Hearst 2002]. No caso das m´etricas Pk e WindowDiff, quanto\nmenor o valor, melhor o desempenho. No caso da m´etrica F1, quanto maior o valor,\nmelhor o desempenho. A avaliac¸ ˜ao dentro do dom´ınio de treinamento foi realizada com\n20% do dataset WikiSection em portuguˆes. Os datasets WIKI-50, G1 e IBGE s˜ao usados\napenas para avaliac¸ ˜ao fora do dom´ınio de treinamento.\n\n3. Resultados\n\nAs Tabelas 1 e 2 apresentam os resultados de avaliac¸ ˜ao dos modelos usando BERTim-\nbau, criados e avaliados para o portuguˆes, dentro do mesmo dom´ınio, com os datasets\nWikiSection/cidades e WikiSection/doenc¸as. Tamb´em s˜ao apresentados os resultados\npara o inglˆes correspondentes ao modelo BERT Base [Devlin et al. 2018], obtidos por\n[Yu et al. 2023].\n\nModelo\n(en) BERT Base [Yu et al. 2023] 80,16\n87,41\n(pt) BERTimbau Base\n87,59\n(pt) BERTimbau Large\n\nF1\n\nPk WD\n8,22 10,19\n8,55\n7,07\n8,37\n6,89\n\nTabela 1. Resultados dos modelos criados e avaliados com o dataset WikiSec-\ntion / cidades. BERT Base avaliado em ingl ˆes, BERTimbau em portugu ˆes.\n\nModelo\nWD\n(en) BERT Base [Yu et al. 2023] 68,26 18,29 22,06\n76,91 17,16 19,45\n(pt) BERTimbau Base\n77,77 16,55 18,76\n(pt) BERTimbau Large\n\nPk\n\nF1\n\nTabela 2. Resultados dos modelos criados e avaliados com o dataset WikiSec-\ntion / doenc¸ as. BERT Base avaliado em ingl ˆes, BERTimbau em portugu ˆes.\n\nAs m´etricas de avaliac¸ ˜ao obtidas com os modelos BERTimbau para o portuguˆes\ns˜ao melhores e pr´oximas `aquelas apresentadas por [Yu et al. 2023] em inglˆes. Neste caso,\ndevemos considerar tamb´em que o modelo criado para o portuguˆes usando BERTimbau\nLarge ´e maior que o modelo usado em [Yu et al. 2023].\n\n3https://huggingface.co/neuralmind/bert-base-portuguese-cased\n\n\fA Tabela 3 apresenta os resultados da avaliac¸ ˜ao de dois modelos criados para o\nportuguˆes nos dom´ınios de cidades e doenc¸as, usando o BERTimbau Large, e avaliados\nfora do dom´ınio de treinamento, nos datasets WIKI-50, G1 e IBGE.\n\nDataset Modelo / cidades\nPk\nWD\n35,01 35,36\n13,62 17,28\n20,12 21,06\n\nF1\n15,43\n64,66\n43,12\n\nWiki50\nG1\nIBGE\n\nModelo / doenc¸as\nPk\nF1\nWD\n36,02\n32,42\n26,55\n\n12,97 35,98\n54,81 25,61\n43,36 23,40\n\nTabela 3. Avaliac¸ ˜ao fora do dom´ınio de treinamento. Modelos com o BERTimbau\nLarge criados com o dataset WikiSection/cidades e WikiSection/doenc¸ as.\n\nO desempenho do modelo fora do dom´ınio de treinamento foi inferior ao desempe-\nnho dentro do dom´ınio. Os resultados foram melhores para o modelo treinado com o da-\ntaset WiKiSection/cidades. De fato, segundo [Arnold et al. 2019], o conte´udo do dataset\nWikiSection apresenta caracter´ısticas distintas para cada dom´ınio: WiKiSection/doenc¸as\n´e de dom´ınio cient´ıfico restrito com linguagem espec´ıfica, enquanto WiKiSection/cidades\n´e de dom´ınio geral mais diverso, mais pr´oximo de um conte´udo de not´ıcias.\nIsso su-\ngere que a composic¸ ˜ao de dados de treinamento pode ajudar a obter um modelo para\nsegmentac¸ ˜ao textual que generalize melhor para m´ultiplos dom´ınios.\n\n4. Conclus˜ao\n\nNeste trabalho, exploramos a segmentac¸ ˜ao textual para o portuguˆes, seguindo a aborda-\ngem de [Yu et al. 2023], mas utilizando o modelo pr´e-treinado para o portuguˆes BERTim-\nbau [Souza et al. 2023]. Empregamos bases de treinamento e teste constru´ıdas usando\na traduc¸ ˜ao autom´atica de bases existentes, al´em de bases de teste constru´ıdas a par-\ntir de not´ıcias em portuguˆes recuperadas da internet. Obtivemos ´otimos resultados na\nsegmentac¸ ˜ao de texto dentro do mesmo dom´ınio para o portuguˆes, semelhante ao que foi\nobtido por [Yu et al. 2023] para o inglˆes. Nossos resultados sugerem a efic´acia do m´etodo\nempregado para a criac¸ ˜ao do modelo em portuguˆes e a importˆancia de usar uma base de\ntreinamento de dom´ınio diversificado para obter um modelo que generalize melhor para\nm´ultiplos dom´ınios.\n\nPara trabalhos futuros, pretendemos explorar modelos diferentes e buscar uma\ncomposic¸ ˜ao mais variada de dados de treinamento para obter um modelo que generalize\nmelhor para v´arios dom´ınios. Al´em disso, desejamos estudar a segmentac¸ ˜ao textual de\ntranscric¸ ˜oes autom´aticas obtidas com reconhecimento de fala, e explorar a segmentac¸ ˜ao\nde textos muito longos, considerando a t´ıpica limitac¸ ˜ao do contexto de entrada de modelos\nbaseados em Transformer [Vaswani et al. 2023].\n\nAgradecimentos\n\nEste projeto foi apoiado pelo Minist´erio da Ciˆencia, Tecnologia e Inovac¸ ˜oes, com recursos\nda Lei no 8.248, de 23 de outubro de 1991, no ˆambito do PPI-SOFTEX, coordenado pela\nSoftex e publicado PDI 03, DOU 01245.023862/2022-14.\n\n\fReferˆencias\n\nArnold, S., Schneider, R., Cudr´e-Mauroux, P., Gers, F. A., and L¨oser, A. (2019). Sector:\nA neural model for coherent topic segmentation and classification. Transactions of the\nAssociation for Computational Linguistics, 7:169–184.\n\nBeeferman, D., Berger, A. L., and Lafferty, J. D. (1999). Statistical models for text\n\nsegmentation. Machine Learning, 34:177–210.\n\nCardoso, P. C., Pardo, T. A., and Taboada, M. (2017). Subtopic annotation and automatic\n\nsegmentation for news texts in brazilian portuguese. Corpora, 12(1):23–54.\n\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. (2018). BERT: pre-training of deep\n\nbidirectional transformers for language understanding. CoRR, abs/1810.04805.\n\nFrancisco, O. J. (2018). Recuperac¸ ˜ao de informac¸ ˜ao em atas de reuni˜ao utilizando\nsegmentac¸ ˜ao textual e extrac¸ ˜ao de t´opicos. Dissertac¸ ˜ao de mestrado, Universidade\nFederal de S˜ao Carlos, Sorocaba.\n\nGklezakos, D. C., Misiak, T., and Bishop, D. (2024). Treeseg: Hierarchical topic seg-\n\nmentation of large transcripts. arXiv preprint arXiv:2407.12028.\n\nHearst, M. A. (1997). Text tiling: Segmenting text into multi-paragraph subtopic passa-\n\nges. Computational linguistics, 23(1):33–64.\n\nKoshorek, O., Cohen, A., Mor, N., Rotman, M., and Berant, J. (2018). Text segmen-\nIn Proceedings of the 2018 Conference of the\ntation as a supervised learning task.\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers), pages 469–473.\n\nPevzner, L. and Hearst, M. A. (2002). A critique and improvement of an evaluation metric\n\nfor text segmentation. Computational Linguistics, 28(1):19–36.\n\nRetkowski, F. and Waibel, A. (2024). From text segmentation to smart chaptering: A\nnovel benchmark for structuring video transcriptions. In Proceedings of the 18th Con-\nference of the European Chapter of the Association for Computational Linguistics (Vo-\nlume 1: Long Papers), pages 406–419.\n\nSouza, F., Nogueira, R., and Lotufo, R. (2023). Bert models for brazilian portuguese: Pre-\ntraining, evaluation and tokenization analysis. Applied Soft Computing, 149:110901.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L.,\n\nand Polosukhin, I. (2023). Attention is all you need.\n\nYu, H., Deng, C., Zhang, Q., Liu, J., Chen, Q., and Wang, W. (2023). Improving long\ndocument topic segmentation models with enhanced coherence modeling. In Bouamor,\nH., Pino, J., and Bali, K., editors, Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages 5592–5605, Singapore. Association\nfor Computational Linguistics.\n\n\f"
        },
        {
            "titulo": "Synthetic AI Data Pipeline for Domain-Specific Speech-to-Text Solutions",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31114",
            "idioma": "Inglês",
            "storage_key": "files/article_31114_30917.pdf",
            "autores": [
                {
                    "nome": "Anderson Luiz Karl",
                    "afiliacao": "Audo Tecnologia e Saúde",
                    "orcid": "http://orcid.org/0009-0005-4989-8426"
                },
                {
                    "nome": "Guilherme Sales Fernandes",
                    "afiliacao": "Audo Tecnologia e Saúde",
                    "orcid": "https://orcid.org/0009-0001-8100-0074"
                },
                {
                    "nome": "Leonardo Augusto Pires",
                    "afiliacao": "Audo Tecnologia e Saúde",
                    "orcid": "https://orcid.org/0000-0002-8265-5021"
                },
                {
                    "nome": "Yvens R. Serpa",
                    "afiliacao": "Audo Tecnologia e Saúde / Saxion University of Applied Sciences",
                    "orcid": "https://orcid.org/0000-0002-4799-3180"
                },
                {
                    "nome": "Carlos Caminha",
                    "afiliacao": "UFC",
                    "orcid": "https://orcid.org/0009-0000-5788-6680"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "In this article, we propose a pipeline to fine-tune domain-specific Speech-to-Text (STT) models using synthetic data generated by Artificial Intelligence (AI). Our methodology eliminates the need for manually labelled audio data, which is expensive and difficult to obtain, by generating domain-specific data with a Large Language Model (LLM) combined with multiple Text-to-Speech (TTS) solutions. We applied our pipeline to the radiology domain and compared the results with different approaches based on the availability of domain-specific data, varying from the total absence of domain-specific data to the use of only domain-specific high-quality data (ground truth). Our performance improved the accuracy of the baseline by 40.19% and 10.63% for the WhisperX Tiny and Small models, respectively, which, although performed worse than the results from using the ground truth, shows that it is possible to achieve good results with minimal cost and effort. Finally, the result analysis shows a good insight into the amount of action necessary to achieve good results based on the availability of real data.",
            "keywords": [
                "Large Language Models",
                "Text-to-speech",
                "Speech-to-text",
                "Domain-Specific",
                "Model Fine-tuning"
            ],
            "referencias": [
                "Bain, M., Huh, J., Han, T., and Zisserman, A. (2023). Whisperx: Time-accurate speech transcription of long-form audio. INTERSPEECH 2023.",
                "Chan, W., Jaitly, N., Le, Q., and Vinyals, O. (2016). Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), page 4960–4964. IEEE Press.",
                "da Cruz, F. B., de Souza Britto, M. C., Moreira, G. M., and Junior, A. d. S. B. (2022). Robôs substituem juízes? o estado da arte da inteligência artificial no judiciário brasileiro. Revista Antinomias, 3(1):8–41.",
                "Gontier, F., Serizel, R., and Cerisara, C. (2021). Automated audio captioning by finetuning bart with audioset tags. In DCASE 2021-6th Workshop on Detection and Classification of Acoustic Scenes and Events.",
                "Johnson, M., Lapkin, S., Long, V., Sanchez, P., Suominen, H., Basilakis, J., and Dawson, L. (2014). A systematic review of speech recognition technology in health care. BMC Med. Inform. Decis. Mak., 14(1):94.",
                "Koenecke, A., Choi, A. S. G., Mei, K. X., Schellmann, H., and Sloane, M. (2024). Careless whisper: Speech-to-text hallucination harms. In The 2024 ACM Conference on Fairness, Accountability, and Transparency, pages 1672–1681.",
                "Kumar, Y. (2024). A comprehensive analysis of speech recognition systems in healthcare: Current research challenges and future prospects. SN Computer Science, 5.",
                "Silva, M. d. L. M., Mendonça, A. L. C., Neto, E. R. D., Chaves, I. C., Caminha, C., Brito, F. T., Farias, V. A. E., and Machado, J. C. (2024). Facto dataset: A dataset of user reports for faulty computer components. In Anais do VI Dataset Showcase Workshop, pages 1–12. SBC.",
                "Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems.",
                "Yu, D., Deng, L., and Dahl, G. (2010). Roles of pre-training and fine-tuning in contextdependent dbn-hmms for real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning. sn."
            ],
            "artigo_completo": "Synthetic AI Data Pipeline for Domain-Specific Speech-to-Text\nSolutions\n\nAnderson Luiz Karl 1, Guilherme Sales Fernandes1, Leonardo Augusto Pires 1,\nYvens R. Serpa 1,3, Carlos Caminha 2\n\n11Audo Tecnologia e Sa´ude\n2Universidade Federal do Cear´a, UFC, Brasil\n3Saxion University of Applied Sciences, Enschede, Netherlands\n\nAbstract. In this article, we propose a pipeline to fine-tune domain-specific\nSpeech-to-Text (STT) models using synthetic data generated by Artificial In-\ntelligence (AI). Our methodology eliminates the need for manually labelled\naudio data, which is expensive and difficult to obtain, by generating domain-\nspecific data with a Large Language Model (LLM) combined with multiple Text-\nto-Speech (TTS) solutions. We applied our pipeline to the radiology domain\nand compared the results with different approaches based on the availability\nof domain-specific data, varying from the total absence of domain-specific data\nto the use of only domain-specific high-quality data (ground truth). Our per-\nformance improved the accuracy of the baseline by 40.19% and 10.63% for\nthe WhisperX Tiny and Small models, respectively, which, although performed\nworse than the results from using the ground truth, shows that it is possible\nto achieve good results with minimal cost and effort. Finally, the result analysis\nshows a good insight into the amount of action necessary to achieve good results\nbased on the availability of real data.\n\n1. Introduction\nAutomatic audio transcription, commonly referred to as Speech-to-Text (STT), has been\na common practice for many work fields, such as health, justice, education, and business\n[Kumar 2024]. However, precision in recognizing and transcribing language is impor-\ntant to guarantee the correct and efficient use of the transcribed information. That is\nespecially important in domain-specific applications, in which the use of technical terms\nand jargon increases the recognition and transcription challenge [Suh et al. 2024]. How-\never, many of the typically available solutions for this problem are built on generic data.\nDue to that, their results are of lower quality when used in domain-specific scenarios\n[Chan et al. 2016].\n\nA common approach to solving this issue is to build and refine solutions using\ndomain-related contexts, vocabularies and other types of data [Huang et al. 2020]. Nowa-\ndays, it is standard to use generic AI models as the base for STT solutions and fine-tune\nthese models with domain-specific data [Mak et al. 2024]. However, the fine-tuning pro-\ncess is expensive and requires a significant amount of data and effort [Hu et al. 2022]. For\nmedical applications, for example, it is necessary to collect sensitive data, have health pro-\nfessionals check, correct and validate it, and guarantee its privacy and security in regard\nto the involved patients and personnel [Johnson et al. 2014].\n\nNevertheless, the need for high-quality STT solutions is evident in many work\nsectors. In Radiology, for example, it is a common practice to have physicians use STT\n\n\ftools in their work practice to increase productivity over traditional transcription, the lat-\nter in which the professional records a report via voice to be later transcribed manually\nby another professional (usually without a medical background) [Hammana et al. 2015].\nAny errors or delays in this process may result in possible harm and consequences to\nthe patients and their treatments [Vorbeck et al. 2000]. Another common example is\ncourts and judicial procedures, in which a large quantity of domain-specific texts is gen-\nerated and often transcribed manually, resulting in expensive and inefficient processes\n[da Cruz et al. 2022].\n\nIn this context, this work proposes a low-cost pipeline for the training and fine-\ntuning of STT AI models when domain-specific data is required but not readily available.\nOur pipeline is based on the use of AI models to generate synthetic domain-specific data.\nFor that, we have used a Large Language Model (LLM) to produce domain-specific con-\ntent that simulates real use cases. Specifically for this work, we have explored the radiol-\nogy domain, generating data for synthetic radiology reports using an LLM and a specific\nprompting approach. The synthetic data is then converted into audio files through Text-\nto-Speech (TTS) tools. Thus, the fine-tuning process is done entirely using synthetic data\ngenerated via AI. Additionally, due to the focus on being a low-cost solution, the results\nof this work were done by using inexpensive or freely available solutions. Simultane-\nously, this work also presents a comparison analysis of a range of possible final results\ndepending on the availability of domain-specific data.\n\n2. Related Work\n\nAutomatic audio transcription has been a fruitful research field in computer sciences over\nmany years [Yu et al. 2010, Blackley et al. 2019]. Many of the traditional works in this\nfield are focused on the inherited challenges of it, such as handling language subtleties,\nstructure, and fluency [Gontier et al. 2021], and the limitations on the access of adequate\ndatasets [Hu et al. 2022]. These challenges increase when dealing with domain-specific\nscenarios [Samarakoon et al. 2018].\n\nIn regards to datasets, the majority of works in the field use datasets in the\nEnglish language [Casanova et al. 2022]. When working in scenarios with other lan-\nguages, researchers must not only solve the recurrent STT challenges but also adapt\ntheir solutions, such as done by Gruzitis et al.\n[Gruzitis et al. 2022] which adapted\ntheir models to the Latvian language, and the work of Vivancos-Vincente et al.\n[Vivancos-Vicente et al. 2016] for Spanish and Portuguese. Alternatively, the work pro-\nposed by Casanova et al. [Casanova et al. 2022] shows an alternative to training models\nfor different languages based on data augmentation from only one speaker for the targeted\nlanguage, using cross-lingual voice conversion and multi-speaker TTS techniques.\n\nMoreover, access to good domain-specific datasets is a challenge, and its produc-\ntion involves high costs with domain experts, data analysis, and validation. This prob-\nlem is often faced with the use of synthetic data [Li et al. 2018, Rosenberg et al. 2019,\nLaptev et al. 2020, Huang et al. 2020, Yang et al. 2023]. However, synthetic data is fre-\nquently distant from real use cases due to the absence of mistakes and imperfections\nthat are often common in human-made data, which makes it “too perfect” compared to\nreal-world cases. This“perfection problem” is handled with the introduction of synthetic\nerrors and imperfections, such as done by the Synt++ solution proposed by Hu et al.\n\n\f[Hu et al. 2022], in which noise and random artefacts are introduced to the synthetic data\ngeneration so it more closely resembles real-life data.\n\nOnly recently the process of data synthesis using LLM have been explored, such\nas the work presented by V´asquez-Correa et al. [V´asquez-Correa et al. 2023], which gen-\nerates domain-specific synthetic data through prompting to fine-tune an STT solution for\nthe English, Spanish, and Basque languages. Silva et al. [Silva et al. 2024] also uses an\nLLM to generate synthetic data for a hardware failure prediction dataset. Their dataset\nwas generated from problem categories and reports from major component manufacturers\nin the market.\n\nSimilarly, this work proposes a new approach to synthetic data based on prompt-\ning. The synthetic data is then converted into audio files through TTS algorithms and used\nto fine-tune a generic STT AI model. Our approach uses a simple and low-cost generic\nSTT AI model as a means to prove its usefulness in scenarios with minimal resources.\nMoreover, this work presents a comparison analysis of results based on the availability of\ndomain-specific data, varying from the total absence of domain-specific data (our solu-\ntion) to the use of only domain-specific high-quality data (an ideal solution).\n\n3. Methodology\n3.1. Datasets\nTo validate the efficiency of our proposed pipeline, we used a dataset of manually labelled\naudio data from radiology professionals, which was divided into a set for training and\nanother for testing. The training set included 98 audio files from two cisgender male\nradiologists with a total duration of 1 hour, 10 minutes and 8 seconds of audio. The\ntesting dataset consisted of 82 audio files from the same two radiologists, with a total\nduration of 1 hour, 4 minutes and 21 seconds of audio. Both training and testing sets had\nan equal amount of audio files for the two radiologists, and all audio files were spoken in\nPortuguese. All audio files were recorded in real-world scenarios, including background\nnoise from the respective workplaces, audio artefacts, and other common issues. This\ndataset constitutes our ground truth dataset, which was used to compare with the results\nfrom the other approaches explored.\n\n3.2. Methods and Technologies\nThe transformers library by Hugging Faces [Vaswani 2017] was used to fine-tune the STT\nmodel, which was also configured for the Portuguese language. We opted for a traditional\nfine-tuning process using all of the available weights. For the inference, we have used the\nWhisperX model [Bain et al. 2023], which offers a quicker and more precise transcrip-\ntion, with the Ctranslate2 backend for better compatibility and reduced inference time.\nThe main reason for using WhisperX was the presence of an internal Voice Activity De-\ntection (VAD), which considerably reduces the hallucination tendencies and optimizes the\nuse of VRAM [Koenecke et al. 2024].\n\nWe have used GPT-4o as the LLM to generate synthetic domain-specific radiology\nreports using a specific approach and prompts [Islam and Moushi 2024]. The synthetic\nreports were fed into TTS solutions to generate audio files for the fine-tuning process.\n\nAs TTS solutions, we have used the ElevenLabs solution1, which is fairly low cost\n\n1https://elevenlabs.io/\n\n\ffor its quality, and the Google Text-to-Speech2. Both tools allowed for a variety of into-\nnations, speech styles, and variations, which helped to reduce the “perfection problem”\noften produced in synthetic data. Furthermore, the use of two TTS solutions improved the\nrepresentation and diversity of speech patterns and accents.\n\n3.3. Metrics\n\nThe Word Error Rate (WER) metric was used to assess the precision of the STT solutions\n[Ali and Renals 2018]. The WER metric is calculated by the ratio between the number of\ntranscribed errors and the number of words originally spoken. These errors are classified\nas Substitutions (S), Insertions (I), and Deletions (D). The WER formula we used was:\nWER = S+D+I\n\nN , where N is the number of words originally spoken.\n\n4. Results\n\n4.1. Proposed Pipeline\n\nFigure 1. Proposed Pipeline.\n\nAs shown in Figure 1, the proposed pipeline aims to fine-tune an STT model using\na set of synthetic domain-specific data. It starts with a specialist prompt for the LLM. This\nspecialist prompt must consider specific terminology and domain-specific information to\nguarantee that the synthetic data closely resembles real-life data.\n\nThe LLM-generated synthetic data is fed into TTS solutions and converted into\naudio files. It is important to include variations in tone of voice and synthetic noise in this\nprocess to reduce the “perfection problem”. Together, the LLM-generated synthetic data\nand its audio representation compose the AI-labelled dataset. This dataset is then used to\nfine-tune the STT model of choice.\n\n4.2. AI-Labelled Dataset\n\nGPT-4o was used as the LLM tool for the domain-specific synthetic data generation. For\nthat, we first introduced the model to the radiology context and gave it a series of radiology\nspecialities and exam types, such as computer tomography and radiography. Furthermore,\nto guarantee typical report-style phrasing, we instructed the LLM to create phrases and\nsentences in a progressive format, starting from normal descriptions, followed by potential\n\n2https://cloud.google.com/text-to-speech\n\nPromptDomain-Specfic \nTextSynthetic AI \nDatasetFine-Tuned\nSpeech-to-TextDomain-Specfic \nAudioText-to-Speech\nSolution(s)\ffindings and specific diagnostics for those. Finally, the LLM was instructed not to include\nabbreviations and to provide the results in a JSON format without additional text. The\nprompt used can be seen in Figure 2.\n\nYou must generate {number of phrases} phrases in Portuguese that could be present in a {type of report} report\nmade by a physician expert on a specific medical field you will be given as input. Generate the phrases and sentences\nfollowing a logical chain of thought, starting from regular cases and progressing to possible findings and specific diagnostics\nrelated to the given context. Explore multiple phrase types, ranging from basic descriptions to detailed conclusions. Avoid\nusing abbreviations, and every time you need to mention a specific term, use it in its most complete form (for example, use\ncentimetres instead of cm and beats per minute instead of bpm).\nFormat the output: return a JSON object with the phrase list. Do not include any additional text before and after the JSON.\nJSON output example:\n{\n\n\"phrases\": [\n\n\"O paciente apresenta ritmo card´ıaco regular, com 72 batimentos por minuto.\",\n\"A imagem mostra um aumento moderado no tamanho do ventr´ıculo esquerdo.\",\n\"N˜ao h´a evidˆencias de derrame pleural ou ascite.\"\n\n]\n\n}\n\nOutput only the JSON with the {number of phrases} phrases without additional texts.\n\nFigure 2. Prompt used to generate domain-specific radiology texts. The example\nphrases and sentences are written in Portuguese to exemplify better the input we\nused.\n\nAs previously mentioned, we have used two TTS tools for the synthetic audio\ngeneration: ElevenLabs and Google Text-to-Speech. The use of both tools is meant to di-\nversify the generated data with varying speaking patterns, rhythm, intonation and quality.\n\nWe generated 46 minutes and 43 seconds of audio using ElevenLabs in a total of\n980 files. These files were equally split into five different male voices. As for the Google\nText-to-Speech, we generated 58 minutes and 55 seconds of audio, again, in a total of 980\nfiles, using only one male voice available. The dataset for the synthetically generated data\nis available in a GitHub repository3.\n\nFigure 3 (a) and (b) shows the audio length distribution for the synthetic dataset\ncompared to the real, manually labelled data we had. As seen, the overall distribution\nis quite similar, while the synthetic data tends to be shorter, resulting in more files. The\nword cloud in Portuguese for both datasets can be seen in Figure 3 (c) and (d). Both\ndatasets show domain-specific terms, with a greater presence of punctuation terms (com-\nmas, dots, etc) on the real dataset. Alternatively, the synthetic dataset has a higher pres-\nence of phrases such as “N˜ao h´a” or “H´a sinais” (meaning “There is no” and “There are\nsigns of,” respectively in English), showing a tendency to repeat phrase structures with\nthe same starting terms. The distribution of terms and times between TTS tools is fairly\nsimilar.\n\n4.3. Analisys\n\nFigure 4 shows the results for the WER metric for four different scenarios: a base-\nline (WhispherX without fine-tuning); WhispherX fine-tuned using the synthetic data;\nWhispherX using synthetic audio data generated from real radiology reports; WhispherX\n\n3https://github.com/AtkLLM/AI-DrivenSpeechModel-Dataset\n\n\fFigure 3. Histograms of audio length used: (a) histogram of the duration of man-\nually labelled audio files used for training; (b) histogram of the duration of AI-\ngenerated audio files; (c) Word Cloud from Real Data; (d) Word Cloud from Syn-\nthetic Data;\n\nfine-tuned using the ground truth (ideal scenario). The results are shown for both the\nWhisperX Small and Tiny versions, its smallest and more easily accessible forms.\n\nIn both cases, for the WhisperX Tiny and Small versions, the behaviour was fairly\nsimilar, with the Tiny version having considerably higher values for the WER compared\nto the Small version. The baseline, as expected, presented the highest WER value (104.13\nand 37.35), while the ground truth version achieved the lowest one (40.11 and 23.11). It is\nworth mentioning that the ground truth Tiny version while performing significantly worse,\nachieved a similar WER when compared to the baseline Small version (40.11 compared\nto 37.35).\n\nIt is worth noting that the high WER value from the baseline WhisperX Tiny is\nthe result of hallucination, which made the model include words that were not present in\nits results. That made the WER value rise above 100%.\n\nOur proposed pipeline, which used only synthetic data for the fine-tuning process,\nachieved a WER of 62.28 and 33.38 for the Tiny and Small versions, respectively, which\ncorrespond to an improvement of 41.85% and 10.62%. These results were achieved us-\ning only the audio files generated by ElevenLabs, which performed better than the ones\ngenerated by the Google-TTS tool (WER equal to 70.96 and 33.94 for the Tiny and Small\nversions) and by a combination of both ElevenLabs and Google-TTS (WER equal to 66.75\n\n0102030Length(seconds)020406080100FrequencyAudiolengthhistogram-Real246Length(seconds)050100150Frequency(b)Audiolengthhistogram-Synthetic(c)WordCloud-Real(d)WordCloud-Synthetic\fand 33.94 for the Tiny and Small versions).\n\nTo exemplify a case in which there are some real-use data for the fine-tuning, we\nhave tested using only real-case radiology reports (ignoring the LLM step) and producing\nthe audio data from them using the same TTS tools mentioned previously. This new data\nwas used to fine-tune both WhisperX Tiny and Small versions, achieving the WER of\n56.24 and 30.76, respectively, which are 45.99% and 17.64% better than the baseline. For\nthese results, we have only used audio data generated by ElevenLabs since it achieved\nbetter results in previous tests.\n\nOur results show that it is possible to achieve better outcomes by using a com-\npletely synthetic approach. While it still performs worse compared to approaches with\nreal-data approaches, it shows a promising approach that has plenty of room for experi-\nmentation and improvement and incurs a very low cost compared to generating a dataset\nwith real data.\n\nFigure 4. Results from the four approaches using both WhisperX Small and Tiny\nmodels. The WER metric is shown on the y-axis.\n\nThe ground truth results are, as expected, the best results with the lowest WER\nvalues for both models. However, it is also the most expensive approach with its caveats\nand challenges. Moreover, it is not unlikely that its best results are a consequence of some\nlevel of overfitting since the training and test data come from the same physicians using\nthe same equipment in the same environments. On the other hand, the synthetic dataset\nwas composed of a wider variety of voices and intonations that, while similar to the real\nones in terms of context and intonation, are still fairly different. On that, the wider range\nof possible voices from the ElevenLabs tool might explain why it performed better than\nthe Google-TTS tool. From our experiments, the Google-TTS tool tends to generate very\nclean and “perfect” robot-like audio files that are remote from real-use cases.\n\n5. Conclusion\n\nThis work presented a pipeline for fine-tuning domain-specific STT solutions using syn-\nthetic data produced by a combination of LLM prompting and TTS tools. Our proposed\npipeline produces good-quality synthetic data and overcomes the “perfect problem” by\nusing TTS tools for a wider range of voices, intonation, and rhythm. Our findings show\nthat our pipeline improves the results compared to a non-fine-tuned solution.\n\nGiven the results, we can also make assumptions based on the availability of real\ndomain-specific data. As Figure 4 shows, and as expected, the more real data used, the\n\n\fbetter the results. Yet, the difference between the use of some real data (using real-case\nreports data with TTS for audio generation) and 100% synthetic data is not significant\n(about 10% improvement for the Tiny model, and 8% improvement for the Small model,\nwhen comparing both approaches), indicating that in some cases, the synthetic-only ap-\nproach might provide good enough results. Nevertheless, it is worth spending resources\nacquiring domain-specific knowledge and data, especially to produce a specialist LLM\nprompt required by our approach, but it will not necessarily reflect a significant improve-\nment over the synthetic data.\n\nOur choice of using WhisperX Tiny and Small models is focused on providing\na low-cost solution for domain-specific scenarios. Higher WhisperX models are likely\nto provide better results, but they require expensive hardware and more resources for\ntraining. Besides that, higher models would require higher costs to host online for a\nproduction-ready solution. Considering our scenario, considerable investment would be\nrequired to host such a strategy for a single hospital with multiple simultaneous physicians\nworking at the same time daily. Yet, our results indicate that, with the use of a ground truth\ndataset, it might be possible to improve a simpler model through fine-tuning to perform\nas well as a baseline better model, as we saw with the results from the ground truth fine-\ntuned Tiny model compared to the baseline Small model. In our preliminary tests, we\nfound that the baseline WhisperX Medium model has a WER of 28.85, which is slightly\nhigher than the ground truth fine-tuned WhisperX Small model we presented (23.11).\n\nBesides operational costs, the complexity of the AI model used impacts its infer-\nence time (the time it takes to generate the output given the input). Simpler models, such\nas Tiny and Small, have a relative inference time significantly smaller than larger models\n[Bain et al. 2023]. For real-time settings, this is of major importance, such as the one\nexplored in this study for radiology STT solutions.\n\nAs future work, our pipeline could be assessed for other domain-specific contexts,\nas well as more experimentation on the synthetic data variation that further approaches\nreal-case scenarios, including the use of different accents, acoustic conditions, and back-\nground noise. The use of a more diverse ground truth set might also provide better insight\ninto possible overfitting and more realistic results for fine-tuned models trained with it.\nIt is not unlikely that a production-ready solution achieves a WER value closer to the\nresults from our approaches than the current ground truth ones. Besides that, a longer\naudio ground truth dataset could surely provide better insights into our results since it was\nlimited to a little over 1 hour long due to budget and time constraints.\n\nFinally, a more fine-grained analysis of the balance between synthetic and real data\ncould provide further insight into how much effort is needed to create hybrid approaches\nthat more closely resemble real data, including the use of real audio instead of purely\nrelying on TTS Tools. That might provide a great approach for fine-tuning STT models\nwith a fraction of the usual associated costs when using high-quality ground truth sets.\n\n6. Acknowledgements\n\nThe authors would like to thank the Brazilian Agency FUNCAP-CE for its financial sup-\nport under the project NUP 31052.001303/2023-62. We would also like to thank Raiza\nVaz for her help in building the ground truth database.\n\n\fReferences\n\nAli, A. and Renals, S. (2018). Word error rate estimation for speech recognition: e-\nwer. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 20–24.\n\nBain, M., Huh, J., Han, T., and Zisserman, A. (2023). Whisperx: Time-accurate speech\n\ntranscription of long-form audio. INTERSPEECH 2023.\n\nBlackley, S. V., Huynh, J., Wang, L., Korach, Z., and Zhou, L. (2019). Speech recognition\nfor clinical documentation from 1990 to 2018: a systematic review. Journal of the\nAmerican Medical Informatics Association, 26(4):324–338.\n\nCasanova, E., Shulby, C., Korolev, A., Junior, A. C., Soares, A. d. S., Alu´ısio, S.,\nand Ponti, M. A. (2022). Asr data augmentation in low-resource settings using\ncross-lingual multi-speaker tts and cross-lingual voice conversion. arXiv preprint\narXiv:2204.00618.\n\nChan, W., Jaitly, N., Le, Q., and Vinyals, O. (2016). Listen, attend and spell: A neural\nIn 2016 IEEE In-\nnetwork for large vocabulary conversational speech recognition.\nternational Conference on Acoustics, Speech and Signal Processing (ICASSP), page\n4960–4964. IEEE Press.\n\nda Cruz, F. B., de Souza Britto, M. C., Moreira, G. M., and Junior, A. d. S. B. (2022).\nRobˆos substituem ju´ızes? o estado da arte da inteligˆencia artificial no judici´ario\nbrasileiro. Revista Antinomias, 3(1):8–41.\n\nGontier, F., Serizel, R., and Cerisara, C. (2021). Automated audio captioning by fine-\ntuning bart with audioset tags. In DCASE 2021-6th Workshop on Detection and Clas-\nsification of Acoustic Scenes and Events.\n\nGruzitis, N., Dargis, R., Lasmanis, V. J., Garkaje, G., and Gosko, D. (2022). Adapting\nautomatic speech recognition to the radiology domain for a less-resourced language:\nIn Intelligent Sustainable Systems: Selected Papers of WorldS4\nthe case of latvian.\n2021, Volume 1, pages 267–276. Springer.\n\nHammana, I., Lepanto, L., Poder, T., Bellemare, C., and Ly, M.-S. (2015). Speech recog-\nnition in the radiology department: a systematic review. Health Information Manage-\nment Journal, 44(2):4–10.\n\nHu, T.-Y., Armandpour, M., Shrivastava, A., Chang, J.-H. R., Koppula, H., and Tuzel, O.\n(2022). Synt++: Utilizing imperfect synthetic data to improve speech recognition. In\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 7682–7686. IEEE.\n\nHuang, Y., He, L., Wei, W., Gale, W., Li, J., and Gong, Y. (2020). Using personal-\nized speech synthesis and neural language generator for rapid speaker adaptation. In\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 7399–7403. IEEE.\n\nIslam, R. and Moushi, O. M. (2024). Gpt-4o: The cutting-edge advancement in multi-\n\nmodal llm. Authorea Preprints.\n\n\fJohnson, M., Lapkin, S., Long, V., Sanchez, P., Suominen, H., Basilakis, J., and Dawson,\nL. (2014). A systematic review of speech recognition technology in health care. BMC\nMed. Inform. Decis. Mak., 14(1):94.\n\nKoenecke, A., Choi, A. S. G., Mei, K. X., Schellmann, H., and Sloane, M. (2024). Care-\nless whisper: Speech-to-text hallucination harms. In The 2024 ACM Conference on\nFairness, Accountability, and Transparency, pages 1672–1681.\n\nKumar, Y. (2024). A comprehensive analysis of speech recognition systems in healthcare:\n\nCurrent research challenges and future prospects. SN Computer Science, 5.\n\nLaptev, A., Korostik, R., Svischev, A., Andrusenko, A., Medennikov, I., and Rybin, S.\n(2020). You do not need more data: Improving end-to-end speech recognition by text-\nto-speech data augmentation. In 2020 13th International Congress on Image and Sig-\nnal Processing, BioMedical Engineering and Informatics (CISP-BMEI), pages 439–\n444. IEEE.\n\nLi, J., Gadde, R., Ginsburg, B., and Lavrukhin, V. (2018). Training neural speech recog-\nnition systems with synthetic speech augmentation. arXiv preprint arXiv:1811.00707.\n\nMak, F., Govender, A., and Badenhorst, J. (2024). Exploring asr fine-tuning on limited\ndomain-specific data for low-resource languages. Journal of the Digital Humanities\nAssociation of Southern Africa (DHASA), 5.\n\nRosenberg, A., Zhang, Y., Ramabhadran, B., Jia, Y., Moreno, P., Wu, Y., and Wu, Z.\n(2019). Speech recognition with augmented synthesized speech. In 2019 IEEE Au-\ntomatic Speech Recognition and Understanding Workshop (ASRU), pages 996–1002.\nIEEE.\n\nSamarakoon, L., Mak, B., and Lam, A. Y. (2018). Domain adaptation of end-to-end\nspeech recognition in low-resource settings. In 2018 IEEE Spoken Language Technol-\nogy Workshop (SLT), pages 382–388. IEEE.\n\nSilva, M. d. L. M., Mendonc¸a, A. L. C., Neto, E. R. D., Chaves, I. C., Caminha, C., Brito,\nF. T., Farias, V. A. E., and Machado, J. C. (2024). Facto dataset: A dataset of user\nreports for faulty computer components. In Anais do VI Dataset Showcase Workshop,\npages 1–12. SBC.\n\nSuh, J., Na, I., and Jung, W. (2024). Improving domain-specific asr with llm-generated\n\ncontextual descriptions.\n\nV´asquez-Correa, J. C., Arzelus, H., Martin-Do˜nas, J. M., Arellano, J., Gonzalez-Docasal,\nA., and ´Alvarez, A. (2023). When whisper meets tts: Domain adaptation using only\nIn International Conference on Text, Speech, and Dialogue,\nsynthetic speech data.\npages 226–238. Springer.\n\nVaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing\n\nSystems.\n\nVivancos-Vicente, P. J., Castej´on-Garrido, J. S., Paredes-Valverde, M. A., Salas-Z´arate,\nM. d. P., and Valencia-Garc´ıa, R. (2016). Ixhealth: A multilingual platform for ad-\nIn Technologies and Innovation: Second\nvanced speech recognition in healthcare.\nInternational Conference, CITI 2016, Guayaquil, Ecuador, November 23-25, 2016,\nProceedings 2, pages 26–38. Springer.\n\n\fVorbeck, F., Ba-Ssalamah, A., Kettenbach, J., and Huebsch, P. (2000). Report generation\nusing digital speech recognition in radiology. European Radiology, 10:1976–1982.\n\nYang, K., Hu, T.-Y., Chang, J.-H. R., Koppula, H. S., and Tuzel, O. (2023). Text is all\nyou need: Personalizing asr models using controllable speech synthesis. In ICASSP\n2023-2023 IEEE International Conference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 1–5. IEEE.\n\nYu, D., Deng, L., and Dahl, G. (2010). Roles of pre-training and fine-tuning in context-\ndependent dbn-hmms for real-world speech recognition. In Proc. NIPS Workshop on\nDeep Learning and Unsupervised Feature Learning. sn.\n\n\f"
        },
        {
            "titulo": "Evaluating Federated Learning with Homomorphic Encryption for Medical Named Entity Recognition Using Compact BERT Models",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31115",
            "idioma": "Inglês",
            "storage_key": "files/article_31115_30918.pdf",
            "autores": [
                {
                    "nome": "Marcos F. Pontes",
                    "afiliacao": "UFOP",
                    "orcid": "http://orcid.org/0000-0001-8721-0171"
                },
                {
                    "nome": "Rodrigo C. Pedrosa",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0003-2547-3835"
                },
                {
                    "nome": "Pedro H. Lopes",
                    "afiliacao": "UFOP",
                    "orcid": null
                },
                {
                    "nome": "Eduardo J. Luz",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0001-5249-1559"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Medical Named Entity Recognition (NER) identifies and categorizes medical entities from unstructured texts, crucial for health monitoring tasks. Despite advancements with Large Language Models (LLMs), medical NER faces challenges due to limited and dispersed labeled data across institutions, protected under privacy regulations. Federated Learning (FL) offers a solution by enabling decentralized model training while preserving data privacy, but it is vulnerable to byzantine attacks. This research proposes a simple and secure FL protocol using Homomorphic Encryption (HE), called FedHE, that removes the need of trust between the federations and the training coordinator. Encrypted FL imposes significant constraints regarding resources consumption and performance, making the state-of-the-art language models impractical. This research aims to assess how well compact BERT representations work in federated medical NER tasks in comparison to the state-of-the-art approaches. The results showed that compact BERT representations, such as BERTmini are competitive with the state-of-the-art, and are feasible to use in FedHE. However, resource consumption overheads remain a challenge, particularly when the number of clients increase.",
            "keywords": [
                "Cryptography",
                "Federated Learning",
                "Homomorphic Encryption",
                "Named Entity Recognition",
                "BERT"
            ],
            "referencias": [
                "Al Badawi, A. and Polyakov, Y. (2023). Demystifying bootstrapping in fully homomorphic encryption. Cryptology ePrint Archive.",
                "Beutel, D. J., Topal, T., Mathur, A., Qiu, X., Fernandez-Marques, J., Gao, Y., Sani, L., Li, K. H., Parcollet, T., de Gusmao, P. P. B., et al. (2020). Flower: A friendly federated ˜ learning research framework. arXiv preprint arXiv:2007.14390.",
                "Konečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., and Bacon, D. (2016). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492.",
                "Marcolla, C., Sucasas, V., Manzano, M., Bassoli, R., Fitzek, F. H., and Aaraj, N. (2022). Survey on fully homomorphic encryption, theory, and applications. Proceedings of the IEEE, 110(10):1572–1609.",
                "Peng, L., Luo, G., Zhou, S., Chen, J., Xu, Z., Sun, J., and Zhang, R. (2024). An indepth evaluation of federated learning on biomedical natural language processing for information extraction. npj Digital Medicine, 7(1):127.",
                "Tang, B., Cao, H., Wu, Y., Jiang, M., and Xu, H. (2013). Recognizing clinical entities in hospital discharge summaries using structural support vector machines with word representation features. In BMC medical informatics and decision making, volume 13, pages 1–10. Springer",
                "Yi, X., Paulet, R., Bertino, E., Yi, X., Paulet, R., and Bertino, E. (2014). Homomorphic encryption. Springer.",
                "Zhu, L., Liu, Z., and Han, S. (2019). Deep leakage from gradients. Advances in neural information processing systems, 32."
            ],
            "artigo_completo": "Evaluating Federated Learning with Homomorphic\nEncryption for Medical Named Entity Recognition Using\nCompact BERT Models\n\nMarcos F. Pontes 1, Rodrigo C. Pedrosa 1, Pedro H. Lopes 1, Eduardo J. Luz, 1\n\n1 Departamento de Computac¸ ˜ao – Universidade Federal de Ouro Preto (UFOP)\n\nmarcos.rezende@aluno.ufop.edu.br,{rodrigo.silva,silvap,eduluz}@ufop.edu.br\n\nAbstract. Medical Named Entity Recognition (NER) identifies and categorizes\nmedical entities from unstructured texts, crucial for health monitoring tasks. De-\nspite advancements with Large Language Models (LLMs), medical NER faces\nchallenges due to limited and dispersed labeled data across institutions, pro-\ntected under privacy regulations. Federated Learning (FL) offers a solution by\nenabling decentralized model training while preserving data privacy, but it is\nvulnerable to byzantine attacks. This research proposes a simple and secure FL\nprotocol using Homomorphic Encryption (HE), called FedHE, that removes the\nneed of trust between the federations and the training coordinator. Encrypted\nFL imposes significant constraints regarding resources consumption and perfor-\nmance, making the state-of-the-art language models impractical. This research\naims to assess how well compact BERT representations work in federated med-\nical NER tasks in comparison to the state-of-the-art approaches. The results\nshowed that compact BERT representations, such as BERTmini are competitive\nwith the state-of-the-art, and are feasible to use in FedHE. However, resource\nconsumption overheads remain a challenge, particularly when the number of\nclients increase.\n\n1. Introduction\n\nMedical named entity recognition (NER) aims to identify medical entities (e.g., drug\nnames, adverse reactions and symptoms) from unstructured medical texts and classify\nthem into different categories. It can be used in many intelligent healthcare tasks such\nas pharmacovigilance and health monitoring [Tang et al. 2013]. With the recent advance-\nments in the field [Peng et al. 2024] the problem of NER has seen significant improve-\nments. However, in the specific context of medical NER, there are significant challenges\nin the learning process due to the sensitive nature of the data. First, the available labeled\ndata of a single healthcare institution might not be representative enough to adjust a NER\nmodel with good predictive accuracy. Second, collaborative training with data sharing\nis frequently impractical considering the regulations prohibitions and the security risks\nassociated to the data sensitiveness and trust between the parties.\n\nTo leverage massively distributed data and enhance model generalizability, feder-\nated learning (FL) was introduced in [Koneˇcn`y et al. 2016] as a novel learning framework.\nIn an FL training loop, clients collaboratively train a shared global model by exchanging\nmodel weights or gradients while keeping their data stored locally. By bringing the model\nto the data, FL avoids data transfer and achieves competitive performance compared to\nmodels trained with pooled data.\n\n\fRecently, [Peng et al. 2024] provided an in-depth evaluation of federated learning\nin biomedical natural language processing, demonstrating that the BlueBERT (BERTblue)\nmodel, particularly its larger variant (BERTlargeblue), trained using FL outperforms both\nits version trained on data from a single client and GPT-4 when applied in a few-shot\nprompt setting. Clearly, FL, combined with variations of BERT, stands out as an effective\napproach for NER.\n\nAlthough FL’s primary focus is on maintaining rigorous privacy protections by\npreventing data sharing, [Zhu et al. 2019] introduced a new vulnerability in the form of\ninference attacks, showing that private training data can be extracted from the publicly\nshared gradients. To mitigate this risk, one approach is to incorporate an encryption step\ninto the federated learning framework. Specifically, employing Homomorphic Encryption\n(HE) [Yi et al. 2014] within FL allows clients to encrypt their gradients, enabling the cen-\ntral coordinator to aggregate model updates directly on ciphertexts, thereby eliminating\nthe need for decryption.\n\nWhile HE is often considered the gold standard for data-in-use encryption, it im-\nposes significant performance overheads in terms of both computation and communica-\ntion. As a result, deploying state-of-the-art natural language models becomes impractical\ndue to the large number of trainable parameters. Therefore, to implement a more secure\nFL system using HE, smaller models must be selected. In this context, this paper ad-\ndresses two key questions: (i) What is the computational cost of applying HE in FL for\nNER applications? (ii) How much predictive accuracy might be sacrificed by choosing a\nmore secure FL+HE approach with a smaller model? The obtained results showed that\ncompact models, like BERTmini, can perform competitively with state-of-the-art NER\nmodels in a FL+HE setting for different corpora. However, resource overheads — par-\nticularly communication bandwidth and memory utilization—continue to pose significant\nchallenges.\n\n2. Federated Learning\nThe prototypical FL setting consists of a central server S and a set of K distributed clients\nC, such that |C| = K, that jointly cooperate to solve a standard supervised learning task.\nEach client c ∈ C has access to it’s own private training set Dc = {xc,i, yc,i}nc\ni=1. The goal\nof FL is to train a global predictive model whose architecture and parameters θ∗ ∈ Rd\nc=1 pcLc(θ; Dc), where\nare shared amongst all the clients and found to minimize minθ\nLc is the local objective and pc ≥ 0 specifies the individual contribution of the client c\nsuch that (cid:80)K\nn , where\nn = (cid:80)K\nc=1 nc.\nThe local objective function Lc usually is defined as the empirical risk calculated\nover the training set Dc sampled from the client’s local data distribution Lc(θ; Dc) =\n1\ni=1 l(θ; (xc,i, yc,i)), where l is an instance-level loss (e.g., cross-entropy loss or\nnc\nsquared error in the case of classification or regression tasks, respectively).\n\nc=1 pc = 1. Two possible configurations for pc are pc = 1\n\nK or pc = nc\n\n(cid:80)nc\n\n(cid:80)K\n\nIn Federated Learning, to generate a global model θ from locally trained models\nwith parameters θc, an aggregation step is necessary to combine the updates from all\nclients. One of the most widely used methods for aggregation is called FedAvg. In each\nround t, clients perform local training steps on their private datasets Dc to minimize their\nrespective objective functions Lc. After completing the local updates, clients send their\n\n\fupdated parameters θ(t)\nupdates by computing a weighted average, typically defined as θ(t+1) = (cid:80)K\nuses this to update the global model for the next training round.\n\nc back to the central server S. The server then aggregates these\nc , and\n\nc=1 pcθ(t)\n\n2.1. Federated Learning with Fully Homomorphic Encryption\n\nHomomorphic Encryption (HE) allows certain computations (e.g., addition) to be per-\nformed directly on ciphertexts, without decrypting them first. The intuitive idea is that a\nthird party can compute data without actually getting to know that data. This problem is\nsolved with key-based encryption where the encryption process preserves algebraic opera-\ntions. For the addition operator, for example, we would have e(k, a)+e(k, b) = e(k, a+b)\nfor an encryption scheme e(., .), encryption key k, and plaintexts a and b. A third party\ncould thus compute the ciphertext of the value of the addition a + b from the ciphertexts\nof a and b , and return this to the owner who could decrypt this to get the computation\nresult on the plaintext [Al Badawi and Polyakov 2023].\n\nIn the context of FL, the viability of HE is particularly constrained when encrypt-\ning local model updates. The use of large language models, such as BERT with 110M\nparameters, becomes nearly infeasible given the bandwidth and computational overhead\nassociated with processing encrypted gradients. This limitation underscores the need for\nmore efficient encryption techniques, model compression strategies, and the adoption of\nmore compact architectures— the latter being the focus of this paper’s assessment\n\n3. Methodology\n\nThis section describes this work’s proposal for making federated medical NER secure\nwith HE. The proposed solution, called FedHE, aims to be conceived as a generic frame-\nwork on how HE can be used in FL, making them compliant with data privacy regulations\nand enabling scenarios such as medical NER to work without the risk of inference attacks.\n\nThe FedHE protocol uses HE encryption to protect the gradients data. Thus, even\nif byzantine attackers compromise the computing server, they don’t have access to the\ninformation of the gradient data from each learning client. In addition, it is impossible for\nbyzantine attackers to use these encrypted gradient data to train shadow models.\n\nIn this work,\n\nthe cryptographic scheme CKKS (Cheon-Kim-Kim-Song)\n[Marcolla et al. 2022] is used to encrypt clients’ gradients preserving the arithmetic oper-\nations of addition and multiplication by a scalar plaintext number. CKKS is an asymmet-\nric cryptographic scheme that requires key pairs, so a key management service (KMS)\nis required. Notice that this work does not aim to detail neither the encryption scheme\nnor the KMS protocol, but we rely on strategies and algorithms publicly defined in the\nliterature.\n\nThe coordinator algorithm orchestrates the federated network (See Algorithm 1).\nUsually, it defines how the protocol work, establish mechanisms to define the architecture,\nguarantee trust between the clients, and aggregate the locally generated gradients.1\n\nThe FedAvg algorithm is executed homomorphically, without decryption of\nclients updates. Additionally, although the KMS strategy is not specified in this paper,\nwe assume the coordinator has only access to the public key.\n\n\fAlgorithm 1: FedHE Coordinator. The K clients are indexed by c ∈ C,\nT is the total of federated learning rounds and L is the loss function. The\ngoal is to obtain θ∗ that minimizes the clients’ loss function.\n\nState: Local model with parameters θi.\nFunction ServerTrain:\n\ninitialize θ(0)\nrequest public key from KMS\nfor each round t = 0, . . . , T do\n\nnumber of clients: m ← max(C · K, 1)\nclient selection: St ← (random set of m clients)\nfor each client c ∈ St in parallel do\n\n∇enc(L(t+1)\n\nc\n\n) ← ClientT rain(c)\n\nend\nhomomorphic FedAvg: ∇enc(L(t+1)) ← (cid:80)K\nfor each client c ∈ C in parallel do\nClientU pdate(c, ∇enc(L(t+1))\n\nc=1\n\nend\n\nend\n\nnc\n\nn ∇enc(L(t+1)\n\nc\n\n)\n\nThe FedHE client training algorithm is where the actual train happens (See Al-\ngorithm 2). Each client trains on their own private training dataset and share only the\nencrypted gradient updates with the coordinator for model aggregation.\n\nAlgorithm 2: FedHE Client. X represents the training samples while Y\nrepresents the training labels. I, ϵ, η represents the number of local epochs,\nthe tolerance and the learning rate, respectively. The goal is to obtain θ∗\nthat minimizes the loss function L.\n\nState: Local model with parameters θi.\nFunction ClientTrain:\n\nrequest public key from KMS\nfor each epoch i = 0, . . . , I do\n\nforward propagation: ˆYi = f orward(X, θi)\ncompute loss: Li = loss(Y, ˆYi)\nif Li <ϵ then\nbreak\n\nend\nelse\n\nback propagation: ∇Li = backprop(X, θi, Li)\ngradients encryption: ∇enc(Li) = encrypt(∇Li, P ublicKey)\nreturn ∇enc(Li)\n\nend\n\nend\n\nFunction ClientUpdate(∇enc(Lagg)):\n\nrequest private key from KMS\ngradients decryption: ∇Lagg = decrypt(∇enc(Lagg), P rivateKey)\nupdate: θi+1 = θi − η∇Lagg\n\n\fIn conclusion, the FedHE 1 protocol is adaptable to a range of model archi-\ntectures and can be seamlessly integrated into established FL platforms like Flower\n[Beutel et al. 2020], TensorFlow Federated2, FATE3, among others.\n\n4. Results\n\nIn this section, we present the key findings of our analysis on FedHE, focusing on two\npractical aspects: (1) the performance of FedHE trained with compact BERT models\ncompared to state-of-the-art models, and (2) the performance and resource consumption\noverheads associated with FedHE.\n\nNamed Entity Recognition Corpora\n\nWe compared FedHE with alternative training schemes on two biomedical NLP datasets\nand one news dataset, focusing on NER tasks. In NER, the objective is to identify and\nclassify named entities, such as diseases and genes, from a given sequence of tokens.\n\nThe selected corpora were chosen based on two main criteria: they are publicly\navailable, ensuring the reproducibility of results, and they are commonly used in well-\ncited papers, which helps guarantee the quality of the data. A summary of the selected\ndatasets can be found in Table 1.\n\nEntity/Relation Type Corpora Type\n\nCorpus\nCONLL-2003 General\nBC2GM\nBC4CHEMD Drug/Chem\n\nGene\n\nNews articles\nMedline abstract\nPubMed abstract\n\nTrain Dev\n14987 3466\n26006 3251\n94170\n\nTest\n3684\n3251\n\n11772 11771\n\nTable 1. List of NER corpora and their statistics\n\nWhat Are the Performance and Resource Overheads of FedHE?\n\nIn FedHE, the encryption of gradients introduces a substantial increase in data size,\nwhich can significantly impact bandwidth. Table 4 provides a comparative analysis of the\nsize overhead associated with different BERT models when using the CKKS encryption\nscheme. For instance, BERTtiny, which has a plaintext gradient size of 16 MB, increases\nto 340 MB when encrypted. Similarly, BERTmini’s gradient size grows from 42 MB to\n864 MB under the same scheme. The most pronounced effect is seen with BERTblue and\nBERTlarge blue , where the gradient size were 20 times and more than 50 times bigger,\nrespectively. While local training remains unaffected, these increases in ciphertext size\nlead to significant bandwidth overheads. As such, models like BERTblue become im-\npractical for FedHE due to the prohibitive size of encrypted gradients, emphasizing the\nneed for more bandwidth-efficient approaches or smaller models to maintain feasibility\nin federated settings.\n\nTable 4 highlights the impracticality of Large Language Models in FedHE due\nto their exponential growth of memory and bandwidth requirements. Table 3 shows that\n\n1Source code: https://github.com/marcosfpr/fedhe and https://github.com/\n\nmarcosfpr/sealy.\n\n2https://www.tensorflow.org/federated\n3https://fate.fedai.org/\n\n\fBERTmini\n\nModel\nBERTtiny\n\nScheme\nSingle/Central/Federated\nFedHE\nSingle/Central/Federated\nFedHE\nSingle/Central/Federated\nFedHE\nBERTlarge blue Single/Central/Federated\nFedHE\n\nBERTblue\n\n# Params Size\n4M\n4M\n11M\n11M\n108M\n108M\n344M\n344M\n\n16 MB\n340 MB\n42 MB\n864 MB\n415 MB\n8 GB\n1GB\n>50 GB\n\nTable 2. Model Parameters and Size for Different Schemes\n\nwhile operations on encrypted gradients, particularly encryption, become more costly\nwith increased parameter sizes, these do not generally pose a bottleneck in training. How-\never, if federated training involves frequent aggregation rounds and infrequent local train-\ning epochs, these operations could become a significant bottleneck when the number of\nparameters is sufficiently large. Typically, clients perform extensive local training with\nless frequent aggregations, which aligns with both performance and operational efficien-\ncies in FedHE.\n\nDue to BERTlarge blue’s excessive memory demands—over 50 GB in FedHE and\n1 GB in plaintext—along with significant bandwidth and processing requirements, this\nwork will focus on comparing the effectiveness only with the base BERTblue model in-\nstead. Future work can be done to address the comparsion with larger versions of BERT\nsuch as BERTlarge blue.\n\nModel\n\nBERTtiny\n\nBERTmini\n\nType\nEncrypt\nDecrypt\nEncrypt\nDecrypt\n\nMean (s) Std. Dev. (s)\n8.016\n2.179\n21.877\n5.884\n\n0.133\n0.053\n0.251\n0.095\n\n99th Percentile (s)\n8.484\n2.337\n22.671\n6.144\n\nTable 3. Summary Statistics for Encryption and Decryption Times of BERT Mod-\nels\n\nWhile Table 4 highlights significant bandwidth constraints on the server-side in\nFedHE, it is also essential to evaluate the resource and time costs associated with aggre-\ngation operations. Figure 4 sheds light on the performance of aggregation as the number\nof clients increases for the BERTtiny model using the BC2GM corpus. The analysis\nindicates that the homomorphic FedAvg aggregation time does not present an efficiency\nissue in the training process; specifically, BERTtiny completes aggregation in approxi-\nmately 20 seconds with 22 clients, whereas BERTmini requires about 50 seconds with 14\nclients. However, it is important to note that as the number of clients grows, the memory\nrequired to store ciphertext gradients increases significantly. For instance, aggregation for\nBERTmini with 16 clients led to a coordinator crash due to memory insufficiency. While\nsuch issues can be mitigated using external memory strategies, these solutions introduce\nadditional performance overhead.\n\n\f)\ns\n(\n\ni\n\ne\nm\nT\nn\na\ne\n\nM\n\n60\n\n40\n\n20\n\n0\n\n0\n\nAggregation Performance vs Number of Clients\n\nBERTtiny\nBERTmini\n\nMemory Limit Exceeded (>20GB)\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\nNumber of Clients\n\n14\n\n16\n\n18\n\n20\n\n22\n\n24\n\nFigure 1. Aggregation performance for BERT models on BC2GM.\n\nHow Does FedHE with Compact BERT Models Compare to State-of-the-Art BERT\nModels For Medical NER?\n\nAnother fundamental research question for this work is to understand how far compact\nBERT models are from the state-of-the-art models for federated medical NER. In par-\nticular, we would also like to understand if the introduction of HE can skew the overall\nresults. The table 4 shows an F1 comparison of the approaches tested with the state-of-\nthe-art models.\n\nAnother critical research question addressed in this work is evaluating the per-\nformance gap between compact BERT models and state-of-the-art models for federated\nmedical NER. Additionally, we investigate whether the integration of HE affects the over-\nall performance outcomes. Table 4 provides a comparative analysis of F1 scores for the\nevaluated approaches against the state-of-the-art models.\n\nModel Method\n\nCONLL-2003\n\nBC2GM\n\nBC4CHEMD\n\nBERTtiny\n\nTrain\nEval\nTrain\n0.804 ± 0.002\n0.618 ± 0.006\n0.865 ± 0.005\nSingle\n0.841 ± 0.000\n0.728 ± 0.002\n0.953 ± 0.001\nCentral\n0.726 ± 0.010\n0.464 ± 0.005\nFederated 0.624 ± 0.001\n0.816 ± 0.000 0.650 ± 0.014 0.744 ± 0.005\nFedHE\n0.802 ± 0.000\n0.690 ± 0.002\n0.961 ± 0.002\nBERTmini Single\n0.995 ± 0.001\n0.787 ± 0.005\n0.990 ± 0.000\nCentral\nFederated 0.993 ± 0.000\n0.958 ± 0.001\n0.758 ± 0.013\nFedHE\nBERTblue Central\n\nEval\nTrain\n0.391 ± 0.047\n0.537 ± 0.003\n0.460 ± 0.022\n0.605 ± 0.011\n0.598 ± 0.010\n0.448 ± 0.008\n0.621 ± 0.004 0.464 ± 0.005\n0.538 ± 0.004\n0.802 ± 0.000\n0.613 ± 0.002\n0.859 ± 0.001\n0.584 ± 0.000\n0.833 ± 0.000\n0.994 ± 0.000 0.781 ± 0.001 0.998 ± 0.000 0.739 ± 0.004 0.877 ± 0.000 0.590 ± 0.001\n0.683 ± 0.000\n0.999 ± 0.000\n\nEval\n0.593 ± 0.001\n0.645 ± 0.003\n0.613 ± 0.016\n0.604 ± 0.010\n0.594 ± 0.002\n0.763 ± 0.010\n0.703 ± 0.012\n\n0.791 ± 0.009\n\n0.968 ± 0.001\n\n0.992 ± 0.001\n\n0.758 ± 0.041\n\nTable 4. F1 Score comparison of FedHE with various BERT models on medical\nNER datasets. Standard deviations are shown in parentheses. Bold indicates\nFedHE surpasses Federated, while underscored indicates it surpasses Central-\nized evaluation.\n\nTraining Setup\n\nIn all experiments we ran 50 epochs for training the models. The centralized and single-\nclient learning, we conducted 50 local epochs on their private dataset. The single-client\ndata were obtained splitting the dataset in two parts, and taking only one from the corpora.\n\nThe federated and FedHE approaches ran 5 aggregation rounds with 10 local\nepochs each on client data. Effectiveness tests, shown in Table 4, were conducted with\n\n\f2 clients. Standard deviations for federated and FedHE were calculated from all clients,\nwhile single-client and centralized deviations were from 2 runs.\n\nFor training the models, we used Adam optimizer with an initial learning rate of\n2e − 5 and weight decay of 0.1. All experiments were performed on a system equipped\nwith an NVIDIA A100 GPU and at least 32GB RAM available.\n\nDiscussion\n\nThe results in Table 4 highlight the performance of different BERT models in 4 different\nconfigurations (centralized, single client, federated and FedHE) for medical NER tasks.\nThe centralized BERTblue model is used as a baseline, representing the state-of-the-art\nin BERT-based models for medical named entity recognition. This model sets a high\nstandard for comparison, demonstrating its accuracy across the datasets.\n\nImportantly, the application of HE does not skew model effectiveness. On the con-\ntrary, the encryption noise introduced by the encryption and decryption processes does not\ndamage model accuracy. Instead, it sometimes even improves performance, as reflected in\nthe bolded values in the table. The strongest hypothesis for this fact is that the small noise\nadded by the ciphertext operations helped the model to generalize better. This indicates\nthat HE can be effectively integrated without compromising, and potentially enhancing,\nthe model’s performance.\n\nThe analysis also reveals that single-client learning models, such as BERTtiny and\nBERTmini, often achieve higher training accuracy, but generalize with less effectively\ncompared to federated and FedHE approaches. Federated learning and FedHE models\nexhibit superior generalization in all corpora evaluated.\n\nBERTtiny shows lower performance compared to BERTmini, with signficant dif-\nferences in all 4 methods tested for all corpora. BERTmini, using only 11M parameters,\npresented satisfactory results even when compared to the more complex BERTblue with\n108M parameters. This suggests that we can achieve results closer to state-of-the-art us-\ning compact BERT representations without making FL+HE impractical. This work also\nsuggests that evaluating other slightly more complex BERT variants, such as BERTsmall,\ncould provide additional insights and potential improvements in model performance.\n\n5. Conclusion\nOverall, FedHE shows a generic framework for integrating HE in a FL protocol as a\nstrong alternative for federated medical NER tasks. FedHE offers robust performance\nand practical advantages, making it a compelling choice for scenarios where data pri-\nvacy and model effectiveness are critical. The results underscore the viability of FedHE\nin maintaining high performance while incorporating encryption techniques. For future\nwork, we highlight (1) study scenarios where the number of clients is higher and the data\nis non-IID; (2) assess the feasibility of other compact BERT variants such as BERTsmall\nand (3) test the models against other LLM-based baselines such as BERTlarge blue.\n\n6. Acknowledgements\nThe authors would also like to thank the Universidade Federal de Ouro Preto\n(PROPPI/UFOP) for supporting the development of this study.\n\n\fReferences\n\nAl Badawi, A. and Polyakov, Y. (2023). Demystifying bootstrapping in fully homomor-\n\nphic encryption. Cryptology ePrint Archive.\n\nBeutel, D. J., Topal, T., Mathur, A., Qiu, X., Fernandez-Marques, J., Gao, Y., Sani, L., Li,\nK. H., Parcollet, T., de Gusm˜ao, P. P. B., et al. (2020). Flower: A friendly federated\nlearning research framework. arXiv preprint arXiv:2007.14390.\n\nKoneˇcn`y, J., McMahan, H. B., Yu, F. X., Richt´arik, P., Suresh, A. T., and Bacon, D.\n(2016). Federated learning: Strategies for improving communication efficiency. arXiv\npreprint arXiv:1610.05492.\n\nMarcolla, C., Sucasas, V., Manzano, M., Bassoli, R., Fitzek, F. H., and Aaraj, N. (2022).\nSurvey on fully homomorphic encryption, theory, and applications. Proceedings of the\nIEEE, 110(10):1572–1609.\n\nPeng, L., Luo, G., Zhou, S., Chen, J., Xu, Z., Sun, J., and Zhang, R. (2024). An in-\ndepth evaluation of federated learning on biomedical natural language processing for\ninformation extraction. npj Digital Medicine, 7(1):127.\n\nTang, B., Cao, H., Wu, Y., Jiang, M., and Xu, H. (2013). Recognizing clinical entities\nin hospital discharge summaries using structural support vector machines with word\nrepresentation features. In BMC medical informatics and decision making, volume 13,\npages 1–10. Springer.\n\nYi, X., Paulet, R., Bertino, E., Yi, X., Paulet, R., and Bertino, E. (2014). Homomorphic\n\nencryption. Springer.\n\nZhu, L., Liu, Z., and Han, S. (2019). Deep leakage from gradients. Advances in neural\n\ninformation processing systems, 32.\n\n\f"
        },
        {
            "titulo": "Toxic Text Classification in Portuguese: Is LLaMA 3.1 8B All You Need?",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31116",
            "idioma": "Inglês",
            "storage_key": "files/article_31116_30919.pdf",
            "autores": [
                {
                    "nome": "Amanda S. Oliveira",
                    "afiliacao": "BLIP",
                    "orcid": "http://orcid.org/0009-0006-8000-7297"
                },
                {
                    "nome": "Pedro H. L. Silva",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0002-5525-6121"
                },
                {
                    "nome": "Valéria de C. Santos",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0002-7892-4954"
                },
                {
                    "nome": "Gladston Moreira",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0001-7747-5926"
                },
                {
                    "nome": "Vander L. S. Freitas",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0001-7989-0816"
                },
                {
                    "nome": "Eduardo J. S. Luz",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0001-5249-1559"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "The recognition of toxic and hate speech on social media platforms is important due to the significant risks posed to users and the digital ecosystem. Current state-of-the-art models, such as BERTimbau, have set benchmarks for Portuguese text classification, yet challenges remain in accurately detecting toxic content. This paper investigates the effectiveness of fine-tuning a smaller, open-source decoder-only model, LLaMA 3.1 8B 4bit, for this task. We propose an iterative prompt evolution method to optimize the model’s performance. Our results demonstrate that fine-tuning significantly enhances the LLaMA model’s F1-score from 0.61 to 0.75, surpassing BERTimbau in precision and matching the performance of the GPT-4o mini. However, the approach depends on the quality of the language models used for prompt evolution, highlighting the need for further research to enhance robustness in this area.",
            "keywords": [
                "Toxic Text Classification",
                "Llama",
                "LLM"
            ],
            "referencias": [
                "BehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N., and Reddy, S. (2024). Llm2vec: Large language models are secretly powerful text encoders.",
                "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.",
                "da Rocha Junqueira, J., Junior, C. L., Silva, F. L. V., Côrrea, U. B., and de Freitas, L. A.(2023). Albertina in action: An investigation of its abilities in aspect extraction, hate speech detection, irony detection, and question-answering. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 146–155. SBC.",
                "Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2023). Qlora: Efficient finetuning of quantized llms.",
                "dos Santos, W. R. and Paraboni, I. (2023). Predição de transtorno depressivo em redes sociais: Bert supervisionado ou chatgpt zero-shot? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 11–21. SBC.",
                "Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., and et al., A. F. (2024). The llama 3 herd of models.",
                "Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. (2024). Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In The Twelfth International Conference on Learning Representations.",
                "Hammes, L. O. A. and de Freitas, L. A. (2021). Utilizando bertimbau para a classificação de emoções em português. In Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana (STIL), pages 56–63. SBC.",
                "Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. (2019). Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790–2799. PMLR.",
                "Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2021). Lora: Low-rank adaptation of large language models.",
                "Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. (2024). Nv-embed: Improved techniques for training llms as generalist embedding models",
                "Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., and Stanley, K. O. (2023). Evolution through large models. In Handbook of Evolutionary Machine Learning, pages 331–366. Springer.",
                "Leite, J. A., Silva, D., Bontcheva, K., and Scarton, C. (2020). Toxic language detection in social media for brazilian portuguese: New dataset and multilingual analysis. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 914–924.",
                "Meyerson, E., Nelson, M. J., Bradley, H., Gaier, A., Moradi, A., Hoover, A. K., and Lehman, J. (2023). Language model crossover: Variation through few-shot prompting. arXiv preprint arXiv:2302.12170.",
                "Oliveira, A. S., Cecote, T. C., Alvarenga, J. P. R., Freitas, V. L. S., and Luz, E. J. S. (2024). Toxic speech detection in Portuguese: A comparative study of large language models. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 108–116, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Oliveira, A. S., Cecote, T. C., Silva, P. H., Gertrudes, J. C., Freitas, V. L., and Luz, E. J. (2023). How good is chatgpt for detecting hate speech in portuguese? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 94–103. SBC.",
                "OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., and et al., S. A. (2024). Gpt-4 technical report.",
                "Pires, R., Abonizio, H., Almeida, T. S., and Nogueira, R. (2023). Sabia: Portuguese large language models. In Naldi, M. C. and Bianchi, R. A. C., editors, Intelligent Systems, pages 226–240, Cham. Springer Nature Switzerland.",
                "Serras, F. and Finger, M. (2021). verbert: Automating brazilian case law document multilabel categorization using bert. In Anais do XIII Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 237–246, Porto Alegre, RS, Brasil. SBC.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: pretrained bert models for brazilian portuguese. In Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I 9, pages 403–417. Springer.",
                "Zheng, M., Su, X., You, S., Wang, F., Qian, C., Xu, C., and Albanie, S. (2023). Can gpt-4 perform neural architecture search? arXiv preprint arXiv:2304.10970."
            ],
            "artigo_completo": "Toxic Text Classification in Portuguese: Is LLaMA 3.1 8B All\nYou Need?\n\nAmanda S. Oliveira1, Pedro H. L. Silva2, Val´eria de C. Santos2,\nGladston Moreira2, Vander L. S. Freitas2, Eduardo J. S. Luz2\n\n1BLIP\n30.130-174 – Belo Horizonte – MG – Brazil\n\n2Computing Department – Federal University of Ouro Preto (UFOP)\n35.400-000 – Ouro Preto – MG – Brazil\n\namanda.oliveira@blip.ai\n\n{silvap,valeriacs,gladston,vander.freitas,eduluz}@ufop.edu.br\n\nAbstract. The recognition of toxic and hate speech on social media platforms is\nimportant due to the significant risks posed to users and the digital ecosystem.\nCurrent state-of-the-art models, such as BERTimbau, have set benchmarks for\nPortuguese text classification, yet challenges remain in accurately detecting toxic\ncontent. This paper investigates the effectiveness of fine-tuning a smaller, open-\nsource decoder-only model, LLaMA 3.1 8B 4bit, for this task. We propose an\niterative prompt evolution method to optimize the model’s performance. Our\nresults demonstrate that fine-tuning significantly enhances the LLaMA model’s\nF1-score from 0.61 to 0.75, surpassing BERTimbau in precision and matching\nthe performance of the GPT-4o mini. However, the approach depends on the\nquality of the language models used for prompt evolution, highlighting the need\nfor further research to enhance robustness in this area.\n\n1. Introduction\nThe task of recognizing toxic and hate speech has gained substantial attention in recent\nyears, particularly with the surge of user-generated content on social media platforms. As\nthese platforms increasingly shape public discourse, the proliferation of harmful content\npresents significant risks to both individual users and the broader digital environment.\nConsequently, the need for effective moderation tools has escalated, driving research\ntoward automated solutions capable of operating at scale.\n\nCurrent state-of-the-art methods for automated toxic content classification pre-\ndominantly leverage transformer-based architectures, with encoder-only models be-\ning the most common. Within the Portuguese language context, BERTimbau has\nemerged as a leading approach [Souza et al. 2020], demonstrating superior performance\nin various NLP tasks, including emotion classification [Hammes and de Freitas 2021],\ntoxic speech detection [da Rocha Junqueira et al. 2023, Oliveira et al. 2023], news clus-\ntering [Pereira and da Silva 2023], among other tasks [dos Santos and Paraboni 2023,\nSerras and Finger 2021]. The BERTimbau ability to capture subtle nuances in Portuguese\nexpressions has set a high standard in the field, making it the benchmark for multi-class\nclassification tasks. However, despite its effectiveness, the problem of accurately classi-\nfying toxic content remains an open challenge, particularly in the diverse and evolving\nlandscape of online discourse.\n\n\fRecent advancements have shifted towards decoder-only models, such as\nLLM2Vec [BehnamGhader et al. 2024] and NV-Embed [Lee et al. 2024], which have\nshown promising results across multiple languages.\nNotably, OpenAI Chat-\nGPT [OpenAI et al. 2024] 1, a large decoder-only language model, has demonstrated com-\npetitive performance in this domain [Oliveira et al. 2023]. The emergence of open-source\nmodels, like the Meta LLaMA family of models [Dubey et al. 2024], further compels a\nreexamination of existing methodologies, raising research questions about the potential of\nthese newer models.\n\nBuilding on these recent developments,\n\nthis work explores the capabili-\nties of decoder-only models, specifically focusing on the LLaMA 3.1 8B 4bit\nmodel [Dubey et al. 2024]. This model is particularly compelling due to its open-source\nnature, benchmark performance, and relatively smaller size, making it well-suited for\nfine-tuning specialized tasks such as toxic content classification in Portuguese. The key\nresearch questions guiding this investigation are RQ1: Can a fine-tuned LLaMA 3.1 8B\n4bit model achieve or surpass the performance of GPT-4o mini in classifying toxic content\nin Portuguese? RQ2: Can this model outperform the current state-of-the-art BERTimbau-\nbased approach in the same task? To address these questions, we propose a heuristic\napproach that utilizes a larger LLM (GPT-4o-mini) to refine the prompts employed by a\nsmaller LLM, thereby automating prompt engineering. The optimal prompt is then used to\nfine-tune the LLaMA 3.1 8B 4bit model for toxic content classification in social media,\nusing the TolDBr dataset - a large public dataset on this task [Leite et al. 2020]. Our results\nshow that the fine-tuned LLaMA 3.1 8B 4-bit model, operating in zero-shot classification\nmode, outperforms the BERTimbau-based model regarding precision and is on par with\nGPT-4o mini.\n\n2. Materials and Methods\n\nAlthough the primary focus of this work is to investigate the performance of a small\nand open-source language model (with only 8B parameters) for the task of toxic text\ndetection in Portuguese, the choice of prompt is a significant challenge. The quality of the\nprompt heavily influences the LLM’s performance [Brown et al. 2020]. Therefore, this\nwork proposes a straightforward approach to evolving prompts, ultimately using the best\nprompt identified for fine-tuning the model.\n\nThe following subsections describe the dataset selected for benchmarking, which\nis a large and popular dataset by Portuguese language standards for this task. Additionally,\nan outline of the methodology for selecting the best prompt and the approach used for\nfine-tuning the model.\n\n2.1. Told-Br dataset\n\nWe employed the ToLD-br dataset, developed in [Leite et al. 2020] for training and testing\nthe models used in this study. This dataset contains 21,000 tweets, annotated in a binary\nmanner as “toxic” or “non-toxic”. Additionally, the tweets are also classified into different\ncategories of toxicity, such as LGBTphobia, insults, racism, misogyny, and xenophobia.\n\nIn this study, we focused on the binary classification between “toxic” and “non-\ntoxic”, using the corresponding annotations to train and test our models. The dataset was\n\n1https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n\n\fdivided in a stratified manner, with 80% of the tweets allocated to the training set and the\nremaining 20% to the test set.\n\n2.2. Prompt Engineering: Iterative Prompt Refinement\n\nThe challenge in using large language models (LLMs) for zero-shot classification lies in\nidentifying the most effective prompt. This study proposes a heuristic to iteratively refine\nprompts using a larger LLM, intending to enhance classification accuracy in a smaller\nLLM.\n\nOur approach draws on previous research, mainly works by [Oliveira et al. 2024]\nand [Oliveira et al. 2023], which advocate for using in-context learning for social media\npost classification. While these studies explore both zero-shot and few-shot modalities,\nour focus remains exclusively on the zero-shot scenario.\n\nGiven that LLMs have been shown to function effectively as black-box optimiz-\ners [Zheng et al. 2023] and are viable alternatives to mutation and crossover operations in\ngenetic algorithms [Lehman et al. 2023, Meyerson et al. 2023], we draw inspiration from\nthe work presented in [Guo et al. 2024] to propose a simplified algorithm for evolving\nprompts tailored explicitly to the task of toxic speech detection in Portuguese.\n\nThe methodology is structured as Figure 1 illustrates: Initially, a population of\nprompts is initialized, each one specifically designed to classify social media posts as\ntoxic or non-toxic. The prompts then undergo a selection process, retaining only the top-\nperforming ones based on evaluation metrics. Next, operations to evolve the prompt are\napplied utilizing an instruction to a larger LLM, such as GPT-4, which assists in generating\nnew variations by recombining elements of existing successful prompts. This process is\niteratively refined to enhance the quality of the prompts. Finally, the optimal prompt from\nthis cycle is used to fine-tune the model. Algorithm 1 provides a pseudo-code overview of\nthese steps.\n\nFigure 1. The heuristic iterative prompt evolution process begins with an initial set\nof prompts, which are evaluated using the LLaMA model based on their F1-scores.\nThe top-performing prompts are then selected, and GPT-4o mini generates new\nprompts. These new prompts are added back to the population, and the process\nrepeats. The best prompt from this iterative cycle is ultimately selected for further\nuse. All prompts and instructions used in this study were written in Portuguese.\n\n2.3. LLM Fine-Tuning Methodology\n\nThis methodology fine-tunes the model using a quantized version to enhance mem-\nory efficiency and speed. Parameter-Efficient Fine-Tuning (PEFT) [Houlsby et al. 2019,\n\nSelect the top kbest promptsPromptsInitializerAdd the newprompt to thePopulationSet promptsReturn thebest promptGPT-4o miniLLaMAEvaluate F1-scoreGenerate the new promptSet prompts+ ScoreTermination\fPopulation ← []\nfor each prompt in InitialPrompts do\n\nAlgorithm 1 Simplified Prompt Evolution\n1: Function InitializePopulation(InitialPrompts)\n2:\n3:\n4:\n5:\n6:\n7:\n8: Function GenerateNewPrompt(PromptsAndScores)\n9:\n10:\n\nend for\nreturn Population\n\nEvaluate prompt with Llama 3.1 8B, using F1-score\nAdd prompt and its score to Population\n\nPromptsText ← Concatenate each prompt and its F1-Score from PromptsAndScores with line breaks\nSystemInstruction ← “You are an assistant that helps improve AI prompts. You should always\ngenerate a new prompt, using different words or varying lengths, never repeating the same prompt.\nGenerate ONLY the prompt, without comments or explanations”.\n\n11:\n\nInstruction ← “You are evolving a prompt for another LLM. Based on the following prompts and\n\ntheir respective F1-scores, generate a new prompt optimized for the task of classifying hate speech”.\n\nChatGPTInput ← SystemInstruction + Instruction + PromptsText\nNewPrompt ← Call ChatGPT API with ChatGPTInput\nreturn NewPrompt\n\n12:\n13:\n14:\n15: Function Main()\n16:\n17:\n18:\n19:\n20:\n21:\n22:\n23:\n24:\n25:\n26:\n\nInitialPrompts ← Define initial set of prompts\nPopulation ← InitializePopulation(InitialPrompts)\nfor each epoch in range(NumEpochs) do\n\nTopKPrompts ← Select top ‘k’ prompts from Population, based on F1-scores\nPromptsAndScores ← Collect scores and prompts from TopKPrompts\nNewPrompt ← GenerateNewPrompt(PromptsAndScores)\nEvaluate NewPrompt with Llama 3.1 8B using F1-score\nAdd NewPrompt and its score to Population\n\nend for\nBestPrompt ← Select best performing prompt from Population\nreturn BestPrompt\n\nHu et al. 2021] and QLoRA [Dettmers et al. 2023] techniques reduce model complexity,\nfocusing on optimizing QKV projections and Feed Forward Layers. Training data is\ndivided into training and validation sets. Specific prompts, structured as Alpaca prompts,\nalign the model with toxic content classification objectives in Portuguese.\n\nAlpaca Prompt Example:\nBelow is an instruction that describes a task, paired with an input that provides further\ncontext. Write a response that appropriately completes the request.\n\nYou are analyzing a social media post.\n\nInstruction:\ntext contains hate speech, offenses, aggressions, insults,\nswear words, or any form of toxicity, respond only with\n‘yes’. If it is appropriate and non-toxic, respond ‘no’.\n\nIf the\n\nInput: Tweet: every time the weather changes, my throat gets\ninflamed...dammit!\n\nResponse: no.\n\n\f3. Experimental Setup and Results\n\nTwo key experiments are conducted to address the research questions posed in this study.\nFirst, Experiment #1 is designed to identify the best prompt. Using this prompt, the\nLLaMA 3.1 8B model is fine-tuned with the training data from the ToLD-Br dataset\n(Experiment #2). Four versions of the model are fine-tuned, varying parameters related to\nPEFT/QLoRA. An additional experiment is proposed to evaluate the performance of using a\nfine-tuned GPT-4o mini model within the same scenario. All experiments are performed in\na Google Colab environment, utilizing an A100 GPU. Source code is available at https:\n//github.com/oliveiraamanda/ToxicSpeech-Llama-STIL-2024.\n\n3.1. Experiment #1 - Prompt Engineering\n\nTo evolve the prompts using the iterative algorithm proposed here, it is essential first\nto define the cost function to be minimized. The F1-score of binary classification on a\npartition of the training data is selected as the cost function. As our population consists of\nnatural language sentences, the initial individuals must be manually defined for the specific\ndomain. This approach was also adopted in [Guo et al. 2024]. In this work, we base\nour initial prompts on those proposed in [Oliveira et al. 2023] and [Oliveira et al. 2024],\nas these studies serve as a baseline and address the same dataset. Subsequently, we\nderive additional prompts by adding or removing sentences and words, totalizing six\nprompts. The target model, LLaMA 3.1 8B, performs the classification using only 50\ninstances from each class to compute the F1-score, given the high computational cost of\nthis function. Following this, prompts are evolved through an iterative process involving\ntwo models—the target model LLaMA 3.1 8B and GPT-4o mini over 50 epochs. After\nthis period, the best prompt (with the highest F1-score) is selected and presented below.\n“Best Prompt: You are analyzing a social media post. If the text contains hate speech,\noffenses, aggressions, insults, swear words, or any form of toxicity, respond only with\n‘yes’. If it is appropriate and non-toxic, respond ‘no’”.\n\n3.2. Experiment #2 - LLaMA Fine-Tuning Process\n\nTo facilitate fine-tuning on modest hardware, we employed a 4-bit quantized version of\nthe LLaMA 3.1 model, using the QLoRA technique [Dettmers et al. 2023], with 8 billion\nparameters model- LLaMA 3.1 8B [Dubey et al. 2024]2. We used the Hugging Face PEFT\nlibrary3 with the Unsloth library4, setting the learning rate to 2e − 4 and the sequence\nlength to 2048 tokens, while varying the “rank” and “LoRa Alpha” parameters.\n\nThe fine-tuning process used the most effective prompt and involved 3, 000 training\nsteps, with a batch size of 2 and gradient accumulation set to 4, effectively processing\n6000 instances from the training dataset.\n\nResults from experiments varying the parameters “rank” and “LoRa alpha” are\npresented in Table 1, while the fine-tuning loss function using “rank=16” and “LoRa\nalpha=16” is shown in Figure 2.\n\n2https://huggingface.co/unsloth/llama-3-8b-bnb-4bit\n3https://huggingface.co/docs/peft/index\n4https://github.com/unslothai/unsloth\n\n\f3.3. Experiment #3 - GPT-4o mini Fine-Tuning Process\n\nTo fine-tune the GPT4-o mini model, we used the Azure AI Studio platform 5, leveraging\nthe same training data used in Experiment #2. We adopted the best prompt identified\nin Experiment #1 and created a JSONL file where each instance of the training set was\npreceded by the prompt and accompanied by its respective label.\n\nWe chose the 2024-07-18 release of GPT4-o-mini, which was the one available\nfor fine-tuning on Azure. After training, the model was deployed on the Azure platform,\nallowing its use through API calls.\n\nDuring the evaluation, we noted that utilizing Azure Studio, which incorporates\nan additional content moderation layer beyond that provided by OpenAI, led to certain\nmoderation inaccuracies. Approximately 1% of the test set was erroneously categorized\ndue to “content moderation errors.” For these instances, we assigned the label “non-toxic.”\n\n3.4. Results Comparison\n\nidentified through the iter-\nthe most effective prompt\nFor comparative purposes,\native prompt evolution approach is tested with three additional models: Mari-\ntaca 6 AI Sabi´a3 [Pires et al. 2023], OpenAI GPT-4o mini [OpenAI et al. 2024] 7,\nand OpenAI ChatGPT 3.5 Turbo [Brown et al. 2020] 8, as well as the BERTimbau\nmodel [Souza et al. 2020].\n\nThe results presented in Table 2 highlight the significance of fine-tuning the LLaMA\n3.1 8B model. Specifically, fine-tuning improved the F1-score from 0.61 to 0.75, demon-\nstrating a substantial performance gain. Furthermore, when applying the fine-tuning\nmethodology using the prompt proposed in [Oliveira et al. 2023], the F1-score reached\n0.70. However, our prompt evolution approach further improved this to 0.75, indicating\nthat the refined prompt contributed significantly to the model’s performance.\n\nAdditionally, the LLaMA 3.1 8B model, despite being fine-tuned with only 3,000\nsteps and 6,000 instances, performs competitively against other state-of-the-art models like\nGPT-4o mini, Sabi´a3, and BERTimbau. Notably, Sabi´a3, a leading model from Maritaca\nAI, demonstrated comparable accuracy to GPT-4o mini across various high-stakes Brazilian\nexams, such as OAB, ENEM, and ENADE. These results underscore the effectiveness of\nour prompt evolution methodology and the potential of smaller models like LLaMA 3.1\n8B when paired with efficient fine-tuning techniques.\n\nThe results in Table 1 reveal differences in model performance based on the\nconfiguration of the “r” (rank) and “LoRa alpha” parameters. The configuration with\n“r=16” and “alpha=16” achieves the best overall performance, with an F1-Score of 0.75,\nbalancing precision (0.69) and recall (0.83). Increasing “r” to 24 or “alpha” to 24 leads\nto a marked decline in performance, with the model showing symptoms of overfitting,\nparticularly with a dramatic drop in recall. The configuration with “r=8” and “alpha=16”\ndemonstrates high recall (0.935) but at the cost of precision, indicating a bias towards\nover-predicting the positive class.\n\n5https://oai.azure.com/portal\n6https://www.maritaca.ai/\n7https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n8https://chat.openai.com/\n\n\fFigure 2. Loss over steps. Params LoRa alplha = 16 and r = 16.\n\nTable 1.\nmance.\nConfiguration\n\nImpact of different r and LoRa alpha configurations on model perfor-\n\nF1-Score Precision Recall Accuracy\n\nLLaMA 3.1 8B (r=16, alpha=16)\n\n0.75\n\nLLaMA 3.1 8B (r=24, alpha=16)\n\nLLaMA 3.1 8B (r=8, alpha=16)\n\nLLaMA 3.1 8B (r=16, alpha=24)\n\n0.432\n\n0.727\n\n0.327\n\n0.69\n\n0.601\n\n0.595\n\n0.573\n\n0.83\n\n0.338\n\n0.935\n\n0.229\n\n0.76\n\n0.609\n\n0.690\n\n0.584\n\nTable 2. Comparison of Evaluation Metrics for Different Models\n\nModel\n\nF1-Score Precision Recall Accuracy\n\nLLaMA 3.1 8B (original) w/ best prompt\n\nLLaMA 3.1 8B (finetuned) w/ prompt from [Oliveira et al. 2023]\n\nLLaMA 3.1 8B (finetuned) w/ best prompt\n\nChatGPT 3.5T Zero-Shot w/ prompt from [Oliveira et al. 2023]\n\nGPT-4o mini w/ best prompt\n\nGPT-4o mini (finetuned) w/ best prompt\n\nSabi´a 3 w/ best prompt\n\nBERTimbau Finetuned\n\n4. Conclusion\n\n0.61\n\n0.70\n\n0.75\n\n0.73\n\n0.75\n\n0.74\n\n0.75\n\n0.75\n\n0.45\n\n0.71\n\n0.69\n\n0.74\n\n0.75\n\n0.78\n\n0.77\n\n0.75\n\n0.96\n\n0.70\n\n0.83\n\n0.73\n\n0.75\n\n0.74\n\n0.76\n\n0.75\n\n0.46\n\n0.74\n\n0.76\n\n0.74\n\n0.75\n\n0.76\n\n0.75\n\n0.75\n\nIn this study, we investigated whether a smaller, open-source and quantized language model\nlike LLaMA 3.1 8B 4 bits could effectively perform toxic text detection in Portuguese,\nparticularly when optimized using an iterative prompt evolution approach along with\nfinetune. The experiments demonstrated that, with carefully evolved prompts, the model\ncould achieve competitive performance, even with a limited number of training steps\nand instances. This highlights the potential of smaller models when paired with efficient\nprompt engineering techniques.\n\n\fHowever, the approach has its limitations. The success of the prompt evolution\nalgorithm heavily depends on the quality of the underlying language models used for the\ntext evolution operations. This reliance can be a significant constraint, as deficiencies in\nthe language models directly affect the quality of the evolved prompts and, consequently,\nthe overall model performance. Further research is needed to address these dependencies\nand enhance the robustness of the prompt engineering approach.\n\nAcknowledgments\n\nWe would like to express our sincere thanks to Blip, whose generous support and invaluable\nassistance were crucial for the presence of the first author in the event. The authors would\nalso like to thank the Coordenac¸ ˜ao de Aperfeic¸oamento de Pessoal de N´ıvel Superior\n- Brazil (CAPES) - Finance Code 001, Fundac˜ao de Amparo `a Pesquisa do Estado de\nMinas Gerais (FAPEMIG, grants APQ-01518-21), Conselho Nacional de Desenvolvimento\nCient´ıfico e Tecnol´ogico (CNPq, grants 307151/2022-0, 308400/2022-4), and Universidade\nFederal de Ouro Preto (PROPPI/UFOP) for supporting the development of this study.\n\nReferences\n\nBehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N., and Reddy, S.\n\n(2024). Llm2vec: Large language models are secretly powerful text encoders.\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901.\n\nda Rocha Junqueira, J., Junior, C. L., Silva, F. L. V., Cˆorrea, U. B., and de Freitas, L. A.\n(2023). Albertina in action: An investigation of its abilities in aspect extraction, hate\nspeech detection, irony detection, and question-answering. In Anais do XIV Simp´osio\nBrasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 146–155.\nSBC.\n\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2023). Qlora: Efficient\nfinetuning of quantized llms. In Oh, A., Naumann, T., Globerson, A., Saenko, K.,\nHardt, M., and Levine, S., editors, Advances in Neural Information Processing Systems,\nvolume 36, pages 10088–10115. Curran Associates, Inc.\n\ndos Santos, W. R. and Paraboni, I. (2023). Predic¸ ˜ao de transtorno depressivo em redes so-\nciais: Bert supervisionado ou chatgpt zero-shot? In Anais do XIV Simp´osio Brasileiro\nde Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 11–21. SBC.\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A.,\n\nSchelten, A., Yang, A., and et al., A. F. (2024). The llama 3 herd of models.\n\nGuo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. (2024).\nConnecting large language models with evolutionary algorithms yields powerful prompt\noptimizers. In The Twelfth International Conference on Learning Representations.\n\nHammes, L. O. A. and de Freitas, L. A. (2021). Utilizando bertimbau para a classificac¸ ˜ao\nde emoc¸ ˜oes em portuguˆes. In Simp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e da\nLinguagem Humana (STIL), pages 56–63. SBC.\n\n\fHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A.,\nAttariyan, M., and Gelly, S. (2019). Parameter-efficient transfer learning for nlp. In\nInternational conference on machine learning, pages 2790–2799. PMLR.\n\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.\n\n(2021). Lora: Low-rank adaptation of large language models.\n\nLee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. (2024).\nNv-embed: Improved techniques for training llms as generalist embedding models.\n\nLehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., and Stanley, K. O. (2023). Evolution\nthrough large models. In Handbook of Evolutionary Machine Learning, pages 331–366.\nSpringer.\n\nLeite, J. A., Silva, D., Bontcheva, K., and Scarton, C. (2020). Toxic language detec-\ntion in social media for brazilian portuguese: New dataset and multilingual analysis.\nIn Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association\nfor Computational Linguistics and the 10th International Joint Conference on Natural\nLanguage Processing, pages 914–924.\n\nMeyerson, E., Nelson, M. J., Bradley, H., Gaier, A., Moradi, A., Hoover, A. K., and\nLehman, J. (2023). Language model crossover: Variation through few-shot prompting.\narXiv preprint arXiv:2302.12170.\n\nOliveira, A. S., Cecote, T. C., Alvarenga, J. P. R., Freitas, V. L. S., and Luz, E. J. S. (2024).\nToxic speech detection in Portuguese: A comparative study of large language models. In\nGamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R.,\neditors, Proceedings of the 16th International Conference on Computational Processing\nof Portuguese - Vol. 1, pages 108–116, Santiago de Compostela, Galicia/Spain. Associ-\nation for Computational Lingustics.\n\nOliveira, A. S., Cecote, T. C., Silva, P. H., Gertrudes, J. C., Freitas, V. L., and Luz, E. J.\n(2023). How good is chatgpt for detecting hate speech in portuguese?\nIn Anais\ndo XIV Simp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana,\npages 94–103. SBC.\n\nOpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L.,\nAlmeida, D., Altenschmidt, J., Altman, S., and et al., S. A. (2024). Gpt-4 technical\nreport.\n\nPereira, P. H. and da Silva, T. L. C. (2023). Uso de modelagem de t´opicos para agrupamento\nde not´ıcias: uma abordagem usando bertopic. In Anais do XIV Simp´osio Brasileiro de\nTecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 406–410. SBC.\n\nPires, R., Abonizio, H., Almeida, T. S., and Nogueira, R. (2023). Sabi´a: Portuguese large\nlanguage models. In Naldi, M. C. and Bianchi, R. A. C., editors, Intelligent Systems,\npages 226–240, Cham. Springer Nature Switzerland.\n\nSerras, F. and Finger, M. (2021). verbert: Automating brazilian case law document multi-\nlabel categorization using bert. In Anais do XIII Simp´osio Brasileiro de Tecnologia da\nInformac¸ ˜ao e da Linguagem Humana, pages 237–246, Porto Alegre, RS, Brasil. SBC.\n\nSouza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: pretrained bert models\nfor brazilian portuguese. In Intelligent Systems: 9th Brazilian Conference, BRACIS\n\n\f2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I 9, pages 403–417.\nSpringer.\n\nZheng, M., Su, X., You, S., Wang, F., Qian, C., Xu, C., and Albanie, S. (2023). Can gpt-4\n\nperform neural architecture search? arXiv preprint arXiv:2304.10970.\n\n\f"
        },
        {
            "titulo": "Syntactic parsing: where are we going?",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31117",
            "idioma": "Inglês",
            "storage_key": "files/article_31117_30920.pdf",
            "autores": [
                {
                    "nome": "Lucelene Lopes",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0003-0314-140X"
                },
                {
                    "nome": "Thiago Alexandre Salgueiro Pardo",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2111-1319"
                },
                {
                    "nome": "Magali S. Duran",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-3843-4600"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "In this review & opinion paper, we discuss the options and challenges for syntactic parsing. Despite significant advances in recent years, driven primarily by neural network architectures, parsing accuracy appears to be approaching a plateau. This paper proposes a reflection on the factors that may possibly be influencing such results and suggests some future paths.",
            "keywords": [
                "Tools and Resources for NLP",
                "Syntactic representations",
                "Parsing"
            ],
            "referencias": [
                "Alves, D., Bekavac, B., and Tadić, M. (2021). Typological approach to improve dependency parsing for Croatian language. In Dakota, D., Evang, K., and Kübler, S., editors, Proceedings of the 20th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest), pages 1–11, Sofia, Bulgaria. Association for Computational Linguistics.",
                "Attardi, G., Sartiano, D., and Simi, M. (2021). Biaffine dependency and semantic graph parsing for Enhanced Universal dependencies. In Oepen, S., Sagae, K., Tsarfaty, R., Bouma, G., Seddah, D., and Zeman, D., editors, Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies, pages 184–188, Online. Association for Computational Linguistics.",
                "Branco, A., Silva, J. R., Gomes, L., and António Rodrigues, J. (2022). Universal grammatical dependencies for Portuguese with CINTIL data, LX processing and CLARIN support. In Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC), pages 5617–5626, Marseille, France. European Language Resources Association.",
                "Brigada Villa, L. and Giarda, M. (2023). Using modern languages to parse ancient ones: a test on Old English. In Beinborn, L., Goswami, K., Muradolu, S., Sorokin, A., Kumar, R., Shcherbakov, A., Ponti, E. M., Cotterell, R., and Vylomova, E., editors, Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pages 30–41, Dubrovnik, Croatia. Association for Computational Linguistics.",
                "Cassidy, L., Lynn, T., Barry, J., and Foster, J. (2022). TwittIrish: A Universal Dependencies treebank of tweets in Modern Irish. In Muresan, S., Nakov, P., and Villavicencio, A., editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6869–6884, Dublin, Ireland. Association for Computational Linguistics.",
                "de Lhoneux, M., Stymne, S., and Nivre, J. (2017). Arc-hybrid non-projective dependency parsing with a static-dynamic oracle. In Miyao, Y. and Sagae, K., editors, Proceedings of the 15th International Conference on Parsing Technologies, pages 99–104, Pisa, Italy. Association for Computational Linguistics.",
                "de Marneffe, M.-C., Manning, C. D., Nivre, J., and Zeman, D. (2021). Universal Dependencies. Computational Linguistics, 47(2):255–308.",
                "Dione, C. M. B. (2021). Multilingual dependency parsing for low-resource African languages: Case studies on Bambara, Wolof, and Yoruba. In Oepen, S., Sagae, K., Tsarfaty, R., Bouma, G., Seddah, D., and Zeman, D., editors, Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies, pages 84–92, Online. Association for Computational Linguistics.",
                "Dozat, T. and Manning, C. D. (2016). Deep biaffine attention for neural dependency parsing. CoRR, abs/1611.01734.",
                "Duran, M., das Graças Nunes, M., and Pardo, T. A. (2023a). Construções sintáticas do português que desafiam a tarefa de parsing: uma análise qualitativa. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 424–433, Porto Alegre, RS, Brasil. SBC.",
                "Duran, M. S., Nunes, M. d. G. V., and Pardo, T. A. S. (2023b). Avaliação qualitativa do analisador sintático udpipe 2 treinado sobre o córpus jornalístico porttinari-base. Technical report, Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo.",
                "Fernández-González, D. and Gómez-Rodríguez, C. (2023). Dependency parsing with bottom-up hierarchical pointer networks. Information Fusion, 91:494–503.",
                "Gamba, F. and Zeman, D. (2023). Universalising Latin Universal Dependencies: a harmonisation of Latin treebanks in UD. In Grobol, L. and Tyers, F., editors, Proceedings of the Sixth Workshop on Universal Dependencies (UDW, GURT/SyntaxFest), pages 7–16, Washington, D.C. Association for Computational Linguistics.",
                "Ghiffari, F. A. A., Alfina, I., and Azizah, K. (2023). Cross-lingual transfer learning for Javanese dependency parsing. In Li, D., Mahendra, R., Tang, Z. P., Jang, H., Murawaki, Y., and Wong, D. F., editors, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 1–9, Nusa Dua, Bali. Association for Computational Linguistics.",
                "Goldberg, Y. (2016). A primer on neural network models for natural language processing. J. Artif. Int. Res., 57(1):345–420.",
                "Kabiri, R., Karimi, S., and Surdeanu, M. (2022). Informal Persian Universal Dependency treebank. In Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC), pages 7096–7105, Marseille, France. European Language Resources Association.",
                "Kondratyuk, D. and Straka, M. (2019). 75 languages, 1 model: Parsing Universal Dependencies universally. In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2779–2795, Hong Kong, China. Association for Computational Linguistics.",
                "Lopes, L. and Pardo, T. (2024). Towards portparser - a highly accurate parsing system for Brazilian Portuguese following the Universal Dependencies framework. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 401–410, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Lusito, S. and Maillard, J. (2021). A Universal Dependencies corpus for Ligurian. In de Lhoneux, M. and Tsarfaty, R., editors, Proceedings of the Fifth Workshop on Universal Dependencies (UDW, SyntaxFest), pages 121–128, Sofia, Bulgaria. Association for Computational Linguistics.",
                "Mrini, K., Dernoncourt, F., Bui, T., Chang, W., and Nakashole, N. (2019). Rethinking self-attention: An interpretable self-attentive encoder-decoder parser. CoRR, abs/1911.03875.",
                "Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajič, J., Manning, C. D., McDonald, R., Petrov, S., Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D. (2016). Universal Dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC), pages 1659–1666, Portoroz, Slovenia. ELRA.",
                "Pedrazzini, N. and Eckhoff, H. M. (2021). Oldslavnet: A scalable early slavic dependency parser trained on modern language data. Software Impacts, 8:100063.",
                "Qi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. (2020). Stanza: A python natural language processing toolkit for many human languages. In Celikyilmaz, A. and Wen, T.-H., editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 101–108, Online. Association for Computational Linguistics.",
                "Sánchez-Rodríguez, X., Sarymsakova, A., Castro, L., and Garcia, M. (2024). Increasing manually annotated resources for Galician: the parallel Universal Dependencies treebank. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 587–592, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Straka, M. (2018). UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 197–207.",
                "Straka, M., Hajič, J., and Straková, J. (2016). UDPipe: Trainable pipeline for processing CoNLL-U files performing tokenization, morphological analysis, POS tagging and parsing. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC), pages 4290–4297, Portorǒz, Slovenia. European Language Resources Association (ELRA).",
                "Ustün, A., Bisazza, A., Bouma, G., and van Noord, G. (2020). UDapter: Language adaptation for truly Universal Dependency parsing. In Webber, B., Cohn, T., He, Y., and Liu, Y., editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2302–2315, Online. Association for Computational Linguistics.",
                "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
                "Yshaayahu Levi, D. and Tsarfaty, R. (2024). A truly joint neural architecture for segmentation and parsing. In Graham, Y. and Purver, M., editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1408–1420, St. Julian’s, Malta. Association for Computational Linguistics."
            ],
            "artigo_completo": "Syntactic parsing: where are we going?\n\nLucelene Lopes1, Thiago Alexandre Salgueiro Pardo1, Magali S. Duran1\n\n1N´ucleo Interinstitucional de Lingu´ıstica Computacional (NILC)\nInstituto de Ciˆencias Matem´aticas e de Computac¸ ˜ao, Universidade de S˜ao Paulo\nS˜ao Carlos-SP, Brazil\n\nAbstract. In this review & opinion paper, we discuss the options and challenges\nfor syntactic parsing. Despite significant advances in recent years, driven pri-\nmarily by neural network architectures, parsing accuracy appears to be ap-\nproaching a plateau. This paper proposes a reflection on the factors that may\npossibly be influencing such results and suggests some future paths.\n\nMotivation\n\nThe importance of good part of speech tagging and parsing annotation tools for down-\nstream Natural Language Processing (NLP) tasks is acknowledged by several publications\nin the history of the area, including both more classic (symbolic and statistic) approaches\nand new (usually neural-based) ones. In particular, the rise of “Universal Dependencies”\n(UD) framework1 [Nivre et al. 2016, de Marneffe et al. 2021] has sparked renewed inter-\nest in dependency parsing, driving new efforts in syntax studies and parsing in NLP.\n\nThis review & opinion paper attempts to draw a landscape of more recent pars-\ning efforts that align to UD standards, trying to figure out the potential limits of the task\nwith current methods and what other strategies might be adopted for keeping improving\nthe achieved results in the area. Such initiative is bold and naturally subject to failure,\nas natural languages have diverse characteristics and there are always new NLP meth-\nods emerging. Knowing this, this article makes a selection of works from the literature,\nchoosing relatively recent and widely cited approaches in the area in order to draw some\ntentative (and certainly temporally anchored) conclusions.\n\nBesides the possibly interesting work selection and overview that supported this\npaper, our contribution includes an exercise of “keeping the head above water”, showing\nhow far we have come and the imperfections of the landscape.\n\nOn current parsing techniques\n\nThe use of neural networks for detection of patterns, and consequently, the prediction of\npart of speech tags and dependency relations became the preferred method in the area\n[Goldberg 2016]. Within neural networks, several techniques as Long Short-Term Mem-\nory (LSTM) in its various versions [Van Houdt et al. 2020], together with other deep\nlearning techniques [Dozat and Manning 2016], have been employed in the last decade\nwith consistent advances for well resourced languages. The latest evolution brought by\nthe self-attention methods [Vaswani et al. 2017], based on the famous Transformers, goes\nback a few years now, but it is still one of the main reasons for recent improvements.\n\n1https://universaldependencies.org/\n\n\fOverall, although different criteria could be used, in this paper we distinguish the\nparsing efforts according to the generic parsing tools or specific language parsing initia-\ntives; and basic technology employed (e.g., BiLSTM, Deep Biaffine, and Self-Attention).\n\nThe more popular parsing tools, within UD standard, are the UDPipe in its ver-\nsions 1.3 and 2.0 [Straka et al. 2016, Straka 2018], Stanza pipeline [Qi et al. 2020], UD-\nify [Kondratyuk and Straka 2019], and AllenNLP pipeline [Dozat and Manning 2016].\nOther\ntools were developed, but apparently had fewer number of\nusers, as the Diaparser [Attardi et al. 2021], UDapter [ ¨Ust¨un et al. 2020], UUParser\n[de Lhoneux et al. 2017], LAL-Parser [Mrini et al. 2019], and Hierarchical Pointer Net-\nwork algorithm [Fern´andez-Gonz´alez and G´omez-Rodr´ıguez 2023].\n\nless popular\n\nThese parsers usually focus their efforts to cover several languages, being clearly\nmultilingual. Some of these tools were specifically designed to cover the large set of\nlanguages available at the UD repository (which currenlty includes over 150 languages).\nHowever, from a technological point of view, the tools have considerable differences,\nalthough all of them make use of neural network models.\n\nThe technology of Bidirectional LSTM (BiLSTM) [Van Houdt et al. 2020] is fre-\nquently employed by many systems, including UDPipe 2.0, Stanza, and Hierarchical\nPointer Networks algorithm. The Deep Biaffine technology [Dozat and Manning 2016]\nis found in AllenNLP pipeline, but also in tools as Diaparser and UDapter. Self-attention\n[Vaswani et al. 2017] is found in LAL-Parser and UDify tools. Additionally, the men-\ntioned tools show differences on offering a static model or the possibility to perform\nmodel construction through a training set and/or to adopt pre-trained word embeddings.\n\nParsing results\n\nThe best values reported for each of the previously cited parsing methods are shown in\nTable 1. We chose to report only the Label Attachment Score (LAS), as this is usually the\nmost adopted evaluation metric and also one of the most punitive metrics, as it measures\nthe accuracy of the dependency relation identification and the tokens related as head and\ndependent. The table also indicates the language for which the highest LAS was reported.\n\nTable 1. Highest LAS reported for the generic parsing tools.\n\nparsing system\nUUParser\nStanza\nUDPipe 1.3\nUDapter\nDiaparser\nUDify\nUDPipe 2.0\nAllenNLP pipeline\nHier. Pointer Networks\nLAL-parser\n\nhighest LAS\n87.34%\n90.01%\n91.20%\n92.60%\n93.65%\n93.70%\n94.53%\n94.60%\n96.15%\n96.29%\n\nlanguage\nPortuguese\nSpanish\nHindi\nItalian\nItalian\nRussian\nRussian\nEnglish\nEnglish\nEnglish\n\ncited technology publication\n\nBiLSTM\nDeep Biaffine\nNN Classifier\nDeep Biaffine\nDeep Biaffine\nSelf-Attention\nBiLSTM\nDeep Biaffine\nBiLSTM\nSelf-Attention\n\n2017\n2020\n2016\n2020\n2021\n2019\n2018\n2016\n2023\n2019\n\nThe performance of the parsing methods vary considerably according to the lan-\nguage to which they are applied, as the scientific literature has shown. For example,\n\n\ffor UDPipe 2, the reported LAS for Spanish and Italian can be as low as 80.68% and\n77.34%, respectively. For AllenNLP pipeline, LAS for Chinese and Spanish was 85.38%\nand 91.65%, respectively. The values shown in the table may also reflect the number of\ntested languages. While UDify and UDPipe test over more than 70 languages, AllenNLP\npipeline, UUParser, and LAL-parser test for only 6, 5, and 2 languages, respectively.\n\nFocusing only on the highest LAS accuracy as presented in Table 1, it is notice-\nable that the majority of the highest scores are over 90% of accuracy. These numbers\nsuggest that the State Of The Art (SOTA) for LAS is attainable despite of the technology\nemployed, date of publication, and even specificity of each parsing development. Observ-\ning the three best reported results, we see different techniques and that English shows the\nbest scores (probably because English is the best resourced language).\n\nThis fact suggests that, after the spread of neural network-based models, the qual-\nity of the training model plays a more important role than the specific technology em-\nployed. As such, the variations for different languages seem to reflect the quality of the\ntraining data for each language. For example, LAS for UDify for a low resourced lan-\nguage as Breton is as low as 40.19%, which is much lower than the 93.70% maximum\nattained for Russian.\n\nFortunately, the literature is abundant in terms of efforts for specific languages.\nThese works usually are presented either with the construction of a specific corpus for\nthe target language, or transferring learning from a better resourced language towards\nthe low resourced one. Observing the works dedicated to specific languages, we found a\nreasonable number of publications, some of which are summarized in Table 2.\n\nlanguage\nYoruba\n\nwork\n[Dione 2021]\n[Brigada Villa and Giarda 2023]\n[Cassidy et al. 2022]\n[Lusito and Maillard 2021]\n[Baig et al. 2021]\n[Dione 2021]\n[T¨urk et al. 2022]\n[Ghiffari et al. 2023]\n[Pedrazzini and Eckhoff 2021]\n[S´anchez-Rodr´ıguez et al. 2024]\n[Alves et al. 2021]\n[Branco et al. 2022]\n[Kabiri et al. 2022]\n[Gamba and Zeman 2023]\n[Lopes and Pardo 2024]\n\nTable 2. Highest LAS reported by specific language efforts.\noverall approach\nLAS\n31.43%\nTransfer learning\n58.70% Old English Transfer learning\nTransfer learning\n59.34% Indonesian\nCorpus building\nLigurian\n60.74%\nUrdu\n62.90%\nCorpus building\nTransfer learning\nWolof\n67.83%\nCorpus building\nTurkish\n76.04%\n79.22%\nCorpus building\nIrish\nTransfer learning\n79.66% Old Slavic\n84.31%\nCorpus building\nGalician\nTransfer learning\nCroatian\n89.09%\nCorpus building\n92.54% Portuguese\nCorpus building\n92.68%\nCorpus building\n94.61%\nCorpus building\n94.70% Portuguese\n\nPersian\nLatin\n\nThe examples summarized in Table 2 show efforts that can be grouped into at-\ntempts to serve very low resourced languages (as Old English, Old Slavic, Ligurian, Urdu,\nBambara, Wolof, and Indonesian) and low resourced languages (as Turkish, Croatian,\nGalician, Irish, Persian, Latin, and Portuguese). While the very low resourced languages\n\n\fattempts are mostly based on transfer learning, the languages better resourced mostly\ncenter the efforts in building better corpora to be used to train specific models.\n\nThe observation of LAS in Table 2 shows that the best reported results are also\nabove the 90% score of the generic parsing methods (Table 1). Obviously, the hard cases,\nas Yoruba and Old English, show low accuracy despite the efforts, probably because they\nare low-resourced languages. However, it is noticeable the accuracy achieved by transfer\nlearning for Old Slavic and Croatian, as well as the high values for Persian, Latin, and\nPortuguese with the production of high quality training corpora.\n\nWhere can we head to?\n\nThe advent of popular neural network methods in the last decade has brought impressive\nprogress in several areas of NLP, bringing Artificial Intelligence to the center of topics in\nall areas of the human knowledge. For parsing tasks, specifically, using UD standards, we\nnotice the increase of quality since 2016. However, improvements seem to reach a limit\nup to 96% accuracy, and it is noticeable that no specificity show a clear predominance.\n\nIt is also well known that languages with few resources may not be able to\nbenefit from the advantages of SOTA methods.\nIt would be better for theses lan-\nguages to invest in more classic methods or in the improvement of resources through\ncorpora building including careful annotation. Specific techniques like data augmen-\ntation and joint task resolution may also be interesting ways (see, e.g., the work of\n[Yshaayahu Levi and Tsarfaty 2024] for Hebrew parsing). Such paths may also be rel-\nevant for languages already reaching accuracy around 95%, i.e., already delivering SOTA\nresults.\n\nAnother relevant question is if the search for a better accuracy (over 96%) is a re-\nalistic goal. Should we make our peace with these missing 4% due to a natural inaccuracy\nof dependency annotation? Looking at the best method for a specific language (Por-\ntuguese), the authors [Lopes and Pardo 2024] [Duran et al. 2023a] [Duran et al. 2023b]\ndiscuss some reasons for the remaining errors that are also cited in the literature: under-\nrepresented phenomena in the training corpus (that might be solved by data augmentation\nand/or more corpus annotation) and difficult annotation issues (as to decide which is the\nhead of a prepositional phrase) that sometimes may challenge even the humans. Person-\nally, we believe that the above 99% accuracy already achieved for part of speech tagging\nmay be achieved for parsing too. However, it may require to simplify some syntactic\ndistinctions or to look for new approaches to the parsing problem.\n\nThe interested reader may find more information at the POeTiSA project web\n\nportal: https://sites.google.com/icmc.usp.br/poetisa\n\nAcknowledgements\n\nThis work was carried out at the Center for Artificial Intelligence of the University of\nS˜ao Paulo (C4AI - http://c4ai.inova.usp.br/), with support by the S˜ao Paulo\nResearch Foundation (FAPESP grant #2019/07665-4) and by the IBM Corporation. The\nproject was also supported by the Ministry of Science, Technology and Innovation, with\nresources of Law N. 8,248, of October 23, 1991, within the scope of PPI-SOFTEX, coor-\ndinated by Softex and published as Residence in TIC 13, DOU 01245.010222/2022-44.\n\n\fReferences\n\nAlves, D., Bekavac, B., and Tadi´c, M. (2021). Typological approach to improve depen-\ndency parsing for Croatian language. In Dakota, D., Evang, K., and K¨ubler, S., editors,\nProceedings of the 20th International Workshop on Treebanks and Linguistic Theo-\nries (TLT, SyntaxFest), pages 1–11, Sofia, Bulgaria. Association for Computational\nLinguistics.\n\nAttardi, G., Sartiano, D., and Simi, M. (2021). Biaffine dependency and semantic graph\nparsing for EnhancedUniversal dependencies. In Oepen, S., Sagae, K., Tsarfaty, R.,\nBouma, G., Seddah, D., and Zeman, D., editors, Proceedings of the 17th International\nConference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into\nEnhanced Universal Dependencies, pages 184–188, Online. Association for Compu-\ntational Linguistics.\n\nBaig, A., Rahman, M. U., Shah, A. S., and Abbasi, S. (2021). Universal dependencies for\nurdu noisy text. International Journal of Advanced Trends in Computer Science and\nEngineering.\n\nBranco, A., Silva, J. R., Gomes, L., and Ant´onio Rodrigues, J. (2022). Universal gram-\nmatical dependencies for Portuguese with CINTIL data, LX processing and CLARIN\nsupport. In Calzolari, N., B´echet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T.,\nGoggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis,\nS., editors, Proceedings of the Thirteenth Language Resources and Evaluation Con-\nference (LREC), pages 5617–5626, Marseille, France. European Language Resources\nAssociation.\n\nBrigada Villa, L. and Giarda, M. (2023). Using modern languages to parse ancient ones:\nIn Beinborn, L., Goswami, K., Murado˘glu, S., Sorokin, A.,\na test on Old English.\nKumar, R., Shcherbakov, A., Ponti, E. M., Cotterell, R., and Vylomova, E., editors,\nProceedings of the 5th Workshop on Research in Computational Linguistic Typology\nand Multilingual NLP, pages 30–41, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\n\nCassidy, L., Lynn, T., Barry, J., and Foster, J. (2022). TwittIrish: A Universal Dependen-\ncies treebank of tweets in Modern Irish. In Muresan, S., Nakov, P., and Villavicencio,\nA., editors, Proceedings of the 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages 6869–6884, Dublin, Ireland. Asso-\nciation for Computational Linguistics.\n\nde Lhoneux, M., Stymne, S., and Nivre, J. (2017). Arc-hybrid non-projective dependency\nparsing with a static-dynamic oracle. In Miyao, Y. and Sagae, K., editors, Proceedings\nof the 15th International Conference on Parsing Technologies, pages 99–104, Pisa,\nItaly. Association for Computational Linguistics.\n\nde Marneffe, M.-C., Manning, C. D., Nivre, J., and Zeman, D. (2021). Universal Depen-\n\ndencies. Computational Linguistics, 47(2):255–308.\n\nDione, C. M. B. (2021). Multilingual dependency parsing for low-resource African lan-\nguages: Case studies on Bambara, Wolof, and Yoruba. In Oepen, S., Sagae, K., Tsar-\nfaty, R., Bouma, G., Seddah, D., and Zeman, D., editors, Proceedings of the 17th\nInternational Conference on Parsing Technologies and the IWPT 2021 Shared Task on\n\n\fParsing into Enhanced Universal Dependencies, pages 84–92, Online. Association for\nComputational Linguistics.\n\nDozat, T. and Manning, C. D. (2016). Deep biaffine attention for neural dependency\n\nparsing. CoRR, abs/1611.01734.\n\nDuran, M., das Grac¸as Nunes, M., and Pardo, T. A. (2023a). Construc¸ ˜oes sint´aticas do\nportuguˆes que desafiam a tarefa de parsing: uma an´alise qualitativa. In Anais do XIV\nSimp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages\n424–433, Porto Alegre, RS, Brasil. SBC.\n\nDuran, M. S., Nunes, M. d. G. V., and Pardo, T. A. S. (2023b). Avaliac¸ ˜ao qualitativa\ndo analisador sint´atico udpipe 2 treinado sobre o c´orpus jornal´ıstico porttinari-base.\nTechnical report, Instituto de Ciˆencias Matem´aticas e de Computac¸ ˜ao, Universidade\nde S˜ao Paulo.\n\nFern´andez-Gonz´alez, D. and G´omez-Rodr´ıguez, C. (2023). Dependency parsing with\n\nbottom-up hierarchical pointer networks. Information Fusion, 91:494–503.\n\nGamba, F. and Zeman, D. (2023). Universalising Latin Universal Dependencies: a har-\nmonisation of Latin treebanks in UD. In Grobol, L. and Tyers, F., editors, Proceedings\nof the Sixth Workshop on Universal Dependencies (UDW, GURT/SyntaxFest), pages\n7–16, Washington, D.C. Association for Computational Linguistics.\n\nGhiffari, F. A. A., Alfina, I., and Azizah, K. (2023). Cross-lingual transfer learning for\nJavanese dependency parsing. In Li, D., Mahendra, R., Tang, Z. P., Jang, H., Murawaki,\nY., and Wong, D. F., editors, Proceedings of the 13th International Joint Conference\non Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter\nof the Association for Computational Linguistics: Student Research Workshop, pages\n1–9, Nusa Dua, Bali. Association for Computational Linguistics.\n\nGoldberg, Y. (2016). A primer on neural network models for natural language processing.\n\nJ. Artif. Int. Res., 57(1):345–420.\n\nKabiri, R., Karimi, S., and Surdeanu, M. (2022). Informal Persian Universal Dependency\ntreebank. In Calzolari, N., B´echet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T.,\nGoggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis,\nS., editors, Proceedings of the Thirteenth Language Resources and Evaluation Con-\nference (LREC), pages 7096–7105, Marseille, France. European Language Resources\nAssociation.\n\nKondratyuk, D. and Straka, M. (2019). 75 languages, 1 model: Parsing Universal De-\npendencies universally. In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, Proceed-\nings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 2779–2795, Hong Kong, China. Association for Computational Lin-\nguistics.\n\nLopes, L. and Pardo, T. (2024). Towards portparser - a highly accurate parsing system for\nBrazilian Portuguese following the Universal Dependencies framework. In Gamallo,\nP., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., edi-\ntors, Proceedings of the 16th International Conference on Computational Processing\n\n\fof Portuguese - Vol. 1, pages 401–410, Santiago de Compostela, Galicia/Spain. Asso-\nciation for Computational Lingustics.\n\nLusito, S. and Maillard, J. (2021). A Universal Dependencies corpus for Ligurian. In\nde Lhoneux, M. and Tsarfaty, R., editors, Proceedings of the Fifth Workshop on Uni-\nversal Dependencies (UDW, SyntaxFest), pages 121–128, Sofia, Bulgaria. Association\nfor Computational Linguistics.\n\nMrini, K., Dernoncourt, F., Bui, T., Chang, W., and Nakashole, N. (2019). Rethink-\ning self-attention: An interpretable self-attentive encoder-decoder parser. CoRR,\nabs/1911.03875.\n\nNivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajiˇc, J., Manning, C. D., Mc-\nDonald, R., Petrov, S., Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D. (2016).\nIn Proceedings of\nUniversal Dependencies v1: A multilingual treebank collection.\nthe Tenth International Conference on Language Resources and Evaluation (LREC),\npages 1659–1666, Portoroˇz, Slovenia. ELRA.\n\nPedrazzini, N. and Eckhoff, H. M. (2021). Oldslavnet: A scalable early slavic dependency\n\nparser trained on modern language data. Software Impacts, 8:100063.\n\nQi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. (2020). Stanza: A python\nnatural language processing toolkit for many human languages.\nIn Celikyilmaz, A.\nand Wen, T.-H., editors, Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pages 101–108, Online. As-\nsociation for Computational Linguistics.\n\nS´anchez-Rodr´ıguez, X., Sarymsakova, A., Castro, L., and Garcia, M. (2024). Increasing\nmanually annotated resources for Galician: the parallel Universal Dependencies tree-\nbank. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G.,\nand Amaro, R., editors, Proceedings of the 16th International Conference on Compu-\ntational Processing of Portuguese - Vol. 1, pages 587–592, Santiago de Compostela,\nGalicia/Spain. Association for Computational Lingustics.\n\nStraka, M. (2018). UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings\nof the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal\nDependencies, pages 197–207.\n\nStraka, M., Hajiˇc, J., and Strakov´a, J. (2016). UDPipe: Trainable pipeline for process-\ning CoNLL-U files performing tokenization, morphological analysis, POS tagging and\nparsing. In Proceedings of the Tenth International Conference on Language Resources\nand Evaluation (LREC), pages 4290–4297, Portoroˇz, Slovenia. European Language\nResources Association (ELRA).\n\nT¨urk, U., Atmaca, F., ¨Ozates¸, c. B., Berk, G., Bedir, S. T., K¨oksal, A., Bas¸aran, B. O.,\nG¨ung¨or, T., and ¨Ozg¨ur, A. (2022). Resources for turkish dependency parsing:\nin-\ntroducing the boun treebank and the boat annotation tool. Lang. Resour. Eval.,\n56(1):259–307.\n\n¨Ust¨un, A., Bisazza, A., Bouma, G., and van Noord, G. (2020). UDapter: Language\nadaptation for truly Universal Dependency parsing. In Webber, B., Cohn, T., He, Y.,\nand Liu, Y., editors, Proceedings of the 2020 Conference on Empirical Methods in\n\n\fNatural Language Processing (EMNLP), pages 2302–2315, Online. Association for\nComputational Linguistics.\n\nVan Houdt, G., Mosquera, C., and N´apoles, G. (2020). A review on the long short-term\n\nmemory model. Artificial Intelligence Review, 53.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,\nL. u., and Polosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg,\nU. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors,\nAdvances in Neural Information Processing Systems, volume 30. Curran Associates,\nInc.\n\nYshaayahu Levi, D. and Tsarfaty, R. (2024). A truly joint neural architecture for seg-\nmentation and parsing. In Graham, Y. and Purver, M., editors, Proceedings of the 18th\nConference of the European Chapter of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 1408–1420, St. Julian’s, Malta. Association for\nComputational Linguistics.\n\n\f"
        },
        {
            "titulo": "A Robustness Analysis of Automated Essay Scoring Methods",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31118",
            "idioma": "Inglês",
            "storage_key": "files/article_31118_30921.pdf",
            "autores": [
                {
                    "nome": "Rafael T. Anchiêta",
                    "afiliacao": "IFPI",
                    "orcid": "http://orcid.org/0000-0003-4209-9013"
                },
                {
                    "nome": "Rogério F. de Sousa",
                    "afiliacao": "IFPI",
                    "orcid": "https://orcid.org/0000-0003-4589-6157"
                },
                {
                    "nome": "Raimundo S. Moura",
                    "afiliacao": "UFPI",
                    "orcid": "https://orcid.org/0000-0002-1558-3830"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Automated Essay Scoring",
                "Robustness",
                "Adversarial Essays"
            ],
            "referencias": [
                "Barzilay, R. and Lapata, M. (2008). Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.",
                "Beigman Klebanov, B. and Madnani, N. (2020). Automated evaluation of writing – 50 years and counting. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7796–7810, Online. Association for Computational Linguistics.",
                "Cohen, J. (1968). Weighted kappa: nominal scale agreement provision for scaled disagreement or partial credit. Psychological bulletin, 70(4):213–220.",
                "de Sousa, R. F., Marinho, J. C., Neto, F. A. R., Anchiêta, R. T., and Moura, R. S. (2024). PiLN at PROPOR: A BERT-based strategy for grading narrative essays. In Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 2, pages 10–13, Santiago de Compostela, Galicia/Spain. Association for Computational Linguistics.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "Higgins, D. and Heilman, M. (2014). Managing what we can measure: Quantifying the susceptibility of automated scoring systems to gaming behavior. Educational Measurement: Issues and Practice, 33(3):36–46.",
                "Kabra, A., Bhatia, M., Singla, Y. K., Jessy Li, J., and Ratn Shah, R. (2022). Evaluation toolkit for robustness testing of automatic essay scoring systems. In Proceedings of the 5th Joint International Conference on Data Science & Management of Data, pages 90–99, Bangalore, India. Association for Computing Machinery.",
                "Liu, R., Wang, X., Liu, J., and Zhou, J. (2024). A comprehensive analysis of evaluating robustness and generalization ability of models in aes. In Proceedings of the 7th International Symposium on Big Data and Applied Statistics, pages 1–5, Beijing, China. IOP Publishing.",
                "Marinho, J. C., Anchiêta, R. T., and Moura, R. S. (2021). Essay-br: a brazilian corpus of essays. In XXXIV Simpósio Brasileiro de Banco de Dados: Dataset Showcase Workshop, SBBD 2021, pages 53–64, Online. SBC.",
                "Marinho, J. C., Anchiêta, R. T., and Moura, R. S. (2022a). Essay-br: a brazilian corpus to automatic essay scoring task. Journal of Information and Data Management, 13(1):65–76.",
                "Marinho, J. C., C., F., Anchiêta, R. T., and Moura, R. S. (2022b). Automated essay scoring: An approach based on enem competencies. In Anais do XIX Encontro Nacional de Inteligência Artificial e Computacional, pages 49–60, Campinas, Brazil. SBC.",
                "Mello, R. F., Oliveira, H., Wenceslau, M., Batista, H., Cordeiro, T., Bittencourt, I. I., and Isotanif, S. (2024). PROPOR’24 competition on automatic essay scoring of Portuguese narrative essays. In Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 2, pages 1–5, Santiago de Compostela, Galicia/Spain. Association for Computational Linguistics.",
                "Oliveira, H., Ferreira Mello, R., Barreiros Rosa, B. A., Rakovic, M., Miranda, P., Cordeiro, T., Isotani, S., Bittencourt, I., and Gasevic, D. (2023). Towards explainable prediction of essay cohesion in portuguese and english. In Proceedings of the 13th International Learning Analytics and Knowledge Conference, pages 509–519, Arlington TX USA. Association for Computing Machinery.",
                "Page, E. B. (1966). The imminence of... grading essays by computer. The Phi Delta Kappan, 47(5):238–243.",
                "Perelman, L. (2014). When “the state of the art” is counting words. Assessing Writing, 21:104–111.",
                "Tay, Y., Phan, M., Tuan, L. A., and Hui, S. C. (2018). Skipflow: Incorporating neural coherence features for end-to-end automatic text scoring. In Proceedings of the Thirty-second AAAI conference on artificial intelligence, pages 5948–5955, New Orleans, Louisiana, USA. AAAI Press.",
                "Yannakoudakis, H. and Cummins, R. (2015). Evaluating the performance of automated text scoring systems. In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 213–223, Denver, Colorado. Association for Computational Linguistics.",
                "Yoon, S.-Y., Cahill, A., Loukina, A., Zechner, K., Riordan, B., and Madnani, N. (2018). Atypical inputs in educational applications. In Bangalore, S., Chu-Carroll, J., and Li, Y., editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), pages 60–67, New Orleans - Louisiana. Association for Computational Linguistics."
            ],
            "artigo_completo": "A Robustness Analysis of Automated Essay Scoring Methods\n\nRafael T. Anchiˆeta1, Rog´erio F. de Sousa1, Raimundo S. Moura2\n\n1Federal Institute of Piau´ı – (IFPI - Picos)\nLaboratory of Artificial Intelligence, Robotics, and Automation (LIARA)\n\n2Federal University of Piau´ı – (UFPI - Teresina)\nLaboratory of Natural Language Processing (LPLN)\n\n{rta, rogerio.sousa}@ifpi.edu.br, rsm@ufpi.edu.br\n\nAbstract. This paper analyzed the robustness of a state-of-the-art Automated\nEssay Scoring (AES) model by applying various linguistically motivated pertur-\nbations to the Essay-BR corpus. Our findings reveal that the AES model failed\nto detect these adversarial modifications, often assigning higher scores to the\ndisturbed essays than to the original ones.\n\n1. Introduction\n\nAutomated Essay Scoring (AES) aims to provide computational models for automati-\ncally grading essays or with minimal involvement of humans [Page 1966]. Although this\nresearch area is over fifty years old [Beigman Klebanov and Madnani 2020], it has re-\ncently gained the attention of the Brazilian community because of publicly available cor-\npora [Marinho et al. 2021, Marinho et al. 2022a]. Several methods to grade an essay or its\ncharacteristics arose based on these resources [de Sousa et al. 2024, Oliveira et al. 2023,\nMarinho et al. 2022b]. Besides, there is a growing interest in the area. For instance,\nrecently occurred the PROPOR’24 Competition, whose goal was to develop computer\nsystems capable of automatically evaluating essays [Mello et al. 2024].\n\nDespite the advances achieved, the Brazilian community has made little effort to\nevaluate the robustness of AES methods, including analyzing their sensitivity to adver-\nsarial perturbations. [Liu et al. 2024] define robustness as the capacity to remain stable\nand reliable under different circumstances. Studies demonstrate that AES methods for\nthe English language are easily fooled [Perelman 2014], reducing the trustworthiness of\nAI-based automated scoring systems [Kabra et al. 2022]. Based on these limitations of\nAES methods for English, we investigated whether AES methods for Portuguese suffer\nfrom robustness problems.\n\nOur objective is to analyze AES methods using adversarial essays. For that, we\napplied a set of perturbations to an essay corpus, including adding unrelated texts, shuf-\nfling, deleting, and repeating paragraphs of an essay. With these linguistically-motivated\ndisturbances, we evaluated a state-of-the-art AES strategy for Portuguese and found that\nthe analyzed model could not deal with adversarial essays, producing, in fact, better re-\nsults for undisturbed essays.\n\nThe remainder of this paper is organized as follows: Section 2 briefly presents\nrelated work. In Section 3, we detailed the performed analysis to verify the robustness\nof an Automated Essay Scoring method for Portuguese. Finally, Section 4 concludes the\npaper and indicates future directions.\n\n\f2. Related Work\n\n[Kabra et al. 2022] proposed a model agnostic adversarial evaluation scheme and asso-\nciated metrics for AES systems to test their natural language understanding capabilities\nand overall robustness. They evaluated models ranging from feature-engineering-based\napproaches to the latest deep-learning algorithms. The authors found that AES models\nare highly overstable such that even heavy modifications (as much as 25%) with content\nunrelated to the topic of the questions do not decrease the score produced by the models.\n\n[Liu et al. 2024] evaluated Automatic Essay Scoring models’ robustness and gen-\neralization capabilities through a comprehensive series of experiments to validate various\nmodels’ efficacy. The authors randomly select a part of the essays and shuffle the order of\nthe sentences or delete a sentence randomly to construct a Chinese adversarial sample set\nfor evaluating the robustness of the models. The results showed that the advanced AES\nmodels have poor robustness and generalization ability, and Large Language Models have\nbetter performance but still need to be improved.\n\n3. Robustness Analysis\n\nThe Essay-BR corpus [Marinho et al. 2021] is organized into training, development, and\ntesting sets, each with 3,198, 686, and 686 essays. We used the test set to generate adver-\nsarial essays. First, we extracted the essays with a score greater than or equal to 680 since\nthe average score of ENEM 2023 was 641.6 points1, resulting in 305 essays. We adopted\nthe strategy of selecting the best essays, avoiding those with several grammatical, struc-\ntural, and argumentative issues. After that, we applied several perturbations to the essays\nto produce adversarial essays. From the original and adversarial essays set, we evaluated\nthe robustness of an Automated Essay Scoring (AES) model.\n\nWe implemented linguistically motivated perturbations to analyze the robustness\nof an AES model, i.e., to check whether the model can detect any difference between\noriginal and modified responses. The perturbations are detailed below.\n\nAdd unrelated text. We added an unrelated paragraph in each essay. We create three sets\nof essays with unrelated content, each indicating the position where an unrelated\ntext was added. The sets are with unrelated texts added at the beginning, middle,\nand end of the essays. We extracted a paragraph from essays with a prompt differ-\nent from the analyzed essay and added it to the essay. This test tries to mimic the\nbehavior of students when they make their responses lengthy by adding irrelevant\ninformation.\n\nAdd song and cake recipe. Although these perturbations add unrelated content to an es-\nsay, they have a very different language structure than written prose in essays. So,\nthis can be used to test a system negatively. Furthermore, it has been observed\nthat students use this strategy in their exams, possibly in an attempt to fool the\nsystem2. We created two sets of perturbations, one for cake recipe and the other\nfor the song. In both sets, we add unrelated content in the middle of the essays.\n\n1https://querobolsa.com.br/revista/redacao-enem-2023-quantos-texto\n\ns-tiraram-nota-mil-quantos-zeraram\n\n2https://g1.globo.com/educacao/noticia/2013/03/queria-testar-correca\n\no-do-enem-diz-jovem-que-pos-receita-na-redacao.html\n\n\fAdd repeated text. For this adversarial strategy, we also created three sets of pertur-\nbations. For each set, we repeated the essay content at the beginning, middle,\nand end of the essays. The motivation for this perturbation is that, according\nto [Kabra et al. 2022], students sometimes repeat sentences or specific keywords\nin their responses to make them longer yet not out of context and to fashion cohe-\nsive paragraphs [Higgins and Heilman 2014, Yoon et al. 2018].\n\nDelete text. Similar to adding repeated text, we created three sets of perturbations in this\nstrategy. For each set, we removed a paragraph at the beginning, middle, and end\nof the essays. According to [Kabra et al. 2022], these tests generally break the\nflow of an argument, delete crucial details from an essay, and decrease wordiness.\nThis perturbation can seriously detract from the coherency and quality of writing\nand frustrate readers.\n\nShuffle text. For this perturbation, we randomly shuffle the content of an essay. The\nmotivation for this adversarial strategy is to analyze important aspects of es-\nsay scoring, such as coherence and organization, which measure the extent to\nwhich the response demonstrates a unified structure and direction of the narra-\ntive [Barzilay and Lapata 2008, Tay et al. 2018].\n\nAfter generating adversarial essays, we evaluated a state-of-the-art automated es-\nsay scoring [de Sousa et al. 2024] based on the BERT model [Devlin et al. 2019]. We as-\nsessed that model using each ENEM competency through the Quadratic Weighted Kappa\n(QWK) metric [Cohen 1968] for original and adversarial essays. QWK is a metric com-\nmonly used to assess AES models [Yannakoudakis and Cummins 2015]. Table 1 shows\nthe results on the original essays, and Table 2 presents the results on the adversarial essays.\n\nTables 1 and 2, from C1 to C5, indicate the five competencies of the ENEM, and\nthe total is the final grade for an essay. In Table 2, we highlight the values greater than or\nequal to the value of the original essays.\n\nAnalyzing the values from the two tables, we can see that only the values of adding\ntext at the beginning and adding a cake recipe were not greater than the original essay\nvalues, indicating that the AES model was able to identify perturbations in the essays,\npenalizing their scores. On the other hand, the scores for adding unrelated text in the mid-\ndle, in the end, and a song were greater than or equal to the values for the original essays.\nAn interesting finding is that, despite adding an unrelated text at the end of an essay, the\nC5 score was not penalized. Competency 5 of the ENEM is dedicated to elaborating a\nproposal to solve the problem. The proposal normally appears at the end of an essay, and\nthe AES model could not detect the unrelated content added to an essay. More than that,\nthe final grade of original and adversarial essays had the same QWK value, suggesting\nthat the AES model failed to capture this perturbation.\n\nFor the perturbation of repeating a text in the essay, the AES model graded the\noriginal and adversarial essays with the same score, mainly in competence four. Com-\npetence 4 evaluates the superficial structure of the text, that is, how the sentences and\nparagraphs are linked through cohesive elements. This way, the AES model should nega-\ntively score such responses. Besides, the scores for repeating a text in the middle and at\nthe end of an essay had the same value as the original essays.\n\nAnother interesting finding is that deleting some parts of the essay improves its\ngrade in various competencies. As we can see, the scores for deleting a text in the essay\n\n\fare greater than or equal to the scores of the original essays, including the final score.\nThese results show that the AES model could not identify a break in the flow of an argu-\nment when essential parts of the essay were removed.\n\nFinally, and perhaps the most interesting finding, is that shuffling the paragraphs\nof an essay produces better results than the original essays. This result demonstrates that\nthe AES model could not determine the cohesion and coherence of the essays. That is,\nthe AES model did not identify the transition between the lines of the essays, verifying\ndisconnected ideas that change the meaning substantially.\n\nThe source code of the AES model and for generating adversarial essays are pub-\n\nlicly available at https://github.com/liara-ifpi/essay-robustness.\n\nTable 1. Quadratic Weighted Kappa results on the original essays.\n\nC1\n\nC2\n\nC3\n\nC4\n\nC5 Total\n\n0.44\n\n0.23\n\n0.29\n\n0.24\n\n0.62\n\n0.46\n\nTable 2. Quadratic Weighted Kappa results on the adversarial essays.\nC1\n\nAdversarial strategy\n\nC5 Total\n\nC4\n\nC3\n\nC2\n\n0.14\nAdd unrelated text at the begging 0.41\n0.42\nAdd unrelated text in the middle\n0.18\n0.43 0.22\nAdd unrelated text at the end\n0.21\n0.38\nAdd song\n0.18\n0.38\nAdd cake recipe\n0.44\n0.20\nRepeat text at the begging\n0.21\n0.43\nRepeat text in the middle\n0.23\n0.43\nRepeat text at the end\n0.40\nDelete text at the begging\n0.19\n0.40 0.23\nDelete text in the middle\n0.41 0.23\nDelete text at the end\n0.47\n0.20\nShuffle text\n\n0.15\n0.24\n0.28\n0.23\n0.22 0.18\n0.23 0.24\n0.24\n0.27\n0.28 0.24\n0.30 0.22\n0.25\n0.29\n0.24\n0.29\n0.24\n0.29\n\n0.17\n0.57\n0.62\n0.20\n0.62\n0.24\n0.20 0.64\n0.61\n0.59\n0.61\n0.62\n0.66\n0.66\n0.63\n0.64\n\n0.40\n0.44\n0.46\n0.42\n0.41\n0.44\n0.46\n0.46\n0.45\n0.46\n0.46\n0.47\n\n4. Conclusion\n\nThis paper presented a robustness analysis for automatic essay scoring focusing on the\nPortuguese language. We used the Essay-BR corpus, which is based on the ENEM com-\npetencies, to perform that analysis. Our strategy was to add several perturbations to\nproduce adversarial essays, aiming to check if a state-of-the-art automated essay scor-\ning model can detect any difference between original and modified responses. From the\nanalysis, we have learned that the automated essay scoring model could not identify the\nperturbations in the essays, producing scores that were even greater than the original re-\nsponses. We hope that this analysis sheds light on this research area and helps develop\nmore robust strategies for automatically grading essays.\n\nFor future work, we intend to develop more perturbations and create a toolkit to\n\nfacilitate the creation of adversarial essays.\n\n\fReferences\n\nBarzilay, R. and Lapata, M. (2008). Modeling local coherence: An entity-based approach.\n\nComputational Linguistics, 34(1):1–34.\n\nBeigman Klebanov, B. and Madnani, N. (2020). Automated evaluation of writing – 50\nyears and counting. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7796–7810, Online. Association for Computational\nLinguistics.\n\nCohen, J. (1968). Weighted kappa: nominal scale agreement provision for scaled dis-\n\nagreement or partial credit. Psychological bulletin, 70(4):213–220.\n\nde Sousa, R. F., Marinho, J. C., Neto, F. A. R., Anchiˆeta, R. T., and Moura, R. S. (2024).\nPiLN at PROPOR: A BERT-based strategy for grading narrative essays. In Proceed-\nings of the 16th International Conference on Computational Processing of Portuguese\n- Vol. 2, pages 10–13, Santiago de Compostela, Galicia/Spain. Association for Com-\nputational Linguistics.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of\nIn Proceedings of the\ndeep bidirectional transformers for language understanding.\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nHiggins, D. and Heilman, M. (2014). Managing what we can measure: Quantifying the\nsusceptibility of automated scoring systems to gaming behavior. Educational Mea-\nsurement: Issues and Practice, 33(3):36–46.\n\nKabra, A., Bhatia, M., Singla, Y. K., Jessy Li, J., and Ratn Shah, R. (2022). Evaluation\ntoolkit for robustness testing of automatic essay scoring systems. In Proceedings of\nthe 5th Joint International Conference on Data Science & Management of Data, pages\n90–99, Bangalore, India. Association for Computing Machinery.\n\nLiu, R., Wang, X., Liu, J., and Zhou, J. (2024). A comprehensive analysis of evaluating\nrobustness and generalization ability of models in aes. In Proceedings of the 7th In-\nternational Symposium on Big Data and Applied Statistics, pages 1–5, Beijing, China.\nIOP Publishing.\n\nMarinho, J. C., Anchiˆeta, R. T., and Moura, R. S. (2021). Essay-br: a brazilian cor-\npus of essays. In XXXIV Simp´osio Brasileiro de Banco de Dados: Dataset Showcase\nWorkshop, SBBD 2021, pages 53–64, Online. SBC.\n\nMarinho, J. C., Anchiˆeta, R. T., and Moura, R. S. (2022a). Essay-br: a brazilian corpus to\nautomatic essay scoring task. Journal of Information and Data Management, 13(1):65–\n76.\n\nMarinho, J. C., C., F., Anchiˆeta, R. T., and Moura, R. S. (2022b). Automated essay scor-\ning: An approach based on enem competencies. In Anais do XIX Encontro Nacional\nde Inteligˆencia Artificial e Computacional, pages 49–60, Campinas, Brazil. SBC.\n\nMello, R. F., Oliveira, H., Wenceslau, M., Batista, H., Cordeiro, T., Bittencourt, I. I., and\nIsotanif, S. (2024). PROPOR’24 competition on automatic essay scoring of Portuguese\n\n\fnarrative essays. In Proceedings of the 16th International Conference on Computa-\ntional Processing of Portuguese - Vol. 2, pages 1–5, Santiago de Compostela, Gali-\ncia/Spain. Association for Computational Linguistics.\n\nOliveira, H., Ferreira Mello, R., Barreiros Rosa, B. A., Rakovic, M., Miranda, P.,\nCordeiro, T., Isotani, S., Bittencourt, I., and Gasevic, D. (2023). Towards explainable\nprediction of essay cohesion in portuguese and english. In Proceedings of the 13th In-\nternational Learning Analytics and Knowledge Conference, pages 509–519, Arlington\nTX USA. Association for Computing Machinery.\n\nPage, E. B. (1966). The imminence of... grading essays by computer. The Phi Delta\n\nKappan, 47(5):238–243.\n\nPerelman, L. (2014). When “the state of the art” is counting words. Assessing Writing,\n\n21:104–111.\n\nTay, Y., Phan, M., Tuan, L. A., and Hui, S. C. (2018). Skipflow: Incorporating neural\ncoherence features for end-to-end automatic text scoring. In Proceedings of the Thirty-\nsecond AAAI conference on artificial intelligence, pages 5948–5955, New Orleans,\nLouisiana, USA. AAAI Press.\n\nYannakoudakis, H. and Cummins, R. (2015). Evaluating the performance of automated\ntext scoring systems. In Proceedings of the Tenth Workshop on Innovative Use of NLP\nfor Building Educational Applications, pages 213–223, Denver, Colorado. Association\nfor Computational Linguistics.\n\nYoon, S.-Y., Cahill, A., Loukina, A., Zechner, K., Riordan, B., and Madnani, N. (2018).\nAtypical inputs in educational applications. In Bangalore, S., Chu-Carroll, J., and Li,\nY., editors, Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 3\n(Industry Papers), pages 60–67, New Orleans - Louisiana. Association for Computa-\ntional Linguistics.\n\n\f"
        },
        {
            "titulo": "Avaliação de Algoritmos de Clusterização para Agrupamento de Descrições de Produtos em Notas Fiscais Eletrônicas",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31119",
            "idioma": "Português",
            "storage_key": "files/article_31119_30922.pdf",
            "autores": [
                {
                    "nome": "Jonas Gabriel L. de Araújo",
                    "afiliacao": "UFPB",
                    "orcid": "http://orcid.org/0009-0006-1736-5593"
                },
                {
                    "nome": "Thaís G. do Rêgo",
                    "afiliacao": "UFPB",
                    "orcid": "https://orcid.org/0000-0002-6608-4900"
                },
                {
                    "nome": "Yuri de A. M. Barbosa",
                    "afiliacao": "UFPB",
                    "orcid": "https://orcid.org/0000-0002-7779-0288"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "A nota fiscal eletrônica é essencial para o processo de auditoria fiscal. Este artigo avalia a eficácia de algoritmos de clusterização para agrupar descrições de produtos em notas fiscais eletrônicas, um desafio devido à falta de padronização nos registros. Usando similaridade de strings e ajustes para unidades de medida, foram testados DBSCAN, HDBSCAN, OPTICS e Agglomerative Clustering. As métricas de avaliação incluíram o Coeficiente de Silhueta, Índice de Calinski-Harabasz e a porcentagem de produtos agrupados. O HDBSCAN apresentou o melhor desempenho inicial, e a subclusterização, apesar de melhorar as métricas, introduziu inconsistências nos agrupamentos.",
            "keywords": [
                "Algoritmos de Clusterização",
                "Notas Fiscais Eletrônicas",
                "Similaridade de Strings",
                "Descrições de Produtos",
                "Auditoria Fiscal"
            ],
            "referencias": [
                "Ahmed, M., Tiun, S., Omar, N., and Sani, N. S. (2022). Short text clustering algorithms, application and challenges: A survey. Applied Sciences.",
                "Ester, M., Kriegel, H.-P., Sander, J., Xu, X., et al. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. In kdd, volume 96, pages 226–231",
                "Jaro, M. A. (1989). Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida. Journal of the American Statistical Association, 84(406):414–420.",
                "Mazzarolo, J., Steinmetz, R., and Mergen, S. (2022). Um estudo sobre a falta de padronização na descrição de produtos em notas fiscais eletrônicas. In Anais da XVII Escola Regional de Banco de Dados, pages 31–40, Porto Alegre, RS, Brasil. SBC.",
                "Ribeiro, L., Brandão, W., Marques, I., Andrade, P., Júnior, R., Oliveira, F., and Kelles, R. (2018). Reconhecimento de entidades nomeadas em itens de produto da nota fiscal eletrônica. 36:116–126.",
                "Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20:53–65.",
                "Schulte, J. P., Giuntini, F. T., Nobre, R. A., Nascimento, K. C. d., Meneguette, R. I., Li, W., Gonçalves, V. P., and Rocha Filho, G. P. (2022). Elinac: Autoencoder approach for electronic invoices data clustering. Applied Sciences, 12(6).",
                "Steinbach, M., Karypis, G., and Kumar, V. (2000). A comparison of document clustering techniques.",
                "."
            ],
            "artigo_completo": "Avaliac¸ ˜ao de Algoritmos de Clusterizac¸ ˜ao para Agrupamento\nde Descric¸ ˜oes de Produtos em Notas Fiscais Eletrˆonicas\n\nJonas Gabriel L. de Ara ´ujo1, Tha´ıs G. do Rˆego1, Yuri de A. M. Barbosa1\n\n1Centro de Inform´atica – Universidade Federal da Para´ıba (UFPB)\nJo˜ao Pessoa - PB - Brasil\n\njonas.araujo@academico.ufpb.br, gaudenciothais@gmail.com, yuri@ci.ufpb.br\n\nAbstract. The electronic invoice is essential for the tax audit process. This pa-\nper evaluates the effectiveness of clustering algorithms in grouping product des-\ncriptions from electronic invoices, a key document in tax audits. Due to the lack\nof standardization in these descriptions, clustering becomes a challenge. Using\nstring similarity and adjustments for different units of measurement, DBSCAN,\nHDBSCAN, OPTICS, and Agglomerative Clustering were tested. Evaluation\nmetrics included the Silhouette Coefficient, Calinski-Harabasz Index, and the\npercentage of grouped products. HDBSCAN showed the best initial perfor-\nmance, and the subclustering stage, while improving metrics, introduced in-\nconsistencies in the groups.\n\nResumo. A nota fiscal eletrˆonica ´e essencial para o processo de auditoria fis-\ncal. Este artigo avalia a efic´acia de algoritmos de clusterizac¸ ˜ao para agrupar\ndescric¸ ˜oes de produtos em notas fiscais eletrˆonicas, um desafio devido `a falta\nde padronizac¸ ˜ao nos registros. Usando similaridade de strings e ajustes para\nunidades de medida, foram testados DBSCAN, HDBSCAN, OPTICS e Agglome-\nrative Clustering. As m´etricas de avaliac¸ ˜ao inclu´ıram o Coeficiente de Silhueta,\n´Indice de Calinski-Harabasz e a porcentagem de produtos agrupados. O HDBS-\nCAN apresentou o melhor desempenho inicial, e a subclusterizac¸ ˜ao, apesar de\nmelhorar as m´etricas, introduziu inconsistˆencias nos agrupamentos.\n\n1. Introduc¸ ˜ao\n\nAs Notas Fiscais Eletrˆonicas (NF-e) s˜ao um marco na modernizac¸ ˜ao dos processos fis-\ncais no Brasil, ao melhorar o controle e a fiscalizac¸ ˜ao tribut´aria, o que j´a resultou\nem avanc¸os na arrecadac¸ ˜ao de impostos e no processo de auditoria [Vieira et al. 2019,\nNeto and Lopo Martinez 2016]. No entanto, a an´alise dessas notas enfrenta desafios\ndevido `a falta de padronizac¸ ˜ao nas descric¸ ˜oes de produtos, com erros ortogr´aficos,\nabreviac¸ ˜oes e variac¸ ˜oes nas unidades de medida [Mazzarolo et al. 2022]. Essa incon-\nsistˆencia dificulta a organizac¸ ˜ao e comparac¸ ˜ao de dados, exigindo t´ecnicas computacio-\nnais para agrupar descric¸ ˜oes similares e auxiliar na auditoria fiscal, que requer a corres-\npondˆencia entre o invent´ario das empresas e as notas emitidas por elas. Dessa forma, o uso\nde algoritmos de agrupamento facilita a fiscalizac¸ ˜ao e melhora a eficiˆencia do processo\n[Ribeiro et al. 2018].\n\nNeste contexto,\n\neste estudo busca avaliar algoritmos de agrupamento,\ncomo DBSCAN [Ester et al. 1996], HDBSCAN [Campello et al. 2013], OPTICS\n[Ankerst et al. 1999] e Agglomerative Clustering (AGG) [Steinbach et al. 2000], para\n\n\fagrupar descric¸ ˜oes de produtos e identificar quais algoritmos oferecem o melhor desem-\npenho na organizac¸ ˜ao e interpretac¸ ˜ao dos dados. Para isso, foi empregada uma m´etrica\npersonalizada no c´alculo da matriz de distˆancias, baseada em similaridade entre strings e\na an´alise de uma segunda etapa de agrupamento dos dados.\n\n2. Trabalhos relacionados\n\nNesta sec¸ ˜ao, ser˜ao abordados alguns trabalhos que contribu´ıram para tentar resolver as\ndiferenc¸as na padronizac¸ ˜ao nas descric¸ ˜oes dos produtos, a fim de melhorar o processo de\nfiscalizac¸ ˜ao tribut´aria no Brasil [Mazzarolo et al. 2022].\n\nO trabalho de [Schulte et al. 2022] apresentou o ELINAC, um modelo que com-\nbina autoencoder e busca bin´aria para agrupar descric¸ ˜oes de produtos em notas fiscais. O\nm´etodo filtra as descric¸ ˜oes, considerando apenas o nome e informac¸ ˜oes num´ericas, como\nquantidade e dosagem. Embora eficiente, ele tem limitac¸ ˜oes ao distinguir produtos com\nvariac¸ ˜oes sutis, como sabor.\n\nA revis˜ao de [Ahmed et al. 2022] aponta que a representac¸ ˜ao vetorial de tex-\ntos curtos ´e desafiadora devido `a alta dimensionalidade e ao ru´ıdo. O estudo de\n[Marinho et al. 2024] comparou representac¸ ˜oes textuais para classificar inconsistˆencias\nem notas fiscais, calculando a similaridade entre a descric¸ ˜ao do produto e a oficial da\nNomenclatura Comum do Mercosul (NCM). Concluiu-se que a distˆancia de edic¸ ˜ao de\nstrings teve melhor desempenho preditivo do que embeddings, apesar de n˜ao considerar a\nsimilaridade entre produtos.\n\nEste estudo se diferencia ao focar na avaliac¸ ˜ao de algoritmos de agrupamento e\nna representac¸ ˜ao de descric¸ ˜oes de NF-es utilizando similaridade de strings. Enquanto\noutros trabalhos abordam redes neurais, detecc¸ ˜ao de fraudes e visualizac¸ ˜ao de dados, este\nestudo explora a efic´acia dos algoritmos de clusterizac¸ ˜ao para organizar e interpretar as\ndescric¸ ˜oes de produtos em notas fiscais.\n\n3. Metodologia\n\nEsta sec¸ ˜ao descreve a base de dados, o c´alculo da matriz de distˆancias e os algoritmos de\nclusterizac¸ ˜ao utilizados.\n\n3.1. Base de dados\n\nForam usadas duas bases: uma base sint´etica com 22 descric¸ ˜oes, contendo ru´ıdos t´ıpicos\n[Mazzarolo et al. 2022], e uma base real cedida pela Secretaria da Fazenda da Para´ıba\n(SEFAZ-PB) com 507 descric¸ ˜oes. As descric¸ ˜oes foram normalizadas, removendo carac-\nteres especiais e convertendo tudo para caracteres mai´usculos.\n\n3.2. Matriz de distˆancias\n\nUma matriz de distˆancias ´e uma matriz quadrada que cont´em as distˆancias entre todos\nos pares de elementos do banco de dados. Neste trabalho, a matriz foi feita a partir\nde uma m´etrica personalizada, baseada na similaridade de Jaro [Jaro 1989]. O valor da\nsimilaridade varia entre 0 e 1, onde 0 indica que as strings n˜ao tˆem correspondˆencias e 1\nindica que as strings s˜ao idˆenticas. Entretanto, para o conceito de distˆancia, quanto mais\npr´oximo de 0, mais pr´oximos s˜ao dois pontos. Dessa forma, para computar a matriz de\ndistˆancia, foi calculado o complemento da similaridade de Jaro, ou seja, 1−JaroSimilarity.\n\n\fAl´em da similaridade textual, foi introduzido um c´alculo adicional para diferen-\nciar produtos com o mesmo nome, mas com medidas distintas, como “200 ML” e “10\nKG”. Isso evita que produtos com variac¸ ˜ao apenas na quantidade sejam considerados\niguais. Para implementar esse ajuste, as medidas foram extra´ıdas por meio da express˜ao\nregular 1 [Lucena et al. 2022], e convertidas em mililitros, gramas ou metros. Quando as\nmedidas diferem, adiciona-se uma penalidade de 0,3 ao complemento da similaridade de\nJaro, valor que foi escolhido ap´os testes com variac¸ ˜oes entre 0,1 e 0,5.\n\n(?:\\ d *[,]?\\ d+?\\s?(?:kg|ml|mm|l| lt | gr | grs |g|metros|m|gb|k|cm|mg)\\b)\n\n(1)\n\n3.3. Algoritmos de Clusterizac¸ ˜ao\nPara o agrupamento,\neste estudo avaliou 4 algoritmos diferentes: DBSCAN\n[Ester et al. 1996], HDBSCAN [Campello et al. 2013], OPTICS [Ankerst et al. 1999] e\nAGG [Steinbach et al. 2000]. Todos os algoritmos usados foram implementados pela bi-\nblioteca scikit-learn, vers˜ao 1.5.1, e nenhuma m´etrica de distˆancia foi passada para os\nalgoritmos, uma vez que a matriz j´a est´a pr´e-computada.\n\nOs algoritmos foram usados em duas etapas:\n\no agrupamento inicial e a\nsubclusterizac¸ ˜ao dos grupos de outliers, aplicada apenas na base real. Para o agrupa-\nmento inicial, foi definida uma distˆancia m´axima de agrupamento de 0,1 e um tamanho\nm´ınimo de cluster sendo igual a 2. Para a segunda etapa, a distˆancia foi igual a 0,2. Os\nparˆametros de distˆancia foram escolhidos ap´os avaliac¸ ˜ao do agrupamento com variac¸ ˜oes\nentre 0,05 e 0,2 e os demais hiperparˆametros possuem os valores padr˜oes da biblioteca.\n\nPara avaliar o resultado dos agrupamentos, foram utilizadas duas m´etricas prin-\ncipais: o Coeficiente de Silhueta [Rousseeuw 1987], que avalia a coes˜ao dos clusters, e\no ´Indice de Calinski-Harabasz (CH) [Cali´nski and JA 1974], que mede a separac¸ ˜ao en-\ntre os grupos. O c´alculo dessas m´etricas foi feito utilizando as distˆancias entre pontos\npr´e-computadas. Al´em disso, foi considerada a porcentagem de produtos agrupados para\navaliar a cobertura dos dados pelos algoritmos de agrupamento.\n\n4. Resultados e discuss˜oes\n\n´E importante\nA Tabela 1 apresenta os resultados da primeira etapa dos experimentos.\nressaltar que o algoritmo AGG n˜ao gera um grupo de outliers identificado como −1, o que\nexigiu um ajuste no c´alculo das m´etricas para esse caso. Especificamente, todos os grupos\nindividuais, que contˆem apenas um produto, foram considerados como pertencentes ao\ngrupo −1, permitindo que as m´etricas fossem calculadas de forma consistente.\n\nNa base sint´etica, DBSCAN, OPTICS e AGG produziram clusters idˆenticos, en-\nquanto o HDBSCAN teve desempenho superior, distinguindo produtos com variac¸ ˜oes de\nsabor, mas n˜ao separando bem produtos de medidas diferentes. Nos dados reais, o HDBS-\nCAN obteve as melhores m´etricas gerais, enquanto o OPTICS teve o maior coeficiente\nde Silhueta, mas o menor ´ındice de CH, sugerindo que seus clusters n˜ao estavam bem\nseparados.\n\nA Tabela 2 apresenta os resultados da subclusterizac¸ ˜ao dos grupos de produtos\nconsiderados outliers na base de dados real. Todas as m´etricas possu´ıram aumentos\nnos valores, quando comparados `a primeira clusterizac¸ ˜ao, especialmente na utilizac¸ ˜ao\ndo HDBSCAN, tanto na primeira, quanto na segunda etapa.\n\n\fTabela 1. Avaliac¸ ˜ao dos algoritmos de clusterizac¸ ˜ao no agrupamento inicial\n\nBase de Dados Algoritmo\n\nSilhueta\n\nCH\n\nProdutos agrupados (%)\n\nBase\nControlada\n\nBase\nSEFAZ-\nPB\n\nDBSCAN\nHDBSCAN\nOPTICS\nAGG\n\nDBSCAN\nHDBSCAN\nOPTICS\nAGG\n\n0,490\n0,563\n0,490\n0,490\n\n0,686\n0,726\n0,730\n0,696\n\n7,97\n15,58\n7,97\n7,97\n\n21,71\n43,40\n17,98\n20,18\n\n98,16\n99,80\n98,16\n98,16\n\n86,19\n94,08\n85,99\n75,79\n\nEmbora as m´etricas tenham melhorado com a segunda etapa de clusterizac¸ ˜ao\nusando o HDBSCAN, surgiram inconsistˆencias nos agrupamentos. Por exemplo, produ-\ntos como “BOM TRIGO PREP. EMULSIF.” e “MARG. MEDALHA DE OURO” foram\nagrupados erroneamente no mesmo cluster. Isso indica que a fase adicional pode priorizar\na melhoria das m´etricas, mas comprometer a consistˆencia semˆantica, tornando os clusters\nmenos ´uteis ou interpret´aveis na pr´atica.\n\nTabela 2. Avaliac¸ ˜ao dos algoritmos de clusterizac¸ ˜ao no segundo agrupamento\n\nPrimeira Etapa\n\nSegunda Etapa\n\nSilhueta\n\nCH\n\nProdutos agrupados (%)\n\nDBSCAN\n\nHDBSCAN\n\nOPTICS\n\nAGG\n\nDBSCAN\nHDBSCAN\nOPTICS\nAGG\n\nDBSCAN\nHDBSCAN\nOPTICS\nAGG\n\nDBSCAN\nHDBSCAN\nOPTICS\nAGG\n\nDBSCAN\nHDBSCAN\nOPTICS\nAGG\n\n0,718\n0,737\n0,717\n0,719\n\n0,729\n0,740\n0.729\n0,729\n\n0,761\n0,779\n0,760\n0,763\n\n0,728\n0,746\n0,727\n0,729\n\n29,02\n79,30\n27,50\n28,06\n\n46,28\n125,55\n46,28\n46,28\n\n25,25\n73,11\n22,96\n23,46\n\n27,42\n75.52\n26.00\n26,53\n\n90,13\n97,63\n89,74\n89,94\n\n94,47\n99,21\n94,47\n94,47\n\n89,94\n97,63\n89,54\n89,74\n\n89,94\n97,43\n89,54\n89,74\n\n5. Considerac¸ ˜oes finais\n\nEste estudo avaliou os algoritmos de clusterizac¸ ˜ao DBSCAN, HDBSCAN, OPTICS e\nAGG para agrupar descric¸ ˜oes de produtos em NF-e, utilizando similaridade de strings\ncomo representac¸ ˜ao de dados. O HDBSCAN apresentou o melhor desempenho inicial,\nmas a segunda etapa de agrupamento gerou inconsistˆencias. DBSCAN e OPTICS tive-\nram m´etricas um pouco inferiores, por´em com menos irregularidades. Sugere-se, como\ntrabalhos futuros, testar o m´etodo em bases maiores e explorar representac¸ ˜oes como em-\nbeddings e redes neurais para padronizac¸ ˜ao.\n\n\fReferˆencias\n\nAhmed, M., Tiun, S., Omar, N., and Sani, N. S. (2022). Short text clustering algorithms,\n\napplication and challenges: A survey. Applied Sciences.\n\nAnkerst, M., Breunig, M. M., Kriegel, H.-P., and Sander, J. (1999). Optics: ordering\n\npoints to identify the clustering structure. SIGMOD Rec., 28(2):49–60.\n\nCali´nski, T. and JA, H. (1974). A dendrite method for cluster analysis. Communications\n\nin Statistics - Theory and Methods, 3:1–27.\n\nCampello, R. J. G. B., Moulavi, D., and Sander, J. (2013). Density-based clustering based\non hierarchical density estimates. In Pei, J., Tseng, V. S., Cao, L., Motoda, H., and\nXu, G., editors, Advances in Knowledge Discovery and Data Mining, pages 160–172,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\n\nEster, M., Kriegel, H.-P., Sander, J., Xu, X., et al. (1996). A density-based algorithm for\ndiscovering clusters in large spatial databases with noise. In kdd, volume 96, pages\n226–231.\n\nJaro, M. A. (1989). Advances in record-linkage methodology as applied to matching\nthe 1985 census of tampa, florida. Journal of the American Statistical Association,\n84(406):414–420.\n\nLucena, L. F., de Menezes e Silva Filho, T., do Rˆego, T. G., and Malheiros, Y. (2022).\nAutomatic recognition of units of measurement in product descriptions from tax in-\nvoices using neural networks. In Pinheiro, V., Gamallo, P., Amaro, R., Scarton, C.,\nBatista, F., Silva, D., Magro, C., and Pinto, H., editors, Computational Processing of\nthe Portuguese Language, pages 156–165, Cham. Springer International Publishing.\n\nMarinho, M., Weigang, L., Oliveira, V., and Borges, V. (2024). Estrat´egias computacio-\nnais baseadas em similaridade de textos e visualizac¸ ˜ao explorat´oria para a identificac¸ ˜ao\nde inconsistˆencias em notas fiscais eletrˆonicas.\n\nMazzarolo, J., Steinmetz, R., and Mergen, S. (2022). Um estudo sobre a falta de\npadronizac¸ ˜ao na descric¸ ˜ao de produtos em notas fiscais eletrˆonicas. In Anais da XVII\nEscola Regional de Banco de Dados, pages 31–40, Porto Alegre, RS, Brasil. SBC.\nNeto, H. and Lopo Martinez, A. (2016). Nota fiscal de serviC¸ os eletr ˆOnica: Uma an ´Alise\ndos impactos na arrecadaC¸ ˜Ao em munic´Ipios brasileiros. Revista de Contabilidade e\nOrganizac¸ ˜oes, 10:49.\n\nRibeiro, L., Brand˜ao, W., Marques, I., Andrade, P., J´unior, R., Oliveira, F., and Kelles,\nR. (2018). Reconhecimento de entidades nomeadas em itens de produto da nota fiscal\neletrˆonica. 36:116–126.\n\nRousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation\nof cluster analysis. Journal of Computational and Applied Mathematics, 20:53–65.\n\nSchulte, J. P., Giuntini, F. T., Nobre, R. A., Nascimento, K. C. d., Meneguette, R. I., Li,\nW., Gonc¸alves, V. P., and Rocha Filho, G. P. (2022). Elinac: Autoencoder approach\nfor electronic invoices data clustering. Applied Sciences, 12(6).\n\nSteinbach, M., Karypis, G., and Kumar, V. (2000). A comparison of document clustering\n\ntechniques.\n\n\fVieira, P. A., Pimenta, D. P., Cruz, A. F. d., and Souza, E. M. S. d. (2019). Efeitos do\nprograma de nota fiscal eletrˆonica sobre o aumento da arrecadac¸ ˜ao do estado. Revista\nde Administrac¸ ˜ao P´ublica, 53(2):481–491.\n\n\f"
        },
        {
            "titulo": "EyetrackingMOS: Proposta de um método de avaliação online para modelos de síntese de fala",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31120",
            "idioma": "Português",
            "storage_key": "files/article_31120_30923.pdf",
            "autores": [
                {
                    "nome": "Gustavo E. Araújo",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0009-0004-5661-4789"
                },
                {
                    "nome": "Julio C. Galdino",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0001-6378-4648"
                },
                {
                    "nome": "Rodrigo de F. Lima",
                    "afiliacao": "USP",
                    "orcid": null
                },
                {
                    "nome": "Leonardo Ishida",
                    "afiliacao": "USP",
                    "orcid": null
                },
                {
                    "nome": "Gustavo W. Lopes",
                    "afiliacao": "USP",
                    "orcid": null
                },
                {
                    "nome": "Miguel Oliveira Jr.",
                    "afiliacao": "UFAL",
                    "orcid": "https://orcid.org/0000-0002-0866-0535"
                },
                {
                    "nome": "Arnaldo Cândido Jr.",
                    "afiliacao": "UNESP",
                    "orcid": "https://orcid.org/0000-0002-5647-0891"
                },
                {
                    "nome": "Sandra M. Aluísio",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0001-5108-2630"
                },
                {
                    "nome": "Moacir A. Ponti",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2059-9463"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Avaliação de modelos de síntese de fala",
                "língua portuguesa",
                "fala espontânea",
                "rastreamento ocular"
            ],
            "referencias": [
                "ALMEIDA, R. A. S. d., OLIVEIRA JR., M., and COZIJN, R. (2021). Paradigma do Mundo Visual: Método de Rastreamento Ocular, chapter 5. Blucher Open Access.",
                "Batista, N. A. R. (2019). Estudo sobre identificação automática de sotaques regionais brasileiros baseada em modelagens estatísticas e técnicas de aprendizado de máquina. Master’s thesis, Unicamp.",
                "Cagliari, L. C. (1992). Prosódia: algumas funções dos supra-segmentos. Cadernos de estudos linguísticos, 23:137–151.",
                "Casanova, E., Shulby, C., Gölge, E., Müller, N. M., de Oliveira, F. S., Junior, A. C., da Silva Soares, A., Aluisio, S. M., and Ponti, M. A. (2021). Sc-glowtts: an efficient zero-shot multi-speaker text-to-speech model.",
                "Casanova, E., Weber, J., Shulby, C. D., Junior, A. C., Gölge, E., and Ponti, M. A. (2022). Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone. In International Conference on Machine Learning, pages 2709–2720. PMLR.",
                "Caseli, H. M. and Nunes, M. G. V., editors (2024). Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português. BPLN, 2 edition.",
                "Choi, Y., Jung, Y., Suh, Y., and Kim, H. (2022). Learning to maximize speech quality directly using mos prediction for neural text-to-speech. IEEE Access, 10:52621–52629.",
                "Cooper, E., Huang, W.-C., Tsao, Y., Wang, H.-M., Toda, T., and Yamagishi, J. (2024). A review on subjective and objective evaluation of synthetic speech. Acoustical Science and Technology, 45(4):161–183.",
                "Hoogeboom, E., Van Den Berg, R., and Welling, M. (2019). Emerging convolutions for generative normalizing flows. In International conference on machine learning, pages 2771–2780. PMLR.",
                "ITU - R (2017). ITU-T Rec. P.10/G.100 (11/2017): Vocabulary for performance, quality of service and quality of experience. Recommendation P.10/G.100, International Telecommunication Union.",
                "ITU - T (1996). Methods for subjective determination of transmission quality. Recommendation P.800, International Telecommunication Union.",
                "Jia, Y., Zhang, Y., Weiss, R. J., Wang, Q., Shen, J., Ren, F., Chen, Z., Nguyen, P., Pang, R., Moreno, I. L., and Wu, Y. (2019). Transfer learning from speaker verification to multispeaker text-to-speech synthesis.",
                "Ju, Z., Wang, Y., Shen, K., Tan, X., Xin, D., Yang, D., Liu, Y., Leng, Y., Song, K., Tang, S., Wu, Z., Qin, T., Li, X.-Y., Ye, W., Zhang, S., Bian, J., He, L., Li, J., and Zhao, S. (2024). Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models.",
                "Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. (2016). Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29.",
                "Le Maguer, S., King, S., and Harte, N. (2024). The limits of the mean opinion score for speech synthesis evaluation. Computer Speech Language, 84:101577.",
                "Ling, L., Fernandes Tavares, T., Barbosa, P., and Batista, N. (2018). Detecção automática de sotaques regionais brasileiros: A importância da validação cross-datasets.",
                "Loizou, P. C. (2011). Speech Quality Assessment, pages 623–654. Springer Berlin Heidelberg, Berlin, Heidelberg.",
                "Mitchell, D. C. (2004). On-line methods in language processing: introduction and historical review. In Carreiras, M. and Clifton Jr., C., editors, The On-line Study of Sentence Comprehension: Eyetracking, ERP and Beyond, pages 15–32. Psychology Press, New York.",
                "Mota, J. A., Ribeiro, S. S. C., and de Oliveira, J. M. (2023). Atlas Linguístico Do Brasil: Comentários às Cartas Linguísticas I-V 3. Universidade Estadual de Londrina. Editora.",
                "Nguyen, T.-N., Pham, N.-Q., and Waibel, A. (2023). Syntacc: Synthesizing multi-accent speech by weight factorization. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE.",
                "Ren, Y., Hu, C., Tao, X., Zhao, Z., Zhang, X., Li, Q., Lei, L., Zhou, S., Liu, J., and Liu, S. (2021). Fastspeech 2: Fast and high-quality end-to-end text to speech. In International Conference on Learning Representations.",
                "Ren, Y., Zhao, Z., Tan, X., Yi, J., Cheng, Y.-L., Yang, J., Qin, T., and Liu, T.-Y. (2022). Naturalspeech: End-to-end text to speech synthesis with human-level quality. In Advances in Neural Information Processing Systems.",
                "Ribeiro, F., Florêncio, D., Zhang, C., and Seltzer, M. (2011). Crowdmos: An approach for crowdsourcing mean opinion score studies. In 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 2416–2419. IEEE.",
                "Sellam, T., Bapna, A., Camp, J., Mackinnon, D., Parikh, A. P., and Riesa, J. (2023). Squeak: Measuring speech naturalness in many languages. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE.",
                "Shen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin, T., Zhao, S., and Bian, J. (2023). Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers.",
                "Tan, X., Chen, J., Liu, H., Cong, J., Zhang, C., Liu, Y., Wang, X., Leng, Y., Yi, Y., He, L., Soong, F., Qin, T., Zhao, S., and Liu, T.-Y. (2022). Naturalspeech: End-to-end text to speech synthesis with human-level quality.",
                "Ynoguti, C. A. (1999). Reconhecimento de Fala Contínua Utilizando Modelos Ocultos de Markov. PhD thesis, Unicamp."
            ],
            "artigo_completo": "EyetrackingMOS: Proposta de um m´etodo de avaliac¸ ˜ao online\npara modelos de s´ıntese de fala\n\nGustavo E. Ara ´ujo1, Julio C. Galdino1, Rodrigo de F. Lima1, Leonardo Ishida1,\nGustavo W. Lopes1, Miguel Oliveira Jr.2, Arnaldo Candido Jr.3, Sandra M. Alu´ısio1,\nMoacir A. Ponti1\n\n1 Universidade de S˜ao Paulo (USP)\n\n2Universidade Federal de Alagoas (UFAL)\n\n3Universidade Estadual de S˜ao Paulo (UNESP)\n\n{gustavo evangelista,juliogaldino,guico21,leoishida,gustavowlopes}@usp.br\n\nmiguel@fale.ufal.br, arnaldo.candido@unesp, {moacir, sandra}@icmc.usp.br\n\nAbstract. Evaluating Text-To-Speech (TTS) systems is challenging, as the in-\ncreasing quality of synthesis makes it difficult to discriminate models’ ability to\nreproduce prosodic attributes, especially for Brazilian Portuguese. Offline eva-\nluation metrics do not capture our genuine reactions to audio stimuli. Therefore,\nwe propose an online evaluation method using eye-tracking. Our experiments\nwith 76 annotators show a reasonable correlation between EyetrackingMOS\nand MOS, as well as a reduction in the total evaluation time. We believe this\nmetric provides precise and potentialy fast information to complement existing\nevaluation methods.\n\nResumo. Avaliar sistemas Text-To-Speech (TTS) ´e um desafio, uma vez que a\nqualidade crescente da s´ıntese imp˜oe obst´aculos em discriminar a capacidade\nde modelos em reproduzir atributos pros´odicos, especialmente para o portuguˆes\nbrasileiro. M´etricas de avaliac¸ ˜ao offline n˜ao medem a reac¸ ˜ao genu´ına de ava-\nliadores aos est´ımulos de ´audios. Prop˜oe-se, portanto, um m´etodo de avaliac¸ ˜ao\nonline com rastreamento de globo ocular. Os experimentos com 76 anotadores\napontam que h´a uma correlac¸ ˜ao razo´avel entre EyetrackingMOS e MOS, assim\ncomo uma reduc¸ ˜ao em sua durac¸ ˜ao total. Desta forma, acredita-se que esta\nm´etrica fornec¸a uma informac¸ ˜ao precisa e potencialmente r´apida para comple-\nmentar os m´etodos de avaliac¸ ˜ao.\n\nIndex Terms: Speech Synthesis Models Evaluation, Portuguese language, spontaneous\nspeech, eyetracking\n\n1. Introduc¸ ˜ao\n\nSistemas de texto-para-fala, do inglˆes Text-To-Speech (TTS) buscam vocalizar um texto\nescrito em n´ıveis pr´oximos a naturalidade de fala humana [Caseli and Nunes 2024]. Os\navanc¸os em Aprendizado Profundo impulsionaram o desenvolvimento de tais sistemas.\nPosteriormente, a utilizac¸ ˜ao de modelos gerativos baseados em fluxo, como os propostos\npor [Kingma et al. 2016] e [Hoogeboom et al. 2019], tem permitido maior flexibilidade\n\n\fna manipulac¸ ˜ao de caracter´ısticas pros´odicas1 da fala sint´etica. Os resultados de modelos\ndo estado da arte j´a reproduzem a identidade dos locutores com bastante naturalidade em\ncondic¸ ˜oes mais amplas de dados [Casanova et al. 2022, Tan et al. 2022].\n\nEntretanto, modelos de s´ıntese ainda encontram obst´aculos na reproduc¸ ˜ao de as-\npectos espec´ıficos da expressividade individual de falantes. Estes aspectos podem ser\nmedidos atrav´es da entoac¸ ˜ao, durac¸ ˜ao e ritmo da fala [Ju et al. 2024], que s˜ao de natu-\nreza pros´odica, o que se agrava em cen´arios de s´ıntese zero-shot [Casanova et al. 2022,\nJu et al. 2024]. Neste contexto, sistemas contemporˆaneos de TTS investigam outras capa-\ncidades al´em da reproduc¸ ˜ao da identidade de um locutor com naturalidade nos resultados,\ndentre elas o interesse em manter a naturalidade ao gerar fala nas variantes internacio-\nnais de uma l´ıngua (accent-robust), como o Synthesizing Multi-Accent Speech By Weight\nFactorization (SYNTACC) [Nguyen et al. 2023]. A possibilidade de s´ıntese de fala com\nsensibilidade de sotaques internacionais, tamb´em levanta hip´oteses de aplicac¸ ˜oes para va-\nriantes lingu´ısticas regionais de uma dada l´ıngua que conta com menos recursos, afim de\navaliar se a qualidade se preserva. O portuguˆes brasileiro ´e uma l´ıngua que contempla\numa grande quantidade de variantes, dadas as dimens˜oes continentais do Brasil, e devido\na fatores hist´oricos, sociais e culturais [Mota et al. 2023].\n\nPara avaliar a qualidade da fala sintetizada nesses sistemas, s˜ao utilizadas diversas\nm´etricas. As m´etricas subjetivas como: Mean Opinion Score (MOS) [ITU - T 1996],\nCrowd MOS [Ribeiro et al. 2011], Similarity MOS (SMOS) [Jia et al. 2019] e Compa-\nrative MOS (CMOS), por um lado, dependem da opini˜ao e percepc¸ ˜ao de um grupo de\nouvintes humanos. Apesar de importante, este perfil de m´etricas pode oferecer risco para\nan´alise de sotaques a depender da correspondˆencia entre o contexto regional/cultural dos\navaliadores e os ´audios sint´eticos, uma vez que a avaliac¸ ˜ao ser´a influenciada por seus\ncontextos culturais, lingu´ısticos e experiˆencias individuais. Por outro lado, as m´etricas\nobjetivas como: Speaker Encoder Cosine Similarity (SECS) [Casanova et al. 2021], Pro-\nsody Similarity with Prompt, Prosody Similarity with Ground Truth e Word Error Rate\n(WER) [Shen et al. 2023] podem n˜ao capturar completamente a percepc¸ ˜ao humana da\nqualidade do ´audio sobre o desempenho na qualidade de expressividade individual e\nrepresentatividade de variantes lingu´ısticas e, por isso, complementam a an´alise subje-\ntiva. A ausˆencia de uma m´etrica padr˜ao e amplamente aceita dificulta a identificac¸ ˜ao de\ntendˆencias e avanc¸os consistentes no campo do TTS, al´em de dificultar o entendimento de\nquais modelos s˜ao mais adequados para determinados cen´arios ou requisitos espec´ıficos\n(cf. [Le Maguer et al. 2024]). Ambos os perfis tˆem sensibilidades a aspectos diferentes e\nlimitac¸ ˜oes que devem ser avaliadas [Cooper et al. 2024].\n\nAmbas as m´etricas tamb´em podem ser observadas quanto a sua resposta aos\nest´ımulos de ´audios fornecidos durante a avaliac¸ ˜ao. Em m´etodos de avaliac¸ ˜ao of-\nfline (MOS, CrowdMOS, SMOS e CMOS), o indiv´ıduo pontua apenas ap´os ouvir\ntodo o est´ımulo, enquanto m´etodos de avaliac¸ ˜ao online permitem que se registre\nsuas impress˜oes `a medida que o est´ımulo ´e recebido,\ntendo como objetivo captu-\nrar reac¸ ˜oes genu´ınas e momentˆaneas. A avaliac¸ ˜ao de est´ımulos de ´audio utilizando\nrastreamento ocular j´a ´e amplamente empregada em contextos lingu´ısticos, como na\n\n1A pros´odia estuda as func¸ ˜oes dos suprassegmentos, que s˜ao essenciais para a melodia da fala (tom,\nentoac¸ ˜ao, tessitura), para a dinˆamica da fala (durac¸ ˜ao, pausa etc.) e para qualidade da voz (volume, registro\netc.) [Cagliari 1992].\n\n\fan´alise de processamento de linguagem, compreens˜ao auditiva, e percepc¸ ˜ao fon´etica\n[ALMEIDA et al. 2021]. No entanto, sua aplicac¸ ˜ao na avaliac¸ ˜ao de sistemas de s´ıntese\nde fala ainda ´e pouco explorada. Buscamos preencher essa lacuna, propondo um novo\nm´etodo, EyetrackingMOS, que utiliza o rastreamento ocular para avaliac¸ ˜ao de qualidade\ndos ´audios forma mais natural, sem que o participante atribua uma nota de forma direta.\n\nAs principais contribuic¸ ˜oes feitas nesse trabalho s˜ao sumarizadas como se segue:\n\n1. Proposta de um novo m´etodo de avaliac¸ ˜ao de sistemas de s´ıntese de fala que inte-\n\ngra o rastreamento ocular, chamado de EyetrackingMOS;\n\n2. Comparac¸ ˜ao entre o EyetrackingMOS e uma adaptac¸ ˜ao do MOS tradicional, des-\n\ntacando suas respectivas vantagens e limitac¸ ˜oes;\n\n3. Apresentac¸ ˜ao dos experimentos, detalhes de configurac¸ ˜ao do modelo e interfaces\nem um reposit´orio2, facilitando a replicabilidade em diferentes cen´arios e promo-\nvendo avanc¸os na pesquisa sobre s´ıntese de fala.\n\n2. Revis˜ao sobre m´etricas subjetivas para an´alise de sistemas de TTS\n\nNa d´ecada de 1990, a International Telecommunication Union (ITU) padronizou diversos\ntipos de testes de audic¸ ˜ao que eram frequentemente usados na telefonia [ITU - T 1996]. A\npontuac¸ ˜ao baseada em opini˜ao pode ser definida como o valor em uma escala predefinida\nque um sujeito atribui `a sua opini˜ao sobre o desempenho de um sistema [ITU - R 2017,\nLoizou 2011]. A pontuac¸ ˜ao m´edia de opini˜ao, do inglˆes Mean Opinion Score (MOS) ´e\num tipo de Absolute Category Rating (ACR) [Ribeiro et al. 2011]. A MOS emergiu como\no descritor mais popular sobre a percepc¸ ˜ao da qualidade de m´ıdia. Para o c´alculo da MOS,\nhumanos avaliam os ´audios sintetizados e naturais e atribuem uma nota de 1 a 5, no qual\no valor final corresponde `a m´edia das notas de todos os avaliadores. A tabela traduzida\ncom a equac¸ ˜ao correspondente pode ser vista no reposit´orio2.\n\nDiversas variac¸ ˜oes da MOS foram desenvolvidas para atender a diferentes neces-\nsidades de avaliac¸ ˜ao. A Crowd Mean Opinion Score (crowdMOS) prop˜oe uma adaptac¸ ˜ao\nao ambiente tradicional de testes MOS, ao utilizar trabalhadores de uma multid˜ao (do\ninglˆes, crowd) pela internet para realizar avaliac¸ ˜oes em ambientes n˜ao controlados, o que\npermite maior diversidade de ouvintes a um custo reduzido, embora com desafios em\ntermos de controle de qualidade [Ribeiro et al. 2011]. A Similarity Mean Opinion Score\n(SMOS)3, por sua vez, foca na avaliac¸ ˜ao da semelhanc¸a entre ´audios sintetizados e de\nreferˆencia, sendo ´util para medir qu˜ao pr´oximo um ´audio gerado est´a de uma voz original\nem termos de caracter´ısticas ac´usticas e vocais [Ren et al. 2021]. J´a a Comparative Mean\nOpinion Score (CMOS) avalia a qualidade relativa entre duas vers˜oes de ´audio sinteti-\nzado, pedindo aos avaliadores que comparem diretamente os ´audios e apontem qual deles\npossui melhor qualidade, utilizando uma escala de -3 a +3 [Ren et al. 2022]. Cada uma\ndessas variantes da MOS foca em diferentes aspectos da qualidade de ´audio, utilizadas\nde acordo com o que se deseja avaliar. Considerando que estudos tˆem utilizado a MOS\ncomo uma medida de naturalidade da fala em tarefas de s´ıntese (cf. [Sellam et al. 2023],\n[Choi et al. 2022]), a descric¸ ˜ao da caracter´ıstica observada pelo avaliador foi adaptada\npara a avaliac¸ ˜ao de naturalidade (veja a coluna 4 da Tabela 1).\n\n2Acesso\n\nem\nEyetrackingMOS-STIL\n\nhttps://github.com/GustavoEvangelistaAraujo/\n\n3Tamb´em abreviado por SimMOS na literatura.\n\n\f3. EyetrackingMOS\n\nO rastreamento ocular ´e amplamente reconhecido como uma das t´ecnicas mais precisas\npara a avaliac¸ ˜ao online do processamento lingu´ıstico [Mitchell 2004, Kaiser 2013]. Os\nvariados movimentos dos olhos durante o processamento de informac¸ ˜oes podem ser utili-\nzados para inferir como essas informac¸ ˜oes s˜ao processadas, seja durante a leitura de texto\n(est´ımulo de leitura) ou ao observar uma imagem (est´ımulo visual). O resultado ´e obtido a\npartir da porcentagem de tempo em que o avaliador olhou para o lados direito e esquerdo,\nos quais mostram figuras relacionadas aos conceitos que se deseja medir. Assim, registra-\nmos a porcentagem de tempo que o participante permanece com o olhar sobre a figura que\nrepresenta a fala natural. Esta medida pode ser avaliada em um intervalo de 0% a 100%\ne mapeada para a escala MOS como apresentado na Tabela 1. Assim como no MOS, ao\nfinal ´e calculada a m´edia das notas de todos os avaliadores.\n\nTabela 1. Mapeamento entre pontuac¸ ˜oes do EyetrackingMOS e MOS\n\nTempo de fixac¸ ˜ao (%) Avaliac¸ ˜ao MOS Qualidade Naturalidade\n81 a 100\n61 a 80\n41 a 60\n21 a 40\n0 a 20\n\nExtremamente natural\nMuito natural\nRazoavelmente natural\nPouco natural\nNada natural\n\nExcelente\nBoa\nRazo´avel\nPobre\nRuim\n\n5\n4\n3\n2\n1\n\n4. Materiais e m´etodos\n\n4.1. Descric¸ ˜ao do conjunto de dados\n\nH´a uma carˆencia de conjuntos de dados de ´audio com variantes lingu´ısticas regi-\nonais. Corpus como BRACCENT, utilizado em [Batista 2019], [Ling et al. 2018] e\n[Ynoguti 1999], n˜ao apresentam volume satisfat´orio de dados, assim como n˜ao tratam\nda fala espontˆanea. Portanto, foi escolhido para este estudo um recorte de ´audios de um\ngrande dataset do Museu da Pessoa4, um museu virtual e colaborativo de hist´orias de\nvida, que s˜ao do tipo entrevistas biogr´aficas, compilado pelo projeto Tarsila5. Detalhes do\nrecorte preliminar do corpus (MuPe-v1) est˜ao dispon´ıveis no reposit´orio2.\n\n4.2. Modelo de s´ıntese de fala\n\nO modelo SYNTACC [Nguyen et al. 2023] ´e uma arquitetura para s´ıntese de fala com\nm´ultiplos sotaques baseada no YourTTS [Casanova et al. 2022]. Similarmente ao an-\ntecessor, utiliza uma arquitetura de codificac¸ ˜ao-decodificac¸ ˜ao baseada em Transformer,\nonde o codificador recebe a sequˆencia de texto como entrada e gera uma representac¸ ˜ao\nintermedi´aria, que ´e posteriormente processada pelo decodificador para gerar o espec-\ntrograma mel, uma representac¸ ˜ao em espectro da frequˆencia ao longo do tempo que ´e\nreconstru´ıda em ´audio por um vocoder.\n\nEsse modelo implementa as seguintes mudanc¸as: na entrada, a arquitetura con-\ncatena 4 embeddings de idiomas trein´aveis em cada caractere de entrada, uma t´ecnica de\nfatorizac¸ ˜ao de pesos (weight factorization), o que permite um treinamento multi-accent.\n\n4https://museudapessoa.org/\n5https://sites.google.com/view/tarsila-c4ai/home\n\n\fEsta abordagem divide os pesos do modelo em componentes compartilhados e espec´ıficos\npara cada variante lingu´ıstica, otimizando o treinamento em cen´arios de poucos recursos.\nIsso possibilita que a s´ıntese de fala seja adapt´avel para o contexto do portuguˆes brasi-\nleiro, sendo poss´ıvel obter um controle expl´ıcito de sotaques pelo congelamento parcial\nde pesos atribu´ıdos a ele e portanto permite que a fala seja sintetizada de forma mais es-\npec´ıfica para cada variante. Detalhes da arquitetura, configurac¸ ˜oes do modelo e etapa de\ntreinamento tamb´em foram disponibilizados no reposit´orio2.\n\n5. Experimentos\n\nA Figura 1 apresenta o fluxo de interac¸ ˜ao do usu´ario neste experimento. Para tanto,\nfoi utilizada a plataforma Gorilla6, uma plataforma paga, com o objetivo de construc¸ ˜ao\ne coleta de tarefas de anotac¸ ˜ao. A sequˆencia de interfaces e o conjunto de dados dos\nest´ımulos tamb´em s˜ao apresentados no reposit´orio2.\n\nFigura 1. Fluxograma da interac¸ ˜ao do usu ´ario em cada experimento\n\nNo experimento elaborado neste trabalho, o processo se inicia com a aceitac¸ ˜ao\ndo Termo de Consentimento Livre e Esclarecido (TCLE). Em seguida, o participante ´e\nconduzido a um tutorial, que tem o objetivo de ambient´a-lo com o experimento subse-\nquente. Para a captura do v´ıdeo, s˜ao utilizadas as cˆameras padr˜oes dos dispositivos pes-\nsoais (apenas computador e notebook) dos usu´arios, caso as configurac¸ ˜oes de iluminac¸ ˜ao\ne qualidade de imagem n˜ao sejam suficientemente boas para permitir que o participante\ncomplete a calibragem sem erros, o participante ´e impedido de continuar. Em conjunto\ncom uma calibragem recorrente, ´e poss´ıvel inferir que a qualidade de rastreamento se\nmantenha desde o in´ıcio at´e o final do experimento. O Gorilla utiliza a biblioteca Web-\ngazer7 para rastreamento ocular. No caso do EyetrackingMOS, ap´os o tutorial, o usu´ario\npassa pela etapa de calibragem, que ´e dividida em duas partes. Primeiro, ´e necess´ario\nposicionar corretamente o rosto em relac¸ ˜ao `a cˆamera. Em seguida, o participante deve\nfixar o olhar em uma sequˆencia de pontos que aparecem aleatoriamente nas extremidades\nda ´area ´util da tela. S˜ao apresentados 10 pontos no total, no qual os 5 primeiros pontos\ntornam-se uma referˆencia de rastreamento, e os 5 seguintes s˜ao repetidos como validac¸ ˜ao\ndos anteriores. Caso haja uma discrepˆancia significativa entre a referˆencia e a validac¸ ˜ao\nem um dos pontos (considerado como a tolerˆancia do teste), a calibragem ´e considerada\nfalha, e o usu´ario precisa repetir o processo. Ap´os uma calibragem bem-sucedida, o parti-\ncipante prossegue e visualiza uma tela com duas imagens vetoriais ilustrativas (um robˆo e\numa figura humana, que trocam de posic¸ ˜ao de forma aleat´oria a cada est´ımulo) enquanto\nouve o ´audio. A pausa ´e uma tela subsequente ao final do ´audio com apenas um sinal\nde “+” por 3 segundos, feita para poder reposicionar o olhar do usu´ario no meio da tela.\n\n6https://gorilla.sc/\n7https://webgazer.cs.brown.edu/\n\n\fEste ciclo de est´ımulo e pausa ´e repetido 15 vezes, e ent˜ao ´e feita uma nova calibragem\npara garantir a qualidade de rastreamento do globo ocular, sendo realizado quatro ciclos\ncompletos, que totalizam 60 est´ımulos.\n\nPor outro lado, no experimento de MOS, ap´os o TCLE e o tutorial, o participante\n´e exposto a 60 est´ımulos de ´audio. Durante o tutorial, s˜ao apresentados trˆes exemplos de\n´audios sintetizados, correspondentes `as pontuac¸ ˜oes 1, 3 e 5, para ajudar o participante a\nalinhar suas expectativas. O participante pode ouvir cada est´ımulo mais de uma vez antes\nde decidir sua pontuac¸ ˜ao, utilizando a Tabela 1 como referˆencia para todas as 60 amostras\nde ´audio, conforme descrito na literatura de avaliac¸ ˜ao de modelos de s´ıntese.\n\nA divis˜ao das listas de ´audios para avaliac¸ ˜ao foi realizada considerando dois ti-\npos de ´audios: sintetizados e naturais. Esses ´audios foram organizados em duas listas:\nLista A e Lista B, que foram atribu´ıdas aos participantes de forma equilibrada. 30 ´audios\nnaturais foram colocados na Lista A, enquanto seus correspondentes sintetizados foram\nalocados na lista B. Da mesma forma, 30 ´audios sintetizados foram inclu´ıdos na Lista\nA, com os seus correspondentes naturais na lista B. Quanto aos participantes, conforme\n[Loizou 2011], a proporc¸ ˜ao de avaliac¸ ˜oes subjetivas deve ser de 10 especialistas para 20\nn˜ao especialistas. Foram escolhidos 76 anotadores dentre 28 especialistas e 48 n˜ao es-\npecialistas, distribu´ıdos entre 4 grupos de 19 participantes. Ambos experimentos foram\nelaborados desta mesma forma, o que assegurou uma diversidade de perspectivas nas\navaliac¸ ˜oes, permitindo uma an´alise comparativa abrangente entre as opini˜oes de especia-\nlistas e n˜ao especialistas sobre os ´audios apresentados.\n\n6. Resultados preliminares\n\nA Figura 2 ilustra a relac¸ ˜ao entre os valores mensurados pelo EyetrackingMOS, conver-\ntidos para a escala MOS, e os valores mensurados diretamente pelo MOS. Cada ponto\nazul representa um par de medidas, com o eixo horizontal correspondendo aos valores\ndo EyetrackingMOS convertidos para a escala MOS e o eixo vertical representando os\nvalores obtidos diretamente pelo MOS. A linha verde trac¸ada no gr´afico indica a linha\nde tendˆencia linear, mostrando a direc¸ ˜ao geral da correlac¸ ˜ao entre as duas vari´aveis. A\nFigura 3 ilustra a dispers˜ao das pontuac¸ ˜oes obtidas tanto pelo MOS quanto pelo rastrea-\nmento ocular (EyetrackingMOS) para ´audios reais e sintetizados. Em ambos os testes, os\nparticipantes conseguiram separar razoavelmente bem os ´audios reais dos sintetizados.\n\nFigura 2. Gr ´afico de dispers ˜ao\nentre EyetrackingMOS e MOS\n\nFigura 3. Gr ´afico de dispers ˜ao\npor classe\n\nNo gr´afico de dispers˜ao por MOS (Figura 4), observa-se uma distinc¸ ˜ao clara en-\ntre os ´audios reais, que tendem a receber pontuac¸ ˜oes mais altas, e os sintetizados, que\n\n\fse concentram nas faixas intermedi´arias e baixas. No entanto, no gr´afico de dispers˜ao\npor rastreamento ocular (Figura 5), a separac¸ ˜ao entre ´audios reais e sintetizados ´e me-\nnos evidente. Essa maior dispers˜ao nos resultados do rastreamento ocular ´e esperada, j´a\nque nesse m´etodo os est´ımulos s˜ao percebidos apenas uma vez, enquanto nos m´etodos\noffline como o MOS, o anotador pode ouvir o est´ımulo repetidas vezes antes de tomar sua\ndecis˜ao, resultando em uma separac¸ ˜ao mais clara entre os tipos de ´audio. Assim, o rastre-\namento ocular oferece uma avaliac¸ ˜ao mais detalhada, capturando variac¸ ˜oes mais sutis na\npercepc¸ ˜ao da qualidade dos ´audios.\n\nFigura 4. Gr ´afico de dispers ˜ao por\nMOS\n\nFigura 5. Gr ´afico de dispers ˜ao por\nrastreamento ocular\n\nOs resultados apresentados na Tabela 2 indicam uma correlac¸ ˜ao razo´avel entre o\nEyetrackingMOS e o MOS, com uma m´etrica R² de 56%, sugerindo que o Eyetracking-\nMOS explica 56% da variˆancia observada no MOS. O desvio padr˜ao do erro entre as duas\nm´etricas ´e de 0,72 unidades, mostrando que, em geral, elas tendem a ser pr´oximas, com\numa diferenc¸a m´edia de menos de uma unidade. Al´em disso, o MOS tende a classificar\num n´umero maior de ´audios com a nota m´axima ou valores pr´oximos, enquanto o Eye-\ntrackingMOS oferece uma an´alise mais detalhada, por sua escala ser de 0 a 100, o que\n´e observado em ´audios de alta qualidade. Essa dispers˜ao indica que, embora exista uma\ncorrelac¸ ˜ao razo´avel entre as duas m´etricas, conforme evidenciado pela inclinac¸ ˜ao positiva\nda linha de tendˆencia, as medidas n˜ao s˜ao perfeitamente alinhadas, refletindo diferenc¸as\nna maneira como cada m´etodo capta e avalia a qualidade dos ´audios.\n\nTabela 2. Medidas de performance estat´ıstica\n\nMedida\nPearson\nMean Squared Error (MSE)\nRooted Mean Squared Error (RMSE)\nR2\nSpearman\n\nInterpretac¸ ˜ao geral\nValor\nCorrelac¸ ˜ao moderada\n0.744\nErro m´edio baixo\n0.710\nErro m´edio baixo\n0.844\n0.553 Explica 55% da variˆancia\nCorrelac¸ ˜ao moderada\n0.714\n\nTamb´em foi realizada uma an´alise da concordˆancia entre os avaliadores den-\ntro de seus respectivos grupos, utilizando o coeficiente de Kendall’s W para avaliar a\nconsistˆencia das respostas (Tabela 3). Em resumo, o grupo EyetrackingMOS apresen-\ntou maior consistˆencia nas avaliac¸ ˜oes, com alta concordˆancia na maioria dos est´ımulos,\nenquanto o grupo MOS demonstrou uma maior variabilidade, com concordˆancia que\nvariou de alta at´e nenhuma, indicando poss´ıveis desafios na avaliac¸ ˜ao uniforme dos\n\n\fest´ımulos por este grupo. Com relac¸ ˜ao ao tempo, EyetrackingMOS e MOS tomaram\nem m´edia 12:07min e 12:30min dos participantes, respectivamente. As medianas foram\nde 11:38min e 10:41min, respectivamente. Nota-se que o teste MOS tende a ser em torno\nde 1 minuto mais r´apido que o EyetrackingMOS que pode ser justificada pelo tempo das\n4 calibrac¸ ˜oes do rastreamento ocular.\n\nTabela 3. Medidas de concord ˆancia para cada grupo de experimentos\n\nGrupo\nEyetrackingMOS\n\nIntervalo de Kendall’s W Interpretac¸ ˜ao geral\n0.6719 a 0.9579\n\nAlta concordˆancia geral, algumas\nvariac¸ ˜oes\nGrande variac¸ ˜ao, alta concordˆancia\na nenhuma concordˆancia\n\nMOS\n\n0.0000 a 0.9474\n\n7. Conclus˜ao e trabalhos futuros\nConforme os resultados preliminares, o EyetrackingMOS e MOS tˆem uma correlac¸ ˜ao\nrazo´avel. Paralelamente, a utilizac¸ ˜ao de uma medida de avaliac¸ ˜ao subjetiva com rastre-\namento ocular oferece vantagens significativas, uma vez que permite capturar reac¸ ˜oes\ngenu´ınas e s´ıncronas aos est´ımulos apresentados. Al´em disso, o controle mais rigoroso\nsobre a quantidade de est´ımulos recebidos por cada participante pode reduzir a variac¸ ˜ao na\nconcordˆancia e aumentar a quantidade de est´ımulos por sess˜ao. A reac¸ ˜ao mecˆanica ocular\ntamb´em pode reduzir variac¸ ˜oes na concordˆancia, causadas pelas diferentes interpretac¸ ˜oes\ndas descric¸ ˜oes de pontuac¸ ˜ao de m´etricas subjetivas. A escala de 0 a 100 para cada in-\ndiv´ıduo oferece uma avaliac¸ ˜ao mais detalhada e precisa, permitindo uma maior granu-\nlaridade na an´alise das respostas, ao contr´ario das escalas limitadas a poucos pontos.\nEmbora a produc¸ ˜ao dessa medida seja mais complexa e demorada, o benef´ıcio de ob-\nter uma an´alise mais transparente das reac¸ ˜oes dos participantes justifica seu uso como\ncomplemento do MOS tradicional.\n\nComo\n\nfuturos,\n\ntrabalhos\n\ndiferentes\n\nexperimentar\n\ntecnolo-\npretende-se\ngias/plataformas de captac¸ ˜ao ocular para comparar a precis˜ao da captac¸ ˜ao. Tamb´em ´e\nimportante obter dados estat´ısticos com uma distinc¸ ˜ao das pontuac¸ ˜oes fornecidas entre\nos grupos de especialistas e n˜ao especialistas. Al´em disso, a selec¸ ˜ao de vari´aveis deve ser\nrefinada, como, por exemplo, calcular a fixac¸ ˜ao no espac¸o intermedi´ario entre as imagens,\no que pode oferecer uma compreens˜ao mais detalhada das reac¸ ˜oes dos participantes. Por\nfim, explorar maneiras de realizar esses testes gratuitamente, seja por meio de parcerias,\nuso de plataformas de crowdsourcing ou outras abordagens que reduzam os custos e\nampliem o acesso aos participantes.\n\n8. Agradecimentos\nEste trabalho foi realizado no Centro de Inteligˆencia Artificial (C4AI-USP), com o\napoio da Fundac¸ ˜ao de Amparo `a Pesquisa do Estado de S˜ao Paulo (FAPESP, bolsa\n#2019/07665-4) e da IBM Corporation. O projeto tamb´em foi apoiado pelo Minist´erio\nda Ciˆencia, Tecnologia e Inovac¸ ˜oes, com recursos da Lei nº 8.248, de 23 de outubro de\n1991, no ˆambito do PPI-SOFTEX, coordenado pela Softex e publicado como Residˆencia\nem TIC 13, DOU 01245.010222/2022-44. Agradecimentos tamb´em s˜ao dirigidos ao Pro-\ngrama de Excelˆencia Acadˆemica (PROEX) da Coordenac¸ ˜ao de Aperfeic¸oamento de Pes-\nsoal de N´ıvel Superior - Brasil (CAPES), nº 88887.841258/2023-00.\n\n\fReferˆencias\n\nALMEIDA, R. A. S. d., OLIVEIRA JR., M., and COZIJN, R. (2021). Paradigma do\nMundo Visual: M´etodo de Rastreamento Ocular, chapter 5. Blucher Open Access.\n\nBatista, N. A. R. (2019). Estudo sobre identificac¸ ˜ao autom´atica de sotaques regionais\nbrasileiros baseada em modelagens estat´ısticas e t´ecnicas de aprendizado de m´aquina.\nMaster’s thesis, Unicamp.\n\nCagliari, L. C. (1992). Pros´odia: algumas func¸ ˜oes dos supra-segmentos. Cadernos de\n\nestudos lingu´ısticos, 23:137–151.\n\nCasanova, E., Shulby, C., G¨olge, E., M¨uller, N. M., de Oliveira, F. S., Junior, A. C.,\nda Silva Soares, A., Aluisio, S. M., and Ponti, M. A. (2021). Sc-glowtts: an efficient\nzero-shot multi-speaker text-to-speech model.\n\nCasanova, E., Weber, J., Shulby, C. D., Junior, A. C., G¨olge, E., and Ponti, M. A.\n(2022). Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion\nfor everyone. In International Conference on Machine Learning, pages 2709–2720.\nPMLR.\n\nCaseli, H. M. and Nunes, M. G. V., editors (2024). Processamento de Linguagem Natural:\n\nConceitos, T´ecnicas e Aplicac¸ ˜oes em Portuguˆes. BPLN, 2 edition.\n\nChoi, Y., Jung, Y., Suh, Y., and Kim, H. (2022). Learning to maximize speech quality di-\nrectly using mos prediction for neural text-to-speech. IEEE Access, 10:52621–52629.\n\nCooper, E., Huang, W.-C., Tsao, Y., Wang, H.-M., Toda, T., and Yamagishi, J. (2024). A\nreview on subjective and objective evaluation of synthetic speech. Acoustical Science\nand Technology, 45(4):161–183.\n\nHoogeboom, E., Van Den Berg, R., and Welling, M. (2019). Emerging convolutions for\ngenerative normalizing flows. In International conference on machine learning, pages\n2771–2780. PMLR.\n\nITU - R (2017). ITU-T Rec. P.10/G.100 (11/2017): Vocabulary for performance, qua-\nlity of service and quality of experience. Recommendation P.10/G.100, International\nTelecommunication Union. https://www.itu.int/rec/T-REC-P.10-201711-I/en.\n\nITU - T (1996). Methods for subjective determination of transmission quality. Recom-\n\nmendation P.800, International Telecommunication Union.\n\nJia, Y., Zhang, Y., Weiss, R. J., Wang, Q., Shen, J., Ren, F., Chen, Z., Nguyen, P., Pang,\nR., Moreno, I. L., and Wu, Y. (2019). Transfer learning from speaker verification to\nmultispeaker text-to-speech synthesis.\n\nJu, Z., Wang, Y., Shen, K., Tan, X., Xin, D., Yang, D., Liu, Y., Leng, Y., Song, K., Tang,\nS., Wu, Z., Qin, T., Li, X.-Y., Ye, W., Zhang, S., Bian, J., He, L., Li, J., and Zhao,\nS. (2024). Naturalspeech 3: Zero-shot speech synthesis with factorized codec and\ndiffusion models.\n\nKaiser, E. (2013). Experimental paradigms in psycholinguistics. In Podesva, R. J. and\nSharma, D., editors, Research Methods in Linguistics, pages 135–168. Cambridge Uni-\nversity Press, Cambridge.\n\n\fKingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M.\n(2016). Improved variational inference with inverse autoregressive flow. Advances in\nneural information processing systems, 29.\n\nLe Maguer, S., King, S., and Harte, N. (2024). The limits of the mean opinion score for\n\nspeech synthesis evaluation. Computer Speech Language, 84:101577.\n\nLing, L., Fernandes Tavares, T., Barbosa, P., and Batista, N. (2018). Detecc¸ ˜ao autom´atica\n\nde sotaques regionais brasileiros: A importˆancia da validac¸ ˜ao cross-datasets.\n\nLoizou, P. C. (2011). Speech Quality Assessment, pages 623–654. Springer Berlin Hei-\n\ndelberg, Berlin, Heidelberg.\n\nMitchell, D. C. (2004). On-line methods in language processing: introduction and histo-\nrical review. In Carreiras, M. and Clifton Jr., C., editors, The On-line Study of Sentence\nComprehension: Eyetracking, ERP and Beyond, pages 15–32. Psychology Press.\n\nMota, J. A., Ribeiro, S. S. C., and de Oliveira, J. M. (2023). Atlas Lingu´ıstico Do Brasil:\nComent´arios `as Cartas Lingu´ısticas 1-V. 3. Ed. Universidade Estadual de Londrina.\n\nNguyen, T.-N., Pham, N.-Q., and Waibel, A. (2023). Syntacc: Synthesizing multi-accent\nspeech by weight factorization. In ICASSP 2023-2023 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE.\n\nRen, Y., Hu, C., Tao, X., Zhao, Z., Zhang, X., Li, Q., Lei, L., Zhou, S., Liu, J., and Liu, S.\n(2021). Fastspeech 2: Fast and high-quality end-to-end text to speech. In International\nConference on Learning Representations.\n\nRen, Y., Zhao, Z., Tan, X., Yi, J., Cheng, Y.-L., Yang, J., Qin, T., and Liu, T.-Y. (2022).\nNaturalspeech: End-to-end text to speech synthesis with human-level quality. In Ad-\nvances in Neural Information Processing Systems.\n\nRibeiro, F., Florˆencio, D., Zhang, C., and Seltzer, M. (2011). Crowdmos: An approach\nfor crowdsourcing mean opinion score studies. In 2011 IEEE international conference\non acoustics, speech and signal processing (ICASSP), pages 2416–2419. IEEE.\n\nSellam, T., Bapna, A., Camp, J., Mackinnon, D., Parikh, A. P., and Riesa, J. (2023).\nSquid: Measuring speech naturalness in many languages. In ICASSP 2023-2023 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages\n1–5. IEEE.\n\nShen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin, T., Zhao, S., and Bian, J. (2023).\nNaturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing\nsynthesizers.\n\nTan, X., Chen, J., Liu, H., Cong, J., Zhang, C., Liu, Y., Wang, X., Leng, Y., Yi, Y., He, L.,\nSoong, F., Qin, T., Zhao, S., and Liu, T.-Y. (2022). Naturalspeech: End-to-end text to\nspeech synthesis with human-level quality.\n\nYnoguti, C. A. (1999). Reconhecimento de Fala Cont´ınua Utilizando Modelos Ocultos\n\nde Markov. PhD thesis, Unicamp.\n\n\f"
        },
        {
            "titulo": "Modestos e Sustentáveis: O Ajuste Eficiente Beneficia Modelos de Língua de Menor Escala em Português?",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31121",
            "idioma": "Português",
            "storage_key": "files/article_31121_30924.pdf",
            "autores": [
                {
                    "nome": "Gabriel Assis",
                    "afiliacao": "UFF",
                    "orcid": "http://orcid.org/0009-0000-2674-0427"
                },
                {
                    "nome": "Arthur Vasconcelos",
                    "afiliacao": "UFF",
                    "orcid": null
                },
                {
                    "nome": "Lívia de Azevedo",
                    "afiliacao": "UFF",
                    "orcid": null
                },
                {
                    "nome": "Mariza Ferro",
                    "afiliacao": "UFF",
                    "orcid": "https://orcid.org/0000-0003-0191-582X"
                },
                {
                    "nome": "Aline Paes",
                    "afiliacao": "UFF",
                    "orcid": "https://orcid.org/0000-0002-9089-7303"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Modelos de Língua têm estabelecido novos padrões de desempenho em tarefas textuais. Porém, tais modelos exigem grandes volumes de dados e recursos computacionais intensivos. Este estudo explora o uso de técnicas de Ajuste Fino Eficiente de Parâmetros (PEFT), especificamente LoRA e GreenTrainer, aplicadas a modelos especializados para o portugues, OPT-PTBR e PTT5. Almeja-se avaliar se as técnicas de PEFT mantém o desempenho dos modelos enquanto mitigam os impactos financeiros e ambientais do uso intensivo de recursos, mesmo em modelos menores. Os resultados mostram que o GreenTrainer, particularmente, oferece desempenho competitivo em relação ao Ajuste Fino completo, enquanto reduz significativamente demandas computacionais.",
            "keywords": [
                "ajuste eficiente de parâmetros",
                "modelos de língua de menor escala",
                "recursos limitados",
                "sumarização"
            ],
            "referencias": [
                "Cabral, B., Claro, D., and Souza, M. (2024). Exploring Open Information Extraction for Portuguese Using Large Language Models. In Proceedings of the 16th International Conference on Computational Processing of Portuguese, pages 127–136.",
                "Freitas, C. (2024). Dataset e corpus. In Caseli, H. M. and Nunes, M. G. V., editors, Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português, book chapter 13. BPLN, 2 edition.",
                "Fu, J., Ng, S.-K., Jiang, Z., and Liu, P. (2024). GPTScore: Evaluate as You Desire. In Duh, K., Gomez, H., and Bethard, S., editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6556–6576, Mexico City, Mexico. Association for Computational Linguistics.",
                "Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR.",
                "Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations.",
                "Huang, K., Yin, H., Huang, H., and Gao, W. (2024). Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation. In The Twelfth International Conference on Learning Representations.",
                "Kato, M. A., Martins, A. M., and Nunes, J. (2023). The Syntax of Portuguese. Cambridge Syntax Guides. Cambridge University Press.",
                "Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T. (2019). Quantifying the Carbon Emissions of Machine Learning. arXiv preprint arXiv:1910.09700.",
                "Li, P., Yang, J., Islam, M. A., and Ren, S. (2023). Making AI less “Thirsty’’: Uncovering and Addressing the Secret Water Footprint of AI models.",
                "Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
                "Maslej, N., Fattorini, L., Perrault, R., Parli, V., Reuel, A., Brynjolfsson, E., Etchemendy, J., Ligett, K., Lyons, T., Manyika, J., Niebles, J. C., Shoham, Y., Wald, R., and Clark, J. (2024). Artificial Intelligence Index Report 2024.",
                "Paes, A., Vianna, D., and Rodrigues, J. (2024). Modelos de linguagem. In Caseli, H. M. and Nunes, M. G. V., editors, Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português, book chapter 15. BPLN, 2 edition.",
                "Paiola, P. H. (2022). Sumarização abstrativa de textos em português utilizando aprendizado de máquina. Mestrado em ciências da computação, Universidade Estadual Paulista Júlio de Mesquita Filho, [s.l.]. Programa de Pós-Graduação em Ciência da Computação.",
                "Paiola, P. H., Garcia, G. L., Jodas, D. S., Correia, J. V. M., Sugi, L. A., and Papa, J. P. (2024). RecognaSumm: A Novel Brazilian Summarization Dataset. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, pages 575–579, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1).",
                "Schwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. (2020). Green AI. Communications of the ACM, 63(12):54–63.",
                "Souza, J. W. d. C., Cardoso, P. C. F., and Paixão, C. A. (2024). Sumarização automática. In Caseli, H. M. and Nunes, M. G. V., editors, Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português, book chapter 22. BPLN, 2 edition.",
                "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY, USA. Curran Associates Inc.",
                "Yang, Y., Zhou, J., Wong, N., and Zhang, Z. (2024). LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models. In Duh, K., Gomez, H., and Bethard, S., editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3161–3176, Mexico City, Mexico. Association for Computational Linguistics.",
                "Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2020). BERTScore: Evaluating Text Generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
                "Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y. (2024b). GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection."
            ],
            "artigo_completo": "Modestos e Sustent´aveis: O Ajuste Eficiente Beneficia\nModelos de L´ıngua de Menor Escala em Portuguˆes?\n\nGabriel Assis, Arthur Vasconcelos, L´ıvia de Azevedo, Mariza Ferro, Aline Paes\n\nInstituto de Computac¸ ˜ao, Universidade Federal Fluminense, Niter´oi, RJ, Brasil\n\n{assisgabriel,athurbittencourt,liviaazevedosilva}@id.uff.br,\n{mariza,alinepaes}@ic.uff.br\n\nAbstract. Language Models have established new performance standards in\ntext-based tasks. Yet, these models require substantial amounts of data and\ncomputational power. This article investigates Parameter Efficient Fine-Tuning\n(PEFT) techniques, specifically LoRA and GreenTrainer, on Portuguese speci-\nalized models OPT-PTBR and PTT5. We aim to evaluate whether PEFT main-\ntains model performance while reducing the financial and environmental costs\nassociated with intensive resource consumption, even in small-scale models.\nOur results show that GreenTrainer, in particular, delivers performance compa-\nrable to full Fine-Tuning while significantly reducing computational demands.\n\nResumo. Modelos de L´ıngua tˆem estabelecido novos padr˜oes de desempenho\nem tarefas textuais. Por´em, tais modelos exigem grandes volumes de dados\ne recursos computacionalis intensivos. Este estudo explora o uso de t´ecnicas\nde Ajuste Fino Eficiente de Parˆametros (PEFT), especificamente LoRA e Gre-\nenTrainer, aplicadas a modelos especializados para o portuguˆes, OPT-PTBR e\nPTT5. Almeja-se avaliar se as t´ecnicas de PEFT mantˆem o desempenho dos mo-\ndelos enquanto mitigam os impactos financeiros e ambientais do uso intensivo\nde recursos, mesmo em modelos menores. Os resultados mostram que o Green-\nTrainer, particularmente, oferece desempenho competitivo em relac¸ ˜ao ao Ajuste\nFino completo, enquanto reduz significativamente demandas computacionais.\n\n1. Introduc¸ ˜ao\n\nModelos de L´ıngua (MLs) Computacionais tˆem como objetivo representar componen-\ntes da l´ıngua humana de forma simplificada usando representac¸ ˜oes num´ericas, mas ten-\ntando preservar seus fundamentos l´exicos, sint´aticos e semˆanticos [Paes et al. 2024].\nNo contexto atual do Processamento de Linguagem Natural\n(PLN), MLs Neu-\nrais — baseados em redes neurais — que empregam a arquitetura Transfor-\nmer [Vaswani et al. 2017] destacam-se por alcanc¸arem resultados no estado-da-arte em\ndiversas tarefas [Wolf et al. 2020]. Particularmente, MLs de larga escala (Large Lan-\nguage Models, LLMs) [Zhao et al. 2023, Paes et al. 2024] estabeleceram novos padr˜oes\npara tarefas generativas, como a sumarizac¸ ˜ao [Fu et al. 2024]. Tais modelos se carac-\nterizam pelo seu vasto n´umero de parˆametros que possibilitam a observac¸ ˜ao de habili-\ndades emergentes, ao resolverem tarefas para as quais n˜ao foram explicitamente treina-\ndos [Paes et al. 2024]. Como consequˆencia, LLMs passaram a ser integrados como com-\nponentes de software e partes essenciais de agentes de conversac¸ ˜ao, expandindo seu uso\npara al´em dos ambientes acadˆemicos e corporativos e tornando-os acess´ıveis por qualquer\nindiv´ıduo com um computador.\n\n\fDessa forma, aumentou-se a demanda pelo desenvolvimento e acesso de LLMs,\nacompanhados por um crescimento expressivo no n´umero de parˆametros desses mode-\nlos [Maslej et al. 2024]. Contudo, o aumento em larga escala de parˆametros apresenta\ndesafios not´aveis, incluindo a necessidade de vastos volumes de dados e um intenso con-\nsumo de recursos computacionais [Zhao et al. 2023]. Neste cen´ario, a Inteligˆencia Arti-\nficial Verde (IA Verde) desponta como uma ´area dedicada a elucidar e reduzir os impac-\ntos computacionais — tanto ambientais, como socioeconˆomicos — do desenvolvimento\nde soluc¸ ˜oes em IA [Schwartz et al. 2020]. Atualmente, o desenvolvimento e a pesquisa\nem modelos de l´ıngua s˜ao dominados por entidades privadas, e com uma concentrac¸ ˜ao\nsignificativa nos Estados Unidos, Uni˜ao Europeia e China [Maslej et al. 2024]. Essa\nconcentrac¸ ˜ao representa um entrave, pois limita a diversificac¸ ˜ao de pesquisa em outras\nregi˜oes, como o Brasil, que enfrentam restric¸ ˜oes de recursos. Al´em disso, a sustentabili-\ndade ambiental emerge como uma quest˜ao cr´ıtica, dado, por exemplo, o alto uso de tempo\nem GPUs para treinamento e operac¸ ˜ao de MLs, que tem como consequˆencias um elevado\nconsumo energ´etico e seu equivalente em emiss˜oes de di´oxido de carbono (CO2eq) e uso\nde ´agua pot´avel [Li et al. 2023].\n\nNo contexto de adaptac¸ ˜ao de MLs, t´ecnicas como o Ajuste Fino (Fine-Tuning)\ne, mais ainda, o Ajuste Fino Eficiente de Parˆametros (Parameter Efficient Fine-Tuning,\nPEFT) [Xu et al. 2023] emergem como abordagens para adaptar LLMs de forma a aliviar\nessas limitac¸ ˜oes. Ambas as abordagens aproveitam o conhecimento previamente codifi-\ncado em MLs Pr´e-Treinados (Pre-trained Language Models, PLMs) [Ding et al. 2023] e\nos adaptam para dom´ınios ou tarefas espec´ıficas. Entretanto, enquanto a primeira abor-\ndagem pode alterar todos os parˆametros do modelo pr´e-treinado, a segunda abordagem\nfoca na adaptac¸ ˜ao considerando explicitamente a limitac¸ ˜ao de recursos. Todavia, diver-\nsos m´etodos de PEFT dependem da selec¸ ˜ao de parˆametros a serem alterados, o que pode\nacarretar em degradac¸ ˜ao de desempenho [Yang et al. 2024].\n\nsob a premissa de que esses modelos\n\nOs m´etodos de PEFT s˜ao tipicamente avaliados em LLMs com bilh˜oes\nde parˆametros,\ns˜ao superparametriza-\ndos [Ding et al. 2023]. Embora haja uma motivac¸ ˜ao natural para reduzir o consumo de\nrecursos por parte desses modelos, sua aplicac¸ ˜ao em grande escala, mesmo que de forma\nmais eficiente, n˜ao elimina completamente as barreiras impostas ao uso de MLs dessa\nmagnitude. Surge, ent˜ao, uma quest˜ao relevante: quais seriam os impactos da aplicac¸ ˜ao\nde t´ecnicas de PEFT em modelos de menor escala em relac¸ ˜ao a sua capacidade de re-\nalizar tarefas espec´ıficas? Adicionalmente, o portuguˆes destaca-se como uma l´ıngua di-\nversificada, apresentando particularidades estruturais significativas, como a relac¸ ˜ao de\nordem das palavras e as variac¸ ˜oes nas desinˆencias, que podem alterar o significado de\numa frase [Kato et al. 2023]. Nesse contexto, outra quest˜ao importante se apresenta: a\naplicac¸ ˜ao de t´ecnicas de PEFT em modelos de menor escala para o portuguˆes afetaria\nnegativamente o desempenho e a representac¸ ˜ao do idioma?\n\nPara responder tais quest˜oes, este artigo contribui com uma avaliac¸ ˜ao entre a abor-\ndagem de ajuste fino completo e t´ecnicas de PEFT, especificamente Low-Rank Adap-\ntation (LoRA) [Hu et al. 2022] e GreenTrainer [Huang et al. 2024], em dois PLMs es-\npec´ıficos para o portuguˆes: OPT-PTBR1, com 125 milh˜oes de parˆametros, e PTT5-\nbase [Carmo et al. 2020], com 223 milh˜oes de parˆametros. Nossos resultados demons-\n\n1https://huggingface.co/monilouise/opt125M_portuguese\n\n\ftram que as t´ecnicas eficientes produzem desempenhos competitivos em relac¸ ˜ao ao ajuste\nfino completo, mesmo em modelos de menor escala. Notavelmente, a t´ecnica GreenTrai-\nner apresentou resultados com menor degradac¸ ˜ao e, em alguns casos, at´e superiores ao\najuste fino completo. Com essa an´alise, buscamos contribuir para a atenuac¸ ˜ao dos impac-\ntos socioeconˆomicos e ambientais do treinamento de MLs, sem deixar de considerar as\nparticularidades do idioma portuguˆes.\n\n2. Fundamentac¸ ˜ao Te´orica\n\nEsta sec¸ ˜ao visa elucidar conceitos fundamentais tratados no trabalho e essenciais no con-\ntexto de ajuste de MLs, especificamente acerca de PLMs e m´etodos de PEFT.\n\n2.1. Ajuste de Modelos de L´ıngua Pr´e-treinados\n\nOs PLMs s˜ao modelos que passam por uma etapa chamada de pr´e-treinamento, cujo ob-\njetivo ´e incorporar informac¸ ˜oes lingu´ısticas relevantes a partir de um grande volume de\ncorpora. Todavia, esses modelos podem n˜ao representar adequadamente informac¸ ˜oes es-\npec´ıficas de certos dom´ınios ou tarefas n˜ao abordadas durante o pr´e-treinamento. Para\ntratar dessa quest˜ao, adota-se amplamente o ajuste fino dos PLMs, no qual os pesos dos\nmodelos s˜ao atualizados para tarefas ou dom´ınios particulares por meio do treinamento\nsobre um novo conjunto de dados espec´ıfico, tipicamente na tarefa final pretendida. Dessa\nforma, ´e poss´ıvel aproveitar o conhecimento previamente codificado sem a necessidade\nde repetir a etapa de pr´e-treinamento, realizando um processo direcionado e geralmente\nmenos oneroso [Paes et al. 2024].\n\n2.2. Ajuste Fino Eficiente de Parˆametros\n\nO conjunto de t´ecnicas de PEFT reduz a demanda por recursos computacionais para\najuste de PLMs. Esses m´etodos s˜ao divididos por [Xu et al. 2023] em aditivo, parcial,\nreparametrizado, unificado e h´ıbrido. O ajuste aditivo introduz uma quantidade me-\nnor de parˆametros adicionais ajust´aveis, evitando o ajuste dos parˆametros pr´oprios do\nmodelo pr´e-treinado. O ajuste parcial atualiza apenas um subconjunto dos parˆametros\npr´e-treinados. A reparametrizac¸ ˜ao utiliza transformac¸ ˜oes de baixo posto da ´Algebra Li-\nnear para reduzir o n´umero de parˆametros trein´aveis. O m´etodo unificado prop˜oe um\nframework coeso que simplifica a integrac¸ ˜ao de t´ecnicas de ajuste fino, garantindo con-\nsistˆencia e eficiˆencia na adaptac¸ ˜ao dos modelos. Por fim, o m´etodo h´ıbrido combina\ndiversas t´ecnicas de PEFT. Em comum, todos os m´etodos ajustam um n´umero reduzido\nde parˆametros dos MLs.\n\ndo\n\nessas\n\ntodas\n\nDentre\n\nt´ecnicas,\n\no m´etodo\n\nreparametrizado\nLoRA [Hu et al. 2022] se destaca como um dos m´etodos de PEFT mais utilizados\npara o ajuste de modelos em diferentes tarefas ao proporcionar consistentemente a\nreduc¸ ˜ao no n´umero de parˆametros trein´aveis e consequente reduc¸ ˜ao na demanda de\nmem´oria [Zhao et al. 2024a, Yang et al. 2024]. Essa estrat´egia utiliza matrizes adicionais\nde baixo posto A e B, que substituem a matriz de pesos original W. A computac¸ ˜ao final\ndos modelos ´e realizada por meio da express˜ao W + A × B, permitindo a adaptac¸ ˜ao dos\npesos com uma quantidade significativamente menor de recursos computacionais.\n\ntipo\n\nEmbora eficaz, a LoRA ainda requer a computac¸ ˜ao dos gradientes de ativac¸ ˜ao\ndurante a etapa de backpropagation no treinamento de modelos, o que limita seu potencial\n\n\fm´aximo de reduc¸ ˜ao de recursos. A estrat´egia GreenTrainer [Huang et al. 2024] surge\ncomo uma alternativa que visa reduzir diretamente as operac¸ ˜oes necess´arias para ajustes\ndos modelos, sem desconsiderar a backpropagation. Ela seleciona tensores espec´ıficos\npara ajuste a cada ´epoca de treinamento, com base na importˆancia de cada tensor para a\ndiminuic¸ ˜ao da loss, caracterizando-se assim como uma t´ecnica de ajuste parcial. Al´em\ndisso, ela permite a configurac¸ ˜ao do hiperparˆametro ρ, que determina a porcentagem de\noperac¸ ˜oes mantidas em relac¸ ˜ao ao ajuste fino completo.\n\nDesse modo, ao considerar uma t´ecnica consolidada e amplamente reconhecida\ncomo a LoRA, e uma abordagem emergente e competitiva, como o GreenTrainer, este\nestudo visa realizar uma avaliac¸ ˜ao inicial acerca do impacto dessas abordagens no de-\nsempenho de PLMs de menor escala em tarefas finais, bem como na reduc¸ ˜ao de seus\ncustos e impactos computacionais.\n\n3. Trabalhos Relacionados\n\nTrabalhos recentes tˆem desenvolvido MLs espec´ıficos para o portuguˆes utilizando\nm´etodos eficientes. Como ilustrac¸ ˜ao, [Carmo et al. 2020] realizaram tanto o ajuste com-\npleto de parˆametros quanto o ajuste restrito aos embeddings do vocabul´ario — um m´etodo\nparcial — no treinamento de um ML voltado para o portuguˆes. Os resultados indicam\nque, embora competitivo, o ajuste restrito aos embeddings ´e inferior ao ajuste com-\npleto. Al´em disso, os estudos de [Garcia et al. 2024] e [Cabral et al. 2024] introduzi-\nram LLMs ajustados especificamente para tarefas em portuguˆes baseados na arquitetura\nLlama [Touvron et al. 2023], empregando a t´ecnica de reparametrizac¸ ˜ao LoRA.\n\nOutros trabalhos avaliam o impacto de t´ecnicas de PEFT sobre o desempenho\nde PLMs. [Yang et al. 2024] comparam o ajuste fino completo a t´ecnicas como LoRA,\nPrefix-tuning [Li and Liang 2021] e o uso de adaptadores [Houlsby et al. 2019] em mo-\ndelos de menor escala da arquitetura BERT [Devlin et al. 2019] em tarefas n˜ao genera-\ntivas, destacando o desempenho da estrat´egia LoRA e a competitividade das demais es-\ntrat´egias de PEFT em relac¸ ˜ao ao ajuste completo nesse contexto. Contudo, tratando-se da\navaliac¸ ˜ao realizada sobre modelos generativos da arquitetura Llama, as t´ecnicas baseadas\nem LoRA se sobressaem. Resultados similares s˜ao reportados em [Huang et al. 2024],\nque, ao proporem a estrat´egia GreenTrainer, realizaram uma avaliac¸ ˜ao comparativa com\noutras t´ecnicas de PEFT, concluindo por sua competitividade. O estudo avaliou PLMs\nmultil´ıngues ou predominantemente voltados ao inglˆes, com parˆametros variando entre\n350 milh˜oes e 7 bilh˜oes, revelando o potencial da aplicac¸ ˜ao de t´ecnicas eficientes at´e\nmesmo nos modelos com menor n´umero de parˆametros.\n\nNo melhor de nosso conhecimento, n˜ao h´a, ainda, trabalho que avalie o uso da\nabordagem GreenTrainer para MLs em portuguˆes. Adicionalmente, nenhum dos trabalhos\nmencionados apresenta uma an´alise comparativa que considere a relac¸ ˜ao do desempenho\nde MLs de menor escala no idioma e o impacto de seu consumo em termos de tempo e\nCO2eq. Desse modo, este estudo visa oferecer novas perspectivas que abordem tanto a\nefic´acia preditiva de modelos, quanto os custos associados a sua etapa de ajuste.\n\n4. Avaliac¸ ˜ao de T´ecnicas de Ajuste Eficiente\n\nEsta sec¸ ˜ao detalha os MLs avaliados, a tarefa de PLN selecionada e as m´etricas de\navaliac¸ ˜ao adotadas para investigar o impacto das t´ecnicas LoRA e GreenTrainer tanto\n\n\fno desempenho textual quanto no consumo computacional. Destaca-se que o ajuste fino\ncompleto dos parˆametros dos modelos foi adotado como baseline.\n\n4.1. Modelos de L´ıngua Selecionados\nA selec¸ ˜ao de MLs para tarefas espec´ıficas ´e influenciada pelo n´umero de parˆametros\ne pelo corpus de pr´e-treinamento, fatores cruciais para a viabilidade de execuc¸ ˜ao em\ndiferentes plataformas de hardware e para as capacidades de gerac¸ ˜ao de texto do mo-\ndelo. Modelos maiores geralmente demandam mais recursos computacionais, enquanto\no corpus de pr´e-treinamento determina a adequac¸ ˜ao do modelo `as necessidades da ta-\nrefa [Freitas 2024]. No contexto de recursos limitados, foram escolhidos dois mode-\nlos: o OPT-PTBR2, com 125 milh˜oes de parˆametros, baseado na arquitetura Open Pre-\ntrained Transformer (OPT) [Zhang et al. 2022] e adaptado para o portuguˆes do Brasil, e\no PTT5-base [Carmo et al. 2020], com 223 milh˜oes de parˆametros, utilizando a arquite-\ntura T5 [Raffel et al. 2020] e pr´e-treinado com um corpus de p´aginas web em portuguˆes\ndo Brasil. A escolha de modelos menores alinha-se com a necessidade de operar em am-\nbientes com recursos limitados, mantendo a avaliac¸ ˜ao da qualidade da gerac¸ ˜ao de textos\nem portuguˆes.\n\n4.2. A Tarefa de PLN Aplicada: Sumarizac¸ ˜ao\nPara garantir a compatibilidade com a implementac¸ ˜ao p´ublica do GreenTrainer3, a ta-\nrefa de sumarizac¸ ˜ao textual foi selecionada. A sumarizac¸ ˜ao por meio de MLs con-\nsiste em condensar as informac¸ ˜oes de um texto, gerando uma nova vers˜ao que\npreserva de forma concisa o conte´udo essencial do original. Essa tarefa ´e ampla-\nmente estudada em PLN, incluindo no contexto do portuguˆes brasileiro [Paiola 2022,\nPontes et al. 2022, Feltrin et al. 2023], com LLMs recentemente estabelecendo novos\npadr˜oes de gerac¸ ˜ao [Souza et al. 2024]. Fatores como a coocorrˆencia de termos relevantes\ne a fidelidade entre texto original e gerado s˜ao importantes para determinar a qualidade\nde um resumo. Igualmente relevantes s˜ao aspectos como a aderˆencia a formalidade e pre-\ncis˜ao gramatical pretendidos. Por exemplo, no contexto jornal´ıstico, resumos de not´ıcias\npol´ıticas podem exigir um n´ıvel de formalidade distinto daquele necess´ario para resumos\nde eventos recentes em um reality show popular, embora, em ambos os casos, a correc¸ ˜ao\ngramatical seja tipicamente fundamental. Assim, a tarefa de sumarizac¸ ˜ao posiciona-se\napropriadamente para a avaliac¸ ˜ao da aplicac¸ ˜ao de t´ecnicas de ajuste de modelos, uma\nvez que a adequac¸ ˜ao a contextos e dom´ınios espec´ıficos ´e fundamental para garantir a\nqualidade das gerac¸ ˜oes textuais [Paes et al. 2024].\n\n4.3. M´etricas de Avaliac¸ ˜ao\nCom o objetivo de avaliar de forma integrada a qualidade do desempenho generativo e\nos custos e impactos computacionais, trˆes grupos de m´etricas foram usados na an´alise\nde gerac¸ ˜ao de sum´arios. O primeiro grupo visa medir a aderˆencia dos resumos gerados\nem relac¸ ˜ao aos textos de referˆencia e ´e composto pelas m´etricas ROUGE [Lin 2004] e\nBERTScore [Zhang et al. 2020]. A m´etrica ROUGE, amplamente utilizada nesse con-\ntexto, compara a sobreposic¸ ˜ao de n-gramas entre o sum´ario autom´atico e a referˆencia, en-\nquanto o BERTScore utiliza modelos de l´ıngua baseados em BERT para avaliar a similari-\ndade semˆantica entre os textos. Neste estudo, a m´etrica ROUGE ´e apresentada pela m´edia\n\n2https://huggingface.co/monilouise/opt125M_portuguese\n3https://github.com/pittisl/GreenTrainer/\n\n\fde suas variantes, ROUGE-1, ROUGE-2, ROUGE-L e ROUGE-S [Souza et al. 2024],\nque se diferenciam na forma de computar os n-gramas, sendo os resultados expressos em\nvalores percentuais. O BERTScore, por sua vez, ´e expresso em termos da sua compo-\nnente F1. O segundo grupo de m´etricas visa mensurar explicitamente o impacto e o con-\nsumo de recursos associados ao ajuste dos modelos, incluindo a contagem do n´umero de\n(peta) operac¸ ˜oes de ponto flutuante por segundo (PFLOPS), que quantifica as operac¸ ˜oes\naritm´eticas necess´arias para o ajuste, o tempo de treinamento dos modelos e a quantidade\nequivalente de CO2 emitida durante o processo, estimada pela ferramenta dispon´ıvel por\n[Lacoste et al. 2019]. Por fim, o terceiro grupo aproveita do extenso conjunto de m´etricas\nfornecidas pelo portal NILC-metrix [Leal et al. 2023]4 para avaliar a qualidade de escrita\ndos textos gerados. Essas m´etricas extraem valores de diversos indicadores lingu´ısticos\npara avaliar informac¸ ˜oes sobre morfossintaxe, coes˜ao e coerˆencia.\n\n5. Experimentos\nEsta sec¸ ˜ao apresenta os experimentos conduzidos, detalhando as configurac¸ ˜oes utilizadas\ne os resultados obtidos.\n\n5.1. Configurac¸ ˜oes Experimentais\n\nHiperparˆametros Considerando a premissa de recursos limitados, os modelos foram\ntreinados por apenas uma ´epoca, com uma taxa de aprendizado de 2 · 10−5 e um tamanho\nde lote de 4. Para a tarefa de sumarizac¸ ˜ao, foram definidos: max input length de 512,\nmax output length de 128, repetition penalty de 2,5 e length penalty de 1,0. No\nque se refere aos parˆametros do LoRA, utilizou-se r = 8, lora alpha = 32 e uma taxa\nde dropout de 0,1. O GreenTrainer foi testado com ρ de 0,5 e 0,7, e implementado\nconforme [Huang et al. 2024]. Tamb´em ao encontro desse trabalho, o modelo OPT-PTBR\nfoi configurado com a estrutura “TL;DR” para sumarizac¸ ˜ao, enquanto o modelo PTT5\nusou o prefixo “sumarize: [sequˆencia de entrada]”. Por fim, o BERTScore foi computado\nutilizando o modelo BERT multilingual5, dada a incompatibilidade da m´etrica com um\nmodelo pr´oprio para o portuguˆes.\n\nConjunto de Dados A tarefa de sumarizac¸ ˜ao ocorreu com a base Recogna-\nSumm [Paiola et al. 2024]. Esse conjunto possui origem diversificada, sendo composto\npor not´ıcias de diferentes fontes de informac¸ ˜ao. Tal diversidade resulta em uma colec¸ ˜ao\nde documentos que abrangem uma variedade de t´opicos e estilos jornal´ısticos. Ademais,\no RecognaSumm cont´em cerca de 135 mil instˆancias em que, para os prop´ositos deste tra-\nbalho, foram selecionadas apenas as colunas referentes ao texto da not´ıcia e ao sum´ario,\nesse ´ultimo servindo como referˆencia padr˜ao nas m´etricas de avaliac¸ ˜ao. Adota-se a sub-\ndivis˜ao pr´e-estabelecida do conjunto de dados, de 81,2 mil instˆancias para treinamento e\n27,1 mil para validac¸ ˜ao e teste cada.\n\n5.2. Resultados Experimentais\n\nA Tabela 1 combina os resultados do primeiro e do segundo grupo de m´etricas avaliados.\nA porcentagem indica a variac¸ ˜ao positiva ou negativa em relac¸ ˜ao ao ajuste fino completo,\ncom resultados com diferenc¸a percentual inferior a 1% marcados com 0%. Por fins de\nsimplificac¸ ˜ao, as configurac¸ ˜oes de ρ para o GreenTrainer s˜ao denotadas GT-ρ.\n\n4http://fw.nilc.icmc.usp.br:23380/metrixdoc\n5https://huggingface.co/google-bert/bert-base-multilingual-cased\n\n\fTabela 1. Comparac¸ ˜ao de efici ˆencia, impacto ambiental e m ´etricas textuais.\n\nModelo\n\nEstrat´egia\n\nPFLOPS\n\nTempo (h)\n\nCO2eq (kg)\n\nROUGE\n\nBERTScore\n\nOPT-PTBR\n(125M params)\n\nPTT5\n(220M params)\n\nAjuste fino\nGT-0.5\nGT-0.7\nLoRA\n\nAjuste fino\nGT-0.5\nGT-0.7\nLoRA\n\n16,59\n8,18 (51%↓)\n11,61 (30%↓)\n11,06 (33%↓)\n\n15,67\n8,76 (44%↓)\n11,50 (27%↓)\n10,45 (33%↓)\n\n3,00\n1,45 (52%↓)\n2,17 (28%↓)\n2,28 (24%↓)\n\n3,73\n2,38 (36%↓)\n2,87 (23%↓)\n2,61 (30%↓)\n\n0,24\n0,12 (50↓%)\n0,17 (29↓%)\n0,18 (25↓%)\n\n0,30\n0,19 (37↓%)\n0,23 (23↓%)\n0,21 (30↓%)\n\n7,48\n4,60 (39%↓)\n7,94 (6%↑)\n7,23 (3%↓)\n\n27,82\n27,16 (2%↓)\n27,56 (1%↓)\n26,20 (6%↓)\n\n0,652\n0,662 (2%↑)\n0,682 (5%↑)\n0,672 (3%↑)\n\n0,742\n0,739 (0%↓)\n0,741 (0%↓)\n0,734 (1%↓)\n\nOs resultados indicam que a estrat´egia GT-0.7 apresentou ou a menor degradac¸ ˜ao,\nou uma melhora no desempenho textual em comparac¸ ˜ao com o ajuste fino em todos os\ncasos avaliados. Em termos de desempenho computacional, seus resultados s˜ao pr´oximos\naos da LoRA, embora ligeiramente inferiores. Observa-se que a configurac¸ ˜ao GT-0.5, a\nmais eficiente em termos de consumo, apresentou uma queda significativa nos resultados\ngenerativos para o OPT-PTBR na m´etrica ROUGE. No entanto, essa mesma configurac¸ ˜ao\nn˜ao resultou em grandes quedas para o modelo PTT5, indicando que a robustez inerente\ndo modelo deve ser considerada ao aplicar estrat´egias de eficiˆencia dr´astica. Na verdade,\nessa configurac¸ ˜ao foi superior `a estrat´egia LoRA para esse modelo.\n\n(a.) ROUGE x PFLOPS\n\n(b.) ROUGE x Tempo\n\n(c.) BERTSore x PFLOPS\n\n(d.) BERTScore x Tempo\n\n30\n\n25\n\n20\n\n15\n\n10\n\n5\n\n30\n\n25\n\n20\n\n15\n\n10\n\n5\n\n8\n\n11\n\n14\n\n17\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\n0.74\n\n0.72\n\n0.7\n\n0.68\n\n0.66\n\n0.64\n\n8\n\n11\n\n14\n\n17\n\n0.74\n\n0.72\n\n0.7\n\n0.68\n\n0.66\n\n0.64\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\nOPT-PTBR\n\nPTT5\n\nGT-0.5\n\nGT-0.7\n\nLoRA\n\nAjuste Fino\n\nFigura 1. Comparativo entre os desempenhos computacionais e textuais.\n\nA Figura 1 contrasta as m´etricas textuais com as medidas de desempenho. Essa\ncomparac¸ ˜ao reforc¸a a estrat´egia GT-0.7 como a que gera resultados mais pr´oximos do\najuste fino, seguida pela estrat´egia LoRA. Fica evidente tamb´em a leve superioridade da\neconomia da estrat´egia LoRA em relac¸ ˜ao `a GT-0.7. Em termos de economia computa-\ncional, a estrat´egia LoRA se posiciona consistentemente entre as configurac¸ ˜oes de GT,\nembora, em termos de resultados generativos, seja inferior `a GT-0.5 para o modelo PTT5.\nAl´em disso, particularmente para o modelo OPT-PTBR, os resultados de BERTScore obti-\ndos pelo ajuste eficiente foram superiores ao ajuste completo. Por fim, a Figura 1 tamb´em\ndemonstra a superioridade geral do modelo PTT5 na execuc¸ ˜ao da tarefa, ressaltando o\nimpacto que a escolha adequada do PLM pode implicar.\n\nA Tabela 2 apresenta a distˆancia euclidiana m´edia, calculada com base em cinco\nconjuntos de m´etricas do NILC-metrix, entre uma amostra de 100 sum´arios gerados para\ncada configurac¸ ˜ao avaliada e suas respectivas referˆencias. Antes do c´alculo, os valores\nforam normalizados para o intervalo de 0 a 1. Os melhores resultados est˜ao destacados\nem negrito, enquanto os segundos melhores est˜ao sublinhados. De modo geral, os valo-\nres semelhantes observados dentro do mesmo modelo, independentemente da estrat´egia\n\n\fTabela 2. M ´etricas em sintaxe, morfologia e sem ˆantica do conjunto NILC-metrix.\n\nModelo\n\nEstrat´egia\n\nCoes˜ao\nReferencial\n\nCoes˜ao\nSemˆantica\n\nInformac¸ ˜oes\nSemˆanticas\n\nComplexidade\nSint´atica\n\nInformac¸ ˜oes\nMorfossint´aticas\n\nOPT-PTBR\n(125M params)\n\nPTT5\n(220M params)\n\nAjuste fino\nGT-0.5\nGT-0.7\nLoRA\n\nAjuste fino\nGT-0.5\nGT-0.7\nLoRA\n\n0,259\n0,269\n0,252\n0,279\n\n0,340\n0,310\n0,326\n0,247\n\n0,667\n0,679\n0,652\n0,760\n\n0,491\n0,505\n0,446\n0,692\n\n0,587\n0,595\n0,560\n0,600\n\n0,513\n0,481\n0,492\n0,560\n\n0,276\n0,255\n0,281\n0,264\n\n0,253\n0,237\n0,267\n0,258\n\n1,150\n1,143\n1,058\n1,158\n\n0,838\n0,853\n0,851\n0,962\n\nde ajuste, indicam que as t´ecnicas de PEFT n˜ao comprometem significativamente a ca-\npacidade de escrita dos modelos de l´ıngua quando comparadas ao ajuste fino. Notavel-\nmente, as configurac¸ ˜oes do GT obtiveram os melhores resultados em v´arias ocorrˆencias.\nNo entanto, uma avaliac¸ ˜ao comparativa entre os modelos revela que o PTT5 consistente-\nmente apresenta desempenho superior, especialmente na avaliac¸ ˜ao de Informac¸ ˜oes Mor-\nfossint´aticas. Esse resultado pode estar relacionado `a etapa de pr´e-treinamento, mais ro-\nbusta nesse modelo, sugerindo que uma execuc¸ ˜ao adequada dessa fase possibilita uma\nestrat´egia de ajuste mais eficiente. No entanto, uma dualidade surge, pois uma etapa de\npr´e-treinamento mais robusta pode resultar em custos mais elevados. De maneira geral,\nesses resultados corroboram os anteriormente descritos, indicando que, al´em da escolha\nda estrat´egia de ajuste, a selec¸ ˜ao do modelo mostra-se crucial.\n\n6. Considerac¸ ˜oes Finais\nEste trabalho conduziu experimentos com estrat´egias de ajuste fino eficiente, empregando\ndois modelos de menor escala treinados em portuguˆes para a tarefa de sumarizac¸ ˜ao tex-\ntual. Os resultados indicam que a estrat´egia do GreenTrainer ´e competitiva em relac¸ ˜ao\n`a estrat´egia j´a estabelecida LoRA. Dependendo da escolha do parˆametro ρ, a estrat´egia\npode, inclusive, alcanc¸ar um equil´ıbrio superior entre a degradac¸ ˜ao de desempenho e o\nganho de eficiˆencia computacional. Al´em disso, os resultados revelam que a aplicac¸ ˜ao\nde estrat´egias eficientes pode implicar degradac¸ ˜oes significativas, dependendo da escolha\ndo modelo. Trabalhos futuros incluem avaliar essas estrat´egias em outros modelos e tare-\nfas, visando obter melhores indicativos sobre a generalizac¸ ˜ao, al´em de considerar novas\nestrat´egias como LoRETTA [Yang et al. 2024] e GaLore [Zhao et al. 2024b].\n\nPor fim, visando `a transparˆencia, explicitamos os custos totais desta pesquisa,\ntotalizando R$1.642,48 em uso de recursos em nuvem. Os experimentos, realizados\nna Google Cloud Platform na regi˜ao us-central1, resultaram em emiss˜oes estimadas de\n14,52 kgCO2eq, com 364 horas de computac¸ ˜ao em duas GPUs T4 (TDP de 70W) e uma\neficiˆencia de carbono de 0,57 kgCO2eq/kWh.\n\nAgradecimentos\nOs autores agradecem ao financiamento do Conselho Nacional de Desenvolvimento Ci-\nent´ıfico e Tecnol´ogico (CNPq), bolsa 307088/2023-5, da Fundac¸ ˜ao Carlos Chagas Fi-\nlho de Amparo `a Pesquisa do Estado do Rio de Janeiro (FAPERJ), processos SEI-\n260003/002930/2024, SEI-260003/000614/2023, e da Coordenac¸ ˜ao de Aperfeic¸oamento\nde Pessoal de N´ıvel Superior (CAPES) — C´odigo Financeiro 001. Tamb´em agradecem\naos recursos do programa Google Cloud Research Credits, c´odigo GCP19980904.\n\n\fReferˆencias\n\nCabral, B., Claro, D., and Souza, M. (2024). Exploring Open Information Extraction for\nPortuguese Using Large Language Models. In Proceedings of the 16th International\nConference on Computational Processing of Portuguese, pages 127–136.\n\nCarmo, D., Piau, M., Campiotti, I., Nogueira, R., and Lotufo, R. (2020). PTT5: Pretrai-\n\nning and validating the T5 model on Brazilian Portuguese data. arXiv.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. In Burstein, J., Doran,\nC., and Solorio, T., editors, Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Techno-\nlogies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\n\nDing, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu, S., Chen, Y., Chan, C.-M.,\nChen, W., Yi, J., Zhao, W., Wang, X., Liu, Z., Zheng, H.-T., Chen, J., Liu, Y., Tang,\nJ., Li, J., and Sun, M. (2023). Parameter-efficient fine-tuning of large-scale pre-trained\nlanguage models. Nature Machine Intelligence, 5(3):220–235.\n\nFeltrin, G., Vianna, D., and da Silva, A. (2023). Um Estudo sobre M´etricas de Avaliac¸ ˜ao\npara Sumarizac¸ ˜ao de Ac´ord˜aos. In Anais do XXXVIII Simp´osio Brasileiro de Bancos\nde Dados, pages 295–305, Porto Alegre, RS, Brasil. SBC.\n\nFreitas, C. (2024). Dataset e corpus. In Caseli, H. M. and Nunes, M. G. V., editors, Pro-\ncessamento de Linguagem Natural: Conceitos, T´ecnicas e Aplicac¸ ˜oes em Portuguˆes,\nbook chapter 13. BPLN, 2 edition.\n\nFu, J., Ng, S.-K., Jiang, Z., and Liu, P. (2024). GPTScore: Evaluate as You Desire. In\nDuh, K., Gomez, H., and Bethard, S., editors, Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers), pages 6556–6576, Mexico City,\nMexico. Association for Computational Linguistics.\n\nGarcia, G. L., Paiola, P. H., Morelli, L. H., Candido, G., J´unior, A. C., Jodas, D. S.,\nAfonso, L., Guilherme, I. R., Penteado, B. E., and Papa, J. P. (2024).\nIntroducing\nBode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task. ar-\nXiv preprint arXiv:2401.02909.\n\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A.,\nAttariyan, M., and Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP.\nIn Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th International\nConference on Machine Learning, volume 97 of Proceedings of Machine Learning\nResearch, pages 2790–2799. PMLR.\n\nHu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen,\nW. (2022). LoRA: Low-Rank Adaptation of Large Language Models. In International\nConference on Learning Representations.\n\nHuang, K., Yin, H., Huang, H., and Gao, W. (2024). Towards Green AI in Fine-tuning\nLarge Language Models via Adaptive Backpropagation. In The Twelfth International\nConference on Learning Representations.\n\n\fKato, M. A., Martins, A. M., and Nunes, J. (2023). The Syntax of Portuguese. Cambridge\n\nSyntax Guides. Cambridge University Press.\n\nLacoste, A., Luccioni, A., Schmidt, V., and Dandres, T. (2019). Quantifying the Carbon\n\nEmissions of Machine Learning. arXiv preprint arXiv:1910.09700.\n\nLeal, S. E., Duran, M. S., Scarton, C. E., Hartmann, N. S., and Alu´ısio, S. M. (2023).\nNILC-Metrix: assessing the complexity of written and spoken language in Brazilian\nPortuguese. Language Resources and Evaluation, pages 1–38.\n\nLi, P., Yang, J., Islam, M. A., and Ren, S. (2023). Making AI less “Thirsty’: Uncovering\n\nand Addressing the Secret Water Footprint of AI models.\n\nLi, X. L. and Liang, P. (2021). Prefix-Tuning: Optimizing Continuous Prompts for Ge-\nneration. In Zong, C., Xia, F., Li, W., and Navigli, R., editors, Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npages 4582–4597, Online. Association for Computational Linguistics.\n\nLin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries.\n\nIn\nText Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for\nComputational Linguistics.\n\nMaslej, N., Fattorini, L., Perrault, R., Parli, V., Reuel, A., Brynjolfsson, E., Etchemendy,\nJ., Ligett, K., Lyons, T., Manyika, J., Niebles, J. C., Shoham, Y., Wald, R., and Clark,\nJ. (2024). Artificial Intelligence Index Report 2024.\n\nPaes, A., Vianna, D., and Rodrigues, J. (2024). Modelos de linguagem.\n\nIn Caseli,\nH. M. and Nunes, M. G. V., editors, Processamento de Linguagem Natural: Conceitos,\nT´ecnicas e Aplicac¸ ˜oes em Portuguˆes, book chapter 15. BPLN, 2 edition.\n\nPaiola, P. H. (2022). Sumarizac¸ ˜ao abstrativa de textos em portuguˆes utilizando apren-\ndizado de m´aquina. Mestrado em ciˆencias da computac¸ ˜ao, Universidade Estadual\nPaulista J´ulio de Mesquita Filho, [s.l.]. Programa de P´os-Graduac¸ ˜ao em Ciˆencia da\nComputac¸ ˜ao.\n\nPaiola, P. H., Garcia, G. L., Jodas, D. S., Correia, J. V. M., Sugi, L. A., and Papa, J. P.\n(2024). RecognaSumm: A Novel Brazilian Summarization Dataset. In Gamallo, P.,\nClaro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors,\nProceedings of the 16th International Conference on Computational Processing of Por-\ntuguese - Vol. 1, pages 575–579, Santiago de Compostela, Galicia/Spain. Association\nfor Computational Lingustics.\n\nPontes, L., Oliveira, H., and Boldt, F. (2022). Avaliac¸ ˜ao de Modelos Neurais para\nIn Anais do XLIX Semin´ario Integrado de Software\n\nSumarizac¸ ˜ao de C´odigo-fonte.\ne Hardware, pages 140–151, Porto Alegre, RS, Brasil. SBC.\n\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W.,\nand Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1).\n\nSchwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. (2020). Green AI. Communications\n\nof the ACM, 63(12):54–63.\n\n\fSouza, J. W. d. C., Cardoso, P. C. F., and Paix˜ao, C. A. (2024). Sumarizac¸ ˜ao autom´atica.\nIn Caseli, H. M. and Nunes, M. G. V., editors, Processamento de Linguagem Natural:\nConceitos, T´ecnicas e Aplicac¸ ˜oes em Portuguˆes, book chapter 22. BPLN, 2 edition.\n\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere,\nB., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,\nL., and Polosukhin, I. (2017). Attention is all you need. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, NIPS’17, page\n6000–6010, Red Hook, NY, USA. Curran Associates Inc.\n\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T.,\nLouf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y.,\nPlu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. (2020).\nTransformers: State-of-the-Art Natural Language Processing. In Liu, Q. and Schlan-\ngen, D., editors, Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pages 38–45, Online. Association for\nComputational Linguistics.\n\nXu, L., Xie, H., Qin, S. J., Tao, X., and Wang, F. L. (2023). Parameter-Efficient Fine-\nTuning Methods for Pretrained Language Models: A critical review and assessment.\nCoRR, abs/2312.12148.\n\nYang, Y., Zhou, J., Wong, N., and Zhang, Z. (2024). LoRETTA: Low-Rank Economic\nTensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language\nModels. In Duh, K., Gomez, H., and Bethard, S., editors, Proceedings of the 2024\nConference of the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1: Long Papers), pages 3161–3176,\nMexico City, Mexico. Association for Computational Linguistics.\n\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li,\nX., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S.,\nSridhar, A., Wang, T., and Zettlemoyer, L. (2022). OPT: Open Pre-trained Transformer\nLanguage Models. arXiv.\n\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2020). BERTScore:\nIn 8th International Conference on Lear-\nEvaluating Text Generation with BERT.\nning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Open-\nReview.net.\n\nZhao, J., Wang, T., Abid, W., Angus, G., Garg, A., Kinnison, J., Sherstinsky, A., Molino,\nP., Addair, T., and Rishi, D. (2024a). LoRA Land: 310 Fine-tuned LLMs that Rival\nGPT-4, A Technical Report. arXiv preprint arXiv:2405.00732.\n\nZhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y. (2024b). GaLore:\n\nMemory-Efficient LLM Training by Gradient Low-Rank Projection.\n\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang,\nJ., Dong, Z., et al. (2023). A survey of large language models. arXiv preprint ar-\nXiv:2303.18223.\n\n\f"
        },
        {
            "titulo": "A Hybrid Machine Learning Method to Author Name Disambiguation",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31122",
            "idioma": "Inglês",
            "storage_key": "files/article_31122_30925.pdf",
            "autores": [
                {
                    "nome": "Natan S. Rodrigues",
                    "afiliacao": "UEG / UnB",
                    "orcid": "http://orcid.org/0000-0002-0785-4397"
                },
                {
                    "nome": "Celia G. Ralha",
                    "afiliacao": "UnB",
                    "orcid": "https://orcid.org/0000-0002-2983-2180"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Digital bibliographic repositories, including publications, authors, and research fields are essential for sharing scientific information. Nevertheless, the information retrieval, extraction, and classification efficiency in such archives is threatened by author name ambiguity. This paper addresses the Author Name Disambiguation (AND) problem by proposing a hybrid machine learning method integrating Bidirectional Encoder Representations from Transformers (BERT), Graph Convolutional Network (GCN), and Graph Enhanced Hierarchical Agglomerative Clustering (GHAC) approaches. The BERT model extracts textual data from scientific documents, the GCN structures global data from academic graphs, and GHAC considers heterogeneous networks’ global context to identify scientific collaboration patterns. We compare the hybrid method with AND state-of-the-art work using a publicly accessible data set consisting of 7,886 documents, 137 unique authors, and 14 groups of ambiguous authors, along with recognized validation metrics. The results achieved a high precision score of 93.8%, recall of 96.3%, F1-measure of 95%, Average Cluster Purity (ACP) of 96.5%, Average Author Purity (AAP) of 97.4% and K-Metric of 96.9%. Compared to the AND baseline approach, the hybrid method presents better results indicating a promising approach.",
            "keywords": [
                "AND",
                "BERT",
                "Digital Bibliographic Repositories",
                "GCN",
                "GHAC"
            ],
            "referencias": [
                "AMiner (2005-2024b). Search and mining of academic social networks.",
                ". Tsinghua University, Beijing, 100084. China.",
                "AMiner (2024a). Aminer dataset. Disponível em",
                ".",
                "Beltagy, I., Cohan, A., and Lo, K. (2019). Scibert: Pretrained contextualized embeddings for scientific text. CoRR, abs/1903.10676.",
                "CiteSeerX (2007-2019). Scientific literature digital library and search engine.",
                ". Pennsylvania State University, University Park, PA 16802, USA.",
                "DBLP (1993-2024). The digital bibliography & library project.",
                ". Schloss Dagstuhl, Leibniz-Zentrum fu ̈r Informatik, LZI GmbH.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. page 4171–4186, Minneapolis, Minnesota, USA. Proceedings of NAACL-HLT 2019, Association for Computational Linguistics.",
                "Ferreira, A. A., Gonçalves, M. A., and Laender, A. H. F. (2020). Automatic disambiguation of author names in bibliographic repositories. Synthesis Lectures on Information Concepts, Retrieval, and Services, 12(1):1–146.",
                "Hussain, I. and Asghar, S. (2017). A survey of author name disambiguation techniques: 2010-2016. Knowledge Eng. Review, 32:e22.",
                "Kim, J. and Owen-Smith, J. (2020). Model reuse in machine learning for author name disambiguation: An exploration of transfer learning. IEEE Access, 8:188378–188389.",
                "Kingma, D. P. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
                "Kipf, T. N. and Welling, M. (2017). Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations (ICLR).",
                "Pooja, K. M., Mondal, S., and Chandra, J. (2022). Exploiting higher order multi-dimensional relationships with self-attention for author name disambiguation. ACM Transactions on Knowledge Discovery from Data, 16(5).",
                "Qiao, Z., Du, Y., Fu, Y., Wang, P., and Zhou, Y. (2019). Unsupervised author disambiguation using heterogeneous graph convolutional network embedding. In 2019 IEEE International Conference on Big Data (Big Data), pages 910–919.",
                "Rodrigues, N. S., Mariano, A. M., and Ralha, C. G. (2024). Author name disambiguation literature review with consolidated meta-analytic approach. International Journal on Digital Libraries, pages 1–21.",
                "Shin, D., Kim, T., Choi, J., and Kim, J. (2014). Author name disambiguation using a graph model with node splitting and merging based on bibliographic information. Scientometrics, 100(1):15–50.",
                "Waqas, H. and Qadir, A. (2022). Completing features for author name disambiguation (AND): An empirical analysis. Scientometrics, 127(2):1039–1063.",
                "Waqas, H. and Qadir, M. A. (2021). Multilayer heuristics based clustering framework (MHCF) for author name disambiguation. Scientometrics, 126(9):7637–7678.",
                "Zhang, S., Tong, H., Xu, J., and Maciejewski, R. (2019). Graph convolutional networks: a comprehensive review. Computational Social Networks, 6(11)."
            ],
            "artigo_completo": "A Hybrid Machine Learning Method to Author Name\nDisambiguation\n\nNatan S. Rodrigues1,2, Celia G. Ralha2\n\n1Instituto Acadˆemico de Ciˆencias Tecnol´ogicas\nUniversidade Estadual de Goi´as (UEG) – Goi´as – GO – Brasil\n\n2Departamento de Ciˆencia da Computac¸ ˜ao – Instituto de Ciˆencias Exatas\nUniversidade de Bras´ılia (UnB) – Bras´ılia, DF – Brasil\n\nnatan.rodrigues@ueg.br, ghedini@unb.br\n\nAbstract. Digital bibliographic repositories, including publications, authors,\nand research ﬁelds are essential for sharing scientiﬁc information. Neverthe-\nless, the information retrieval, extraction, and classiﬁcation efﬁciency in such\narchives is threatened by author name ambiguity. This paper addresses the\nAuthor Name Disambiguation (AND) problem by proposing a hybrid machine\nlearning method integrating Bidirectional Encoder Representations from Trans-\nformers (BERT), Graph Convolutional Network (GCN), and Graph Enhanced\nHierarchical Agglomerative Clustering (GHAC) approaches. The BERT model\nextracts textual data from scientiﬁc documents, the GCN structures global data\nfrom academic graphs, and GHAC considers heterogeneous networks’ global\ncontext to identify scientiﬁc collaboration patterns. We compare the hybrid\nmethod with AND state-of-the-art work using a publicly accessible data set con-\nsisting of 7,886 documents, 137 unique authors, and 14 groups of ambiguous\nauthors, along with recognized validation metrics. The results achieved a high\nprecision score of 93.8%, recall of 96.3%, F1-measure of 95%, Average Cluster\nPurity (ACP) of 96.5%, Average Author Purity (AAP) of 97.4% and K-Metric of\n96.9%. Compared to the AND baseline approach, the hybrid method presents\nbetter results indicating a promising approach.\n\n1. Introduction\nDigital bibliographic repositories are vast reservoirs of bibliographic citation information\n(DBLP [DBLP 2024], ArnetMiner [AMiner 2024b], CiteSeerX [CiteSeerX 2019]). They\noffer functionalities that allow the identiﬁcation of works by scientists, authors, and their\nrespective academic social networks. The DBLP currently lists around 7 million works\nin Computer Science, including journals and conference articles. In January 2024, DBLP\ngathered information on approximately 3.5 million authors, with 227 thousand names\nof researchers and publications manually veriﬁed by the DBLP team, corresponding to\na curation of 34% of all publications in the database.1 ArnetMiner stores information\non approximately 2 million of scientiﬁc works, 1.7 million of authors, and 8 million of\nbibliographic citations [AMiner 2024a].2\n\nBy storing millions of information from bibliographic records, digital reposito-\nries become an essential source of information for the global academic and scientiﬁc\n\n1https://dblp.org/\n2https://www.aminer.org/\n\n\fcommunity, allowing retrieval, extraction and classiﬁcation of relevant publications in a\ncentralized manner [Ferreira et al. 2020]. In addition to these bibliographic features, such\ndigital libraries provide helpful analysis useful for better decision-making by scientiﬁc\nfunding agencies and academic institutions [Hussain and Asghar 2017].\n\nHowever, a common problem in digital bibliographic repositories is automatic\nAuthor Name Disambiguation (AND). The AND problem occurs when different authors\nhave the same name record or when an author has multiple name records in the same data\nset. Such a problem can signiﬁcantly affect the document and information retrieval perfor-\nmance through Web search engines and obstruct entity integrity for integrated databases.\nEven though the author’s name ambiguity problem has been studied for decades, it re-\nmains without a canonical solution. Thus, research efforts to solve the AND problem are\nessential, especially considering that digital bibliographic repositories are becoming more\nperson-centric than document-centric [Shin et al. 2014].\n\nThis work addresses the AND problem with a novel hybrid method combin-\ning advanced machine learning techniques, such as the Bidirectional Encoder Represen-\ntations from Transformers (BERT) [Devlin et al. 2019], Graph Convolutional Network\n(GCN) [Zhang et al. 2019], and Graph Enhanced Hierarchical Agglomerative Clustering\n(GHAC) [Qiao et al. 2019]. As proposed by [Kipf and Welling 2017], GCN is a powerful\nmachine learning model that extends the Convolutional Neural Network (CNN) to handle\ndata structured as graphs, capturing local and global dependencies within a network. Our\nmethod aims to enhance the AND accuracy in digital bibliographic repositories consid-\nering information retrieval, extraction, and classiﬁcation by applying document content\nsemantic treatment related to a graph representation of relationships between documents,\nauthors, and other scientiﬁc attributes.\n\nAs presented in [Ferreira et al. 2020], there are several approaches to solving AND\nproblem applying various techniques, but no works combining transfer learning with\nGCN and GHAC techniques. Also, according to a recent AND literature review, using\nthe theory of the consolidated meta-analytic approach with quantitative techniques and\nbibliometric aspects, the hybrid method proposed is considered a novel solution to AND\n[Rodrigues et al. 2024].\n\nThe rest of the article includes in Section 2 related work focusing on approaches\nto the AND problem. Section 3 details the AND hybrid method. Section 4 includes the\nconducted experiments with the evaluation metrics. In Section 5, we present the results\nwith discussion. Finally, the conclusion and future work are in Section 6.\n\n2. Related Work\nAs presented in [Rodrigues et al. 2024], largely used AND solving approaches are au-\nthor grouping associated with similarity functions and clustering methods, and some\nworks with author assignment allied to classiﬁcation methods. Also, approaches based\non graphs, word embedding with supervised learning, and heuristics with probabilistic\napplications are common. The literature review highlights author clustering techniques’\nprevalence and effectiveness, especially when addressing issues associated with large bib-\nliographic databases. In this section, we present works most related to our hybrid method.\n\nThe authors in [Kim and Owen-Smith 2020] explore supervised techniques using\ntransfer learning on AND tasks where no labelled data was available for training. The\n\n\fresults show that by training source data that well represent the main characteristics of\nthe target datasets, the developed disambiguation models through transfer learning can\nproduce results comparable to those achieved by traditional machine learning approaches,\nwhich train algorithms on speciﬁcally labelled subsets of the target data.\n\nIn [Waqas and Qadir 2021], the authors propose a method to perform AND based\non heuristic clusters in several layers. They used global characteristics and those related\nto the structure of publications to group them. One of the differences pointed out by the\nauthors is that instead of relying only on keyword information, the approach also consid-\ners the contextual structure of publications for grouping. The authors use an incremental\nclassiﬁcation method to reduce errors after creating clusters. A dataset called CustAND\nwas presented for testing and executing the AND method.\n\nThe approach of [Pooja et al. 2022] uses GCN in conjunction with attention mech-\nanisms for learning representations in a heterogeneous graph of documents. The work\nhighlights the importance of using attention at different levels, both about the types of\nneighbors and relationships, to incorporate relevant context into learning node represen-\ntations. The emphasis on attention allows a detailed analysis of the impact of this mecha-\nnism on capturing semantic and contextual information from documents in a graph. The\nauthors used two ArnetMiner variants as data sets, the ﬁrst with 110 and the second with\n100 ambiguous name references.\n\n3. The Hybrid Method\n\nThe hybrid method has four main steps as presented in Figure 1 and described in the\nsequence.\n\nFigure 1. The hybrid method workﬂow.\n\n3.1. Data Entry and Preprocessing\n\nThe hybrid method’s ﬁrst step deals with the input and preprocessing of document data\n(publications), when data is adjusted and formatted to ensure input suitability for the\nsubsequent steps.\n\n3.2. Graph Creation and Characterization\n\nThis step plays a fundamental role as it creates the structure of the heterogeneous network\nfrom the information received from the previous step and provides contextual information\nfor the AND task.\n\n\f• Creation of a Heterogeneous Network - includes different types of nodes and\nedges. The graph is formally deﬁned as Gheterogeneous = (Nnodes, Eedges), where\nNnodes are nodes representing publications, authors, and words. The edges Eedges\nrepresent the different connections between nodes, such as contains (between\npublications and words), written by (between publications and authors), co −\nauthored (between authors who collaborated on the same publication), and shared−\nword (between publications that share keywords).\n\n• Embedding Extraction with BERT - BERT uses transfer learning pretraining its\nparameters on large sets of unlabeled texts with only minor modiﬁcations to per-\nform tasks in a given domain. BERT converts every word in a text into a vector\nrepresentation that captures the word’s meaning given the context in which it ap-\npears. This representation can be combined to obtain a representation of entire\nsentences. In this work, we use the SciBERT variant of BERT pre-trained on sci-\nentiﬁc texts, which is particularly effective at capturing the contextual and seman-\ntic information of academic documents [Beltagy et al. 2019]. SciBERT calculates\nthe embeddings of publications based on titles and abstracts. These embeddings\nare incorporated as features of the nodes that represent the publications in the\ngraph. The algorithm is described in the sequence.\nGiven a set of N documents with titles and abstracts, where each document i has\na title Ti and an abstract Ri, the embedding extraction process with SciBERT can\nbe detailed as follows:\n\n1. Tokenization of Titles and Abstracts: each title Ti and abstract Ri are tok-\nenized into sequences of separate tokens, represented by {ti,1, ti,2, . . . , ti,Li}\nand {ri,1, ri,2, . . . , ri,Mi}, respectively.\n\n2. SciBERT Embedding Generation: the token sequences of the titles Ti and\nabstracts Ri are processed by BERT, which produces a vector of embed-\ndings for each document. These embeddings capture the semantics of the\ntexts, reﬂecting the main topics and contextual relations.\n\n3. Graph Embedding: the embeddings resulting from SciBERT are used as\nnodes features that represent the publications in the heterogeneous graph.\nIn our algorithm, SciBERT performs the embedding extraction on the titles and\nabstracts of the documents. These embeddings are used as node features in the\nheterogeneous graph, allowing the subsequent GCN to use these representations\nto analyze the interactions between publications, authors, and words, including\nco-authorship relationships. An edge index represents sparsely the connections\nbetween nodes. This format allows the GCN to process large heterogeneous net-\nworks while maintaining essential connectivity information among entities.\n\n3.3. Learning Using GCN\n\nAfter extracting embeddings with SciBERT and constructing the heterogeneous graph,\nthe titles and abstract embeddings are used as features of the nodes in the network. The\npropagation operations in the GCN layers use these embeddings to compute representa-\ntions of neighboring nodes. The need to apply a GCN model to a heterogeneous network,\ninstead of other traditional deep learning techniques arises from the particularities of net-\nworks, where relationships between different types of nodes and graph structures must be\ncaptured effectively.\n\n\fIn the GCN step, this proposed hybrid method initially processes the textual data\nto create a vocabulary with a feature matrix, where each row corresponds to the embed-\nding of a node, such as documents, authors, or keywords. The edge index represents the\nconnections between nodes, preserving the essential relationships in the heterogeneous\nnetwork.\n\nTo capture local and global dependencies within the heterogeneous network data,\neach layer of the GCN updates the node representations based on the connections and\nfeatures deﬁned by the edge index. The proposed GCN model uses activation functions\nto introduce nonlinearities in the model. In our work, we use the ReLU function (σ(x) =\nmax(0, x)) at different stages of GCN (widely used to mitigate the vanishing gradient\nproblem). GCN training is performed by minimizing the MSE loss function, deﬁned\nas L = 1\ni=1(Zi − Xi)2, where N is the number of nodes in the network, Zi the\nN\nﬁnal GCN output for node i, and Xi the original feature vector of node i. The Adam\noptimizer [Kingma 2014] was used to adjusts the model’s weights, adjusting an initial\nlearning rate as needed.\n\n(cid:80)N\n\nFinally, GCN produces embeddings of the nodes in the network, represented as\nlow-dimensional vectors that capture both the nodes’ initial features and the network\nstructure. These embeddings are used for subsequent tasks, such as agglomerative hi-\nerarchical clustering, which will be performed in the next step.\n\n3.4. Generating Hierarchical Agglomerative Clustering\nThe disambiguated authors’ clustering results are generated based on their representa-\ntions in the heterogeneous network. The goal is to group documents with similar char-\nacteristics the interactions between publications, authors, and keywords using the GHAC\nmethod [Qiao et al. 2019]. The GHAC is an agglomerative hierarchical clustering algo-\nrithm that integrates network structural information considering the average similarity of\nthe embeddings between the connected nodes. The algorithm is suitable for complex\nheterogeneous networks as the one built from the embeddings generated by GCN.\n\nInitially, each document is an individual cluster. The iterative algorithm proceeds\nto merge the clusters with the highest average similarity between their components until\nreaching the desired number of clusters. The similarity between the two clusters is de-\nﬁned based on the normalized inner products of the node embeddings, allowing GHAC to\ncapture the semantic and contextual data relationships.\n\nDocuments are grouped to maximize the internal cohesion of the clusters while\npreserving the semantic and structural interaction characteristics between the different\ntypes of entities in the network. This method not only groups documents based on local\nsimilarities but also considers the global context of the heterogeneous network, making it\nparticularly effective in organizing complex academic networks and identifying underly-\ning co-authorship and scientiﬁc collaboration patterns.\n\n4. Experiments\nTo validate the hybrid method, we conducted experiments comparing to the multi-layer\napproach with clustering techniques of [Waqas and Qadir 2021] as a baseline, using the\npublic data set CustAND,3 which is composed of 14 ambiguous name groups with 137\n\n3https://github.com/humaira699/CustAND_Full.git\n\n\fdistinct authors and 7,886 documents [Waqas and Qadir 2022]. This dataset is valuable\nfor AND studies with various attributes and complex data relationships. The execution\npipeline and the code for implementing this method are available in the repository.4\n\n4.1. Experimental Setup\n\nWe used the document titles and abstracts to extract embeddings with SciBERT. We then\nconcatenated these features to form the input text tokenized using the BERT tokenizer\nlimited to 512 tokens. The output was a 768-dimensional embedding representing each\ndocument. The empirically deﬁned GCN conﬁguration includes three layers with an em-\nbedding size of 768, ReLU activation function, Mean Squared Error (MSE) loss function,\nand the optimization performed with a 0.001 learning rate for the Adam algorithm. We\nexecuted the training for 200 epochs with a batch size of 128. Python language was used\nto execute the experiments in a Google Colab L4 environment with the hardware acceler-\nator L4, GPU with 22.5 GB of RAM, CPU with 53 GB of RAM, 201.2 GB disk, and the\nruntime type conﬁgured for Python 3.\n\n4.2. Evaluation Metrics\n\nThe precision, recall, F1-measure, and speciﬁc metrics for clustering, such as Average\nCluster Purity (ACP), Average Author Purity (AAP), and K-Metric metrics commonly\npresented in the AND literature are used to evaluate the experimental results.5\n\nPrecision measures the proportion of correctly classiﬁed documents relative to the\ntotal number of author documents, assessing the algorithm’s ability to assign documents\nto authors correctly as Precision = Documents Correctly Classiﬁed\n. Recall evaluates the ability of\nTotal Documents Classiﬁed\nthe algorithm to retrieve all documents from a real author, measuring the retrieval capacity\nof the algorithm about real authors as Recall = Documents Correctly Retrieved\nTotal Documents from Real Author. F1-measure is\nthe harmonic mean of precision and recall, providing a balanced metric between these\nmetrics (general performance metric) as F1-measure = 2×Precision×Recall\nPrecision+Recall\n\n.\n\n(cid:80)q\n\n(cid:80)R\n\nACP evaluates the average purity of the clusters generated by the algorithm about\nthe theoretical clusters. ACP measures how well the documents were grouped into clus-\nters that represent real authors as ACP = 1\n, where N is the total size of\nN\nthe publication/paper records in the test set, q is the number of hybrid method/predicted\nclusters, R is the number of manually generated reference/real clusters, nij is the number\nof elements in common between the hybrid method-predicted clusters i and the reference\nclusters j, and nj is the number of elements in the reference cluster j. The purer the clus-\nters, the higher the ACP value. AAP measures how fragmented or cohesive the clusters\npredicted by the algorithm are relative to the reference clusters. A higher AAP indicates\nthat the clusters are less fragmented, as AAP = 1\nN\n\nn2\nij\nni\n\n(cid:80)R\n\n(cid:80)q\n\nj=1\n\nj=1\n\ni=1\n\ni=1\n\n.\n\nn2\nij\nnj\n\nK-metric determines the trade-off between the average purity of clusters (ACP)\nand the average purity of authors (AAP). It is a metric that provides a single measure\nthat considers both the quality of clusters and the quality of document attribution to real\nACP × AAP. K-metric helps evaluate the overall performance\nauthors, as K-metric =\n\n√\n\n4https://github.com/natansr/adan_hybrid_method.git\n5Cluster purity measures how well the items in a cluster belong to the same real class. For AND it\nreﬂects the authorship records belonging to a single author within a cluster. A higher purity indicates a\nmore homogeneous cluster where one is the ideal value [Ferreira et al. 2020].\n\n\fof the disambiguation algorithm by balancing the quality of clusters and the quality of\ndocument attribution.\n\n1 ), the ACP is 0.888 ( 1\n\nFigure 2 presents an illustrative example with geometric ﬁgures corresponding to\nan authorship record, where equal ﬁgures represent the same author. There are three the-\noretical clusters and four empirical ones, with one empirical cluster not pure and two au-\nthorship circle records fragmented across two clusters. The results of the metrics applied\nto this example, considering the ACP with the empirical clusters include in the ﬁrst two\n3 ), the third and fourth clusters two different authors ( 12\nclusters three author records ( 32\n2 ),\n2 + 12\nand the last cluster has a single record ( 12\n1 )).\nThe AAP values numerators remain the same, but the denominators reﬂect the number of\nrecords in the theoretical clusters. For instance, 32\n4 represents three records from the same\nauthor in an empirical cluster out of four in the theoretical one. The ﬁnal AAP value is\n9 × ( 32\n0.722 ( 1\n2 )), and the K-metric is the geometric mean of ACP and\n√\n0.888 × 0.722 = 0.8). Precision is 0.857 considering the sum of three author-\nAAP (\nship record pairs from the same author in the ﬁrst and second empirical clusters and none\nin the last three clusters. The denominator sums the total number of authorship record\npairs from each empirical ( 3+3+0+0+0\n3+3+1+0 ). Recall is 0.6 using the same Precision numerator\nwith the denominator the sum of the authorship record pairs that refer to the same author\nin the theoretical clusters 6, 3, and 1 in the ﬁrst, second, and third theoretical clusters,\n). Finally, the F1-measure = 2×(0.857×0.6)\nrespectively ( 3+3+0+0+0\n\n9 ×( 32\n\n4 + 12\n\n4 + 32\n\n3 + 12\n\n2 + 12\n\n3 + 12\n\n3 + 32\n\n2 + 12\n\n0.857+0.6 = 0.7.\n\n6+3+1\n\nFigure 2. Theoretical and empirical clusters.\n\n5. Results and Discussion\n\nIn this section, we present our hybrid method results with the evaluation metrics (Sec-\ntion 4.2) for the CustAND dataset with 14 groups of ambiguous names compared to the\nbaseline work of [Waqas and Qadir 2021]. In the CustAND dataset, an example of an\nambiguous name group for “A Choudhary” consists of 12 distinct authors that share the\nsame name in document citation, namely “Ashish Choudhary”, “Amit Choudhary”, “Anil\nChoudhary”, “Arvind Choudhary”, “Anupam Choudhary”, “Ajay Choudhary”, “Abhishek\nChoudhary”, “Aniruddha Choudhary”, “Anjali Choudhary”, “Arjun Choudhary”, “Akshay\nChoudhary”, and “Arun Choudhary”. Table 1 summarizes the metrics for each ambiguous\nname group presenting average values for our method and the baseline.\n\nAnalysis of our method performance metrics for the 14 ambiguous name groups\nof the CustAND dataset reveals attractive results. Compared to the results reported by\n[Waqas and Qadir 2021], the average precision across the 14 groups is slightly lower\n(93.8% versus 94.6%), which may indicate a loss of precision when classifying docu-\nments for speciﬁc authors. However, the higher Recall (96.3% versus 92.5%) suggests\n\n\fthat the method applied to ambiguous groups has a better recall capacity and is more ef-\nﬁcient in identifying all documents of an author. The F1-measure of 95% across the 14\ngroups, compared to 93.5% for [Waqas and Qadir 2021], demonstrates that the method\nachieves a better balance between Precision and Recall.\n\nThe ACP and AAP metrics across the 14 groups also outperform the baseline with\nvalues of 96.5% and 97.4%, compared to 95.8% and 87%, respectively. These results\nsuggest a higher average purity of the generated clusters and a lower fragmentation of\nthe predicted clusters, reﬂecting a more cohesive and representative grouping of the real\nauthors. Finally, the 96.9% K-metric in the 14 clusters of our method is signiﬁcantly\nhigher than the 91.24% reported by [Waqas and Qadir 2021], indicating that our method\nachieves a superior balance between the quality of the clusters and the correct attribution\nof documents to authors.\n\nTable 1. Performance metrics by ambiguous name group.\n\nAmbiguous\nName Group\nA Choudhary\nJ Martin\nM A Qadir\nJ Mitchell\nA Gupta\nJ Robinson\nA Kumar\nJ Smith\nBin Li\nS Kim\nD Eppstein\nZ Zhang\nJ Lee\nK Tanaka\nBaseline [Waqas and Qadir 2021]\nOur Method\n\n# Authors Precision Recall F1-measure ACP AAP K-metric\n\n12\n9\n15\n10\n8\n12\n9\n12\n8\n10\n3\n10\n8\n11\n137\n137\n\n1.000\n1.000\n1.000\n1.000\n0.853\n1.000\n1.000\n0.938\n0.592\n0.754\n1.000\n1.000\n1.000\n1.000\n0.946\n0.938\n\n1.000\n1.000\n1.000\n1.000\n0.878\n1.000\n1.000\n0.988\n0.671\n0.944\n1.000\n1.000\n1.000\n1.000\n0.925\n0.963\n\n1.000\n1.000\n1.000\n1.000\n0.865\n1.000\n1.000\n0.964\n0.632\n0.839\n1.000\n1.000\n1.000\n1.000\n0.935\n0.950\n\n1.000\n1.000\n1.000\n1.000\n0.875\n1.000\n1.000\n0.972\n0.763\n0.897\n1.000\n1.000\n1.000\n1.000\n0.958\n0.965\n\n1.000\n1.000\n1.000\n1.000\n0.875\n1.000\n1.000\n0.972\n0.889\n0.895\n1.000\n1.000\n1.000\n1.000\n0.870\n0.974\n\n1.000\n1.000\n1.000\n1.000\n0.875\n1.000\n1.000\n0.972\n0.826\n0.896\n1.000\n1.000\n1.000\n1.000\n0.912\n0.969\n\n6. Conclusion\n\nThe main objective of this work was accomplished by proposing and evaluating the res-\nolution capacity of the AND problem using a hybrid method that involves transfer learn-\ning with SciBERT, GCN, and GHAC. When comparing the effectiveness of our hybrid\nmethod with the state-of-the-art work of [Waqas and Qadir 2021], using the CustAND\ndataset, we note that the proposed method outperformed the baseline regarding average\naccuracy, considering ﬁve of six commonly used metrics of precision, recall, F1-measure,\nACP, AAP, and K-metric.\n\nFuture experiments include comparison to [Pooja et al. 2022] including the use\nof other machine learning methods, diverse textual extract information methods, and the\nadoption of graph neural networks approaches, such as Graph Attention Network (GAT)\nand GraphSAGE with larger datasets. Also, a manageable data entry implementation for\nthe end user as a graphical user interface to make the solution more user-friendly.\n\n\fReferences\nAMiner (2005-2024b). Search and mining of academic social networks. https://\n\nwww.aminer.org/. Tsinghua University, Beijing, 100084. China.\n\nAMiner (2024a). Aminer dataset. Dispon´ıvel em https://www.aminer.cn/\n\ndata/?nav=openData.\n\nBeltagy, I., Cohan, A., and Lo, K. (2019). Scibert: A pretrained language model for\n\nscientiﬁc text. CoRR, abs/1903.10676.\n\nCiteSeerX (2007-2019). Scientiﬁc literature digital library and search engine. https:\n//citeseerx.ist.psu.edu/index. Pennsylvania State University, University\nPark, PA 16802, USA.\n\nDBLP (1993-2024). The digital bibliography & library project. https://dblp.\nuni-trier.de/. Schloss Dagstuhl, Leibniz-Zentrum f¨ur Informatik, LZI GmbH.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. page 4171–4186, Min-\nneapolis, Minnesota, USA. Proceedings of NAACL-HLT 2019, Association for Com-\nputational Linguistics.\n\nFerreira, A. A., Gonc¸alves, M. A., and Laender, A. H. F. (2020). Automatic disambigua-\ntion of author names in bibliographic repositories. Synthesis Lectures on Information\nConcepts, Retrieval, and Services, 12(1):1–146.\n\nHussain, I. and Asghar, S. (2017). A survey of author name disambiguation techniques:\n\n2010-2016. Knowledge Eng. Review, 32:e22.\n\nKim, J. and Owen-Smith, J. (2020). Model reuse in machine learning for author name\ndisambiguation: An exploration of transfer learning. IEEE Access, 8:188378–188389.\n\nKingma, D. P. (2014). Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980.\n\nKipf, T. N. and Welling, M. (2017). Semi-supervised classiﬁcation with graph convolu-\n\ntional networks. International Conference on Learning Representations (ICLR).\n\nPooja, K. M., Mondal, S., and Chandra, J. (2022). Exploiting higher order multi-\ndimensional relationships with self-attention for author name disambiguation. ACM\nTransactions on Knowledge Discovery from Data, 16(5).\n\nQiao, Z., Du, Y., Fu, Y., Wang, P., and Zhou, Y. (2019). Unsupervised author disam-\nbiguation using heterogeneous graph convolutional network embedding. In 2019 IEEE\nInternational Conference on Big Data (Big Data), pages 910–919.\n\nRodrigues, N. S., Mariano, A. M., and Ralha, C. G. (2024). Author name disambiguation\nliterature review with consolidated meta-analytic approach. International Journal on\nDigital Libraries, pages 1–21.\n\nShin, D., Kim, T., Choi, J., and Kim, J. (2014). Author name disambiguation using\na graph model with node splitting and merging based on bibliographic information.\nScientometrics, 100(1):15–50.\n\nWaqas, H. and Qadir, A. (2022). Completing features for author name disambiguation\n\n(AND): An empirical analysis. Scientometrics, 127(2):1039–1063.\n\n\fWaqas, H. and Qadir, M. A. (2021). Multilayer heuristics based clustering framework\n\n(MHCF) for author name disambiguation. Scientometrics, 126(9):7637–7678.\n\nZhang, S., Tong, H., Xu, J., and Maciejewski, R. (2019). Graph convolutional networks:\n\na comprehensive review. Computational Social Networks, 6(11).\n\n\f"
        },
        {
            "titulo": "PropBank e anotação de papéis semânticos para a língua portuguesa: O que há de novo?",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31123",
            "idioma": "Português",
            "storage_key": "files/article_31123_30926.pdf",
            "autores": [
                {
                    "nome": "Cláudia Freitas",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0001-6807-8558"
                },
                {
                    "nome": "Thiago Alexandre Salgueiro Pardo",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2111-1319"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "O artigo introduz o Porttinari-base PropBank (PBP): o corpus Porttinari-base com uma camada de papéis semânticos. A anotação foi feita sobre dependências sintáticas, usando regras linguísticas e sob inspeção humana. Foram anotados mais de 40 mil argumentos, e os resultados são discutidos à luz de trabalhos que investigam a generalização das classes do PropBank.",
            "keywords": [
                "PropBank",
                "anotação semântica",
                "papéis semânticos",
                "Dependências Universais"
            ],
            "referencias": [
                "Bick, E. (2007). Automatic semantic role annotation for portuguese. In Proceedings of TIL 2007 - 5th Workshop on Information and Human Language Technology, pages 1713–1716, Rio de Janeiro. Sociedade Brasileira de Computação (SBC).",
                "Branco, A., Carvalheiro, C., Pereira, S., Silveira, S., Silva, J., Castro, S., and Graça, J. (2012). A PropBank for Portuguese: the CINTIL-PropBank. In Calzolari, N., Choukri, K., Declerck, T., Doğan, M. U., Maegaard, B., Mariani, J., Moreno, A., Odijk, J., and Piperidis, S., editors, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 1516–1521, Istanbul, Turkey. European Language Resources Association (ELRA).",
                "de Marneffe, M.-C., Manning, C. D., Nivre, J., and Zeman, D. (2021). Universal Dependencies. Computational linguistics, 47(2):255–308.",
                "de Souza, E. and Freitas, C. (2021). ET: A workstation for querying, editing and evaluating annotated corpora. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 35–41, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "Dong, X. L. (2023). Generations of knowledge graphs: The crazy ideas and the business impact. Proc. VLDB Endow., 16(12):4130–4137.",
                "Duran, M., Lopes, L., das Graças Nunes, M., and Pardo, T. (2023). The dawn of the porttinari multigenre treebank: Introducing its journalistic portion. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 115–124, Porto Alegre, RS, Brasil. SBC.",
                "Duran, M. S. (2014). Manual de anotação do PropBank-Br v2. Technical report, ICMC-USP.",
                "Duran, M. S. and Aluísio, S. M. (2011). Propbank-br: a Brazilian Portuguese corpus annotated with semantic role labels. In Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology.",
                "Duran, M. S. and Freitas, C. (2024). Guia de anotação de papéis semânticos seguindo o modelo PropBank no corpus Porttinari-base. (no prelo). Technical report, ICMC-USP.",
                "Duran, M. S., Torres, L. S., Viviani, M. C., Hartmann, N., and Aluísio, S. M. (2014). Seleção e preparação de sentenças do corpus PLN-BR para compor o corpus de anotação de papéis semânticos Propbank-Br.v2. Technical report, Núcleo Interinstitucional de Linguística Computacional.",
                "Evans, R. and Orasan, C. (2019). Sentence simplification for semantic role labelling and information extraction. In Mitkov, R. and Angelova, G., editors, Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 285–294, Varna, Bulgaria. INCOMA Ltd.",
                "Freitas, C. (2023). Dataset e corpus. In Caseli, H. and Volpe Nunes, M. d. G., editors, Processamento de Linguagem Natural: conceitos, técnicas e aplicações em Português, pages 1–37. BPLN.",
                "Freitas, C. (2024). Anotação de papéis semânticos no corpus Porttinari-base: Procedimentos, resultados e análise. (no prelo). Technical report, ICMC-USP.",
                "Freitas, C., Souza, E., Castro, M. C., Cavalcanti, T., Ferreira da Silva, P., and Corrêa Cordeiro, F. (2023). Recursos linguísticos para o PLN específico de domínio: o Petrolês. Linguamática, 15(2):51–68.",
                "Gung, J. and Palmer, M. (2021). Predicate representations and polysemy in VerbNet semantic parsing. In Zarrieß, S., Bos, J., van Noord, R., and Abzianidze, L., editors, Proceedings of the 14th International Conference on Computational Semantics (IWCS), pages 51–62, Groningen, The Netherlands (online). Association for Computational Linguistics.",
                "Han, H. and Choi, J. (2020). Establishing strong baselines for the new decade: Sequence tagging, syntactic and semantic parsing with bert. In Proceedings of the Thirty-Third International Florida Artificial Intelligence Research Society Conference (FLAIRS 2020).",
                "Hartmann, N. S., Duran, M. S., and Aluísio, S. M. (2016). Automatic semantic role labeling on non-revised syntactic trees of journalistic texts. In Silva, J., Ribeiro, R., Quaresma, P., Adami, A., and Branco, A., editors, Computational Processing of the Portuguese Language, pages 202–212, Cham. Springer International Publishing.",
                "Levin, B. (1993). English Verb Classes and Alternations: a preliminary investigation. The University of Chicago Press, London.",
                "Levin, B. and Rappaport Hovav, M. (2005). Argument Realization. Cambridge University Pres, Cambridge.",
                "Li, T., Kazeminejad, G., Brown, S., Srikumar, V., and Palmer, M. (2023). Learning semantic role labeling from compatible label sequences. In Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15561–15572, Singapore. Association for Computational Linguistics.",
                "Merlo, P. and Van Der Plas, L. (2009). Abstraction and generalisation in semantic role labels: PropBank, VerbNet or both? In Su, K.-Y., Su, J., Wiebe, J., and Li, H., editors, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 288–296, Suntec, Singapore. Association for Computational Linguistics.",
                "Mohebbi, M., Razavi, S. N., and Balafar, M. A. (2022). Computing semantic similarity of texts based on deep graph learning with ability to use semantic role label information. Scientific Reports, 12(1).",
                "Palmer, M., Gildea, D., and Kingsbury, P. (2005). The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71–106.",
                "Pardo, T., Duran, M., Lopes, L., Felippo, A., Roman, N., and Nunes, M. (2021). Porttinari - a large multi-genre treebank for brazilian portuguese. In Anais do XIII Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 1–10, Porto Alegre, RS, Brasil. SBC.",
                "Rodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., Cardoso, H. L., and Osório, T. (2023). Advancing neural encoding of portuguese with transformer albertina pt-*. In Moniz, N., Vale, Z., Cascalho, J., Silva, C., and Sebastião, R., editors, Progress in Artificial Intelligence, pages 441–453, Cham. Springer Nature Switzerland.",
                "Sanches Duran, M. and Aluísio, S. (2015). Automatic generation of a lexical resource to support semantic role labeling in Portuguese. In Palmer, M., Boleda, G., and Rosso, P., editors, Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 216–221, Denver, Colorado. Association for Computational Linguistics.",
                "Tenney, I., Das, D., and Pavlick, E. (2019a). BERT rediscovers the classical NLP pipeline. In Korhonen, A., Traum, D., and Màrquez, L., editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593–4601, Florence, Italy. Association for Computational Linguistics.",
                "Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R. T., Kim, N., Durme, B. V., Bowman, S. R., Das, D., and Pavlick, E. (2019b). What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations.",
                "Wallis, S. (2003). Completing parsed corpora: From correction to evolution. In Abeillé, A., editor, Treebanks: Building and Using Parsed Corpora, pages 61–71. Springer Netherlands, Dordrecht.",
                "Wang, N., Li, J., Meng, Y., Sun, X., Qiu, H., Wang, Z., Wang, G., and He, J. (2022). An MRC framework for semantic role labeling. In Calzolari, N., Huang, C.-R., Kim, H., Pustejovsky, J., Wanner, L., Choi, K.-S., Ryu, P.-M., Chen, H.-H., Donatelli, L., Ji, H., Kurohashi, S., Paggio, P., Xue, N., Kim, S., Hahm, Y., He, Z., Lee, T. K., Santus, E., Bond, F., and Na, S.-H., editors, Proceedings of the 29th International Conference on Computational Linguistics, pages 2188–2198, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.",
                "Yi, S.-t., Loper, E., and Palmer, M. (2007). Can semantic roles generalize across genres? In Sidner, C., Schultz, T., Stone, M., and Zhai, C., editors, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 548–555, Rochester, New York. Association for Computational Linguistics."
            ],
            "artigo_completo": "PropBank e anotac¸ ˜ao de pap´eis semˆanticos para a l´ıngua\nportuguesa: O que h´a de novo?\n\nCl´audia Freitas1, Thiago Alexandre Salgueiro Pardo1\n\n1N´ucleo Interinstitucional de Lingu´ıstica Computacional (NILC)\nInstituto de Ciˆencias Matem´aticas e de Computac¸ ˜ao, Universidade de S˜ao Paulo\n\nclaudiafreitas@usp.br, taspardo@icmc.usp.br\n\nintroduces Porttinari-base PropBank (PBP):\n\nAbstract. This paper\nthe\nPorttinari-base corpus with a semantic role layer. The annotation was perfor-\nmed on syntactic dependencies, using linguistic rules and under human inspec-\ntion. More than 40,000 arguments were annotated, and the results are discussed\nin light of works investigating the generalization of PropBank labels.\n\nResumo. O artigo introduz o Porttinari-base PropBank (PBP): o corpus\nPorttinari-base com uma camada de pap´eis semˆanticos. A anotac¸ ˜ao foi feita so-\nbre dependˆencias sint´aticas, usando regras lingu´ısticas e sob inspec¸ ˜ao humana.\nForam anotados mais de 40 mil argumentos, e os resultados s˜ao discutidos `a luz\nde trabalhos que investigam a generalizac¸ ˜ao das classes do PropBank.\n\n1. Introduc¸ ˜ao\nEntre os m´etodos utilizados para representar computacionalmente informac¸ ˜ao semˆantica\nem textos est´a a anotac¸ ˜ao de pap´eis semˆanticos (ou SRL – Semantic Role Labeling).\nPap´eis semˆanticos s˜ao respons´aveis por indicar quem fez o quˆe, para quem, onde, quando,\ncomo, por quˆe, para quˆe, com o quˆe, com quem, etc., e assim estruturam de maneira\nexpl´ıcita e interpret´avel a informac¸ ˜ao contida em enunciados lingu´ısticos. Enquanto ta-\nrefa, a anotac¸ ˜ao de pap´eis semˆanticos tem como objetivo atribuir etiquetas a argumentos\nde predicadores, indicando o papel que estes argumentos exercem em uma frase.\n\nA anotac¸ ˜ao de pap´eis semˆanticos permite criar representac¸ ˜oes semˆanticas est´aveis\nao longo de diferentes realizac¸ ˜oes lingu´ısticas, e as frases 1 a 5 ilustram este ponto. Sin-\ntaticamente, “porta” exerce diferentes func¸ ˜oes, assim como “chave”. Na atribuic¸ ˜ao de\npap´eis semˆanticos, “porta” ´e a “coisa abrindo” em todas as 5 frases, e “chave” ´e o instru-\nmento de abertura em todas as 5 frases, n˜ao importa a func¸ ˜ao sint´atica que exerc¸am. Na\nfrase “O tempo abriu no feriado”, entretanto, ter´ıamos uma outra estrutura argumental (e\noutra representac¸ ˜ao semˆantica), j´a que estar´ıamos diante de um outro sentido de “abrir”.\n\n1. A chave abriu a porta.\n2. Ela abriu a porta com a chave.\n3. A porta foi aberta com a chave.\n4. A porta foi aberta por ela com a chave.\n5. A porta abriu com a chave.\n\nUm PropBank (Proposition Bank, ou Banco de Proposic¸ ˜oes) ´e um corpus que\ncont´em anotac¸ ˜ao de pap´eis semˆanticos, relacionando verbos1 e seus argumentos di-\nretamente `as estruturas sint´aticas de um treebank e conforme o modelo sugerido por\n\n1Atualmente, substantivos e adjetivos tamb´em podem ser considerados.\n\n\f[Palmer et al. 2005],\nsintaxe-semˆantica [Palmer et al. 2005, Levin 1993, Levin and Rappaport Hovav 2005].\n\nteoricamente, pap´eis semˆanticos estariam na interface\n\nj´a que,\n\nApesar de ser um fenˆomeno lingu´ıstico amplamente estudado, n˜ao h´a consenso\nrelativo ao conjunto de pap´eis semˆanticos da l´ıngua. No PropBank, a diversidade te´orica\nacerca dos pap´eis semˆanticos ´e contornada com a utilizac¸ ˜ao de i) argumentos numera-\ndos, que v˜ao de Arg0 a Arg5; e ii) argumentos modificadores, um conjunto mais am-\nplo de argumentos. A motivac¸ ˜ao para o conjunto limitado de pap´eis numerados ´e fa-\ncilitar a generalizac¸ ˜ao para o aprendizado de m´aquina, ainda que alguns estudos mos-\ntrem que este objetivo ´e apenas parcialmente alcanc¸ado [Merlo and Van Der Plas 2009,\nGung and Palmer 2021, Li et al. 2023]. A diferenc¸a entre argumentos numerados e argu-\nmentos modificadores est´a sobretudo na natureza da relac¸ ˜ao sint´atica que o argumento\nmant´em com o verbo (exigˆencia, nos argumentos numerados, vs opcionalidade, nos ar-\ngumentos modificadores). A distinc¸ ˜ao entre os argumentos tamb´em ´e motivada pela as-\nsistematicidade semˆantica dos pap´eis com relac¸ ˜ao aos verbos, e por isso argumentos nu-\nmerados s˜ao espec´ıficos de verbos (o Arg0 do verbo “abrir” ´e “quem abre”, e o Arg0\nde “alagar” ´e “causador do alagamento”). Os ArgM, por outro lado, tˆem uma semˆantica\nespec´ıfica e previs´ıvel (indicada pelo nome da etiqueta, como ArgM-tmp para “tempo” e\nArgM-cau para “causa”), e podem se associar a qualquer verbo.\n\nA semˆantica dos argumentos numerados ´e revelada com o alinhamento entre a\nanotac¸ ˜ao do corpus e o recurso lexical associado ao PropBank, os chamados Frame Files\n– em nosso caso, dispomos do Verbo-Brasil [Sanches Duran and Alu´ısio 2015]. Assim,\na frase (2), segundo o estilo PropBank, ´e anotada como indicado abaixo. Um PropBank,\nportanto, n˜ao ´e apenas um corpus anotado, mas a associac¸ ˜ao entre um corpus anotado e\num l´exico, que indica como os elementos devem ser anotados e o que eles significam.\n\n• Ela[Arg0] abriu a porta[Arg1] com a chave[Arg2].\n\nNeste artigo, apresentamos o Porttinari-base PropBank (PBP), que corresponde\n`a adic¸ ˜ao de uma camada de pap´eis semˆanticos a todas as frases do corpus jornal´ıstico\nPorttinari-base, que comp˜oe o treebank Porttinari [Pardo et al. 2021, Duran et al. 2023].\nO Porttinari-base ´e padr˜ao ouro na anotac¸ ˜ao sint´atica conforme a teoria Universal Depen-\ndencies (UD) [de Marneffe et al. 2021]. Com o PBP, o Porttinari-base passa a ser dupla-\nmente padr˜ao ouro – nas dependˆencias sint´aticas e nos pap´eis semˆanticos —, contribuindo\npara a disponibilizac¸ ˜ao de recursos de alto n´ıvel para o processamento do portuguˆes.\n\n2. Motivac¸ ˜ao e trabalhos relacionados\n\nO PropBank foi criado com o objetivo de treinar modelos no aprendizado supervisionado.\nLevando em conta a onipresenc¸a de arquiteturas neurais e grandes modelos de l´ıngua,\ndiscutimos brevemente a relevˆancia para o PLN da anotac¸ ˜ao de pap´eis semˆanticos.\n\nEntre as cr´ıticas ao atual paradigma de IA, est˜ao a falta de transparˆencia e de ex-\nplicabilidade dos m´etodos e resultados. Neste contexto, pap´eis semˆanticos oferecem ma-\nneiras interpret´aveis de representar semanticamente enunciados verbais, podendo servir\nde insumo, por exemplo, para a criac¸ ˜ao de grafos de conhecimento [Mohebbi et al. 2022],\no que torna este tipo de representac¸ ˜ao semˆantica relevante para a investigac¸ ˜ao acerca da\narticulac¸ ˜ao entre os grandes modelos de l´ıngua (os LLMs – Large Language Models) e\nfontes de conhecimento estruturado [Dong 2023]. Na articulac¸ ˜ao entre redes neurais e\n\n\fSRL, [Mohebbi et al. 2022] prop˜oem uma abordagem de aprendizado de grafos profun-\ndos para computar similaridade semˆantica de documentos, usando pap´eis semˆanticos.\n\nA anotac¸ ˜ao de SRL tamb´em pode ser usada para avaliac¸ ˜ao de modelos de l´ıngua\nquanto `a capacidade de representar informac¸ ˜ao semˆantica estruturada e interpret´avel\n[Tenney et al. 2019a, Tenney et al. 2019b, Han and Choi 2020]. Nessa vertente, nota-\nmos a escassez de conjuntos de validac¸ ˜ao criados para o portuguˆes, que nos faz uti-\nlizar conjuntos de dados traduzidos do inglˆes [Rodrigues et al. 2023]. A utilidade da\nanotac¸ ˜ao de pap´eis semˆanticos no PLN tamb´em pode ser indireta. Considerando o pa-\nradigma de avaliac¸ ˜ao extr´ınseca, [Evans and Orasan 2019] utilizam SRL para verificar se\na simplificac¸ ˜ao textual ´e capaz de facilitar o desempenho na tarefa de SRL. Uma vez que\na tarefa de SRL pode ser considerada um passo al´em da an´alise sint´atica, diferentes mo-\ndelos de representac¸ ˜ao sint´atica tamb´em podem ser avaliados em func¸ ˜ao do desempenho\nobtido na anotac¸ ˜ao SRL, como sugerido em [Freitas 2023].\n\nDesde 2011, o portuguˆes disp˜oe do PropBank-BR [Duran and Alu´ısio 2011], que\nanotou pap´eis semˆanticos ao estilo PropBank sobre a parte brasileira do treebank Bosque,\nem sua vers˜ao de sintaxe de constituintes disponibilizada pela Linguateca. Este PropBank\nlevou `a formulac¸ ˜ao (e adaptac¸ ˜ao do inglˆes) de diretivas de anotac¸ ˜ao e permitiu a criac¸ ˜ao\ndo recurso l´exico que codifica os sentidos dos verbos e descreve seus frames sint´aticos,\no Verbo-Brasil [Sanches Duran and Alu´ısio 2015]. No entanto, esse recurso ´e ainda limi-\ntado. Tendo em vista a criac¸ ˜ao de um material maior e mais lexicalmente diversificado,\nfoi criado o PropBank-BR v2 [Duran et al. 2014, Hartmann et al. 2016]. Diferentemente\nda vers˜ao anterior, este material foi constru´ıdo sobre ´arvores sint´aticas n˜ao revistas. Em\nambos os casos, o processo de anotac¸ ˜ao seguiu o PropBank original, com a anotac¸ ˜ao\nfeita de maneira linear, frase a frase. O portuguˆes conta ainda com o CINTIL-PropBank\n[Branco et al. 2012], um corpus de frases anotadas com estrutura de constituintes e pap´eis\nsemˆanticos, criado de maneira semiautom´atica, com algumas etiquetas anotadas automa-\nticamente, e com um conjunto de pap´eis semˆanticos que ´e uma adaptac¸ ˜ao dos argumen-\ntos numerados de [Palmer et al. 2005]. Por fim, em uma abordagem baseada em regras,\n[Bick 2007] faz SRL seguindo a Constraint Grammar.\n\n3. O Porttinari-base PropBank\n\nNo PBP, a anotac¸ ˜ao de pap´eis semˆanticos foi baseada em dependˆencias, sendo cada argu-\nmento um ´unico token. A anotac¸ ˜ao foi feita em um arquivo no formato CoNLL-U, que\nconsiste em um texto simples com 10 campos separados por caracteres de tabulac¸ ˜ao2. A\nanotac¸ ˜ao foi feita nas colunas 10 (nomeada de MISC) para a atribuic¸ ˜ao dos argumentos,\ne 9 (coluna DEPS) para a anotac¸ ˜ao dos frames. Foram anotados argumentos expl´ıcitos e\nimpl´ıcitos, como sujeitos omitidos. A Figura 1 apresenta a codificac¸ ˜ao da frase “J´unior j´a\npresidiu a JBS, mas vendeu a sua parte”. Para facilitar a leitura, omitimos os conte´udos\ndas colunas 3 a 6. A coluna 9 indica os frames dos verbos “presidir” e “vender” (res-\npectivamente, presidir.01 e vender.01). A coluna 10 informa os pap´eis semˆanticos e seus\npredicadores. Por exemplo, o token 1, “Junior”, ´e Arg0 do token 3 (“presidir”) e Arg0 do\ntoken 8 (“vender”), mesmo que esta ´ultima informac¸ ˜ao n˜ao esteja expl´ıcita na frase.\n\nA anotac¸ ˜ao do PBP utilizou 26 etiquetas. Diferentemente do Propbank original,\ncriamos, no PBP, etiquetas que especificam alguns casos da classe mais geral ArgM-adv:\n\n2https://universaldependencies.org/format.html\n\n\fFigura 1. Anotac¸ ˜ao de pap ´eis sem ˆanticos nas colunas 9 e 10 do CoNLL-U\n\nArgM-conseq, para indicar consequˆencia; ArgM-cond, para indicar condic¸ ˜oes e ArgM-\ncomp, para indicar comparac¸ ˜oes. A divergˆencia com relac¸ ˜ao `a lista de pap´eis do PropBank\noriginal est´a na classe ArgM-src (source, fonte da informac¸ ˜ao), para ocorrˆencias como\nDe acordo com a pol´ıcia, trata-se de uma ”pris˜ao significativa”para as investigac¸ ˜oes.\nNo PropBank original, este tipo de construc¸ ˜ao n˜ao deve ser anotado, mas as anotamos\npela relevˆancia argumentativa/ret´orica. Em consonˆancia com as vers˜oes do PropBank-\nBR, utilizamos etiquetas espec´ıficas para verbos auxiliares (de tempo, modo, aspecto e\nvoz) para o pronome -se.\n\nO material foi anotado com base no Manual de anotac¸ ˜ao do PropBank-\nBR v2 [Duran 2014] e nos frames verbais elencados no recurso Verbo-Brasil. Ao\nlongo do projeto, as diretivas de anotac¸ ˜ao foram enriquecidas e atualizadas, dando\norigem a [Duran and Freitas 2024]. Com relac¸ ˜ao a seus antecessores brasileiros, a\nanotac¸ ˜ao PBP difere quanto `a independˆencia da camada sint´atica no que se refere `a\nidentificac¸ ˜ao/segmentac¸ ˜ao de argumentos (se a segmentac¸ ˜ao da an´alise sint´atica e a\nsegmentac¸ ˜ao de argumentos indicada no Verbo-Brasil divergirem, seguimos o Verbo-\nBrasil).\n\n´E interessante destacar as interferˆencias da sintaxe UD na tarefa de SRL. No PBP,\nas divergˆencias entre anotac¸ ˜oes sint´atica e semˆantica foram motivadas pela impossibili-\ndade, em UD, de cruzar arcos sint´aticos (exemplo 1), e em frases com verbos auxiliares,\numa vez que a sintaxe UD ´e bastante econˆomica quanto ao que deve ser considerado verbo\nauxiliar. Na anotac¸ ˜ao UD do Porttinari, est˜ao anotados como auxiliares apenas auxiliares\nde tempo e voz. Na atribuic¸ ˜ao de pap´eis semˆanticos, por´em, esta economia tem como\nresultado a (falsa) necessidade de atribuir pap´eis a elementos que, em portuguˆes, n˜ao\nest˜ao atuando como verbos plenos (“possam”no exemplo 2), e que portanto n˜ao deveriam\nreceber pap´eis semˆanticos – e o resultado ´e uma construc¸ ˜ao sem sentido.\n\n1. O defeito, que a Takata demorou a reconhecer, foi revelado em...\n\n(a) Codificac¸ ˜ao UD: Takata demorou o defeito\n(b) Codificac¸ ˜ao PBP: Takata reconheceu o defeito\n\n2. O projeto prevˆe que as deduc¸ ˜oes s´o possam ocorrer a partir de 2021\n\n(a) Codificac¸ ˜ao UD: deduc¸ ˜oes possam; prevˆe possam; possam ocorrer\n(b) Codificac¸ ˜ao PBP: ocorrer deduc¸ ˜oes; prevˆe ocorrer\n\nO fato de verbos de ligac¸ ˜ao serem considerados AUX, com relac¸ ˜oes de de-\npendˆencia “especiais” (os argumentos sint´aticos – sujeito e predicativo – ficam disso-\nciados do verbo “ser”, e o n´ucleo do sintagma ´e o elemento nominal predicativo) tamb´em\nlevou `a divergˆencia de anotac¸ ˜oes, j´a que o verbo “ser” tem pap´eis semˆanticos para as\nposic¸ ˜oes de sujeito e de predicativo do sujeito.\n\n\f4. Metodologia\n\nEm termos gerais, a anotac¸ ˜ao de pap´eis semˆanticos no PBP seguiu as seguintes etapas,\nalgumas delas concomitantes:\n\n1. Identificac¸ ˜ao do predicador, que em nosso caso foram apenas os verbos;\n2. Consulta ao Verbo-Brasil para selec¸ ˜ao do frame adequado;\n3. Anotac¸ ˜ao dos argumentos numerados conforme descritos no Verbo-Brasil;\n4. Anotac¸ ˜ao dos argumentos modificadores conforme descritos nas diretivas;\n5. Caso necess´ario, criac¸ ˜ao de frames provis´orios para verbos novos ou sentidos no-\n\nvos de formas verbais j´a presentes no Verbo-Brasil;\n\n6. Aplicac¸ ˜ao de regras de validac¸ ˜ao para detectar problemas na anotac¸ ˜ao.\n\nTodo o processo de anotac¸ ˜ao foi feito com base em regras linguisticamente moti-\nvadas e de maneira n˜ao-linear [Wallis 2003], seguindo o exemplo de [Freitas et al. 2023].\nNisto, diferimos do processo de anotac¸ ˜ao do PropBank original, no qual cada frase era\nanotada inteiramente de uma vez, e do PropBank-BR vers˜oes 1 e 2, que seguiu a mesma\nmetodologia. A anotac¸ ˜ao foi feita com a ferramenta ET [de Souza and Freitas 2021], uti-\nlizando o ambiente Interrogat´orio.\n\nA anotac¸ ˜ao foi feita em 3 fases: (i) anotac¸ ˜ao de elementos expl´ıcitos, sempre\nque poss´ıvel usando regras com padr˜oes l´exico-sint´aticos derivados dos exemplos de Du-\nran (2014) e das frases-exemplo no Verbo-Brasil; (ii) anotac¸ ˜ao de elementos impl´ıcitos\n(que envolveu sobretudo a propagac¸ ˜ao de sujeitos, feita com regras) e (iii) aplicac¸ ˜ao final\nde regras de validac¸ ˜ao e detecc¸ ˜ao de inconsistˆencias. Quase todo o processo foi semi-\nautom´atico, utilizando regras que associam um padr˜ao de busca a uma regra de anotac¸ ˜ao,\nsempre com revis˜ao humana. Na propagac¸ ˜ao de sujeitos omitidos de verbos na forma\ninfinitiva, explicitamos argumentos apenas se estes fossem recuper´aveis (frase 1). Em\ncaso de d´uvida ou em caso de argumentos n˜ao recuper´aveis (frase 2), nada foi feito. Em\n[Freitas 2024] est˜ao detalhados os procedimentos e regras utilizados.\n\n1. O presidente centrista optou por garantir pela a primeira vez em anos que. . . (O\n\npresidente garantiu)\n\n2. S˜ao mecˆanicas que pressionam a entender que isso tem custo. (n˜ao ´e poss´ıvel\n\ndeterminar quem entender´a)\n\nA anotac¸ ˜ao de elementos impl´ıcitos levou a um outro tipo de desalinhamento entre\nsintaxe e semˆantica. Na frase “A Folha pediu [contato com o general Mour˜ao] , para que\ncomentasse suas declarac¸ ˜oes, mas (...)”, o segmento “contato com o general Mour˜ao” ´e\nArg1 do verbo “pedir”, mas apenas “general Mour˜ao” ´e sujeito/Arg0 de “comentar”.\n\nA validac¸ ˜ao final consistiu na aplicac¸ ˜ao de 4 regras (Figura 2), que buscavam\nfrases com condic¸ ˜oes suspeitas. Foram encontrados 101 casos suspeitos, e apenas dois\ndeles (derivados da regra 4) eram falsos positivos. Todos os erros foram corrigidos ma-\nnualmente. Diferentemente das regras utilizadas no processo de anotac¸ ˜ao, as regras de\nvalidac¸ ˜ao podem ser aplicadas para a verificac¸ ˜ao final de outros corpora com anotac¸ ˜ao de\npap´eis semˆanticos. As regras de anotac¸ ˜ao, por sua vez, n˜ao foram criadas com o objetivo\nde serem generaliz´aveis para outros corpora, mas de criar um material padr˜ao ouro de\nqualidade e no menor tempo poss´ıvel. No entanto, a elaborac¸ ˜ao de um anotador baseado\nem regras, que aproveite estas regras e o corpus j´a anotado, ´e algo bastante poss´ıvel.\n\n\fFigura 2. Regras para detecc¸ ˜ao de erros\n\nA anotac¸ ˜ao foi feita por uma ´unica pessoa, por 7 meses, a partir das informac¸ ˜oes\ncontidas no Manual de anotac¸ ˜ao do PropBank v2 e no Verbo-Brasil. A fim de avaliar a\nqualidade da anotac¸ ˜ao, foi feita uma concordˆancia inter-anotadores a posteriori, tomando\ncomo base uma amostra com as 100 frases com a maior quantidade de argumentos anota-\ndos no Propbank-BR v2, sobre o qual se baseiam as diretivas de anotac¸ ˜ao e o Verbo-Brasil.\n´E\nEssa amostra foi reanotada sint´atica e semanticamente, e os resultados comparados.\nimportante notar tamb´em que, embora tenhamos escolhido as 100 frases com a maior\nquantidade de argumentos anotados, nem todos os verbos do PropBank-BR v2 tˆem seus\nargumentos anotados, apenas aqueles mais frequentes no corpus. A comparac¸ ˜ao, medida\ncom o ´ındice kappa, foi sobre 443 tokens anotados por ambas as anotac¸ ˜oes, e resultou\nem uma convergˆencia de .90 (como comparac¸ ˜ao, no Propbank original, e considerando\napenas a classificac¸ ˜ao de pap´eis incluindo Arg-M, o kappa foi de .93).\n\nO processo de anotac¸ ˜ao dos frames dos verbos foi concomitante `a anotac¸ ˜ao dos\npap´eis semˆanticos. Uma vez que o foco da anotac¸ ˜ao PBP esteve na atribuic¸ ˜ao dos pap´eis\nsemˆanticos, o processo de atribuic¸ ˜ao de sentidos aos verbos n˜ao foi exaustivo. Al´em de\nn˜ao exaustiva, a anotac¸ ˜ao privilegiou o alinhamento com verbos n˜ao monossˆemicos, uma\nvez que a anotac¸ ˜ao de verbos monossˆemicos poderia ser feita (e foi) de forma autom´atica.\n\nApesar de j´a dispor de um recurso como o Verbo-Brasil, um corpus novo sem-\npre traz novas formas verbais e novos sentidos para verbos j´a descritos. Para os sentidos\n(ainda) sem frames, foi feita uma busca por um verbo similar no Verbo-Brasil, e a soluc¸ ˜ao\nfoi indicada em um documento para posterior aprimoramento do Verbo-Brasil. Ao fi-\nnal do processo, foram documentados cerca de 350 sentidos de verbos sem frames, com\nexemplos do corpus e soluc¸ ˜oes provis´orias em boa parte deles. A anotac¸ ˜ao de frames\nmonossˆemicos foi feita de maneira autom´atica para os casos de verbos monossˆemicos (ou\nseja, que s´o dispunham de um frame). Este procedimento levou `a inclus˜ao de mais de 500\nframes no PBP.\n\nA relac¸ ˜ao estreita entre anotac¸ ˜ao sint´atica e anotac¸ ˜ao de pap´eis semˆanticos, por\num lado, e as interferˆencias da anotac¸ ˜ao sint´atica UD, por outro, levaram `a criac¸ ˜ao de\ndiferentes vers˜oes do corpus, e com isso tamb´em criamos condic¸ ˜oes para investigar o\npapel de diferentes representac¸ ˜oes lingu´ısticas no aprendizado de SRL. Cada uma das\nvers˜oes ´e gerada automaticamente a partir de uma vers˜ao base.\n\n1. PBP na vers˜ao UD: Esta vers˜ao se caracteriza pela atribuic¸ ˜ao de pap´eis\nsemˆanticos apenas aos elementos considerados verbos plenos na UD. Em con-\nsequˆencia: (i) n˜ao foram anotados os pap´eis de argumentos do verbo “ser”; (ii)\n\n\fforam anotados os pap´eis de argumentos de verbos considerados plenos pela UD,\nmas considerados auxiliares no Verbo-Brasil. No entanto, para diferenci´a-los dos\ndemais argumentos, receberam a etiqueta Arg1 d e Arg0 d. Se desej´avel, ambas\nas etiquetas podem ser substitu´ıdas por Arg1 e Arg0, respectivamente.\n\n2. PBP na vers˜ao cl´assica: Esta vers˜ao prioriza o conceito de proposic¸ ˜ao, e se carac-\nteriza pela atribuic¸ ˜ao de pap´eis conforme o modelo PropBank, independentemente\ndo que foi considerado verbo pleno pela UD. Em consequˆencia: (i) foram anota-\ndos os pap´eis de argumentos do verbo “ser”; (ii) n˜ao foram anotados os pap´eis de\nargumentos de verbos que, apesar de considerados plenos na UD, s˜ao considera-\ndos auxiliares no Verbo-Brasil e na documentac¸ ˜ao [Duran and Freitas 2024]; e iii)\nforam anotados como modificadores auxiliares os verbos que, apesar de conside-\nrados plenos pela anotac¸ ˜ao UD, s˜ao considerados auxiliares no PBP (ArgM-mod,\nArgM-asp), al´em daqueles considerados sempre auxiliares (ArgM-tml; ArgM-\npas), seguindo a decis˜ao do PropBank-BR v2 [Duran 2014].\n\n5. Resultados e conclus˜oes\n\nForam anotados 45.813 argumentos verbais e 13.395 instˆancias verbais contˆem anotac¸ ˜ao\nde frames, distribu´ıdos em quase 1.018 frames distintos (60,8% dos verbos possui\nanotac¸ ˜ao de frame). As vers˜oes anteriores do PropBank-BR continham cerca de 7 mil\nargumentos anotados. A Figura 3 apresenta a distribuic¸ ˜ao dos argumentos por func¸ ˜ao\nsint´atica e a Figura 4 traz, com mais detalhes, os pap´eis semˆanticos mais frequentes para\ncada relac¸ ˜ao sint´atica (deprel). Todos os n´umeros se referem `a vers˜ao cl´assica, alinhada `a\nvers˜ao 1.0 do treebank Porttinari-base.\n\nVemos que a associac¸ ˜ao mais frequente ´e entre obj e Arg1, com 92,03%, seguida\nda associac¸ ˜ao entre nsubj e Arg0, com 63,49%. O artigo de [Palmer et al. 2005] traz o\nmesmo tipo de an´alise para o PropBank original. No entanto, uma vez que cada gram´atica\nrecorta e define os elementos que lhes parecem relevantes, ´e necess´ario algum cuidado\nnessa comparac¸ ˜ao. O elemento sentencial/oracional S, presente no PropBank original\n(derivado da anotac¸ ˜ao do Penn Treebank), n˜ao existe em UD, e est´a distribu´ıdo entre\nalguns casos de xcomp e de ccomp, por exemplo.\n\nDe forma geral, analisando os dados do PBP, extra´ımos as seguintes informac¸ ˜oes:\n(a) Arg1 se distribui principalmente entre as relac¸ ˜oes obj, nsubj, obl, xcomp e ccomp;\n(b) Arg0 se concentra em nsubj; (c) Arg2 se distribui principalmente entre obl e xcomp;\n(d) ArgM-tmp e ArgM-mnr se distribuem entre obl, advmod e advcl; (e) ArgM-loc se\nconcentra em obl. Com a perspectiva da sintaxe: (a) obj participa de Arg1 e parece ser a\ngeneralizac¸ ˜ao mais f´acil: “se ´e um obj, ent˜ao ´e Arg1”; (b) xcomp participa igualmente de\nArg1 e Arg2; (c) obl participa de Arg2, Arg1, tempo, modo e local; (d) advmod participa\nde neg, adv, tempo, dis e modo; (e) advcl participa de tempo, finalidade, modo e Arg2.\n\nOs dados apontam para uma regularidade entre func¸ ˜oes sint´aticas e pap´eis nume-\nrados – especificamente, pap´eis Arg0 e Arg1. Os demais argumentos numerados, com\nbaixa frequˆencia, s˜ao de mais dif´ıcil generalizac¸ ˜ao. Estas constatac¸ ˜oes convergem com\nresultados para o inglˆes, que verificam que a anotac¸ ˜ao ao estilo PropBank captura me-\nlhor regularidades sint´aticas, sobretudo para argumentos de frequˆencia alta, em oposic¸ ˜ao\nao estilo VerbNet de anotac¸ ˜ao de pap´eis semˆanticos [Merlo and Van Der Plas 2009]. Li-\ndos de outra forma, embora o objetivo de um PropBank seja muitas vezes servir como\n\n\fFigura 3. Distribuic¸ ˜ao de pap ´eis sem ˆanticos por relac¸ ˜ao sint ´atica\n\nFigura 4. Pap ´eis sem ˆanticos mais frequentes para cada relac¸ ˜ao sint ´atica\n\nmaterial de treino para IA, a forma de codificar os pap´eis talvez seja mais indicada (ou\ntamb´em seja indicada) para um anotador baseado em regras e que considere sintaxe. De\nfato, [Palmer et al. 2005] relatam que um anotador simples baseado em regras tem de-\nsempenho de 83%, sendo 85% o estado-da-arte em inglˆes, considerando cen´arios dif´ıceis\nde avaliac¸ ˜ao, com verbos n˜ao vistos na fase de treino [Wang et al. 2022].\n\nApesar de raros, estudos sobre a capacidade de generalizac¸ ˜ao da anotac¸ ˜ao ao\nestilo PropBank tˆem mostrado que, quando comparada `a anotac¸ ˜ao ao estilo VerbNet,\na anotac¸ ˜ao PropBank leva a resultados inferiores no que se refere a argumentos Arg2\na Arg5 e, em termos gerais, os bons resultados obtidos com a anotac¸ ˜ao PropBank se\nreferem aos argumentos mais frequentes [Merlo and Van Der Plas 2009, Yi et al. 2007,\nGung and Palmer 2021, Li et al. 2023]. No entanto, todos os estudos foram feitos para\na l´ıngua inglesa, e apenas a disponibilizac¸ ˜ao de recursos similares para o portuguˆes nos\npermitiria verificar os resultados para a nossa l´ıngua.\n\nPor fim, a criac¸ ˜ao de duas vers˜oes (UD e Cl´assica) tamb´em permite comparar a\nanotac¸ ˜ao PBP com outros PropBanks que tenham seguido mais fielmente a anotac¸ ˜ao UD.\nAs diferentes vers˜oes do corpus, bem como documentac¸ ˜ao lingu´ıstica detalhada e regras\nde anotac¸ ˜ao utilizadas, est˜ao p´ublicas no portal web do projeto POeTiSA3.\n\nAgradecimentos\n\nEste trabalho foi realizado no ˆambito do Centro de Inteligˆencia Artificial da Universidade\nde S˜ao Paulo (C4AI4), com o apoio da Fundac¸ ˜ao de Amparo `a Pesquisa do Estado de S˜ao\nPaulo (processo FAPESP #2019/07665-4) e da IBM. Este projeto tamb´em foi apoiado\npelo Minist´erio da Ciˆencia, Tecnologia e Inovac¸ ˜oes, com recursos da Lei N. 8.248, de 23\nde outubro de 1991, no ˆambito do PPI-Softex, coordenado pela Softex e publicado como\nResidˆencia em TIC 13, DOU 01245.010222/2022-44. Os autores agradecem tamb´em a\nElvis de Souza pela preparac¸ ˜ao dos arquivos para disponibilizac¸ ˜ao.\n\n3https://sites.google.com/icmc.usp.br/poetisa/resources-and-tools\n4http://c4ai.inova.usp.br/\n\n\fReferˆencias\n\nBick, E. (2007). Automatic semantic role annotation for portuguese.\n\nIn Proceedings\nof TIL 2007 - 5th Workshop on Information and Human Language Technology, pages\n1713–1716, Rio de Janeiro. Sociedade Brasileira de Computac¸ ˜ao (SBC).\n\nBranco, A., Carvalheiro, C., Pereira, S., Silveira, S., Silva, J., Castro, S., and Grac¸a, J.\n(2012). A PropBank for Portuguese: the CINTIL-PropBank. In Calzolari, N., Choukri,\nK., Declerck, T., Do˘gan, M. U., Maegaard, B., Mariani, J., Moreno, A., Odijk, J., and\nPiperidis, S., editors, Proceedings of the Eighth International Conference on Language\nResources and Evaluation (LREC’12), pages 1516–1521, Istanbul, Turkey. European\nLanguage Resources Association (ELRA).\n\nde Marneffe, M.-C., Manning, C. D., Nivre, J., and Zeman, D. (2021). Universal Depen-\n\ndencies. Computational linguistics, 47(2):255–308.\n\nde Souza, E. and Freitas, C. (2021). ET: A workstation for querying, editing and evalua-\nting annotated corpora. In Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations, pages 35–41, Online and\nPunta Cana, Dominican Republic. Association for Computational Linguistics.\n\nDong, X. L. (2023). Generations of knowledge graphs: The crazy ideas and the business\n\nimpact. Proc. VLDB Endow., 16(12):4130–4137.\n\nDuran, M., Lopes, L., das Grac¸as Nunes, M., and Pardo, T. (2023). The dawn of the\nporttinari multigenre treebank: Introducing its journalistic portion. In Anais do XIV\nSimp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages\n115–124, Porto Alegre, RS, Brasil. SBC.\n\nDuran, M. S. (2014). Manual de anotac¸ ˜ao do PropBank-Br v2. Technical report, ICMC-\n\nUSP.\n\nDuran, M. S. and Alu´ısio, S. M. (2011). Propbank-br: a Brazilian Portuguese corpus\nannotated with semantic role labels. In Proceedings of the 8th Brazilian Symposium in\nInformation and Human Language Technology.\n\nDuran, M. S. and Freitas, C. (2024). Guia de anotac¸ ˜ao de pap´eis semˆanticos seguindo o\nmodelo PropBank no corpus Porttinari-base. (no prelo). Technical report, ICMC-USP.\n\nDuran, M. S., Torres, L. S., Viviani, M. C., Hartmann, N., and Alu´ısio, S. M. (2014).\nSelec¸ ˜ao e preparac¸ ˜ao de sentenc¸as do corpus PLN-BR para compor o corpus de\nanotac¸ ˜ao de pap´eis semˆanticos Propbank-Br.v2. Technical report, N´ucleo Interinsti-\ntucional de Lingu´ıstica Computacional.\n\nEvans, R. and Orasan, C. (2019). Sentence simplification for semantic role labelling\nIn Mitkov, R. and Angelova, G., editors, Proceedings\nand information extraction.\nof the International Conference on Recent Advances in Natural Language Processing\n(RANLP 2019), pages 285–294, Varna, Bulgaria. INCOMA Ltd.\n\nFreitas, C. (2023). Dataset e corpus. In Caseli, H. and Volpe Nunes, M. d. G., editors,\nProcessamento de Linguagem Natural: conceitos, t´ecnicas e aplicac¸ ˜oes em Portuguˆes,\npages 1–37. BPLN.\n\nFreitas, C. (2024). Anotac¸ ˜ao de pap´eis semˆanticos no corpus Porttinari-base: Procedi-\n\nmentos, resultados e an´alise. (no prelo). Technical report, ICMC-USP.\n\n\fFreitas, C., Souza, E., Castro, M. C., Cavalcanti, T., Ferreira da Silva, P., and Corrˆea Cor-\ndeiro, F. (2023). Recursos lingu´ısticos para o PLN espec´ıfico de dom´ınio: o Petrolˆes.\nLinguam´atica, 15(2):51–68.\n\nGung, J. and Palmer, M. (2021). Predicate representations and polysemy in VerbNet\nIn Zarrieß, S., Bos, J., van Noord, R., and Abzianidze, L., edi-\nsemantic parsing.\ntors, Proceedings of the 14th International Conference on Computational Semantics\n(IWCS), pages 51–62, Groningen, The Netherlands (online). Association for Compu-\ntational Linguistics.\n\nHan, H. and Choi, J. (2020). Establishing strong baselines for the new decade: Sequence\ntagging, syntactic and semantic parsing with bert. In Proceedings of the Thirty-Third\nInternational Florida Artificial Intelligence Research Society Conference (FLAIRS\n2020).\n\nHartmann, N. S., Duran, M. S., and Alu´ısio, S. M. (2016). Automatic semantic role\nlabeling on non-revised syntactic trees of journalistic texts. In Silva, J., Ribeiro, R.,\nQuaresma, P., Adami, A., and Branco, A., editors, Computational Processing of the\nPortuguese Language, pages 202–212, Cham. Springer International Publishing.\n\nLevin, B. (1993). English Verb Classes and Alternations: a preliminary investigation.\n\nThe University of Chicago Press, London.\n\nLevin, B. and Rappaport Hovav, M. (2005). Argument Realization. Cambridge University\n\nPres, Cambridge.\n\nLi, T., Kazeminejad, G., Brown, S., Srikumar, V., and Palmer, M. (2023). Learning\nsemantic role labeling from compatible label sequences. In Bouamor, H., Pino, J., and\nBali, K., editors, Findings of the Association for Computational Linguistics: EMNLP\n2023, pages 15561–15572, Singapore. Association for Computational Linguistics.\n\nMerlo, P. and Van Der Plas, L. (2009). Abstraction and generalisation in semantic role\nlabels: PropBank, VerbNet or both? In Su, K.-Y., Su, J., Wiebe, J., and Li, H., editors,\nProceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th\nInternational Joint Conference on Natural Language Processing of the AFNLP, pages\n288–296, Suntec, Singapore. Association for Computational Linguistics.\n\nMohebbi, M., Razavi, S. N., and Balafar, M. A. (2022). Computing semantic similarity of\ntexts based on deep graph learning with ability to use semantic role label information.\nScientific Reports, 12(1).\n\nPalmer, M., Gildea, D., and Kingsbury, P. (2005). The proposition bank: An annotated\n\ncorpus of semantic roles. Computational linguistics, 31(1):71–106.\n\nPardo, T., Duran, M., Lopes, L., Felippo, A., Roman, N., and Nunes, M. (2021). Porttinari\nIn Anais do XIII Simp´osio\n- a large multi-genre treebank for brazilian portuguese.\nBrasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 1–10, Porto\nAlegre, RS, Brasil. SBC.\n\nRodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., Cardoso, H. L., and Os´orio,\nT. (2023). Advancing neural encoding of portuguese with transformer albertina pt-*.\nIn Moniz, N., Vale, Z., Cascalho, J., Silva, C., and Sebasti˜ao, R., editors, Progress in\nArtificial Intelligence, pages 441–453, Cham. Springer Nature Switzerland.\n\n\fSanches Duran, M. and Alu´ısio, S. (2015). Automatic generation of a lexical resource to\nsupport semantic role labeling in Portuguese. In Palmer, M., Boleda, G., and Rosso,\nP., editors, Proceedings of the Fourth Joint Conference on Lexical and Computational\nSemantics, pages 216–221, Denver, Colorado. Association for Computational Linguis-\ntics.\n\nTenney, I., Das, D., and Pavlick, E. (2019a). BERT rediscovers the classical NLP pipeline.\nIn Korhonen, A., Traum, D., and M`arquez, L., editors, Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 4593–4601, Florence,\nItaly. Association for Computational Linguistics.\n\nTenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R. T., Kim, N., Durme, B. V.,\nBowman, S. R., Das, D., and Pavlick, E. (2019b). What do you learn from context?\nprobing for sentence structure in contextualized word representations. In International\nConference on Learning Representations.\n\nWallis, S. (2003). Completing parsed corpora: From correction to evolution. In Abeill´e,\nA., editor, Treebanks: Building and Using Parsed Corpora, pages 61–71. Springer\nNetherlands, Dordrecht.\n\nWang, N., Li, J., Meng, Y., Sun, X., Qiu, H., Wang, Z., Wang, G., and He, J. (2022).\nAn MRC framework for semantic role labeling. In Calzolari, N., Huang, C.-R., Kim,\nH., Pustejovsky, J., Wanner, L., Choi, K.-S., Ryu, P.-M., Chen, H.-H., Donatelli, L.,\nJi, H., Kurohashi, S., Paggio, P., Xue, N., Kim, S., Hahm, Y., He, Z., Lee, T. K.,\nSantus, E., Bond, F., and Na, S.-H., editors, Proceedings of the 29th International\nConference on Computational Linguistics, pages 2188–2198, Gyeongju, Republic of\nKorea. International Committee on Computational Linguistics.\n\nYi, S.-t., Loper, E., and Palmer, M. (2007). Can semantic roles generalize across gen-\nIn Sidner, C., Schultz, T., Stone, M., and Zhai, C., editors, Human Language\nres?\nTechnologies 2007: The Conference of the North American Chapter of the Association\nfor Computational Linguistics; Proceedings of the Main Conference, pages 548–555,\nRochester, New York. Association for Computational Linguistics.\n\n\f"
        },
        {
            "titulo": "No Argument Left Behind: Overlapping Chunks for Faster Processing of Arbitrarily Long Legal Texts",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31124",
            "idioma": "Inglês",
            "storage_key": "files/article_31124_30927.pdf",
            "autores": [
                {
                    "nome": "Israel Fama",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0001-6325-4153"
                },
                {
                    "nome": "Bárbara Bueno",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0009-0004-7455-3342"
                },
                {
                    "nome": "Alexandre Alcoforado",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-3184-1534"
                },
                {
                    "nome": "Thomas Palmeira Ferraz",
                    "afiliacao": "Télécom Paris / Institut Polytechnique de Paris",
                    "orcid": "https://orcid.org/0000-0002-5385-9164"
                },
                {
                    "nome": "Arnold Moya",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0009-0001-2377-379X"
                },
                {
                    "nome": "Anna Helena Reali Costa",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0001-7309-4528"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "In a context where the Brazilian judiciary system, the largest in the world, faces a crisis due to the slow processing of millions of cases, it becomes imperative to develop efficient methods for analyzing legal texts. We introduce uBERT, a hybrid model that combines Transformer and Recurrent Neural Network architectures to effectively handle long legal texts. Our approach processes the full text regardless of its length while maintaining reasonable computational overhead. Our experiments demonstrate that uBERT achieves superior performance compared to BERT+LSTM when overlapping input is used and is significantly faster than ULMFiT for processing long legal documents.",
            "keywords": [
                "Natural Language Processing",
                "Legal NLP",
                "Text Classification",
                "Deep Learning",
                "Transformers"
            ],
            "referencias": [
                "Beltagy, I., Peters, M. E., and Cohan, A. (2020). Longformer: The long-document transformer.",
                "CNJ (2024). Conselho nacional de justiça - cnj. Accessed: 2024-08-05.",
                "Cui, J., Shen, X., Nie, F., Wang, Z., Wang, J., and Chen, Y. (2022). A survey on legal judgment prediction: Datasets, metrics, models and challenges. arXiv preprint arXiv:2204.04859v1.",
                "Demšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1–30.",
                "Ferraz, T. P., Alcoforado, A., Bustos, E., Oliveira, A. S., Gerber, R., Müller, N., d’Almeida, A. C., Veloso, B. M., and Costa, A. H. R. (2021). Debacer: a method for slicing moderated debates. In Anais do XVIII Encontro Nacional de Inteligência Artificial e Computacional, pages 667–678. Sociedade Brasileira de Computação-SBC.",
                "Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.-W. (2020). Realm: Retrieval-augmented language model pre-training.",
                "Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontañón, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. (2020). Big bird: Transformers for longer sequences. CoRR, abs/2007.14062",
                "Zhang, L., Wang, W., Yu, K., huang, J., Lyu, Q., Xue, H., and Hetang, C. (2023). Sliding-bert: Striding towards conversational machine comprehension in long contex. Adv. Artif. Intell. Mach. Learn., 3:1325–1339.",
                "Zhu, H., Mak, D., Gioannini, J., and Xia, F. (2020). NLPStatTest: A toolkit for comparing NLP system performance. In Wong, D. and Kiela, D., editors, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pages 40–46, Suzhou, China. Association for Computational Linguistics."
            ],
            "artigo_completo": "No Argument Left Behind: Overlapping Chunks for Faster\nProcessing of Arbitrarily Long Legal Texts\n\nIsrael Fama1*, B´arbara Bueno1*, Alexandre Alcoforado1,\nThomas Palmeira Ferraz2, Arnold Moya1, Anna Helena Reali Costa1\n\n1Escola Polit´ecnica, Universidade de S˜ao Paulo (USP), S˜ao Paulo, SP, Brazil\n\n2T´el´ecom Paris, Institut Polytechnique de Paris, Palaiseau, France\n\n{israelfama, barbarabueno, anna.reali}@usp.br\n\nAbstract. In a context where the Brazilian judiciary system, the largest in the\nworld, faces a crisis due to the slow processing of millions of cases, it becomes\nimperative to develop efficient methods for analyzing legal texts. We introduce\nuBERT, a hybrid model that combines Transformer and Recurrent Neural Net-\nwork architectures to effectively handle long legal texts. Our approach processes\nthe full text regardless of its length while maintaining reasonable computational\noverhead. Our experiments demonstrate that uBERT achieves superior perfor-\nmance compared to BERT+LSTM when overlapping input is used and is signif-\nicantly faster than ULMFiT for processing long legal documents.\n\n1. Introduction\n\nLegal NLP can be defined as the application of Natural Language Processing (NLP) tech-\nniques within the legal domain. This subfield of NLP has been experiencing rapidly\ngrowing interest from both academia and industry: [Katz et al. 2023] reports a signifi-\ncant increase in the volume of publications, rising from fewer than 30 papers in 2013 to\nnearly 120 in 2022. Brazil possesses the largest judiciary system in the world, comprising\n18,000 judges distributed across 91 courts. At the time of writing, there are more than 84\nmillion ongoing legal cases [CNJ 2024]. These numbers indicate both the need and the\nopportunity for innovative solutions to manage and analyze vast amounts of legal data.\n\nWe turn our focus to Legal Judgment Prediction (LJP), which involves predicting\ncourt decisions. Although predicting decisions may be a complex task, we argue it can\nbe reduced to a Text Classification task, which has seen a marked increase in studies\n[Li et al. 2022], fueled by advancements in deep learning. In particular, the Transformer\narchitecture emerged as a paradigm shift [Hasan 2022] for many NLP tasks. However,\nit still has limitations when handling long texts, which poses significant challenges in the\nlegal domain, where documents are usually long and complex.\n\nThere is fruitful research being done on enhancing the input size limita-\ntion for Transformers, such as Retrieval-Augmented Language Models (RALMs)\n[Guu et al. 2020]. Current retrieval techniques, however, often trust embedding mod-\nels which also can be sub-optimal when dealing with legal documents, where a single\nword in the whole document can make a difference. Also, these methods demand sub-\nstantial computational resources and large document stores to achieve good performance.\n\n*These authors contributed equally to this work.\n\n\fOther methods combine input in a sequential way, often leveraging properties of Recur-\nrent Neural Networks to process longer sequences [Wan et al. 2019], although those will\nalso usually truncate the text if it is too long. But for documents in the legal domain, such\nas judicial decisions, most of the documents are usually composed of reasoning from the\njudge. Therefore, it is of our interest to have a method that uses the full text as input.\n\nIn this paper, we propose uBERT, a hybrid model that combines an encoder-based\nTransformer with a Recurrent Neural Network, capable of processing long texts. We\npropose an experimental setup with data from legal decisions, and compare uBERT to\nbaselines BERT+LSTM, Big Bird and ULMFiT in the classification task. Our results\nshow that uBERT slightly outperforms BERT+LSTM as long as overlapping input is in-\ntroduced. Also, ULMFiT performs better for long texts, but is 4x slower than uBERT.\n\nThe remainder of this paper is structured as follows: Sect. 2 reviews related work\non Legal NLP and long text classification; Sect. 3 outlines our proposal, including the\nformalization of the target task and the introduction of our model; Sect. 4 outlines the\nexperiments we setup to assess our model in terms of performance and efficiency. Finally,\nwe present the results and conclude with a discussion of the findings.\n\n2. Related Work\n\nTransformer-Based Approaches for Long Text Processing in Legal NLP: Long-\nformer [Beltagy et al. 2020] employs a sparse attention mechanism, extending the input\nsize limit to 4096 tokens, which is eight times the limit of BERT [Devlin et al. 2019]).\n[Hoang et al. 2023] applied this architecture to classify legal texts from the Indian Legal\nDocuments Corpus – ILDC [Malik et al. 2021], but they did not process the entire text.\n\n[Pappagari et al. 2019] introduced RoBERT, a method that splits long texts into\noverlapping chunks for recurrent encoding. While similar in concept to our architecture,\na direct comparison is not possible due to limited details on their overlap and recurrence\nstrategies. Moreover, RoBERT was evaluated on shorter texts compared to our dataset.\n\nThe overlapping algorithm in our approach, uBERT, is a specific case of Sliding-\nBERT’s method [Zhang et al. 2023], with the stride set to half the overlap. Unlike Slid-\ningBERT, where tokens can appear in multiple chunks, we limit overlaps to two chunks to\nreduce computational overhead while preserving context continuity. This choice is driven\nby efficiency, not language differences.\n\n[Menezes-Neto and Clementino 2022] introduced BrCAD-51, a dataset designed\nfor Legal Judgment Prediction (LJP), and evaluated three architectures for this task:\nULMFiT [Howard and Ruder 2018], BigBird [Zaheer et al. 2020], and BERT+LSTM.\nULMFiT, a transfer learning model that fine-tunes a pre-trained language model for down-\nstream NLP tasks, was the only architecture capable of processing the entire text as input.\nBigBird, a sparse-attention model, addresses the 512-token limit by focusing on subsets\nof tokens, thereby reducing computational complexity, and was configured to handle texts\nup to 7,680 tokens. For BERT+LSTM, documents were split into 512-token chunks, with\ntruncation applied to middle chunks if a document required more than 15. While simi-\n\n1This dataset consists of decisions issued by the Brazilian Federal Small Claims Court (FSCC). These decisions can be appealed\nto the Appellate Panel (AP), which re-examines the case and either reverses or affirms the initial ruling. Each data point in BrCAD-5\nrepresents the text of a decision issued by the FSCC. The task proposed by the authors is to predict whether the AP will reverse or\naffirm the initial ruling based on the decision text.\n\n\flar in approach, uBERT differs from BERT+LSTM in that it uses a chunk overlapping\nstrategy and imposes no limit on the number of chunks, ensuring the entire text is utilized\nwithout truncation.\n\nCritiques and Limitations in Legal NLP Research: The legal\nindustry has\nbeen slow to adopt NLP advancements,\nrelying heavily on manual work by\nlawyers. [Mahari et al. 2023] identify a key issue: Legal NLP research often fails to align\nwith the practical needs of legal practitioners.\n[Medvedeva and Mcbride 2023] further\nhighlight a significant gap in Legal Judgment Prediction (LJP) research, criticizing the\nuse of poorly designed datasets that rely on biased case facts extracted from judgments.\nThis approach leads to models with overly optimistic performance that offer limited prac-\ntical value to legal practitioners.\n\nThis work aims to bridge the gap between research and practice in the field of\nLegal NLP. We propose an architecture capable of processing virtually infinite-length le-\ngal texts and evaluate it on the BrCAD-5 dataset, which [Medvedeva and Mcbride 2023]\nregard as a well-designed benchmark.\n\n3. Proposal\n\nText classification can be formalized as follows. Given a document d that represents\na judicial decision, the goal is to make a prediction y ∈ {0, 1}, by learning a binary\nclassifier f such that f (d) = y. The positive class y = 1 represents a decision that will be\nreversed by an Appellate Panel (AP). Since legal documents are often long, when using\nTransformer-based models, conventional approaches usually truncate text from d, which\nis sub-optimal for the task [Pappagari et al. 2019]. This can hinder performance on the\nLegal Judgment Prediction task, since some relevant part of the text may be cut off.\n\nBelieving that the text as a whole is more useful when learning a classifier, we pro-\npose unlimited BERT, or uBERT, an efficient architecture that combines an encoder-based\nTransformer with a Recurrent Neural Network, utilizing an overlapping algorithm during\nboth training and inference to handle an unlimited number of input tokens. This approach\nis similar to the BERT+LSTM model used by [Menezes-Neto and Clementino 2022], but\nintroduces modifications to maintain local context (through overlapping chunks) and ac-\ncommodate documents of virtually any size. Although the quadratic memory complexity\nof the self-attention mechanism presents a challenge for scaling input indefinitely, we\nleverage the RNN’s capacity to process long sequences, enabling it to take chunk em-\nbeddings and output a comprehensive document embedding. Several studies, such as\n[Hoang et al. 2023], have explored the combination of attention mechanisms and recur-\nrence. Our model builds on this concept but applies overlapping during both training and\ninference, and does not limit the number of chunks processed by the encoder.\n\nFigure 1 depicts the uBERT architecture. It shows key aspects to understand how\n\nour model works.\n\nLet E be an encoder-based Transformer, with dim being the dimensionality of\nthe output vector of the final layer, and R be a Recurrent Neural Network. Let maxtok\nbe the maximum number of tokens E can process as input. Let maxc be the maximum\nnumber of chunks of maxtok tokens that E can process in parallel with a single run. We\nsplit document d into n chunks of size maxtok tokens, starting in the first token. For each\n\n\fFigure 1: uBERT architecture.\n\nrun, we extract the hidden states from the last four layers of E, and concatenate them\nto form the representation of each chunk. This is based on the idea that different layers\ncapture different linguistic features [Tenney et al. 2019]. Specifically, BERT+LSTM, the\nbaseline most similar to our proposed architecture, extracts the hidden states from the last\nfour layers. While other layers could be used for extraction, we retained this approach\nfor consistency in model comparison. In each single run, we process [1, maxc] chunks in\nparallel, generating [1, maxc] vectors of embeddings, each with dimensionality 4 × dim.\nWe iteratively process chunks from d until an embedding vector is generated for each\nchunk and thus preserving the entire text content of d.\n\nThen, we concatenate the embedding vectors maintaining the order of the respec-\ntive chunks, generating a tensor of dimensionality (n, 4 × dim). We process this tensor\nwith the RNN sequentially, capturing the dependencies between them and generating a\ncontextually enhanced representation for each chunk.\n\nSplitting text by token count can disrupt its flow, so we use token overlap between\nchunks during both training and inference to maintain continuity. This technique, similar\nto that used by [Hoang et al. 2023] but applied more broadly, helps preserve the text’s\nnatural structure.\n\nOur token overlap algorithm can be formalized as follows. Consider the judicial\ndecision d as the tokenized sequence S = {t1, . . . , tk}, where k is the number of tokens\nin d. We define the overlap size, z, as the number of tokens each chunk shares with its\n(cid:5)\nadjacent neighbors. Thus, any chunk shares (cid:4) z\nwith the subsequent one. The first and last chunks, having only one adjacent chunk, share\n(cid:4) z\n2\n\n(cid:5) tokens with their respective neighbors.\n\n(cid:5) tokens with the previous chunk and (cid:4) z\n\n2\n\n2\n\nFigure 2: Overlapping chunks example.\n\nFigure 2 provides a simple example for clarification. In this example, the chunk\n\nTokensRaw textOverlapping chunksbatch_1batch_nEBERT_output_1BERT_output_nChunk embeddingsRf\fsize is 4, and z = 2. As shown, chunk c2 = {t4, t5, t6, t7} shares token t4 with chunk c1\nand token t7 with chunk c3. Note that the first and last chunks share only one token with\nthe neighboring chunk.\n\n4. Experiments\n\nIn this section, we design experiments to assess our proposed model, uBERT, and validate\nits effectiveness on the legal domain. We split our experiments into 3, one for each of the\nfollowing research questions:\n\nRQ1: Would an encoder-based model benefit from using the entire text in terms of\nperformance improvement?\n\nWe examine the impact of processing the whole documents using multiple encoder passes.\nWe first tested if simply increasing text chunks to process the whole text without using\noverlap (uBERT 0) improves performance over BERT+LSTM, which processes only par-\ntial text in a single pass. Then, we investigated the effect of introducing overlaps (0 to\n510 tokens2) between chunks to observe if the added local context enhances predictions.\n\nRQ2: If performance improves, does it come with reasonable computational over-\nhead?\n\nWe compare the inference time of our architecture against all baseline models to deter-\nmine if it offers a performance gain and to assess the associated computational overhead.\n\nRQ3: Is our architecture better for processing longer texts?\n\nWe explore the relationship between document length and model performance. We tested\nthe models on the full test set as well as on its subsets, the 10% and the 1% longest texts.\nThis experiment involved statistical analysis to determine whether longer texts lead to\nbetter or worse predictions.\n\nData: We used the BrCAD-5 dataset3 in our experiments. The task is a binary classi-\nfication, with Class 1 indicating that the AP reverses the previous decision, and Class 0\nindicating it affirms. The dataset is imbalanced, with 22% of the data points belonging\nto Class 1. Although this imbalance ratio is consistent across all dataset splits, it varies\nsignificantly with text length.\n\nModels: In this work, our model (uBERT) uses BERT as the encoder and LSTM as the\nRNN, with maxtok set to 512 tokens and up to 15 chunks (maxc) processed in parallel.\nOur training procedure follows the approach of [Menezes-Neto and Clementino 2022],\nwhere we fine-tune the last layer of BERT and the LSTM. The fine-tuning is conducted for\n1 epoch utilizing the One Cycle learning rate scheduler. Our inference procedure mirrors\nthe training process.\n\nBaselines: our baseline models are ULMFiT (forward, backward and bidirectional)4, Big\nBird and BERT+LSTM. Notably, only ULMFiT and uBERT process the full text.\n\n2The typical input size for BERT models is 512 tokens. Our overlap algorithm first slices the text and distributes the tokens. Only\n\nafter this process are the special tokens [CLS] and [SEP] added, resulting in the well-known 512-token limit.\n\n3This dataset is divided in training, validation, and test sets: the training set includes 380,673 documents, while the validation and\n\ntest sets contain 76,342 and 76,299 entries, respectively.\n\n4ULMFiT incorporates a forward language model (predicting the next token), a backward language model (predicting the previous\n\ntoken), and a bidirectional model that combines the two, allowing it to capture contextual information in both directions.\n\n\fComputational Infrastructure and Resources: the experiments were conducted using\nGoogle’s Colab infrastructure, specifically an NVIDIA A100 GPU with 40 GB of RAM.\n\nEvaluation Metrics: We evaluate all models using the Macro F1 score and Matthews\nCorrelation Coefficient (MCC). The Macro F1 score is a well-established metric across\nNLP fields, representing the harmonic mean of precision and recall, while MCC, though\nless common, is frequently used in the Legal Judgment Prediction (LJP) subfield, as noted\nby [Cui et al. 2022]. MCC measures the correlation between predicted and actual clas-\nsifications by accounting for true positives, true negatives, false positives, and false neg-\natives, making it suitable for imbalanced classes5. Additionally, MCC is the metric used\nby [Menezes-Neto and Clementino 2022], making it necessary for us to use it as well\nfor model comparison. To compare different baselines and configurations of our uBERT\nmodel, we employed bootstrap resampling to obtain 95% confidence intervals, followed\nby Wilcoxon-Holm post-hoc analysis to assess statistical significance with α = 5%, fol-\nlowing similar approaches [Demˇsar 2006, Zhu et al. 2020, Ferraz et al. 2021].\n\n5. Results\n\nTable 1 presents the results for all model configurations on the full test dataset, as well as\nthe 10% and 1% longest texts. The baseline models were not run on the full test set in this\nstudy due to computational resource limitations. The results reported here are reproduced\nfrom [Menezes-Neto and Clementino 2022], which is why Table 1 does not include in-\nference times for the full test set. Figure 3 displays the macro F1-scores across varying\ntext lengths, while Figure 4 ranks the models using the MCC metric. Although MCC is\neffective for within-dataset comparisons, it is less suitable across datasets with differing\nclass imbalance; hence, we rely on the macro F1-score for cross-dataset comparisons.\n\nTable 1: uBERT performance across various overlap sizes compared with baselines.\n\nDataset:\n\nFull Test Set\n(76.299 documents)\nImbalance Ratio = 0.28\n\n10% Set\n(7.632 documents)\nImbalance Ratio = 0.32\n\n1% Set\n(763 documents)\nImbalance Ratio = 0.54\n\nArchitecture\n\nMacro-F1↑ MCC↑ Macro-F1↑ MCC↑\n\nInf.Time↓ Macro-F1↑ MCC↑\n\nInf.Time↓\n\nBaselines\nULMFiT - fwd\nULMFiT - bwd\nULMFiT - bidir\nBig Bird\nBERT+LSTM\nOurs\nuBERT 0\nuBERT 150\nuBERT 205\nuBERT 300\nuBERT 408\nuBERT 510\n\n65.1 %\n65.7 %\n66.9 %\n52.0 %\n64.1 %\n\n63.9 %\n63.3 %\n64.7 %\n64.7 %\n64.0 %\n64.6 %\n\n0.32\n0.35\n0.37\n0.27\n0.33\n\n0.33\n0.32\n0.35\n0.35\n0.33\n0.35\n\n64.9 %\n63.4 %\n64.8 %\n44.0 %\n63.2 %\n\n62.6 %\n62.2 %\n62.6 %\n63.0 %\n63.2 %\n63.0 %\n\n0.32\n0.35\n0.34\n0.23\n0.31\n\n0.31\n0.30\n0.31\n0.31\n0.32\n0.31\n\n1h 18min\n1h 18min\n1h 18min\n22min\n12min\n\n13min\n14min\n15min\n17min\n19min\n21min\n\n72.3 %\n59.9 %\n69.3 %\n30.0 %\n64.0 %\n\n61.3 %\n59.4 %\n62.0 %\n63.0 %\n64.3 %\n64.2 %\n\n0.47\n0.33\n0.43\n0.08\n0.36\n\n0.34\n0.32\n0.35\n0.36\n0.38\n0.38\n\n11min:22s\n14min:44s\n14min:44s\n2min:58s\n1min:29s\n\n1min:51s\n2min:04s\n2min:09s\n2min:23s\n2min:42s\n3min:08s\n\n5MCC ranges from -1 to 1, where 1 indicates perfect prediction, 0 indicates no better than random chance, and -1 indicates total\n\ndisagreement between prediction and observation.\n\n\fFigure 3: Macro-F1 score x Avg. Tokens/Group across different groups of same size ranked by the length.The error\n\nbars represent 95% confidence intervals obtained with bootstrap resampling.\n\n(a) All Text\n\n(b) 10% Longest Text\n\n(c) 1% Longest Text\n\nFigure 4: Critical difference diagram showing pairwise statistical comparison between baselines and varying overlap\nsizes for uBERT using the MCC. Connecting bars represent no statistical difference between methods.\n\nProcessing the Full Text Requires Overlap Comparing the BERT+LSTM baseline,\nwhich middle-truncates text when it exceeds input size, with our uBERT without over-\nlap (uBERT 0), which uses the full text, we found that uBERT either underperformed or\nmatched the baseline across all metrics. Notably, it performed worse on the 1% longest\ntexts, where middle-truncation by BERT+LSTM occurs. This suggests that merely\nprocessing the entire text is insufficient for longer inputs. We hypothesize that non-\noverlapping chunks introduce noise due to abrupt segmentation, which degrades perfor-\nmance. Our results support this, showing that introducing overlap in uBERT configura-\ntions improves both Macro-F1 and MCC scores. The following uBERT configurations\noutperformed BERT+LSTM with statistical significance: uBERT 300, uBERT 510\nand uBERT 205 on full test set; uBERT 408 and uBERT 300 on 10% longest; and\nuBERT 408, uBERT 300 and uBERT 510 on 1% longest. Thus, incorporating over-\nlap is crucial for maintaining semantic consistency and improving performance on\nlonger texts.\n\nuBERT with Overlap is still Significantly Faster than ULMFiT As expected, intro-\nducing overlap in the uBERT architecture increased computational time overhead. How-\never, across the full dataset and the 10% longest texts, uBERT configurations delivered\nbetter results than the BERT+LSTM baseline with comparable inference times. Notably,\nuBERT 408 achieved a 4x faster inference than ULMFiT on the 10% longest texts. For\nthe 1% longest texts, the increased length required two passes of uBERT 4086, resulting\n\n6With zero overlap, uBERT can process a maximum of 7,650 tokens in a single encoder pass. This limit arises because uBERT\nhandles up to 15 chunks of 510 tokens each (excluding special tokens [CLS] and [SEP]). Therefore, documents longer than 7,650\ntokens require at least two encoder passes.\n\n02000400060008000Text Length (Avg. Tokens/Group)0.30.40.50.60.70.80.9Macro F1-ScoreULMFiT-bwdULMFiT-bidirULMFiT-fwdBERT_LSTMuBERT 0uBERT 150uBERT 205uBERT 300uBERT 408uBERT 51012345678910ubert_150ULMFiT-fwdubert_408ubert_0BERT_LSTMubert_205ubert_510ubert_300ULMFiT-bwdULMFiT-bidir12345678910ubert_150ubert_510ubert_205ubert_0BERT_LSTMubert_300ubert_408ULMFiT-fwdULMFiT-bidirULMFiT-bwd12345678910ubert_150ULMFiT-bwdubert_0ubert_205BERT_LSTMubert_510ubert_300ubert_408ULMFiT-bidirULMFiT-fwd\fin 1.8x slower inference compared to BERT+LSTM, which needed to middle-truncate\nin all cases. Despite this, uBERT 408 slightly outperformed BERT+LSTM, narrowing\nthe performance gap with ULMFiT while maintaining a faster inference, highlighting the\nefficiency and effectiveness of our approach. In summary, in all subsets, uBERT config-\nurations reduced the BERT+LSTM gap being significantly faster than ULMFiT.\n\nULMFiT Outperforms uBERT on Longer Texts As shown in Figure 3, model dif-\nferences become more clear with increasing text length. Big Bird consistently underper-\nforms on longer texts, which is why it was excluded from the comparison charts. While\nsome uBERT configurations outperform BERT+LSTM on longer texts, F1 scores in\nboth models degrade compared to full test dataset performance. In contrast, ULMFiT\nmodels improve on longer texts compared to the full dataset. This suggests that our archi-\ntecture mitigates the degradation for longer text that is inherent to the BERT+LSTM\napproach, but still falls short of ULMFiT models, that handle better longer text but\nat a cost of 4x slower inference time.\n\n6. Conclusion and Future Work\n\nOur experiments demonstrate that the uBERT model improves the handling of legal texts\ncompared to baseline encoder-based models, particularly on longer texts, due to its capa-\nbility to process entire documents using overlapping chunks. Despite the increased com-\nputational overhead, uBERT remains faster than ULMFiT. uBERT slightly outperforms\nBERT+LSTM, but still falls short of ULMFiT. Thus, further refinement is needed to fully\nmatch ULMFiT‘s performance. Notably, even ULMFiT, the top-performing model in\nour experiments, achieves relatively low Macro-F1 scores, suggesting that processing the\nfull text alone is insufficient for high performance on this task. In this direction, future\nresearch should expand the evaluation methodology by analyzing correctly and incor-\nrectly classified cases across all tested models to assess whether specific characteristics\nof judicial decisions make them more prone to misclassification by certain models. Such\nan analysis, however, requires a multidisciplinary approach, including expert input from\nhighly skilled legal practitioners.\n\nFuture research should also explore different chunking strategies to enhance text\nprocessing. Comparing syntactic chunking, which is based on grammatical structure,\nwith semantic chunking, which is based on content meaning, could provide valuable in-\nsights. As this study focuses on a Portuguese-language dataset, evaluating these chunking\napproaches across datasets in multiple languages would help determine if optimal chunk-\ning strategies vary with language, contributing to more robust long-text segmentation and\nmodel performance across diverse linguistic contexts.\n\nAcknowledgements\n\nThis work was supported by CAPES (Finance Code 001), CNPQ (grant 312360/23-1),\nPrograma Unificado de Bolsas de Estudo para Apoio `a Formac¸ ˜ao de Estudantes (PUB-\nUSP), USP-IBM-FAPESP Center for Artificial Intelligence (FAPESP grant 2019/07665-\n4), and Secretaria da Fazenda do Estado do Rio Grande do Sul (SEFAZ-RS), Brazil.\n\n\fReferences\nBeltagy, I., Peters, M. E., and Cohan, A. (2020). Longformer: The long-document trans-\n\nformer.\n\nCNJ (2024). Conselho nacional de justic¸a - cnj. Accessed: 2024-08-05.\n\nCui, J., Shen, X., Nie, F., Wang, Z., Wang, J., and Chen, Y. (2022). A survey on le-\ngal judgment prediction: Datasets, metrics, models and challenges. arXiv preprint\narXiv:2204.04859v1.\n\nDemˇsar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal\n\nof Machine Learning Research, 7:1–30.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Bert: Pre-training\narXiv preprint\n\ntransformers for language understanding.\n\nof deep bidirectional\narXiv:1810.04805v2.\n\nFerraz, T. P., Alcoforado, A., Bustos, E., Oliveira, A. S., Gerber, R., M¨uller, N.,\nd’Almeida, A. C., Veloso, B. M., and Costa, A. H. R. (2021). Debacer: a method for\nslicing moderated debates. In Anais do XVIII Encontro Nacional de Inteligˆencia Arti-\nficial e Computacional, pages 667–678. Sociedade Brasileira de Computac¸ ˜ao-SBC.\n\nGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.-W. (2020). Realm: Retrieval-\n\naugmented language model pre-training.\n\nHasan, M. (2022). Transformers in natural language processing.\n\nHoang, T. D., Bui, C. M., and Bui, N. (2023). Viettel-AI at SemEval-2023 task 6: Legal\ndocument understanding with longformer for court judgment prediction with expla-\nnation. In Ojha, A. K., Do˘gru¨oz, A. S., Da San Martino, G., Tayyar Madabushi, H.,\nKumar, R., and Sartori, E., editors, Proceedings of the 17th International Workshop on\nSemantic Evaluation (SemEval-2023), pages 862–868, Toronto, Canada. Association\nfor Computational Linguistics.\n\nHoward, J. and Ruder, S. (2018). Fine-tuned language models for text classification.\n\nCoRR, abs/1801.06146.\n\nKatz, D., Hartung, D., Gerlach, L., Jana, A., and Bommarito, M. (2023). Natural language\n\nprocessing in the legal domain. arXiv preprint arXiv:2302.12039v1.\n\nLi, Q., Peng, H., Li, J., Xia, C., Yang, R., Sun, L., Yu, P. S., and He, L. (2022). A\nsurvey on text classification: From traditional to deep learning. ACM Trans. Intell.\nSyst. Technol., 13(2).\n\nMahari, R., Stammbach, D., Ash, E., and Pentland, A. (2023). The law and NLP: Bridging\nIn Bouamor, H., Pino, J., and Bali, K., editors, Findings\ndisciplinary disconnects.\nof the Association for Computational Linguistics: EMNLP 2023, pages 3445–3454,\nSingapore. Association for Computational Linguistics.\n\nMalik, V., Sanjay, R., Nigam, S. K., Ghosh, K., Guha, S. K., Bhattacharya, A., and Modi,\nA. (2021). ILDC for CJPE: Indian legal documents corpus for court judgment predic-\ntion and explanation. In Zong, C., Xia, F., Li, W., and Navigli, R., editors, Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pages 4046–4062, Online. Association for Computational Linguistics.\n\n\fMedvedeva, M. and Mcbride, P. (2023). Legal judgment prediction: If you are going to\ndo it, do it right. In Preot,iuc-Pietro, D., Goanta, C., Chalkidis, I., Barrett, L., Spanakis,\nG., and Aletras, N., editors, Proceedings of the Natural Legal Language Processing\nWorkshop 2023, pages 73–84, Singapore. Association for Computational Linguistics.\n\nMenezes-Neto, E. J. d. and Clementino, M. B. M. (2022). Using deep learning to predict\noutcomes of legal appeals better than human experts: A study with data from brazilian\nfederal courts. PLOS ONE, 17(7):1–20.\n\nPappagari, R., ˙Zelasko, P., Villalba, J., Carmiel, Y., and Dehak, N. (2019). Hierarchical\n\ntransformers for long document classification.\n\nTenney, I., Das, D., and Pavlick, E. (2019). Bert rediscovers the classical nlp pipeline.\n\nWan, L., Seddon, M., Papageorgiou, G., and Bernardoni, M. (2019). Long-length legal\n\ndocument classification. arXiv preprint arXiv:1912.06905v1.\n\nZaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Onta˜n´on, S., Pham, P.,\nRavula, A., Wang, Q., Yang, L., and Ahmed, A. (2020). Big bird: Transformers for\nlonger sequences. CoRR, abs/2007.14062.\n\nZhang, L., Wang, W., Yu, K., huang, J., Lyu, Q., Xue, H., and Hetang, C. (2023). Sliding-\nbert: Striding towards conversational machine comprehension in long contex. Adv.\nArtif. Intell. Mach. Learn., 3:1325–1339.\n\nZhu, H., Mak, D., Gioannini, J., and Xia, F. (2020). NLPStatTest: A toolkit for compar-\ning NLP system performance. In Wong, D. and Kiela, D., editors, Proceedings of the\n1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin-\nguistics and the 10th International Joint Conference on Natural Language Processing:\nSystem Demonstrations, pages 40–46, Suzhou, China. Association for Computational\nLinguistics.\n\n\f"
        },
        {
            "titulo": "Sumarização Automática de Artigos de Notícias em Português: Da Extração à Abstração com Abordagens Clássicas e Modelos de Neurais",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31125",
            "idioma": "Português",
            "storage_key": "files/article_31125_30928.pdf",
            "autores": [
                {
                    "nome": "Marcio Alves Sarmento",
                    "afiliacao": "IFES",
                    "orcid": "http://orcid.org/0009-0008-7333-7866"
                },
                {
                    "nome": "Hilário Tomaz Alves de Oliveira",
                    "afiliacao": "IFES",
                    "orcid": "https://orcid.org/0000-0003-0643-7206"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "A sumarização automática de texto tem como objetivo a criação de um resumo com as informações mais relevantes extraídas de um ou mais documentos textuais. Apesar dos avanços obtidos na área, pesquisas envolvendo documentos escritos em português do Brasil ainda são escassas. Este artigo apresenta uma análise envolvendo diferentes abordagens de sumarização, desde baselines clássicas, passando por sistemas extrativos, o ajuste fino de diferentes arquiteturas dos modelos PPT5 e FLAN -T5, até o uso de modelos de linguagem de larga escala para sumarização abstrativa. Experimentos foram realizados considerando três bases de dados de artigos de notícias escritos em português. Os resultados demonstraram que os modelos ajustados para a tarefa de sumarização abstrativa obtiveram resultados competitivos com base nas medidas do ROUGE-L e do BERTScore com modelos maiores, como o GPT-4o.",
            "keywords": [
                "Sumarização Automática de Texto (SAT)",
                "Processamento de Linguagem Natural (PLN)",
                "Redes Neurais",
                "Artigos de Notícias",
                "Português do Brasil",
                "Modelos de Linguagem",
                "Sumarização Extrativa",
                "Sumarização Abstrativa",
                "Large Language Models (LLMs)"
            ],
            "referencias": [
                "Cardoso, P. C., Maziero, E. G., Jorge, M. L. C., Seno, E. M., Di Felippo, A., Rino, L. H. M., Nunes, M. d. G. V., and Pardo, T. A. (2011). Cstnews-a discourse-annotated corpus for single and multi-document summarization of news texts in Brazilian Portuguese. In Proceedings of the 3rd RST Brazilian Meeting, pages 88–105.",
                "Carmo, D., Piau, M., Campiotti, I., Nogueira, R., and Lotufo, R. (2020). Ptt5: Pre-training and validating the T5 model on Brazilian Portuguese data. arXiv preprint arXiv:2008.09144.",
                "Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. (2024). Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1–53.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "Gomes, L. and de Oliveira, H. (2019). A multi-document summarization system for news articles in Portuguese using integer linear programming. In Anais do XVI Encontro Nacional de Inteligência Artificial e Computacional, pages 622–633. SBC.",
                "Levitin, D. J. (2014). Organized Mind: Thinking Straight in the Age of Information Overload (9780698157224). Barnes & Noble.",
                "Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81.",
                "OpenAI (2024). OpenAI models.",
                "Paiola, P. H., Garcia, G. L., Jodas, D. S., Correia, J. V. M., Sugi, L. A., and Papa, J. P. (2024). Recognasumm: A novel Brazilian summarization dataset. In Proceedings of the 16th International Conference on Computational Processing of Portuguese, pages 575–579.",
                "Pardo, T. A. S. and Rino, L. H. M. (2003). Temário: Um corpus para sumarização automática de textos. São Carlos: Universidade de São Carlos, Relatório Técnico.",
                "Sodré, L. and de Oliveira, H. (2019). Avaliando algoritmos de regressão para sumarização automática de textos em português do Brasil. In Anais do XVI Encontro Nacional de Inteligência Artificial e Computacional, pages 634–645. SBC.",
                "Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., et al. (2024). Gemma: Open models based on Gemini research and technology. arXiv preprint arXiv:2403.08295.",
                "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.",
                "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.",
                "Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2019). Bertscore: Evaluating text generation with BERT. arXiv preprint arXiv:1904.09675."
            ],
            "artigo_completo": "Sumarizac¸ ˜ao Autom´atica de Artigos de Not´ıcias em\nPortuguˆes: Da Extrac¸ ˜ao `a Abstrac¸ ˜ao com Abordagens\nCl´assicas e Modelos Neurais\n\nMarcio Alves Sarmento1, Hil´ario Tomaz Alves de Oliveira1\n\n1Programa de P´os-graduac¸ ˜ao em Computac¸ ˜ao Aplicada (PPComp)\nInstituto Federal do Esp´ırito Santo (IFES) – Campus Serra\n\nmalves.sarmento@gmail.com, hilario.oliveira@ifes.edu.br\n\nAbstract. Automatic text summarization aims to generate a summary that cap-\ntures the most relevant information from one or more textual documents.\nAlthough there have been significant advances in this field, research on docu-\nments written in Brazilian Portuguese remains limited. This article provides an\nanalysis of various summarization approaches, ranging from classical baseli-\nnes to extractive methods, including the fine-tuning of different architectures of\nthe P P T 5 and F LAN -T 5 and the use of large language models for abstrac-\ntive summarization. Experiments were conducted using three datasets of news\narticles written in Portuguese. The results showed that the models fine-tuned\nfor the abstractive summarization achieved competitive performance based on\nROUGE-L and BERTScore when compared to larger models like GPT-4o.\n\nResumo. A sumarizac¸ ˜ao autom´atica de texto tem como objetivo a criac¸ ˜ao de\num resumo com as informac¸ ˜oes mais relevantes extra´ıdas de um ou mais do-\ncumentos textuais. Apesar dos avanc¸os obtidos na ´area, pesquisas envolvendo\ndocumentos escritos em portuguˆes do Brasil ainda s˜ao escassas. Este artigo\napresenta uma an´alise envolvendo diferentes abordagens de sumarizac¸ ˜ao, desde\nbaselines cl´assicas, passando por sistemas extrativos, o ajuste fino de diferen-\ntes arquiteturas dos modelos P P T 5 e F LAN -T 5, at´e o uso de modelos de\nlinguagem de larga escala para sumarizac¸ ˜ao abstrativa. Experimentos foram\nrealizados considerando trˆes bases de dados de artigos de not´ıcias escritos em\nportuguˆes. Os resultados demonstraram que os modelos ajustados para a ta-\nrefa de sumarizac¸ ˜ao abstrativa obtiveram resultados competitivos com base nas\nmedidas do ROUGE-L e do BERTScore com modelos maiores, como o GPT-4o.\n\n1. Introduc¸ ˜ao\n\nA crescente demanda por informac¸ ˜oes impulsiona o desenvolvimento de tecnologias ca-\npazes de processar e sintetizar grandes volumes de dados de forma r´apida e eficiente. Em\num cen´ario em que a produc¸ ˜ao de conte´udo digital online ´e cada vez mais abundante,\ntorna-se cada vez mais desafiador para os leitores acompanhar todas as not´ıcias relevantes\n[Levitin 2014, Zhang et al. 2024]. Nesse contexto, sistemas de Sumarizac¸ ˜ao Autom´atica\nde Texto (SAT) podem ser ferramentas ´uteis para auxiliar os usu´arios, oferecendo a ca-\npacidade de gerar resumos concisos que capturam as informac¸ ˜oes mais relevantes de um\ntexto ou de m´ultiplos documentos relacionados, permitindo uma assimilac¸ ˜ao mais r´apida\ndo conte´udo [Lin and Ng 2019, Zhang et al. 2022].\n\n\fA SAT ´e uma ´area de pesquisa em Processamento de Linguagem Natural\n(PLN) que busca gerar resumos de documentos textuais de forma autom´atica. Exis-\ntem duas abordagens principais para a tarefa de SAT: a Extrativa e a Abstrativa\n[Nenkova and McKeown 2012]. A sumarizac¸ ˜ao extrativa seleciona as frases mais re-\nlevantes diretamente do texto original para compor o resumo, enquanto a sumarizac¸ ˜ao\nabstrativa envolve a reescrita do conte´udo de forma mais condensada e frequentemente\nutiliza t´ecnicas de gerac¸ ˜ao de linguagem natural, sendo capaz de criar novas frases que\nn˜ao necessariamente aparecem no texto original [Zhang et al. 2022]. A sumarizac¸ ˜ao pode\nser aplicada tanto a um ´unico documento (monodocumento) quanto a um conjunto de do-\ncumentos (multidocumento).\n\nNos ´ultimos anos, houve uma mudanc¸a no foco das pesquisas na ´area de SAT\ndas abordagens extrativas para as abstrativas [Lin and Ng 2019]. Essa mudanc¸a foi im-\npulsionada pelo desenvolvimento de algoritmos baseados em redes neurais profundas,\nespecialmente na arquitetura Transformer [Vaswani 2017], capazes de gerac¸ ˜ao de lingua-\ngem natural. Assim, diversas abordagens surgiram usando modelos neurais pr´e-treinados\ne, mais recentemente, modelos de linguagem de larga escala, do inglˆes Large Language\nModels (LLMs) [Zhang et al. 2022]. Contudo, apesar dos resultados promissores usando\nessas abordagens neurais, elas imp˜oem diversos desafios, como a necessidade de grandes\nbases de dados para treinamento e demandam muitos recursos computacionais. Apesar\ndos avanc¸os na ´area, a maioria das pesquisas tem como foco a l´ıngua inglesa, e poucos\nestudos tˆem sido dedicados ao portuguˆes, especialmente para a sumarizac¸ ˜ao abstrativa\n[Zhang et al. 2022]. Essa lacuna limita a aplicabilidade de sistemas de sumarizac¸ ˜ao au-\ntom´aticos projetados especificamente para o portuguˆes, que enfrenta a ausˆencia de mode-\nlos e bases de dados para suportar essas pesquisas [Paiola et al. 2024].\n\nEste artigo busca contribuir para o desenvolvimento da tarefa de SAT em por-\ntuguˆes do Brasil por meio de uma investigac¸ ˜ao de algoritmos de sumarizac¸ ˜ao aplicados\na artigos de not´ıcias. O estudo abrange desde o uso de sistemas extrativos, comumente\nusados como baselines de comparac¸ ˜ao, o ajuste fino dos modelos P T T 5 e F LAN -T 5\npara sumarizac¸ ˜ao abstrativa, at´e o uso de LLMs de c´odigo aberto e propriet´arios, como\no GPT4-o1, Llama 3 [Touvron et al. 2023] e o Gemma [Team et al. 2024]. Para isso, fo-\nram realizados experimentos em trˆes bases de dados, o Tem´ario e Recognnasum para a\ntarefa de sumarizac¸ ˜ao monodocumento e o CSTNews para sumarizac¸ ˜ao multidocumento.\nO desempenho dos modelos foi avaliado usando as medidas de avaliac¸ ˜ao autom´aticas do\nROUGE-L e BERTScore, que s˜ao usualmente adotadas na literatura.\n\nOs c´odigos desenvolvidos e os resumos gerados neste trabalho est˜ao p´ublicos em\n\nhttps://github.com/laicsiifes/benchmark_ptbr_summ.\n\n2. Trabalhos Relacionados\n\nA literatura da ´area de SAT ´e vasta e existem diversos surveys que fornecem uma vis˜ao\nampla do desenvolvimento da ´area desde a sua origem [Nenkova and McKeown 2012,\nLin and Ng 2019, Zhang et al. 2022]. Por limitac¸ ˜oes de espac¸o, esta sec¸ ˜ao foca apenas\nem trabalhos que envolveram documentos escritos em portuguˆes ou que usaram t´ecnicas\nadotadas nos experimentos realizados neste estudo.\n\n1https://openai.com/index/hello-gpt-4o/\n\n\fDiversos indicadores de relevˆancia vˆem sendo explorados para a execuc¸ ˜ao da ta-\nrefa de sumarizac¸ ˜ao extrativa [Leite and Rino 2008, Oliveira et al. 2016a]. Em sua maio-\nria, esses indicadores baseiam-se em t´ecnicas estat´ısticas, como frequˆencia e centralidade,\nou em heur´ısticas, como a posic¸ ˜ao das sentenc¸as nos documentos. O estudo conduzido\npor Oliveira et al. [Oliveira et al. 2016a] avaliou diferentes t´ecnicas para mensurar a re-\nlevˆancia de sentenc¸as em tarefas de SAT de artigos jornal´ısticos em inglˆes. Os autores\nanalisaram os m´etodos individualmente e em combinac¸ ˜ao, utilizando-os como atributos\nem algoritmos de classificac¸ ˜ao. Leite e Rino [Leite and Rino 2008] investigaram uma\nabordagem combinando m´ultiplas features e algoritmos de aprendizado de m´aquina para\na sumarizac¸ ˜ao extrativa de documentos em portuguˆes.\n\nNo trabalho de Sodr´e e Oliveira [Sodr´e and de Oliveira 2019], os autores in-\nvestigaram a estrat´egia de combinar alguns dos indicadores analisados por Oliveira et\nal. [Oliveira et al. 2016a] e aplicaram algoritmos de regress˜ao para estimar um escore\nde relevˆancia das sentenc¸as na tarefa de sumarizac¸ ˜ao de artigos jornal´ısticos em por-\ntuguˆes. Gomes e Oliveira [Gomes and de Oliveira 2019] propuseram um sistema usando\nProgramac¸ ˜ao Linear Inteira (PLI) para sumarizac¸ ˜ao extrativa multidocumento. O sistema\ndesenvolvido usa bigramas como conceitos e aplica m´etodos estat´ısticos tradicionais para\nidentificar as informac¸ ˜oes mais relevantes para a construc¸ ˜ao do resumo.\n\nDiferentemente dos trabalhos anteriores, Paiola et al. [Paiola et al. 2022] investi-\ngaram a tarefa de sumarizac¸ ˜ao abstrativa. Os autores usaram diversas bases de dados em\nportuguˆes (TeM´ario, CSTNews, WikiLingua e XL-Sum) e um sistema de traduc¸ ˜ao para\naplicar modelos treinados em inglˆes. Em [Paiola et al. 2024], os autores apresentam a\nbase de dados do RecognaSumm, um conjunto de dados contendo mais de 135 mil arti-\ngos de not´ıcias para a tarefa de SAT. Os autores realizaram diferentes an´alises da base de\ndados proposta e avaliaram o desempenho do modelo base do P T T 5 para estabelecer um\ndesempenho de referˆencia para comparac¸ ˜oes futuras.\n\nEste trabalho busca expandir os anteriores ao realizar uma an´alise mais ampla con-\nsiderando trˆes bases de dados (Tem´ario, RecognaSumm e CSTNews) para sumarizac¸ ˜ao\nmonodocumento e multidocumento, al´em de envolver desde t´ecnicas de sumarizac¸ ˜ao ex-\ntrativas tradicionais de ponderac¸ ˜ao das frases assim como os usados nos trabalhos em\n[Oliveira et al. 2016a, Sodr´e and de Oliveira 2019], adaptac¸ ˜ao do sistema de PLI pro-\nposto em [Gomes and de Oliveira 2019], ajuste fino e avaliac¸ ˜ao de diferentes tamanhos\nde arquiteturas (small, base e large) dos modelos P T T 5 e F LAN -T 5 e o uso de LLMs\nde c´odigo aberto (Llama3 e Gemma2) e propriet´arios (GP T -3.5 e GP T -4o).\n\n3. Materiais e M´etodos\n\n3.1. Bases de Dados\n\nNeste trabalho, foram utilizadas trˆes bases de dados comumente usadas na literatura para\na tarefa de SAT no dom´ınio de artigos de not´ıcias escritas em portuguˆes.\n\nTeM´ario. Esse conjunto de dados ´e formado por 100 textos jornal´ısticos, prove-\nnientes da Folha de S.Paulo e do Jornal do Brasil. Os artigos, que abordam uma variedade\nde temas, foram selecionados por sua linguagem clara e objetiva. Todos os textos pos-\nsuem resumos elaborados por um especialista, o que garante a qualidade dos resumos de\nreferˆencia [Pardo and Rino 2003].\n\n\fCSTNews. Essa base de dados ´e formada por 50 conjuntos de not´ıcias, cada um\ncom aproximadamente quatro artigos sobre o mesmo tema, coletados manualmente em\nsites de not´ıcias como Folha de S˜ao Paulo e Estad˜ao. Essa abordagem permitiu a selec¸ ˜ao\nde not´ıcias com linguagem clara e acess´ıvel, provenientes de diferentes fontes sobre um\nmesmo assunto [Cardoso et al. 2011].\n\nRecognaSumm. Com o objetivo de construir um conjunto de dados robusto para\nestudos de sumarizac¸ ˜ao de textos, Paiola et al. [Paiola et al. 2024] coletaram 135.272 ar-\ntigos de not´ıcias usando sistemas de web crawlers personalizados. A diversidade tem´atica\ndos artigos foi garantida pela coleta de dados em diferentes portais de not´ıcias e catego-\nrias. A base de dados ´e dividida em trˆes subconjuntos: treinamento, validac¸ ˜ao e teste.\nPor conta de limitac¸ ˜oes de hardware, foi feita uma filtragem no conjunto de treinamento,\nsendo removidos os artigos com resumos contendo menos do que 25 palavras.\n\nNa Tabela 1 s˜ao apresentadas algumas estat´ısticas das bases de dados usadas nos\nexperimentos. Para cada base, foi computado o total de documentos ou grupos, m´edia e\nDesvio Padr˜ao (DP) de frases e palavras no texto dos artigos.\n\nTabela 1. Estat´ısticas das bases de dados usadas nos experimentos.\nBase de Dados Conjunto Docs / Grupos M´edia (DP) Frases M´edia (DP) Palavras\n\nTeM´ario\nCSTNews\n\nRecognaSumm\n\n´Unico\n´Unico\nTreino\nValidac¸ ˜ao\nTeste\n\n100\n50\n64.347\n21.538\n21.493\n\n32,4 (10,38)\n47,06 (19,47)\n27,07 (24,82)\n26,73 (24,15)\n27,05 (24,88)\n\n618,67 (163,93)\n939,56 (331,42)\n527,33 (468,38)\n519,91 (458,68)\n526,41 (470,02)\n\n3.2. Modelos de Sumarizac¸ ˜ao\n\nOs seguintes modelos de sumarizac¸ ˜ao foram investigados:\n\nBaselines. Foram utilizadas como baselines oito t´ecnicas de ponderac¸ ˜ao de\nfrases [Sodr´e and de Oliveira 2019]. As t´ecnicas utilizadas foram: Bushy Path, Cen-\ntralidade das Frases, Frequˆencia de Palavras, Frequˆencia de Entidades Nomeadas,\nFrequˆencia do Termo - Frequˆencia Inversa das Sentenc¸as (TF-ISF), Posic¸ ˜ao das Fra-\nses, Similaridade Agregada, TextRank. Essas t´ecnicas foram usadas em conjunto\ncom uma abordagem cl´assica de sumarizac¸ ˜ao extrativa composta por trˆes etapas\n[Nenkova and McKeown 2012]:\n\n• Pr´e-processamento: O documento ou grupo de documentos de entrada ´e pr´e-\nprocessado usando v´arias t´ecnicas tradicionais de PLN, como divis˜ao do texto em\nfrases, palavras, lematizac¸ ˜ao, identificac¸ ˜ao das classes gramaticais e reconheci-\nmento de entidades nomeadas.\n\n• Ponderac¸ ˜ao das frases: Nesta etapa, cada uma das oito t´ecnicas de ponderac¸ ˜ao\nde frases ´e aplicada para analisar cada frase do(s) documento(s) de entrada e gerar\num valor que deve refletir sua relevˆancia para ser inclu´ıdo no resumo. Todos os\nvalores gerados s˜ao normalizados no intervalo de 0 a 1.\n\n• Gerac¸ ˜ao de resumo: As frases com os maiores valores de relevˆancia gera-\ndas na etapa anterior s˜ao inseridas iterativamente no resumo at´e que o tamanho\nm´aximo desejado seja atingido. Uma nova frase ´e inserida no resumo somente\n\n\fse sua similaridade de cosseno com as frases j´a inseridas for menor que 0, 5\n[Nenkova and McKeown 2012].\n\nSistema de PLI. Foi utilizado o sistema de PLI proposto por Gomes e Oliveira\n[Gomes and de Oliveira 2019] para sumarizac¸ ˜ao multidocumento e uma adaptac¸ ˜ao de um\nsistema similar apresentado em [Oliveira et al. 2016b] para a tarefa de sumarizac¸ ˜ao mo-\nnodocumento.\n\nModelos Pr´e-treinados. Foram utilizados os modelos P T T 5 [Carmo et al. 2020]\ne F LAN -T 5 em suas arquiteturas small, base e large, que se diferenciam pelo tama-\nnho da arquitetura. O P T T 5 ´e uma vers˜ao em portuguˆes do modelo de linguagem T 5,\npr´e-treinado no BrWac, um grande corpus de p´aginas da web em portuguˆes brasileiro.\nO F LAN -T 5 ´e um modelo multil´ıngue desenvolvido pela google que foi treinado para\nm´ultiplas tarefas de PLN [Chung et al. 2024].\n\nLLMs. Os recentes avanc¸os no progresso de LLMs tˆem impulsionado o de-\nsenvolvimento de diversas aplicac¸ ˜oes. Neste trabalho, foram utilizados os modelos:\nGemma 2 9B [Team et al. 2024], o Llama 3.1 8B [Touvron et al. 2023] e os modelos\nText-davinci-003, GPT-3.5 Turbo, GPT-4o e GPT-4o mini desenvolvidos pela empresa\nOpenAI [OpenAI 2024].\n\n3.3. Desenho Experimental\n\nA an´alise de desempenho dos modelos de sumarizac¸ ˜ao foi dividida em dois experimentos.\nNo primeiro experimento, foi utilizada somente a base de dados do RecognaSumm, sendo\nconsiderados os sistemas extrativos (baselines e o sistema de PLI) e foram treinados seis\nmodelos de sumarizac¸ ˜ao abstrativos baseados no P T T 5 e F LAN -T 5. O segundo ex-\nperimento foi realizado usando as bases de dados do Tem´ario e CSTNews. Para esse\nexperimento, foram usados os sistemas extrativos (baselines e o sistema de PLI), os mo-\ndelos abstrativos baseados no P T T 5 e F LAN -T 5 treinados no primeiro experimento\ne os modelos de LLMs. Em todas as abordagens avaliadas, foi configurado o tamanho\nm´aximo de resumo para 150 palavras.\n\nPara os m´etodos de baselines e o sistema de PLI foram usadas implementac¸ ˜oes\npr´oprias. Os modelos da OpenAI foram acessados usando a API oficial disponibilizada\npela empresa. A implementac¸ ˜ao dos modelos P T T 5, F LAN -T 5 e dos LLMs do Gemma\n2 9B e Llama 3.1 8B foi baseada na biblioteca Transformers2 e foram usados os modelos\npr´e-treinados disponibilizados publicamente pelos autores e empresas na plataforma do\nHugging Face3. Para o ajuste fino das trˆes arquiteturas dos modelos P T T 5 e F LAN -T 5,\no tamanho m´aximo de entrada foi definido para 512 tokens e o tamanho m´aximo do re-\nsumo a ser gerado foi configurado para 150 tokens. Os modelos foram ajustados por no\nm´aximo 20 ´epocas, sendo utilizada a estrat´egia de parada antecipada com uma paciˆencia\nde 5 ´epocas. Para evitar sobreajuste dos modelos, foi feito um monitoramento do treina-\nmento, no qual, ao final de cada ´epoca, o modelo resultante ´e aplicado no conjunto de\nvalidac¸ ˜ao e ´e computada a medida do ROUGE-L, sendo armazenado somente o modelo\ncom maior valor. Para a gerac¸ ˜ao dos resumos, foi usado o algoritmo de decodificac¸ ˜ao do\nBeam Search com tamanho 5 de largura.\n\n2https://huggingface.co/docs/transformers/index\n3https://huggingface.co/\n\n\fBaseado no trabalho de [Zhang et al. 2024], o seguinte prompt foi usado nos\nLLMs para gerac¸ ˜ao dos resumos: “Escreva um resumo em PORTUGU ˆES DO BRASIL\npara o artigo de not´ıcias a seguir com no M ´AXIMO 150 palavras. ARTIGO: {TEXTO}.”,\nonde {T EXT O} foi substitu´ıdo pelo conte´udo completo do(s) artigo(s) de not´ıcias.\n\nO desempenho dos modelos foi avaliado utilizando as medidas de avaliac¸ ˜ao do\nRecall-Oriented Understudy for Gisting Evaluation Longest Common Subsequence (LCS)\n(ROUGE-L) [Lin 2004] e a do BERTScore [Zhang et al. 2019]. O ROUGE-L computa a\nmaior cadeia em comum entre um resumo candidato e o resumo de referˆencia, enquanto\no BERTScore calcula a similaridade do cosseno entre dois textos usando representac¸ ˜oes\nde embeddings extra´ıdas do modelo Bidirecional Encoder Representations from Trans-\nformers (BERT) [Devlin et al. 2019]. Por quest˜oes de espac¸o, s˜ao reportados somente a\nm´etrica do f1-score, que combina as m´etricas de precis˜ao e revocac¸ ˜ao. Apesar de terem\ndiversas limitac¸ ˜oes, essas medidas s˜ao alternativas v´alidas `a realizac¸ ˜ao de avaliac¸ ˜oes ma-\nnuais e, conforme an´alise feita em Zhang et al. [Zhang et al. 2024], elas apresentaram\ncorrelac¸ ˜ao moderada com avaliac¸ ˜oes humanas na tarefa de sumarizac¸ ˜ao.\n\n4. Resultados\n\n4.1. Experimento na base de dados do RecognaSumm\n\nNa Tabela 2 s˜ao apresentados os resultados dos experimentos na base de dados do Re-\ncognaSumm. Analisando o desempenho dos baselines, pode-se observar que a t´ecnica\nda Posic¸ ˜ao das Frases foi a que obteve os melhores resultados. Essa t´ecnica consiste\nem selecionar as n primeiras frases do documento para compor o resumo at´e que o ta-\nmanho m´aximo do resumo desejado seja alcanc¸ado. Essa t´ecnica tem sido um dos ba-\nselines mais competitivos para sumarizac¸ ˜ao de artigos de not´ıcias [Oliveira et al. 2016a].\nO sistema baseado em PLI obteve melhor desempenho do que quase todos os baselines,\ncom excec¸ ˜ao da Posic¸ ˜ao das Frases. Os modelos P T T 5 e F LAN -T 5 demonstraram\nmelhor desempenho geral do que as demais abordagens analisadas. Em especial, os me-\nlhores desempenhos neste experimento foram obtidos pelos modelos F LAN -T 5Large e\nP T T 5Large em ambas as medidas de avaliac¸ ˜ao. Os resultados obtidos usando a arquite-\ntura base foram muito pr´oximos `as arquiteturas da large, sendo que eles s˜ao menores e\nconsomem menos recursos computacionais.\n\nCom base nos resultados, fica evidente que os modelos ajustados para sumarizac¸ ˜ao\nabstrativos geraram resumos melhores do que as t´ecnicas de baselines e que o sistema ex-\ntrativo de PLI nas medidas do ROUGE-L e BERTScore. Essa superioridade demonstra a\nefic´acia dos modelos P T T 5 e F LAN -T 5 para a tarefa de gerac¸ ˜ao de resumos abstrati-\nvos. Entretanto, ao considerar o uso desses modelos, ´e importante levar em conta o custo\ncomputacional associado a cada um, tanto para o treinamento quanto para a gerac¸ ˜ao dos\nresumos. Portanto, a relac¸ ˜ao custo-benef´ıcio deve ser ponderada na escolha da aborda-\ngem, especialmente em cen´arios com recursos computacionais limitados.\n\n4.2. Experimento nas bases de dados do Tem´ario e CSTNews\n\nA Tabela 3 apresenta os resultados do experimento nas bases de dados do TeM´ario e\nCSTNews. As abordagens avaliadas incluem os m´etodos de baselines, o sistema extra-\ntivo usando PLI, os modelos do P T T 5 e F LAN -T 5 treinados no RecognaSumm e os\nLLMs analisados. Os resultados obtidos neste experimento foram bastante diversificados.\n\n\fTabela 2. Resultados do experimento usando o corpus RecognaSumm.\n\nAbordagem\n\nBushy Path\nCentralidade das Frases\nFrequˆencia de Palavras\nFreq. Entidades Nomeadas\nPosic¸ ˜ao das Frases\nSimilaridade Agregada\nTextRank\nTF-ISF\nSistema PLI\nP T T 5Small\nP T T 5Base\nP T T 5Large\nF LAN -T 5Small\nF LAN -T 5Base\nF LAN -T 5Large\n\nROUGE-L\n0,249 (0,086)\n0,249 (0,087)\n0,240 (0,085)\n0,242 (0,088)\n0,279 (0,099)\n0,249 (0,085)\n0,206 (0,072)\n0,235 (0,084)\n0,270 (0,095)\n0,315 (0,125)\n0,337 (0,132)\n0,346 (0,134)\n0,314 (0,130)\n0,338 (0,140)\n0,349 (0,143)\n\nBERTScore\n0,691 (0,037)\n0,690 (0,038)\n0,686 (0,038)\n0,681 (0,038)\n0,701 (0,040)\n0,689 (0,039)\n0,674 (0,034)\n0,684 (0,037)\n0,694 (0,038)\n0,713 (0,045)\n0,722 (0,045)\n0,726 (0,046)\n0,714 (0,045)\n0,724 (0,048)\n0,729 (0,048)\n\nBaselines\n\nExtrativo\n\nAbstrativos\n\nNa base de dados do Tem´ario, os modelos Text-davinci-003 e GPT-4o obtiveram o me-\nlhor desempenho nas medidas do ROUGE-L e BERTScore, respectivamente. Na base do\nCSTNews, o baseline de Posic¸ ˜ao das Frases apresentou o melhor resultado no ROUGE-L\ne o GPT-3.5 Turbo no BERTScore.\n\nTabela 3. Resultados do experimento usando o Tem ´ario e o CSTNews.\nCSTNews\n\nTem´ario\n\nAbordagens\n\nSistema\n\nBaselines\n\nExtrativo\n\nAbstrativos\n\nLLMs\n\nBushy Path\nCent. das Frases\nFreq. de Palavras\nFreq. Ent. Nom.\nPosic¸ ˜ao das Frases\nSim, Agregada\nTextRank\nTF-ISF\nSistema PLI\nP T T 5Small\nP T T 5Base\nP T T 5Large\nF LAN -T 5Small\nF LAN -T 5Base\nF LAN -T 5Large\nGemma 2 9B\nLlama 3.1 8B\nText-davinci-003\nGPT-3.5 Turbo\nGPT-4o\nGPT-4o Mini\n\nROUGE-L\n0,396 (0,069)\n0,384 (0,063)\n0,375 (0,069)\n0,389 (0,076)\n0,402 (0,070)\n0,390 (0,070)\n0,350 (0,059)\n0,379 (0,072)\n0,396 (0,065)\n0,348 (0,064)\n0,346 (0,062)\n0,339 (0,062)\n0,241 (0,053)\n0,242 (0,048)\n0,225 (0,049)\n0,354 (0,046)\n0,320 (0,037)\n0,424 (0,075)\n0,402 (0,074)\n0,417 (0,062)\n0,402 (0,059)\n\nBERTScore\n0,694 (0,024)\n0,690 (0,025)\n0,686 (0,023)\n0,683 (0,024)\n0,686 (0,022)\n0,696 (0,025)\n0,685 (0,021)\n0,685 (0,024)\n0,687 (0,023)\n0,679 (0,024)\n0,681 (0,023)\n0,678 (0,025)\n0,654 (0,021)\n0,658 (0,022)\n0,654 (0,021)\n0,690 (0,020)\n0,671 (0,019)\n0,705 (0,027)\n0,705 (0,025)\n0,713 (0,025)\n0,705 (0,021)\n\nROUGE-L\n0,447 (0,067)\n0,454 (0,066)\n0,452 (0,063)\n0,434 (0,068)\n0,482 (0,047)\n0,419 (0,050)\n0,415 (0,060)\n0,451 (0,061)\n0,477 (0,049)\n0,393 (0,065)\n0,384 (0,055)\n0,385 (0,055)\n0,304 (0,070)\n0,290 (0,064)\n0,294 (0,070)\n0,383 (0,034)\n0,338 (0,037)\n0,472 (0,048)\n0,455 (0,061)\n0,452 (0,043)\n0,444 (0,039)\n\nBERTScore\n0,720 (0,029)\n0,723 (0,031)\n0,721 (0,029)\n0,705 (0,032)\n0,733 (0,024)\n0,712 (0,024)\n0,709 (0,027)\n0,718 (0,032)\n0,736 (0,033)\n0,713 (0,031)\n0,712 (0,028)\n0,715 (0,026)\n0,700 (0,027)\n0,700 (0,033)\n0,696 (0,033)\n0,717 (0,023)\n0,699 (0,026)\n0,738 (0,029)\n0,740 (0,025)\n0,731 (0,023)\n0,730 (0,023)\n\nOs m´etodos de baselines, o sistema extrativo baseado em PLI e os LLMs apresen-\ntaram resultados pr´oximos com base nas medidas de avaliac¸ ˜ao. Por outro lado, os mo-\ndelos ajustados do P T T 5 e F LAN -T 5 demonstraram desempenho inferior aos demais,\nespecialmente os modelos do F LAN -T 5. Esse baixo desempenho pode ser atribu´ıdo ao\n\n\ffato desses modelos consistentemente gerarem resumos com tamanhos bem inferiores aos\ndemais, mesmo sendo definido um tamanho m´aximo de 150 palavras. Essa caracter´ıstica\naconteceu por conta do treinamento desses modelos no Recognasumm, que possui resu-\nmos de referˆencia bem menores do que os do Tem´ario e do CSTNews.\n\nApesar dos resultados quantitativos serem pr´oximos, ao analisar os resumos gera-\ndos pelas abordagens extrativas e abstrativas, fica evidente que os resumos extrativos, em\ngeral, possuem muitas informac¸ ˜oes contidas nos resumos de referˆencias, mas os resumos\npossuem diversos problemas de coerˆencia e coes˜ao textual. Por outro lado, os resumos\nabstrativos s˜ao mais sucintos e, em sua maioria, apresentam uma boa qualidade textual\nem termos de coerˆencia, coes˜ao e estrutura ortogr´afica e gramatical. Os LLMs do Gemma\n2 9B e do Llama 3.1 8B apresentaram uma tendˆencia de terminar de forma brusca os re-\nsumos, por exemplo, no meio de uma frase. Cabe ressaltar que nenhum LLM foi ajustado\npara a tarefa de sumarizac¸ ˜ao.\n\nPor fim, ´e importante enfatizar que os LLMs, como Gemma, Llama e especial-\nmente os modelos da OpenAI, possuem um custo consideravelmente maior do que os\ndemais modelos avaliados neste trabalho. Essa caracter´ıstica deve ser considerada em\naplicac¸ ˜oes pr´aticas, na qual a relac¸ ˜ao custo-benef´ıcio ´e determinante. Nesse contexto,\nabordagens extrativas, como o sistema baseado PLI ou mesmo os baselines, podem ofe-\nrecer uma alternativa que equilibra desempenho com menor custo computacional. Em\ncen´arios com recursos computacionais moderados, os modelos ajustados do P T T 5 e\nF LAN -T 5 podem ser as melhores opc¸ ˜oes.\n\n5. Conclus˜oes\n\nEste trabalho apresentou uma an´alise comparativa de v´arias abordagens para sumarizac¸ ˜ao\nautom´atica de texto, considerando desde tradicionais m´etodos de ponderac¸ ˜ao de frases\nat´e modelos de linguagem de grande escala, para sumarizac¸ ˜ao abstrativa e extrativa de\nartigos de not´ıcias escritas em portuguˆes do Brasil. Essa avaliac¸ ˜ao fez uso de trˆes bases\nde dados e de duas medidas de avaliac¸ ˜ao autom´atica comumente usadas na literatura. Os\nresultados obtidos demonstram que os modelos de LLMs s˜ao promissores para a tarefa\nde criac¸ ˜ao autom´atica de resumos, mas s˜ao sistemas com uma alta complexidade que\nrequerem muitos recursos computacionais. Portanto, modelos especializados para a tarefa\nde sumarizac¸ ˜ao ou sistemas extrativos ainda podem ser opc¸ ˜oes vi´aveis, especialmente em\ncen´arios de poucos recursos.\n\nEm trabalhos futuros, pretendemos expandir este trabalho visando: (i) analisar\no desempenho de modelos de LLM de c´odigo aberto, considerando diferentes cen´arios\nde utilizac¸ ˜ao, como zero shot-learning, few-shot learning e fazendo o ajuste fino desses\nmodelos para a tarefa de sumarizac¸ ˜ao; e (ii) realizar uma avaliac¸ ˜ao manual de um sub-\nconjunto dos resumos gerados para complementar as an´alises autom´aticas.\n\nAgradecimentos\n\nOs autores agradecem ao Ifes, apoio da FAPES e CAPES (processo 2021-2S6CD, nº\nFAPES 132/2021) por meio do PDPG (Programa de Desenvolvimento da P´os-Graduac¸ ˜ao,\nParcerias Estrat´egicas nos Estados).\n\n\fReferˆencias\nCardoso, P. C., Maziero, E. G., Jorge, M. L. C., Seno, E. M., Di Felippo, A., Rino, L.\nH. M., Nunes, M. d. G. V., and Pardo, T. A. (2011). Cstnews-a discourse-annotated\ncorpus for single and multi-document summarization of news texts in brazilian portu-\nguese. In Proceedings of the 3rd RST Brazilian Meeting, pages 88–105.\n\nCarmo, D., Piau, M., Campiotti, I., Nogueira, R., and Lotufo, R. (2020). Ptt5: Pre-\ntraining and validating the t5 model on brazilian portuguese data. arXiv preprint ar-\nXiv:2008.09144.\n\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X.,\nDehghani, M., Brahma, S., et al. (2024). Scaling instruction-finetuned language mo-\ndels. Journal of Machine Learning Research, 25(70):1–53.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of\nIn Proceedings of the\ndeep bidirectional transformers for language understanding.\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nGomes, L. and de Oliveira, H. (2019). A multi-document summarization system for news\narticles in portuguese using integer linear programming. In Anais do XVI Encontro\nNacional de Inteligˆencia Artificial e Computacional, pages 622–633. SBC.\n\nLeite, D. S. and Rino, L. H. M. (2008). Combining multiple features for automatic text\nsummarization through machine learning. In International Conference on Computati-\nonal Processing of the Portuguese Language, pages 122–132. Springer.\n\nLevitin, D. J. (2014). Organized Mind: Thinking Straight in the Age of Information\n\nOverload (9780698157224). Barnes & Noble.\n\nLin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In Text\n\nsummarization branches out, pages 74–81.\n\nLin, H. and Ng, V. (2019). Abstractive summarization: A survey of the state of the art.\nIn Proceedings of the AAAI conference on artificial intelligence, volume 33, pages\n9815–9822.\n\nNenkova, A. and McKeown, K. (2012). A survey of text summarization techniques. In\n\nMining text data, pages 43–76. Springer.\n\nOliveira, H., Ferreira, R., Lima, R., Lins, R. D., Freitas, F., Riss, M., and Simske, S. J.\n(2016a). Assessing shallow sentence scoring techniques and combinations for single\nand multi-document summarization. Expert Systems with Applications, 65:68–86.\n\nOliveira, H., Lima, R., Lins, R. D., Freitas, F., Riss, M., and Simske, S. J. (2016b).\nA concept-based integer linear programming approach for single-document summa-\nIn 2016 5th Brazilian Conference on Intelligent Systems (BRACIS), pages\nrization.\n403–408. IEEE.\n\nOpenAI (2024). Openai models. https://openai.com/api/.\n\nPaiola, P. H., de Rosa, G. H., and Papa, J. P. (2022). Deep learning-based abstractive\nsummarization for brazilian portuguese texts. In Xavier-Junior, J. C. and Rios, R. A.,\neditors, Intelligent Systems, pages 479–493, Cham. Springer International Publishing.\n\n\fPaiola, P. H., Garcia, G. L., Jodas, D. S., Correia, J. V. M., Sugi, L. A., and Papa, J. P.\n(2024). Recognasumm: A novel brazilian summarization dataset. In Proceedings of\nthe 16th International Conference on Computational Processing of Portuguese, pages\n575–579.\n\nPardo, T. A. S. and Rino, L. H. M. (2003). Tem´ario: Um corpus para sumarizac¸ ˜ao au-\n\ntom´atica de textos. S˜ao Carlos: Universidade de S˜ao Carlos, Relat´orio T´ecnico.\n\nSodr´e, L. and de Oliveira, H. (2019). Avaliando algoritmos de regress˜ao para sumarizac¸ ˜ao\nautom´atica de textos em portuguˆes do brasil. In Anais do XVI Encontro Nacional de\nInteligˆencia Artificial e Computacional, pages 634–645. SBC.\n\nTeam, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L.,\nRivi`ere, M., Kale, M. S., Love, J., et al. (2024). Gemma: Open models based on\ngemini research and technology. arXiv preprint arXiv:2403.08295.\n\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere,\nB., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\n\nVaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing\n\nSystems.\n\nZhang, M., Zhou, G., Yu, W., Huang, N., and Liu, W. (2022). A comprehensive survey\nof abstractive text summarization based on deep learning. Computational intelligence\nand neuroscience, 2022(1):7132226.\n\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2019). Bertscore:\n\nEvaluating text generation with bert. arXiv preprint arXiv:1904.09675.\n\nZhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown, K., and Hashimoto, T. B. (2024).\nBenchmarking large language models for news summarization. Transactions of the\nAssociation for Computational Linguistics, 12:39–57.\n\n\f"
        },
        {
            "titulo": "Geração Automática de Perguntas em Português do Brasil Usando os Modelos PTT5 e FLAN-T5",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31126",
            "idioma": "Português",
            "storage_key": "files/article_31126_30929.pdf",
            "autores": [
                {
                    "nome": "Tiago Felipe V. braga",
                    "afiliacao": "IFES",
                    "orcid": "http://orcid.org/0009-0003-8351-513X"
                },
                {
                    "nome": "Bruno Cardoso Coutinho",
                    "afiliacao": "IFES",
                    "orcid": "https://orcid.org/0000-0002-4183-7865"
                },
                {
                    "nome": "Hilário Tomaz Alves de Oliveira",
                    "afiliacao": "IFES",
                    "orcid": "https://orcid.org/0000-0003-0643-7206"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Este artigo apresenta uma análise comparativa dos modelos neurais pré-treinados do PTT5 e FLAN-T5 para a geração automática de perguntas em português do Brasil. Para isso, foram utilizados dois conjuntos de dados, PIRA e FairyTaleQA, para avaliar a capacidade desses modelos de gerar perguntas a partir de dois cenários: (i) considerando apenas o contexto e (ii) usando o contexto e a resposta esperada. As medidas do ROUGE-L e do BERTScore foram usadas para avaliar as perguntas geradas, além de uma análise baseada no GPT-4o. Os resultados demonstram que o modelo PTT5",
            "keywords": [
                "Geração Automática de Perguntas",
                "Processamento de Linguagem Natural",
                "PTT5",
                "FLAN-T5",
                "Modelos de Linguagem Pré-treinados",
                "PIRÁ",
                "FairyTaleQA",
                "ROUGE-L",
                "BERTScore",
                "GPT-4o",
                "Avaliação de Modelos de Linguagem",
                "Transformers",
                "Aprendizado de Máquina",
                "Inteligência Artificial"
            ],
            "referencias": [
                "Lin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
                "Wagner Filho, J. A., Wilkens, R., Idiart, M., and Villavicencio, A. (2018). The brwac corpus: A new open resource for brazilian portuguese. In Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018)."
            ],
            "artigo_completo": "Gerac¸ ˜ao Autom´atica de Perguntas em Portuguˆes do Brasil\nUsando os Modelos PTT5 e FLAN-T5\n\nTiago Felipe V. Braga1, Bruno Cardoso Coutinho2, Hil´ario Tomaz Alves de Oliveira1\n\n1Programa de P´os-graduac¸ ˜ao em Computac¸ ˜ao Aplicada (PPComp)\nInstituto Federal do Esp´ırito Santo (IFES) – Campus Serra\n\n2Coordenadoria do Curso T´ecnico em Inform´atica\nInstituto Federal do Esp´ırito Santo (IFES) – Campus Serra\n\ntiagofvx@gmail.com, {bccout,hilario.oliveira}@ifes.edu.br\n\nAbstract. This paper performs a comparative analysis of the pre-trained neural\nmodels of P T T 5 and F LAN -T 5 for Brazilian Portuguese automatic question\ngeneration. To this end, two datasets, PIR ´A and FairyTaleQA, were used to eva-\nluate the ability of these models to generate questions from two scenarios: (i)\nconsidering only the context and (ii) using the context and the expected answer.\nThe ROUGE-L and BERTScore measures were used to assess the generated\nquestions, in addition to an analysis based on GP T -4o. The results demons-\ntrated that the P T T 5Large model consistently outperformed the other models,\ngenerating 93.06% of valid questions in PIR ´A and 82.32% in FairyTaleQA ba-\nsed on the GP T -4o evaluation.\n\nResumo. Este artigo apresenta uma an´alise comparativa dos modelos neurais\npr´e-treinados do P T T 5 e F LAN -T 5 para a gerac¸ ˜ao autom´atica de pergun-\ntas em portuguˆes do Brasil. Para isso, foram utilizados dois conjuntos de da-\ndos, PIR ´A e FairyTaleQA, para avaliar a capacidade desses modelos de gerar\nperguntas a partir de dois cen´arios: (i) considerando apenas o contexto e (ii)\nusando o contexto e a resposta esperada. As medidas do ROUGE-L e do BERTS-\ncore foram usadas para avaliar as perguntas geradas, al´em de uma an´alise ba-\nseada no GP T -4o. Os resultados demonstram que o modelo P T T 5Large apre-\nsentou consistentemente desempenho superior aos demais modelos, gerando\n93,06% de perguntas v´alidas no PIR ´A e 82,32% no FairyTaleQA na avaliac¸ ˜ao\nbaseada no GP T -4o.\n\n1. Introduc¸ ˜ao\n\nA Gerac¸ ˜ao de Perguntas (QG, do inglˆes Question Generation)\n´e uma tarefa da\n´area de Processamento de Linguagem Natural (PLN), que envolve a criac¸ ˜ao au-\ntom´atica de perguntas a partir de um dado texto ou conjuntos de dados textuais\n[Zhang et al. 2021]. Usando t´ecnicas de PLN e algoritmos de Aprendizado de M´aquina\n(AM), os sistemas de QG visam gerar perguntas gramaticalmente corretas e contextu-\nalmente relevantes [da Rocha Junqueira et al. 2024]. Diante da grande abundˆancia de\ninformac¸ ˜oes digitais, sistemas de QG possuem diversas potenciais ´areas de aplicac¸ ˜ao\n[Mulla and Gharpure 2023]. Na ´area da educac¸ ˜ao, a aplicac¸ ˜ao de abordagens de QG\npode contribuir para o desenvolvimento de materiais de avaliac¸ ˜ao, question´arios pr´aticos,\nno desenvolvimento de sistemas de tutoria, aprimorando processos de aprendizagem e\n\n\favaliac¸ ˜ao [Kurdi et al. 2020]. No ˆambito dos sistemas de perguntas e respostas (QA, do\ninglˆes Question Answering), abordagens de QG tˆem sido usadas para o treinamento de\nmodelos com pouca supervis˜ao ou para fins de aumento de dados [Puri et al. 2020].\n\nApesar do crescente interesse em pesquisas envolvendo a tarefa de QG, a mai-\noria desses estudos concentra-se predominantemente na l´ıngua inglesa, onde h´a di-\nversos recursos e bases de dados dispon´ıveis para experimentac¸ ˜ao e desenvolvimento\n[Zhang et al. 2021, Mulla and Gharpure 2023]. Em contrapartida, as pesquisas foca-\ndas na l´ıngua portuguesa, especialmente para o portuguˆes do Brasil, ainda s˜ao li-\nmitadas, resultando em uma escassez tanto de estudos quanto de bases de dados\n[da Rocha Junqueira et al. 2024, Leite et al. 2024]. Essa lacuna imp˜oe desafios adicio-\nnais para o avanc¸o no desenvolvimento e na aplicac¸ ˜ao pr´atica de sistemas de QG, uma\nvez que a adaptac¸ ˜ao de modelos e t´ecnicas desenvolvidas para o inglˆes nem sempre se\ntraduzem diretamente em resultados eficazes em outros idiomas, dada a complexidade e\nas particularidades lingu´ısticas inerentes de linguagem natural.\n\nEste artigo tem como objetivo investigar a aplicac¸ ˜ao de modelos neurais de lingua-\ngem pr´e-treinados baseados na arquitetura Transformers [Vaswani 2017], mais especifi-\ncamente os modelos P T T 5 [Carmo et al. 2020] e F LAN -T 5 [Chung et al. 2024] para a\ntarefa de QG em portuguˆes do Brasil. Para isso, foi realizado o ajuste fino desses mo-\ndelos em suas arquiteturas small, base e large, utilizando as bases de dados do PIR ´A\n[Paschoal et al. 2021], nativa em portuguˆes, e uma vers˜ao traduzida da base de dados\nFairytaleQA [Leite et al. 2024] para o portuguˆes do Brasil. Os experimentos foram rea-\nlizados em dois cen´arios: no primeiro, as perguntas foram geradas a partir somente de\num dado contexto; no segundo, as perguntas foram geradas considerando tanto o con-\ntexto quanto uma resposta pr´evia. A avaliac¸ ˜ao dos resultados foi realizada por meio das\nmedidas autom´aticas do ROUGE-L e BERTScore, que s˜ao comumente utilizadas para\navaliar abordagens de QG em termos de similaridade l´exica e semˆantica das perguntas\ngeradas com as perguntas de referˆencia. Al´em disso, foi realizado um experimento adi-\ncional utilizando o modelo GP T -4o para avaliar as perguntas geradas. Esse experimento\nteve como objetivo complementar as avaliac¸ ˜oes quantitativas anteriores, proporcionando\numa an´alise adicional da qualidade das perguntas geradas.\n\nAs principais contribuic¸ ˜oes deste artigo incluem: (i) o ajuste fino e a avaliac¸ ˜ao\nde diferentes arquiteturas dos modelos P T T 5 e F LAN -T 5 para a tarefa de QG em por-\ntuguˆes do Brasil; e (ii) uma extensa investigac¸ ˜ao considerando duas bases de dados, PIRA\ne FairytaleQA, e duas variac¸ ˜oes da tarefa. O c´odigo-fonte desenvolvido neste trabalho est´a\np´ublico em um reposit´orio do GitHub1.\n\n2. Trabalhos Relacionados\n\nAs abordagens de gerac¸ ˜ao de perguntas podem ser classificadas em m´etodos convencio-\nnais e baseados em modelos neurais [Zhang et al. 2021]. Os m´etodos convencionais de\nQG baseiam-se principalmente na aplicac¸ ˜ao de regras heur´ısticas para transformar os tex-\ntos em perguntas relacionadas. Recentemente, com a evoluc¸ ˜ao das arquiteturas de redes\nneurais profundas, houve uma mudanc¸a de paradigma na tarefa para a adoc¸ ˜ao de modelos\nneurais, permitindo assim, o desenvolvimento de abordagens orientadas a dados e com-\npletamente trein´aveis, na qual a selec¸ ˜ao de conte´udo e a construc¸ ˜ao de perguntas podem\n\n1https://github.com/laicsiifes/question_generation_ptbr\n\n\fser otimizadas de forma combinada. Embora exista uma vasta literatura sobre QG em\ndiversos idiomas [Kurdi et al. 2020, Zhang et al. 2021, Mulla and Gharpure 2023], por\nlimitac¸ ˜ao de espac¸o, esta sec¸ ˜ao foca em trabalhos envolvendo o portuguˆes do Brasil.\n\nEm [Leite and Lopes Cardoso 2022], os autores apresentam um estudo que envol-\nveu o treinamento do modelo P T T 5 para a gerac¸ ˜ao de perguntas utilizando uma vers˜ao\nem portuguˆes do conjunto de dados SQuAD 1.1. Os resultados obtidos foram encora-\njadores, com desempenho equipar´avel com a implementac¸ ˜ao em inglˆes do modelo T 5,\nevidenciando a efic´acia dos modelos baseados na arquitetura Transformers e estabele-\ncendo baselines para futuras comparac¸ ˜oes para a tarefa de QG em portuguˆes. Oliveira\net al. [Oliveira et al. 2023] abordam o desafio de gerar e classificar distratores (opc¸ ˜oes\nincorretas) para quest˜oes de m´ultipla escolha em portuguˆes. Os autores desenvolveram e\ncombinam v´arios m´etodos de gerac¸ ˜ao de distratores, incluindo extrac¸ ˜ao baseada em con-\ntexto, manipulac¸ ˜ao num´erica e similaridade semˆantica a partir de recursos como WordNet.\n\nJunqueira et al. [da Rocha Junqueira et al. 2024] apresentaram uma investigac¸ ˜ao\ndo desempenho dos modelos T 5, F LAN -T 5 e BART -P T para a gerac¸ ˜ao de perguntas\nfactuais em portuguˆes do Brasil. Para mitigar o problema da escassez de dados, foi uti-\nlizada uma vers˜ao em portuguˆes brasileiro do SQuAD v1.1, obtida por meio de traduc¸ ˜ao\nautom´atica. Leite et al. [Leite et al. 2024] realizaram a construc¸ ˜ao de vers˜oes traduzidas\nautomaticamente da base de dados FairytaleQA, que ´e um conjunto de dados comumente\nusado para o desenvolvimento de sistemas de perguntas e respostas em inglˆes. Foram\ndesenvolvidas vers˜oes do FairytaleQA para o portuguˆes de Portugal, portuguˆes do Brasil,\nespanhol e francˆes, que podem ser usadas em pesquisas da ´area de QG e QA. Al´em disso,\nforam realizados experimentos usando modelos neurais baseados na arquitetura T 5.\n\nEste trabalho difere dos anteriores ao: (i) treinar e avaliar diferentes tamanhos\nde arquitetura dos modelos P T T 5 e F LAN -T 5, (ii) considerar dois cen´arios da tarefa\nde QG, (iii) adotar uma base de dados escrita nativamente em portuguˆes (PIR ´A) e outra\nobtida por meio de traduc¸ ˜ao autom´atica (FairytaleQA), e (iv) analisar o desempenho dos\nmodelos usando uma abordagem com o modelo GP T -4o, al´em de tradicionais medidas\nde avaliac¸ ˜ao consideradas em trabalhos anteriores.\n\n3. Materiais e M´etodos\n\n3.1. Bases de Dados\n\nNeste trabalho foram utilizados dois conjuntos de dados, o PIR ´A [Paschoal et al. 2021]\ne o FairyTaleQA [Xu et al. 2022]. Essas bases de dados foram selecionadas por serem\nusadas em trabalhos da literatura na tarefa de gerac¸ ˜ao de perguntas ou de sistemas de\nperguntas e respostas em portuguˆes do Brasil. Al´em disso, elas possuem trˆes componentes\nessenciais para a tarefa de QG: (i) contexto textual, (ii) pergunta associada e (iii) resposta\ncorrespondente.\n\nO PIR ´A ´e uma base de dados bil´ıngue (portuguˆes-inglˆes) focada em quest˜oes\noceˆanicas e da costa brasileira. A base cont´em 2.261 textos extra´ıdos de trechos de re-\nlat´orios das Nac¸ ˜oes Unidas sobre o oceano e de resumos relacionados ao litoral brasileiro\n[Paschoal et al. 2021]. As perguntas e respostas foram criadas manualmente em um pro-\ncesso de revis˜ao em pares por avaliadores humanos. Ap´os uma an´alise da base de dados,\nfoi observado que alguns exemplos n˜ao apresentam as respostas para as perguntas. Por\n\n\fisso, esses exemplos foram removidos, j´a que a resposta ´e um elemento importante para\nos experimentos realizados neste trabalho.\n\nO FairyTaleQA ´e uma base de dados comumente usada para avaliar sistemas de\nperguntas e respostas em inglˆes. Essa base foi criada por especialistas em educac¸ ˜ao e\n´e composta por textos narrativos infantis. Leite et al. [Leite et al. 2024] realizaram um\nprocesso de traduc¸ ˜ao do FairyTaleQA para diversos idiomas, incluindo o portuguˆes de\nPortugal e do Brasil. Neste trabalho, foi utilizada a vers˜ao traduzida para o portuguˆes do\nBrasil, que compreende 10.580 perguntas e respostas derivadas de 278 hist´orias infantis.\n\nNa Tabela 1 s˜ao apresentadas para cada base de dados as estat´ısticas do total de\nexemplos em cada conjunto (treinamento, validac¸ ˜ao e teste) e o tamanho m´edio e desvio\npadr˜ao do total de palavras para cada componente (contexto, pergunta e resposta). Para\ngerar essas estat´ısticas, foi utilizada a ferramenta spaCy2 para o processamento dos textos.\n\nTabela 1. Estat´ıstica das bases de dados do PIR ´A e FairyTaleQA\n\nBase de Dados\n\nConjunto\n\nExemplos\n\nComponente M´edia de Palavras (Desvio Padr˜ao)\n\nTreino\n\n1.756\n\nPIR ´A\n\nValidac¸ ˜ao\n\n215\n\nTeste\n\n216\n\nTreino\n\n8.548\n\nFairyTaleQA\n\nValidac¸ ˜ao\n\n1.025\n\nTeste\n\n1.007\n\nContexto\nPergunta\nResposta\nContexto\nPergunta\nResposta\nContexto\nPergunta\nResposta\nContexto\nPergunta\nResposta\nContexto\nPergunta\nResposta\nContexto\nPergunta\nResposta\n\n274,73 (141,41)\n13,83 (5,62)\n14,32 (11,76)\n273,98 (157,08)\n13,65 (5,38)\n15,04 (12,06)\n250,58 (128,98)\n13,36 (5,68)\n14,92 (14,50)\n182,51 (94,53)\n10,23 (3,38)\n6,98 (5,73)\n170,08 (74,18)\n10,93 (3,40)\n7,52 (5,96)\n168,92 (73,77)\n10,48 (3,30)\n6,80 (5,31)\n\n3.2. Modelos Avaliados\n\nNeste trabalho, foram avaliados os modelos P T T 5 e o F LAN -T 5, baseados na arquite-\ntura Text-to-Text Transfer Transformer (T5) [Raffel et al. 2020]. Apesar de existirem dife-\nrentes tamanhos de arquitetura, as trˆes comumente usadas s˜ao “small”, “base” e “large”.\nElas possuem um n´umero crescente de parˆametros, o que geralmente resulta em maior\ncapacidade de aprendizado, mas tamb´em em um maior custo computacional. Esses mo-\ndelos foram escolhidos devido ao seu desempenho promissor em tarefas de PLN e por\nterem sido explorados em trabalhos anteriores.\n\nO P T T 5 ´e uma adaptac¸ ˜ao do modelo T 5, especificamente pr´e-treinada para o\nportuguˆes do Brasil [Carmo et al. 2020]. O modelo foi pr´e-treinado no corpus BrWac\n[Wagner Filho et al. 2018], uma extensa colec¸ ˜ao de p´aginas web em portuguˆes, contendo\naproximadamente 2,7 bilh˜oes de tokens. Foram utilizados os modelos P T T 5Small, que\npossui aproximadamente 60 milh˜oes de parˆametros, o P T T 5Base, com cerca de 220\nmilh˜oes de parˆametros, e o P T T 5Large, que apresenta aproximadamente 740 milh˜oes\nde parˆametros.\n\n2https://spacy.io/\n\n\fO F LAN -T 5 [Chung et al. 2024] ´e uma vers˜ao aprimorada do T 5 pr´e-treinado\nem m´ultiplas tarefas de PLN. Esse modelo foi pr´e-treinado majoritariamente em do-\ncumentos em inglˆes, mas possui suporte a outros idiomas, como o portuguˆes. Foram\navaliadas trˆes variantes deste modelo: o F LAN -T 5Small com cerca de 80 milh˜oes de\nparˆametros, o F LAN -T 5Base contendo aproximadamente 250 milh˜oes de parˆametros, e\no F LAN -T 5Large apresentando cerca de 780 milh˜oes de parˆametros. Sua inclus˜ao tem\no objetivo de avaliar como um modelo com treinamento diversificado se comporta em\ncomparac¸ ˜ao a modelos especializados em um ´unico idioma e tarefa.\n\n3.3. Metodologia Experimental\n\nA metodologia experimental utilizada neste trabalho envolveu o desenvolvimento, ajuste\nfino e a avaliac¸ ˜ao dos modelos investigados, especificamente ajustados para dois cen´arios\nda tarefa de gerac¸ ˜ao de perguntas, conforme ilustrado na Figura 1. Para cada cen´ario, seis\nmodelos foram treinados, considerando os trˆes tamanhos de arquitetura e os dois modelos\nP T T 5 e F LAN -T 5. No primeiro cen´ario, foi analisada a variac¸ ˜ao da tarefa que gera\nperguntas a partir somente do contexto, como entrada. J´a no segundo cen´ario, os modelos\nrecebem tanto o contexto quanto uma resposta como entrada e devem gerar uma pergunta\ncomo sa´ıda.\n\nFigura 1. Cen ´arios da tarefa de QG analisados.\n\nOs modelos P T T 5 e F LAN -T 5 foram implementados usando a biblioteca Trans-\nformers3. O tamanho de entrada m´aximo foi definido para 512 tokens, enquanto a sa´ıda\nfoi configurada para no m´aximo 40 tokens. Durante o treinamento, os modelos foram\najustados por no m´aximo 20 ´epocas, sendo utilizada a estrat´egia de parada antecipada\ncom uma paciˆencia de 5 ´epocas. Para mitigar o sobreajuste dos modelos, ao final de cada\n´epoca, o modelo treinado ´e aplicado no conjunto de validac¸ ˜ao e ´e computada a medida do\nROUGE-L, sendo salvo somente o modelo com maior valor. Durante a gerac¸ ˜ao das per-\nguntas, foi utilizado o algoritmo de Beam Search com uma largura de tamanho 5. Esses\nvalores foram definidos a partir da an´alise de trabalhos anteriores.\n\nA avaliac¸ ˜ao do desempenho dos modelos foi realizada por meio de duas abor-\ndagens: a aplicac¸ ˜ao de m´etricas autom´aticas de similaridade e uma avaliac¸ ˜ao com\nbase no modelo de linguagem GP T -4o. Para a avaliac¸ ˜ao autom´atica, foi utilizada a\nm´etrica Recall-Oriented Understudy for Gisting Evaluation Longest Common Subse-\nquence (ROUGE-L) [Lin 2004], que mensura a similaridade com base na maior sequˆencia\nde palavras em comum entre as perguntas geradas e as perguntas de referˆencia. Adicional-\nmente, foi utilizada a m´etrica BERTScore [Zhang et al. 2019], que calcula a similaridade\nde cosseno a partir das representac¸ ˜oes em embeddings extra´ıdas do modelo Bidirectional\nEncoder Representations for Transformers (BERT).\n\n3https://huggingface.co/docs/transformers/index\n\n\fPara uma avaliac¸ ˜ao mais hol´ıstica e contextualmente relevante, foi realizada uma\nan´alise usando o modelo GP T -4o. Essa avaliac¸ ˜ao foi pensada porque, dado um con-\ntexto espec´ıfico, ´e poss´ıvel gerar m´ultiplas perguntas v´alidas que n˜ao necessariamente\nprecisam ser idˆenticas `a pergunta de referˆencia presente nas bases de dados usadas nos\nexperimentos. Esta situac¸ ˜ao ´e particularmente relevante no Cen´ario 1, onde apenas o con-\ntexto ´e fornecido como entrada para o modelo. Neste caso, diversas perguntas podem\nser consideradas v´alidas, desde que sejam respond´ıveis com base no contexto fornecido.\nEm contraste, no Cen´ario 2, onde o contexto e a resposta esperada s˜ao fornecidos como\nentrada, a pergunta gerada deve ser semanticamente equivalente `a pergunta de referˆencia.\n\nO processo de avaliac¸ ˜ao utilizando o GP T -4o foi inspirado na t´ecnica de Retrieval\nAugmented Generation (RAG). Esta t´ecnica consiste em fornecer um contexto e uma per-\ngunta para um modelo de linguagem de grande escala (LLM, do inglˆes Large Language\nModel), solicitando que ele responda `a pergunta usando apenas o contexto fornecido ou\nsinalize caso n˜ao seja poss´ıvel [Chen et al. 2024]. Seguindo esta abordagem, foi criado\num prompt4 contendo o contexto original e a pergunta gerada pelos modelos avaliados.\nEste prompt foi ent˜ao submetido ao GP T -4o, com a instruc¸ ˜ao de responder `a pergunta\nutilizando somente o contexto fornecido ou indicar a impossibilidade de resposta. Com\nbase nas respostas do GP T -4o, foi calculado o percentual de perguntas v´alidas (aquelas\nque o LLM conseguiu responder com base no contexto) e inv´alidas (as que n˜ao pude-\nram ser respondidas) para cada modelo avaliado. Deste modo, foi poss´ıvel analisar se\nas perguntas geradas foram relevantes ao contexto, ainda que diferentes da pergunta de\nreferˆencia. Embora esta an´alise seja automatizada, foi realizada uma inspec¸ ˜ao manual em\namostras das sa´ıdas do GP T -4o para verificar sua confiabilidade. Foi observado que, em\ngeral, o LLM identificava corretamente as perguntas v´alidas e inv´alidas.\n\n4. Resultados\n\nNa Tabela 2 s˜ao apresentados os resultados dos experimentos, considerando os cen´arios\n1 e 2, com base nas medidas de avaliac¸ ˜ao do ROUGE-L e BERTScore. No Cen´ario 1\n(apenas contexto), o P T T 5Base e o P T T 5Large apresentaram os melhores desempenhos\npara as bases de dados do FairyTaleQA e PIR ´A, respectivamente. No Cen´ario 2 (contexto\ne resposta esperada), o P T T 5Large superou os demais modelos em ambas as bases. Fica\nevidente que os modelos P T T 5 consistentemente obtiveram melhores resultados do que\nos modelos F LAN -T 5 em ambos os conjuntos de dados e cen´arios, sugerindo que o pr´e-\ntreinamento espec´ıfico em portuguˆes confere vantagens na tarefa de gerac¸ ˜ao de perguntas.\n\nComparando os resultados obtidos em ambos os cen´arios de avaliac¸ ˜ao, observa-se\nque os modelos apresentaram melhores desempenhos no Cen´ario 2 em comparac¸ ˜ao com\no Cen´ario 1. Isso acontece porque no Cen´ario 2, como a resposta ´e dada como entrada, ela\nguia os modelos a gerarem perguntas para aquele contexto e resposta. Assim, a pergunta\ngerada precisa ser semanticamente equivalente `a pergunta de referˆencia. Tal situac¸ ˜ao n˜ao\nocorre no Cen´ario 1, j´a que ´e somente dado o contexto como entrada e, para um mesmo\ncontexto, ´e poss´ıvel gerar diversas perguntas v´alidas. Por isso, para melhor avaliar o\nCen´ario 1, foram realizadas as an´alises usando o modelo GP T -4o.\n\nNa Tabela 3 s˜ao apresentados os resultados da avaliac¸ ˜ao dos modelos no Cen´ario\n1 usando o GP T -4o. Os resultados obtidos apresentam um padr˜ao similar ao primeiro ex-\n\n4O prompt usado est´a dispon´ıvel no reposit´orio do projeto.\n\n\f1\n\nPIR ´A\n\nCen´ario\n\nFairyTaleQA\n\nBase de Dados\n\nTabela 2. Resultados dos experimentos nos cen ´arios 1 e 2.\nModelo\nP T T 5Small\nP T T 5Base\nP T T 5Large\nF LAN -T 5Small\nF LAN -T 5Base\nF LAN -T 5Large\nP T T 5Small\nP T T 5Base\nP T T 5Large\nF LAN -T 5Small\nF LAN -T 5Base\nF LAN -T 5Large\nP T T 5Small\nP T T 5Base\nP T T 5Large\nF LAN -T 5Small\nF LAN -T 5Base\nF LAN -T 5Large\nP T T 5Small\nP T T 5Base\nP T T 5Large\nF LAN -T 5Small\nF LAN -T 5Base\nF LAN -T 5Large\n\nBERTScore\n0,3976\n0,4137\n0,4093\n0,3469\n0,3590\n0,3448\n0,2982\n0,3265\n0,3449\n0,2099\n0,2404\n0,2988\n0,5429\n0,5906\n0,6057\n0,4203\n0,4611\n0,4884\n0,3635\n0,4505\n0,4656\n0,2220\n0,2579\n0,3257\n\nROUGE-L\n0,2491\n0,2699\n0,2668\n0,2412\n0,2497\n0,2354\n0,2109\n0,2266\n0,2280\n0,1581\n0,1723\n0,2219\n0,4230\n0,4786\n0,4938\n0,3190\n0,3672\n0,3810\n0,2625\n0,3506\n0,3640\n0,1680\n0,1934\n0,2620\n\nFairyTaleQA\n\nPIR ´A\n\n2\n\nperimento, mas com algumas diferenc¸as importantes. O P T T 5Large obteve o melhor de-\nsempenho em ambas as bases de dados, com 82,32% das perguntas geradas sendo consi-\nderadas v´alidas no FairyTaleQA e 93,06% no PIR ´A. ´E poss´ıvel observar uma divergˆencia\nentre as medidas autom´aticas e a avaliac¸ ˜ao GP T -4o, particularmente no PIR ´A. Enquanto\nas medidas do ROUGE-L e BERTScore indicaram valores menores para o PIR ´A em\ncomparac¸ ˜ao com o FairyTaleQA, a avaliac¸ ˜ao GP T -4o mostrou uma tendˆencia oposta,\ncom percentuais mais altos de perguntas v´alidas no PIR ´A.\n\nTabela 3. Resultados da avaliac¸ ˜ao do Cen ´ario 1 usando o GP T -4o.\n\nBase de Dados\n\nFairyTaleQA\n\nPIR ´A\n\nModelo\nP T T 5Small\nP T T 5Base\nP T T 5Large\nF LAN -T 5Small\nF LAN -T 5Base\nF LAN -T 5Large\nP T T 5Small\nP T T 5Base\nP T T 5Large\nF LAN -T 5Small\nF LAN -T 5Base\nF LAN -T 5Large\n\nV´alida\n680\n726\n829\n451\n525\n719\n135\n199\n201\n122\n138\n197\n\nInv´alida % V´alida\n\n327\n281\n178\n556\n482\n288\n81\n17\n15\n94\n78\n19\n\n67,53\n72,10\n82,32\n44,79\n52,14\n71,40\n62,50\n92,13\n93,06\n56,48\n63,89\n91,20\n\nNa Figura 2 ´e apresentado um exemplo extra´ıdo da base de dados do PIR ´A, con-\ntendo o contexto, as perguntas geradas pelos modelos P T T 5 e a sa´ıda da an´alise usando o\nGP T -4o. Nesse exemplo, ´e poss´ıvel ver que os modelos P T T 5Large e P T T 5Base foram\ncapazes de gerar perguntas que podem ser respondidas pelo contexto, sendo assim consi-\nderadas v´alidas. Por outro lado, o modelo P T T 5Small gerou uma pergunta confusa sobre\na Petr´oleo Brasileiro SA n˜ao ter comentado sobre a estimativa de produc¸ ˜ao revisada. Ape-\n\n\fsar de ser mencionado no contexto, n˜ao est´a expl´ıcito nele o porquˆe disso. Sendo assim,\nconsiderada inv´alida pela avaliac¸ ˜ao do GP T -4o.\n\nContexto: O BG GROUP produziu recentemente uma nova estimativa na descoberta de ´oleo de\nTupi na Bacia de Santos, afirmando que o campo cont´em 12-30 bilh˜oes boe ou mais. Por um\nlado, a Petroleo Brasileiro SA de Petr´oleo (Petrobras) n˜ao comentou sobre a estimativa de produc¸ ˜ao\nrevisada. Pode-se lembrar que a Petrobras relatou a descoberta para ser de 8 bilh˜oes bbl de luz\nem bruto em 2007. Enquanto isso, alegac¸ ˜oes de BG foram produzidas em uma declarac¸ ˜ao sobre\na estrat´egia de crescimento a longo prazo da empresa, lanc¸ada `a frente da apresentac¸ ˜ao de seus\nresultados do quarto trimestre.\n\nP T T 5Small: Por que a Petroleo Brasileiro SA de Petr´oleo (Petrobras) n˜ao comentou sobre a esti-\nmativa de produc¸ ˜ao revisada?\nAvaliac¸ ˜ao: INV ´ALIDA.\n\nP T T 5Base: Qual foi a estimativa do BG Group para a descoberta de ´oleo de Tupi na Bacia de\nSantos?\nAvaliac¸ ˜ao: V ´ALIDA.\n\nP T T 5Large: Qual a estimativa do BG Group para a quantidade de ´oleo de Tupi na Bacia de Santos?\nAvaliac¸ ˜ao: V ´ALIDA.\n\nFigura 2. Exemplo de contexto extra´ıdo do PIR ´A e perguntas consideradas\nv ´alidas e inv ´alidas pelo GP T -4o.\n\n5. Considerac¸ ˜oes Finais e Trabalhos Futuros\n\nNeste trabalho, foi realizada uma an´alise comparativa dos modelos P T T 5 e F LAN -T 5\npara a tarefa de gerac¸ ˜ao autom´atica de perguntas. Para isso, foram utilizadas as bases de\ndados do PIR ´A e uma vers˜ao traduzida do FairyTaleQA para o portuguˆes do Brasil. O\ndesempenho dos modelos foi avaliado usando uma abordagem tradicional, considerando\nas medidas de avaliac¸ ˜ao do ROUGE-L e do BERTScore. Al´em dessa abordagem, foi re-\nalizada uma an´alise das perguntas geradas pelos modelos usando o GP T -4o, avaliando\nse as perguntas geradas poderiam ser respondidas somente a partir do contexto fornecido.\nOs resultados experimentais demonstraram que o modelo P T T 5Large obteve os melhores\nresultados em quase todos os cen´arios avaliados. Os resultados obtidos indicam a efic´acia\ndo pr´e-treinamento espec´ıfico em portuguˆes, evidenciada pelo desempenho superior con-\nsistente dos modelos P T T 5 em comparac¸ ˜ao com os modelos F LAN -T 5.\n\nApesar dos resultados encorajadores obtidos, o trabalho apresenta diversas\nlimitac¸ ˜oes, que ser˜ao melhor exploradas. Dentre elas, pode-se destacar duas li-\n(i) investigar o desempenho de LLMs, como o Llama 3\nnhas de pesquisa futuras:\n[Touvron et al. 2023], Gemma [Team et al. 2024] e Sabi´a [Almeida et al. 2024]; e (ii) re-\nalizar uma avaliac¸ ˜ao humana para complementar as avaliac¸ ˜oes autom´aticas realizadas.\n\nAgradecimentos\n\nOs autores agradecem ao Ifes, apoio da FAPES e CAPES (processo 2021-2S6CD, nº\nFAPES 132/2021) por meio do PDPG (Programa de Desenvolvimento da P´os-Graduac¸ ˜ao,\nParcerias Estrat´egicas nos Estados).\n\n\fReferˆencias\n\nAlmeida, T. S., Abonizio, H., Nogueira, R., and Pires, R. (2024). Sabi´a-2: A new genera-\n\ntion of portuguese large language models. arXiv preprint arXiv:2403.09887.\n\nCarmo, D., Piau, M., Campiotti, I., Nogueira, R., and Lotufo, R. (2020). Ptt5: Pre-\ntraining and validating the t5 model on brazilian portuguese data. arXiv preprint ar-\nXiv:2008.09144.\n\nChen, J., Lin, H., Han, X., and Sun, L. (2024). Benchmarking large language models in\nretrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 38, pages 17754–17762.\n\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X.,\nDehghani, M., Brahma, S., et al. (2024). Scaling instruction-finetuned language mo-\ndels. Journal of Machine Learning Research, 25(70):1–53.\n\nda Rocha Junqueira, J., Corrˆea, U. B., and Freitas, L. (2024). Transformer models for\nbrazilian portuguese question generation: An experimental study. In The International\nFLAIRS Conference Proceedings, volume 37.\n\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. (2020). A systematic review\nof automatic question generation for educational purposes. International Journal of\nArtificial Intelligence in Education, 30:121–204.\n\nLeite, B. and Lopes Cardoso, H. (2022). Neural question generation for the portuguese\nlanguage: A preliminary study. In EPIA Conference on Artificial Intelligence, pages\n780–793. Springer.\n\nLeite, B., Os´orio, T. F., and Cardoso, H. L. (2024). Fairytaleqa translated: Enabling edu-\ncational question and answer generation in less-resourced languages. arXiv preprint\narXiv:2406.04233.\n\nLin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. In Text\nSummarization Branches Out, pages 74–81, Barcelona, Spain. Association for Com-\nputational Linguistics.\n\nMulla, N. and Gharpure, P. (2023). Automatic question generation: a review of metho-\ndologies, datasets, evaluation metrics, and applications. Progress in Artificial Intelli-\ngence, 12(1):1–32.\n\nOliveira, H. G., Caetano, I., Matos, R., and Amaro, H. (2023). Generating and ranking\n\ndistractors for multiple-choice questions in portuguese. In SLATE, pages 4–1.\n\nPaschoal, A. F., Pirozelli, P., Freire, V., Delgado, K. V., Peres, S. M., Jos´e, M. M., Naka-\nsato, F., Oliveira, A. S., Brand˜ao, A. A., Costa, A. H., et al. (2021). Pir´a: A bilingual\nportuguese-english dataset for question-answering about the ocean. In Proceedings of\nthe 30th ACM International Conference on Information & Knowledge Management,\npages 4544–4553.\n\nPuri, R., Spring, R., Shoeybi, M., Patwary, M., and Catanzaro, B. (2020). Training ques-\ntion answering models from synthetic data. In Webber, B., Cohn, T., He, Y., and Liu,\nY., editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5811–5826, Online. Association for Computational\nLinguistics.\n\n\fRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W.,\nand Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research, 21(140):1–67.\n\nTeam, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L.,\nRivi`ere, M., Kale, M. S., Love, J., et al. (2024). Gemma: Open models based on\ngemini research and technology. arXiv preprint arXiv:2403.08295.\n\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere,\nB., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\n\nVaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing\n\nSystems.\n\nWagner Filho, J. A., Wilkens, R., Idiart, M., and Villavicencio, A. (2018). The brwac\ncorpus: A new open resource for brazilian portuguese. In Proceedings of the eleventh\ninternational conference on language resources and evaluation (LREC 2018).\n\nXu, Y., Wang, D., Yu, M., Ritchie, D., Yao, B., Wu, T., Zhang, Z., Li, T., Bradford,\nN., Sun, B., Hoang, T., Sang, Y., Hou, Y., Ma, X., Yang, D., Peng, N., Yu, Z., and\nWarschauer, M. (2022). Fantastic questions and where to find them: FairytaleQA – an\nauthentic dataset for narrative comprehension. In Muresan, S., Nakov, P., and Villa-\nvicencio, A., editors, Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 447–460, Dublin, Ireland.\nAssociation for Computational Linguistics.\n\nZhang, R., Guo, J., Chen, L., Fan, Y., and Cheng, X. (2021). A review on question\n\ngeneration from natural language text. ACM Trans. Inf. Syst., 40(1).\n\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2019). Bertscore:\n\nEvaluating text generation with bert. arXiv preprint arXiv:1904.09675.\n\n\f"
        },
        {
            "titulo": "Unified Knowledge-Graph for Brazilian Indigenous Languages: An Educational Applications Perspective",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31127",
            "idioma": "Inglês",
            "storage_key": "files/article_31127_30930.pdf",
            "autores": [
                {
                    "nome": "Gustavo Polleti",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0001-5526-2161"
                },
                {
                    "nome": "Fabio Cozman",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-4077-4935"
                },
                {
                    "nome": "Fabrício Gerardi",
                    "afiliacao": "Universität Tübingen",
                    "orcid": null
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Este artigo apresenta um grafo de conhecimento unificado para as línguas indígenas brasileiras (BIL) a partir da perspectiva de aplicações potenciais, com foco particular no domínio educacional. Apresentamos o BILGraph, um protótipo construído para o Bororo e línguas tupis, como Guajajara, Munduruku e Akuntsu. Em seguida, descrevemos o processo de extração de conhecimento e ligação de entidades para construir o grafo a partir de um banco de árvores de dependências e de um banco de dados lexical para línguas Tupi e Bororo. Discutimos as limitações do BILGraph, destacando questões éticas e práticas de implementação.",
            "keywords": [
                "Knowledge Graph",
                "Low-resource languages",
                "Natural Language Processing",
                "Indigenous Languages"
            ],
            "referencias": [
                "Cabral, A. S. and Rodrigues, A. (2003). Dicionário da língua asurini do tocantins. Belém-Pará: UFPA/IFNOPAP/UnB: IL/LALI.",
                "Cong, J. and Liu, H. (2014). Approaching human language with complex networks. Physics of Life Reviews, 11(4):598–618.",
                "Ferraz Gerardi, F. Bororo Dictionary. Forthcoming. Available upon request.",
                "Ferraz Gerardi, F. (2024). Universaldependencies/udbororo−bdt.",
                "Ferraz Gerardi, F. M., Sollberger, D., and Toribio Serrano, L. (2024). Corpus bororo (corbo) (v0.1.1).",
                "Gerardi, F. F., Reichert, S., Aragon, C., Wientzek, T., List, J.-M., and Forkel, R. (2022a). TuLeD. Tupían Lexical Database. Zenodo.",
                "Gerardi, F. F., Reichert, S., Aragon, C., Wientzek, T., List, J.-M., and Forkel, R. (2022b). TuLeD. Tupían Lexical Database (v0.12).",
                "Harrison, C. and Harrison, C. (2013). Dicionário Guajajara-Português. SIL.",
                ", Aboriginal Territories in Cyberspace, Honolulu, HI. Edited by Jason Edward Lewis. English Language Version of ”Ka?ina Hana ?Ōiwi a me ka Waihona ?Ike Hakuhia Pepa Kūlana” available at:",
                ".",
                "Miller, G. A. (1994). WordNet: A lexical database for English. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994.",
                "Monserrat, R. F. (2000). Vocabulário Amondawa-Português, Vocabulário e frases em Arara e Português, Vocabulário Gavião-Português, Vocabulário e frases em Karipuna e Português, Vocabulário e frases em Makurap e Português, Vocabulário e frases em Suruíe Português, Pequeno dicionário em Tupari e Português. Universidade do Caixas do Sul.",
                "Nivre, J., Abrams, M., Agić, Z., Ahrenberg, L., Antonsen, L., Aranzabe, M. J., Arutie, A., Asahara, M., Ateyah, L., Attia, M., et al. (2020a). Universal dependencies v2: An evergrowing multilingual treebank collection.",
                ". Accessed: 2024-08-27.",
                "Nivre, J., de Marneffe, M.-C., Ginter, F., Hajič, J., Manning, C. D., Pyysalo, S., Schuster, S., Tyers, F., and Zeman, D. (2020b). Universal Dependencies v2: An evergrowing multilingual treebank collection. In Calzolari, N., Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno, A., Odijk, J., and Piperidis, S., editors, Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4034–4043, Marseille, France. European Language Resources Association.",
                "Pinhanez, C. S., Cavalin, P., Vasconcelos, M., and Nogima, J. (2023). Balancing social impact, opportunities, and ethical constraints of using ai in the documentation and vitalization of indigenous languages. In Elkind, E., editor, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, pages 6174–6182. International Joint Conferences on Artificial Intelligence Organization. AI for Good.",
                "Polleti, G. (2024). Building a language-learning game for Brazilian indigenous languages: A case study. Technical report, arXiv:2403.14515."
            ],
            "artigo_completo": "Unified Knowledge-Graph for Brazilian Indigenous\nLanguages: An Educational Applications Perspective\n\nGustavo Polleti 1, Fabio Cozman 1, Fabricio Gerardi 2\n\n1Universidade de S˜ao Paulo, Brazil\n\n2Universit¨at T¨ubingen, Germany\n\ngustavo.polleti@gmail.com\n\nAbstract. In this paper we present an unified knowledge-graph for Brazilian\nindigenous languages (BIL) from the perspective of potential applications, with\na particular focus to the educational domain. We present BILGraph, a prototype\nwe built for Bororo and Tupian languages, such as Guajajara, Munduruku and\nAkuntsu. Then we describe the knowledge extraction and entity linking process\nto build the graph from a dependency treebank and a lexical database for Tupian\nand Bororo languages. We discuss the limitations of BILGraph, highlighting\nethical and practical implementation concerns.\n\nResumo. Este artigo apresenta um grafo de conhecimento unificado para as\nl´ınguas ind´ıgenas brasileiras (BIL) a partir da perspectiva de aplicac¸ ˜oes po-\ntenciais, com foco particular no dom´ınio educacional. Apresentamos o BIL-\nGraph, um prot´otipo constru´ıdo para o Bororo e l´ınguas tupis, como Guajajara,\nMunduruku e Akuntsu. Em seguida, descrevemos o processo de extrac¸ ˜ao de co-\nnhecimento e ligac¸ ˜ao de entidades para construir o grafo a partir de um banco\nde ´arvores de dependˆencias e de um banco de dados lexical para l´ınguas Tupi\ne Bororo. Discutimos as limitac¸ ˜oes do BILGraph, destacando quest˜oes ´eticas e\npr´aticas de implementac¸ ˜ao.\n\n1. Introduction\n\nThe development of applications for Brazilian Indigenous languages (BIL) is severely\nlimited by the lack of resources and tools. As is often the case with endangered lan-\nguages, available resources are both scarce and dispersed [Pinhanez et al. 2023]. For\nsome languages, such as Guajajara, Asurini, and Bororo, dictionaries are now available\n[Harrison and Harrison 2013, Cabral and Rodrigues 2003, Ferraz Gerardi ]. For other\nlanguages, treebanks are available through the Universal Dependencies Project (UD)\n[Nivre et al. 2020a], though they vary in length and quality. Some languages, however,\nhave only a handful of miscellaneous resources [Monserrat 2000]. This lack of standard-\nization and proper linked data poses a significant barrier to developing tools and methods\nthat could support language revitalization initiatives and accelerate the production of ped-\nagogical material.\n\nRecent efforts to unify Brazilian Indigenous language resources, such as TuLeD\n[Gerardi et al. 2022a] and the TuDeT treebanks on UD — a lexical database and a de-\npendency treebank for several Tupian languages (still in their initial phase), respec-\ntively — have been pivotal in the development of language-learning applications tar-\ngeted at Indigenous communities [Polleti 2024]. Additionally, the recent publication of\n\n\fFigure 1. BILGraph toy example displaying a sampled subgraph associated with\nthe sentence Bororo “ure karo kowuje”, i.e. “He ate the fish”.\n\nthe Bororo Corpus [Ferraz Gerardi et al. 2024], which is connected to the UD Treebank\n[Ferraz Gerardi 2024], has enabled the use of various computational tools to develop ed-\nucational materials and other online resources; notably a language-learning app for the\nBororo language.1 UD-treebanks [Nivre et al. 2020b] are an important resource since\nthe standardized type of annotation for all languages facilitate the development of new\napplications. On the other hand, heterogeneous and complex network structures, such\nas knowledge graphs, are known for their flexibility in incorporating linguistic charac-\nteristics [Cong and Liu 2014, Miller 1994] and can be effectively utilized to power so-\nphisticated applications, including recommendation systems, information retrieval, and\neducational assistants.\n\nIn this work, we introduce a preliminary version of an unified knowledge graph\nfor Brazilian indigenous languages, which we will refer as “BILGraph”, and we de-\nscribe its knowledge extraction pipeline. We developed a prototype for Tupian lan-\nguages available in Tuled and Tudet [Gerardi et al. 2022b], and the Bororo language\n[Ferraz Gerardi et al. 2024]. We discuss the knowledge graph prototype with a focus on\nthe potential applications. We managed to develop a natural language processing pipeline\nto build BILGraph that can handle semi-structured data from several sources, such as an-\nnotated phrases from treebanks and dictionaries. We discuss the pipeline challenges and\nlimitations. The main contribution of this work is to present a prototype version of BIL-\nGraph as a case of study on building an unified knowledge graph for BIL. We hope the\nknowledge graph and the methods presented in this work can support the development of\nsophisticated applications.\n\nThe paper is organized as follows. Section 2 describes BILGraph’s design and its\ndevelopment, including their data sources and knowledge extraction pipeline. Section 3\ndiscusses the challenges and limitations of our prototype, analyses our processes and\nresources from both a practical implementation and potential applications perspective,\nand offers concluding remarks.\n\n2. BilGraph: Linguistic Knowledge Graph\n\nWe have developed a knowledge extraction pipeline to structure and link language re-\nsources for Brazilian Indigenous Languages (BIL) available in Universal Dependencies\n(UD) treebanks and lexical databases, such as TuLeD and the Bororo dictionary. The re-\nsult of this effort is “BILGraph”, a knowledge graph for BIL that contains four principal\ntypes of nodes: (1) sentence, (2) token, (3) lemma, and (4) concept. Consider the example\ndepicted in Figure 1. The sentence node represents the Bororo treebank phrase ure karo\n\n1https://bilingo-4388e.web.app/\n\n\fkowyje ‘He ate the fish’. This sentence node connects to its token nodes, which repre-\nsent the individual words composing the sentence and their syntactic dependencies. In\nthis example, “kowyje” is the root, with the object “karo” and the nominal subject “ure”\nlinking to it. Each token node is connected to a single sentence node. Each token is fur-\nther linked to a lemma node, which represents the word’s base form and its relationships\nto linguistic classes, including any applicable synonyms. Up to this point, the entities\nand relationships described are those typically found in dependency treebanks. However,\nthe lexical database or dictionary adds another layer by linking lemma nodes to concept\nnodes. Concept nodes represent high-level abstractions that convey meaning across dif-\nferent languages and domains.\nIn our example, the lemmas “karo” and “kowyje” are\nlinked to the concepts “fish” and “eat,” respectively. The goal is to establish the concept\nnodes as a semantic layer that enables interoperability between the treebank sentences and\nother knowledge bases, such as ontologies, multimedia resources (e.g., phonetic or image\ndatabases), and other languages. Using BILGraph, one could easily search for sentences\nin other languages with similar structures or themes by fetching all sentence nodes con-\nnected to a given concept node. For example, a search engine could retrieve the Guajajara\nsentence uPu ipirateteaPu ‘It eats many fishes’ because it is connected to the concept node\n“FISH” as the similar sentence in Bororo ure karo kowyje. Note that the graph structure\nis flexible enough to encode N-N relationships between lemmas and concepts.\n\nThe relationships between sentences, tokens and lemmas can be extracted directly\nfrom UD treebanks, as the treebank sentences are annotated with attributes that allow a\nstraightforward graph representation. To build BILGraph, the real challenge lies in linking\nlemma to concept nodes. In our preliminary version, we applied a simple entity linking\nprocess as follows. For each lemma, we generated a neighborhood set of similar words by\nchanging and trimming characters based on rules. For example, in the Bororo language,\nwe have different spellings where some words exchange “u” for “y”, and words like “boe”\nare often applied, so some of our neighborhood generation rules involved in adding or re-\nmoving prefixes and changing exchangeable letters. The size of the neighborhood was\ndefined considering a similarity threshold based on the Leveshnstein distance. Next, we\nselect from all the vocabulary in our database the words that display high similarity, con-\nsidering again a threshold based on leveshnstein distance, with at least one instance in\nour neighborhood. Finally, we test if dictionary entry or description for each candidate\nhas at least one word in the sentence. So, for example, consider we are trying to link the\nlemma “karo”, from the sentence “He ate the fish”, to its appropriate concepts. Addition-\nally, consider the dictionary description for a word candidate “kabo” is “a type of river\nfish”. In this case, we will establish the link due to the lexical similarity between “karo”\nand “kabo”, and due to the word “fish” that is present in both the dictionary entry and the\nsentence. Note that relying on lexical similarity may lead to innacuracies. For example,\nthe Bororo words “apido” (palm heart) and “apodo” (toucan) have high lexical similarity\nwhile there meanings are not related at all. If a dictionary entry contains both words,\nsuch as “palm hearth, edible for many animals like toucans”, this would lead to incorrect\nlinks being added to the graph. BILGraph’s knowledge extraction pipeline code, with the\nused Leveshentein distance thresholds for each language, and the knowledge graph itself\nis available in Github.2 We adopted the RDF format, where each edge in the graph is\nrepresented as a triple.\n\n2https://github.com/gpadpoll/bilgraph\n\n\f3. Discussion and Concluding Remarks\n\nThe preliminary version of BILGraph introduced in this work represents a significant\nstep forward in advancing resources for Brazilian Indigenous languages. We envision\nthat BILGraph could power typical applications such as information retrieval from texts\nwritten in these languages, with a particular emphasis on its educational potential. The\nprocess of creating educational resources often involves organizing texts based on their\nlinguistic characteristics, themes, and complexity levels. For instance, one might search\nfor specific sentences to teach someone how to ask for food. BILGraph simplifies this\ntask by allowing queries for sentences linked to specific concept nodes. To find sentences\nthat include food-related vocabulary, one can attach a generic ontology to BILGraph’s\nconcept nodes and search for sentences associated with food-related concepts. Moreover,\nBILGraph makes it easy to query sentences based on linguistic features, such as those\nusing possessive pronouns, verb forms, plurals, adverbs, and more. We believe that BIL-\nGraph’s ability to query and organize sentences can enhance the use of treebanks and\nother available BIL resources in the development of educational materials. By organiz-\ning resources in a standardized and unified format, we can develop applications that scale\nacross multiple languages. For example, a query that searches for food-related concepts in\nsentences for one language can be reused for other languages included in BILGraph. We\nare already leveraging BILGraph to develop a curriculum for a Bororo language course,\nwhich will be released as a language-learning app. We aim to extend this approach to\nother languages as they are incorporated into the knowledge graph.\n\nAt this point, our BILGraph prototype falls short in several aspects and remains\na work in progress, from the difficulties of working with limited sources of data to in-\nnacuracies and ethical concerns. BILGraph was built from TuLeD, TuDet and the Bororo\ntreebank and dictionary. All these data sources were developed by compiling several\nsources from the literature, without a proper structured data gathering process. As a re-\nsult, it suffers from incompleness, notably when we consider coverage of dependency\ntrees with translation to portuguese. We only have have Portuguese translations for\n“Bororo”, “Guajajara”, “Munduruku” and “Akuntsu” out of the 9 languages available.\nThe lack of Portuguese translations limits the application of these resources, as for educa-\ntional purposes for example. Furthermore, it reasonable to expect that some innacuracies\nmay have been introduced as part of the entity linking and knowledge extraction pro-\ncess. We haven’t evaluated the correctness in a comprehensive manner yet, except for\nlimited manual inspection by the researchers. Finally, it is worth mentioniong ethical\nconcerns. BILGraph has been developed without the involvement of indigenous commu-\nnity [Pinhanez et al. 2023], except for the case of Bororo, so it is hard to enforce ethical\nguidelines [Lewis et al. 2020], as for example proposed by the Los Pinos Declaration,3\nbefore BILGraph can be properly inspected and validated by actual indigenous speakers.\n\nWe recognize a limitation in distinguishing similar forms that map to different\nlemmas. While various solutions exist, the most effective approach tend to be proba-\nbilistic, improving in accuracy with larger datasets. We also focus on further research in\ndeveloping a pipeline which only uses the target language, without relying on the use af\na dictionary. Overall, we hope BILGraph represents a positive step towards an unified\nsource for BIL resources so that more tools and applications can be developed for them.\n\n3https://unesdoc.unesco.org/ark:/48223/pf0000374030\n\n\fAcknowledgements\n\nThe second author was partially supported by CNPq grant 305753/2022-3. We also thank\nsupport by CAPES -Finance Code 001. The authors of this work would like to thank\nthe Center for Artificial Intelligence (C4AI-USP) and the support from the S˜ao Paulo\nResearch Foundation (FAPESP grant 2019/07665-4) and from the IBM Corporation.\n\nReferences\n\nCabral, A. S. and Rodrigues, A. (2003). Dicion´ario da l´ıngua asurini do tocantins. Bel´em-\n\nPar´a: UFPA/IFNOPAP/UnB: IL/LALI.\n\nCong, J. and Liu, H. (2014). Approaching human language with complex networks.\n\nPhysics of Life Reviews, 11(4):598–618.\n\nFerraz Gerardi, F. Bororo Dictionary. Forthcoming. Available upon request.\n\nFerraz Gerardi, F. (2024). Universaldependencies/udbororo − bdt.\n\nFerraz Gerardi, F. M., Sollberger, D., and Toribio Serrano, L. (2024). Corpus bororo (corbo)\n\n(v0.1.1).\n\nGerardi, F. F., Reichert, S., Aragon, C., Wientzek, T., List, J.-M., and Forkel, R. (2022a).\n\nTuLeD. Tup´ıan Lexical Database. Zenodo.\n\nGerardi, F. F., Reichert, S., Aragon, C., Wientzek, T., List, J.-M., and Forkel, R. (2022b).\n\nTuLeD. Tup´ıan Lexical Database (v0.12).\n\nHarrison, C. and Harrison, C. (2013). Dicion´ario Guajajara-Portuguˆes. SIL.\n\nLewis, J. E., Abdilla, A., Arista, N., Baker, K., Benesiinaabandan, S., Brown, M., Che-\nung, M., Coleman, M., Cordes, A., Davison, J., Duncan, K., Garzon, S., Harrell,\nD. F., Jones, P.-L., Kealiikanakaoleohaililani, K., Kelleher, M., Kite, S., Lagon, O.,\nLeigh, J., Levesque, M., Mahelona, K., Moses, C., Nahuewai, I. I., Noe, K., Olson,\nD., Parker Jones, ¯O., Running Wolf, C., Running Wolf, M., Silva, M., Fragnito, S.,\nand Whaanga, H. (2020). Indigenous protocol and artificial intelligence position paper.\nProject Report 10.11573/spectrum.library.concordia.ca.00986506, Aboriginal Territories\nin Cyberspace, Honolulu, HI. Edited by Jason Edward Lewis. English Language Ver-\nsion of ”Ka?ina Hana ? ¯Oiwi a me ka Waihona ?Ike Hakuhia Pepa K¯ulana” available at:\nhttps://spectrum.library.concordia.ca/id/eprint/990094/.\n\nMiller, G. A. (1994). WordNet: A lexical database for English. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994.\n\nMonserrat, R. F. (2000). Vocabul´ario Amondawa-Portuguˆes, Vocabul´ario e frases em Arara\ne Portuguˆes, Vocabul´ario Gavi˜ao-Portuguˆes, Vocabul´ario e frases em Karipuna e Por-\ntuguˆes, Vocabul´ario e frases em Makurap e Portuguˆes, Vocabul´ario e frases em Suru´ı e\nPortuguˆes, Pequeno dicion´ario em Tupari e Portuguˆes. Universidade do Caixas do Sul.\n\nNivre, J., Abrams, M., Agi´c, Z., Ahrenberg, L., Antonsen, L., Aranzabe, M. J., Arutie, A.,\nAsahara, M., Ateyah, L., Attia, M., et al. (2020a). Universal dependencies v2: An ev-\nergrowing multilingual treebank collection. https://universaldependencies.\norg/. Accessed: 2024-08-27.\n\n\fNivre, J., de Marneffe, M.-C., Ginter, F., Hajiˇc, J., Manning, C. D., Pyysalo, S., Schuster,\nS., Tyers, F., and Zeman, D. (2020b). Universal Dependencies v2: An evergrowing mul-\ntilingual treebank collection. In Calzolari, N., B´echet, F., Blache, P., Choukri, K., Cieri,\nC., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno,\nA., Odijk, J., and Piperidis, S., editors, Proceedings of the Twelfth Language Resources\nand Evaluation Conference, pages 4034–4043, Marseille, France. European Language\nResources Association.\n\nPinhanez, C. S., Cavalin, P., Vasconcelos, M., and Nogima, J. (2023). Balancing social\nimpact, opportunities, and ethical constraints of using ai in the documentation and vital-\nization of indigenous languages. In Elkind, E., editor, Proceedings of the Thirty-Second\nInternational Joint Conference on Artificial Intelligence, IJCAI-23, pages 6174–6182. In-\nternational Joint Conferences on Artificial Intelligence Organization. AI for Good.\n\nPolleti, G. (2024). Building a language-learning game for Brazilian indigenous languages:\n\nA case study. Technical report, arXiv:2403.14515.\n\n\f"
        },
        {
            "titulo": "Boosting not so Large Language Models by using Knowledge Graphs and Reinforcement Learning",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31128",
            "idioma": "Inglês",
            "storage_key": "files/article_31128_30931.pdf",
            "autores": [
                {
                    "nome": "William Jones Beckhauser",
                    "afiliacao": "UFSC",
                    "orcid": "http://orcid.org/0009-0004-2647-3874"
                },
                {
                    "nome": "Renato Fileto",
                    "afiliacao": "UFSC",
                    "orcid": "https://orcid.org/0000-0002-7941-6281"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Ensuring the viability of large language models (LLMs) in situations requiring data privacy with limited on-premise resources is a significant current challenge. This work investigates how to tackle this challenge using knowledge graphs (KGs) and reinforcement learning (RL) to enhance minor LLMs by reducing non-factual responses and response gaps. We evaluated variations of GPT (4o, 4, and 3.5), Llama2 (7b, 13b, and 70b), and Llama3 (8b and 70b) for multi-label classification and information extraction, with or without KG and RL, and also fine-tuned a BERT model. Llama3 8b combined with KG and RL outperformed all other LLM models, and the fine-tuned BERT model too.",
            "keywords": [
                "Large Language Models",
                "Knowledge Graphs",
                "Reinforcement Learning"
            ],
            "referencias": [
                "Erickson, A. (2018). Comparative analysis of the eu’s gdpr and brazil’s lgpd: Enforcement challenges with the lgpd. Brook. J. Int’l L., 44:859.",
                "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744.",
                "Xue, F., Fu, Y., Zhou, W., Zheng, Z., and You, Y. (2024). To repeat or not to repeat: Insights from scaling llm under token-crisis. Advances in Neural Information Processing Systems, 36."
            ],
            "artigo_completo": "Boosting not so Large Language Models by using\nKnowledge Graphs and Reinforcement Learning\n\nWilliam Jones Beckhauser1, Renato Fileto1\n\n1Department of Informatics and Statistics (INE)\nFederal University of Santa Catarina (UFSC), Florianópolis - Brazil\n\n{beckhauserwilliam@gmail.com, r.fileto@ufsc.br}\n\nAbstract. Ensuring the viability of large language models (LLMs) in situations\nrequiring data privacy with limited on-premise resources is a significant current\nchallenge. This work investigates how to tackle this challenge using knowledge\ngraphs (KGs) and reinforcement learning (RL) to enhance minor LLMs by re-\nducing non-factual responses and response gaps. We evaluated variations of\nGPT (4o, 4, and 3.5), Llama2 (7b, 13b, and 70b), and Llama3 (8b and 70b) for\nmulti-label classification and information extraction, with or without KG and\nRL, and also fine-tuned a BERT model. Llama3 8b combined with KG and RL\noutperformed all other LLM models, and the fine-tuned BERT model too.\n\n1. Introduction\n\nLarge language models (LLMs) such as GPT [Liu et al. 2023], Llama [Gao et al. 2023],\nand Gemini [Team et al. 2023] are increasing their parameter count with each new re-\nlease, for performance gains [Xue et al. 2024]. Nevertheless, this technology, usually\navailable in the clouds of large private corporations, remains out of reach for many compa-\nnies and projects that need to operate on local servers [Yao et al. 2024], due to high costs\nand regulations like the General Data Protection Law (LGPD) [Erickson 2018]. These\nenterprises could rely on open-source models with many parameters, but their computa-\ntional requirements are too high to run on-premises [Alizadeh et al. 2023].\n\nNowadays, there is a subtle research movement towards smaller open-source\nLLMs [Shridhar et al. 2023, Shen et al. 2024], and an intense pursuit of optimization\nstrategies. A promising direction is Retrievable Augmented Generation (RAG) using a\nKnowledge Graph (KG) to add relevant formal knowledge to LLMs [Pan et al. 2023].\nThis approach has been tested in various tasks,\nincluding fake news detection\n[Liu et al. 2024], text classification [Shi et al. 2023], and refined node classification in\ncitation graphs and networks [Bruno et al. 2023, He et al. 2023]. In the biomedical do-\nmain, these solutions have been applied in recommendation systems and drug-gene in-\nteraction studies [Xu et al. 2024, Wang et al. 2023], as well as in recruiting for clinical\nstudies [Guan et al. 2023]. However, there are still few concrete examples demonstrating\nconsistent performance gains by using approaches like Graph-RAG [Pan et al. 2023] in\ntypical machine learning tasks, such as multi-label classification or information extrac-\ntion, especially when using open-source LLMs.\n\nThis article contributes to filling this research gap by evaluating the synergism\nof KGs, reinforcement learning (RL) and LLMs. We compare the performance of rela-\ntively small LLMs, like Llama2 (7b and 13b) and Llama3 (8b), with that of larger LLMs\n\n\flike GPT (4, 4o, and 3.5), Llama2 (70b), and Llama3 (70b), each one alone and com-\nbined with the use of KGs and RL, in two tasks: (i) multi-label classification of reviews\nposted by users of a food delivery app in multiple languages and their translations into\nEnglish, and (ii) information extraction from invoices of different types of backhoes. We\npropose and evaluate alternative approaches for exploiting domain specific KGs to enrich\nLLM prompts with relevant context. An RL agent validates responses, restricting them\nto predefined labels, when available, and providing feedback to the models. It randomly\nvalidates some LLM responses with their respective labels throughout the RL process.\nWe also fine-tuned and evaluated a BERT model for performing the same multi-label\nclassification, on the same datasets.\n\nThe main contributions of this article are: (i) a systematic evaluation of language\nmodels, considering each LLM alone and assisted by a KG and/or an RL agent; (ii)\ndemonstrating the superiority of smaller, open-source models, like Llama3 8b, when com-\nbined with KGs; (iii) showcasing the feasibility of feedback systems for language models;\nand (iv) applying LLMs combined with KGs and RL in unexplored fields.\n\n2. Proposed Approach\n\nFigure 1 shows the architecture of our integrated Graph-RAG and RL system for LLMs,\ndesigned to optimize responses in classification and information extraction tasks. The\nprocess starts with a instruction sent to the Prompt Raising module, supplemented by\ndata from annotated corpora (e.g., a backhoe invoice). This module interacts with the\nKG/vector management component to search the Knowledge Base for relevant context\nby accessing the knowledge graph linked to the instruction. The retrieved context is then\nintegrated into the prompt, which the LLM uses to generate a response. The RL Agent\nchecks the LLM’s output against available labels(train data). If inaccuracies arise, feed-\nback is given to the LLM, and interaction results are stored in the Results database.\n\nFigure 1. Proposed process for using knowledge and RL to improve LLM results.\n\n2.1. Pre-production\n\nIn the pre-production phase, we focus on constructing KGs using domain-specific struc-\ntured data sources. For example, in information extraction tests related to backhoe load-\ners, we use tables with product descriptions segmented into products, brands, and mod-\nels. These are organized into a hierarchy of classes and subclasses, with connections\nlike “Product” connected by “offered by” to “Brand,” which in turn connects via “has”\n\nLLMRL AgentInstructionRetrieved\nrelevant\ncontextSearch \nrelated \ncontextAssociated \nlabel searchAssociated label\nresults, if anyContext\nEnriched\nPromptFeedback\nPromptData managenentKG / Vector ManagementPre-productionResultsPrompt \nRaisingDomain specific knowledge (legacy \nKGs & documents)Integration, chucking,\nembedding and \nindexingKnowledge\nBaseAnnotated\nCorpora\fto “Model”. Once the structure is defined and validated, we generate embeddings for\nthe classes, subclasses, and connections using the BGE model [Chen et al. 2024]. The\ngraphs are then implemented in the Neo4j graph database, incorporating the generated\nembeddings. These KGs stored in Neo4j serve as our Knowledge Base.\n\n2.2. Prompt Raising\n\nIn the \"Prompt Raising\" phase, the system processes three inputs: the instruction, textual\ndescriptions on data management, and relevant information from the knowledge base.\nThe third input is obtained via two methods: Graph-RAG Targeted, retrieving highly\nsimilar information, and Graph-RAG Comprehensive, gathering related classes and their\ninterrelationships without filters.\n\nEmbeddings are generated from the inputs using the BGE model, the same one\nemployed for knowledge graphs (KGs). Then, a similarity search compares these embed-\ndings with the knowledge base using cosine similarity. The Graph-RAG Targeted method\nidentifies records with a cosine similarity above 85%, while the Graph-RAG Comprehen-\nsive method retrieves all relevant classes, subclasses, and connections. For example, when\nprocessing \"3C OPEN CAB JCB BACKHOE LOADER\" in a brand-related instruction,\nGraph-RAG Targeted might indicate \"97% probability for JCB and 3% for New Hol-\nland,\" while Graph-RAG Comprehensive would provide broader insights such as \"The\nJCB brand includes models 3CX and 5CX\" and \"New Holland covers models B95C and\nB115C\". Thus, as shown in Figure 2, the output of Prompt Raising consists of Instruction\nand Output Indicator. The Input Data represents a textual description from Data Manage-\nment, and the Context, in this example, is Graph-RAG Targeted, which retrieved the data\nwith the highest cosine similarities from our Knowledge Base.\n\nFigure 2. Graph-RAG-Enhanced Contextual Prompt for Information Extraction.\n\n2.3. LLM\n\nWe configure the LLM and invoke its API using an enriched prompt derived from the\nPrompt Raising stage. Key parameters, like temperature and output token count, are ad-\njusted. Temperature controls prediction randomness, with lower values yielding more\ndeterministic results and higher values increasing creativity. For classification and extrac-\ntion tasks, we limit the output to fewer than 10 tokens. We employ models like Llama2\n(7b, 13b, 70b) and Llama3 (8b, 70b) via the Deepinfra API, as well as GPT models (3.5,\n4.0) via the OpenAI API. With the enriched prompt and optimal model settings, the API\nis called to perform extraction or classification. For example, for the product description\n“3C OPEN CAB JCB BACKHOE LOADER”, the expected response would be “JCB”.\n\n2.4. RL Agent\n\nThe RL Agent processes the output of the LLM model by checking if there is a corre-\nsponding label in the database, as detailed in the enriched prompt. The annotated corpora\n\nYou are tasked with extracting the brand from the product description.\nWhat is the brand contained in the product description? \n\nProduct Description: 3C OPEN CAB JCB BACKHOE LOADER.\n\nThere is a 97% chance of the brand being JCB \nand an 3% chance of it being New Holland.\n\nRespond only with the brand name, no additional details.\n\nInstruction1Output Indicator4Context3Input Data2\finclude a percentage of pre-labeled data randomly distributed, and each new LLM output\nis compared against these corpora. For example, if the LLM classifies a product descrip-\ntion as \"New Holland\" for \"JCB 3C OPEN CAB Backhoe Loader,\" the RL Agent searches\nthe annotated corpora to check if there is a label. If \"New Holland\" is correct or if there\nis no existing label, the response is validated and stored; if incorrect, the agent provides\nfeedback suggesting the correct label. This process is repeated up to five times to correct\nand reinforce the model’s learning. For classification tasks with predefined labels, the RL\nAgent adopts a two-step validation process. First, it checks if the LLM’s classification\nmatches the predefined labels. If it doesn’t match, the agent provides feedback to align\nthe response with the established categories. In the second step, if the classification falls\nwithin the categories, the Agent validates it against the associated label (if any).\n\n3. Application Scenarios\n\n3.1. Multi-label Classification of Food Delivery Reviews\n\nIn our first scenario, we analyzed a dataset of around 4,000 customer reviews\nfrom a European food delivery app, ranging from 0 to 889 characters, available\nin [Beckhauser and Fileto 2024]. After removing duplicates and outliers, 3,451 reviews\nremained. Approximately 80% are in European Portuguese, with the rest in English,\nSpanish, Italian, and Catalan. Given the importance of English in LLM training, we\ncreated a parallel dataset by translating all reviews into English using Googletrans, with\nmanual corrections for about 30 reviews. We then identified key terms for each label\nby removing stop-words in multiple languages using nltk and spaCy and extracting fre-\nquent words with the Counter library. For sentiment analysis, we used the SiEBERT\nmodel [Hartmann et al. 2023], which showed consistent performance, even when com-\npared to GPT-4 [Krugmann and Hartmann 2024]. Sentiment analysis results and dataset\ndetails are summarized in Table 1.\n\nWe manually built a tree-like KG to categorize reviews, distinguishing be-\ntween “Product” (item-related) and “Order” (delivery/service-related). Subcategories like\n“Quantity issue” and “Quality issue” under “Product,” and “Delivery issue” and “Praise\ncomment” under “Order” are further refined with specific keywords.\n\nTable 1. Dataset review distribution by class, subclass, and sentiment.\n\nClass\n\nProduct\n\nProduct\n\nOrder\n\nOrder\n\nSubclass Description\nQuality\nissue\nQuantity\nissue\nDelivery\nissue\nPraise\ncomment\n\nIssues with food preparation,\ntaste, or hygiene.\nDissatisfaction with the amount\nor size of the portions served.\nProblems related to delays,\nwrong deliveries or missing items.\nPositive comments about the\nquality of the service or product.\n\n#Reviews % Pos. Neg.\n\n671\n\n605\n\n19.44\n\n26% 74%\n\n17.53\n\n28% 72%\n\n1196\n\n34.66\n\n26% 74%\n\n979\n\n28.37\n\n98% 2%\n\n3.2. Information Extraction from Invoices\n\nOur second application scenario involves a dataset of approximately 17,000 work machine\npurchase invoice descriptions, including the Mercosur Common Nomenclature (NCM)\n\n\fand unit item values, provided by (blind review). The invoice descriptions range from 16\nto 120 characters. Initially, we filtered the dataset using the NCM code, focusing on the\nfirst four characters, specifically \"8429,\" which covers bulldozers, graders, excavators,\nand similar machinery. We then applied a keyword dictionary to identify relevant terms.\nBackhoe loaders appeared most frequently, with around 1,100 descriptions, becoming the\nprimary focus of our experiments. This dataset lacked initial classifications, containing\nonly raw invoice descriptions. To facilitate future model validation, we manually cat-\negorized the data into predefined classes such as brand, model, and specifications. A\ndictionary comprising brands, models, keywords, orthographic variations, acronyms, and\nabbreviations was used, considering possible typographical errors. Fields not covered by\nthe dictionaries were manually completed, ensuring thorough validation of LLM outputs.\n\nKGs for Backhoe Invoices. Figure 3 shows an extract of an ontology in KG for-\nmat, centered on heavy machinery. It depicts the ’Product’ concept, with ’Backhoe’ as a\nsubclass, linked to 16 brands via the \"offered by\" relation. Brands like ’New Holland’ and\n’JCB’ are highlighted, each connected to specific models through the \"has\" relation. For\ninstance, ’New Holland’ includes models like ’B110B’ and ’LB90,’ while ’JCB’ offers\n’4CX ECO’ and ’3CX.’ In total, 68 models are represented.\n\nFigure 3. KG extraction with concepts and relations from heavy machinery.\n\n4. Experiments\n\nIn this section, we describe the experiments conducted for multi-label classification with\ncustomer reviews, subsection 4.1, and information extraction experiments using back-\nhoe invoice data, subsection 4.2. All datasets and models were tested in various distinct\nscenarios: (1) classification or extraction using only the instruction and corpus, without\nproviding enriched context to the LLM; (2) using only the RL Agent; (3) adding a compre-\nhensive search in the KGs, which returns all classes, subclasses, relationships, and leafs\nas context; (4) using targeted context with similarity search above 85%, utilizing Graph-\nRAG; (5) using Graph-RAG Comprehensive with RL; (6) using Graph-RAG Targeted\nwith RL. Additionally, for the multi-label classification experiments, we will conduct a\ntest with embeddings and fine-tuning using BERT. A more comprehensive description of\nthe experiments developed is available at GitHub1.\n\n4.1. Multilabel Classification of Customer Reviews\n\nIn this subsection, we present the experiments conducted for multi-label classification.\nThe experiments are performed on two subsets of customer review data: the first contains\n\n1https://github.com/WilliamBeckhauser/Boosting-not-so-LLM\n\nProductBackhoeNew\nHollandJCB...B110BLB90...4CX ECO3CX.........subclassOf...subclassOf...subclassOfBrandModeloffered byhas\freviews from customers in various languages, and the second comprises the same reviews\ntranslated into English. Each dataset includes 3,451 reviews. We randomly selected 300\nreviews from each label for the agent to use as a validator during the classification process,\nresulting in 1,200 reviews used solely for reinforcement training on the model.\n\nThe BERT experiment tokenize reviews and split them into training and testing\ndatasets at an 80/20 ratio. We use BERT to produce embeddings and a training function\nwith an AdamW optimizer and a linear scheduler. To optimize hyperparameters, we set\nup an objective function in Optuna, adjusting the learning rate, weight decay, and epochs.\n\nEnglish dataset: In these experiments, the Llama3 8b model, when combined\nwith Graph-RAG Comprehensive and an RL agent, achieved a 64.7% increase in cov-\nerage compared to the “Base” experiment, the highest among all models and scenarios\n(Figure 4). Without Graph-RAG Comprehensive and the agent, coverage dropped dras-\ntically to 27.2%. The Llama3 8b also excelled in precision (93.3%) and F1-Score (92%)\nunder the same conditions.\n\nFigure 4. Multi-label classification of customer reviews in English.\n\nThe GPT-4o model, when used solely with Graph-RAG Comprehensive, recorded\n90.4% coverage, 89.1% precision, and an 89.1% F1-Score, outperforming its configura-\ntion with agents, where these metrics were 72%, 71.1%, and 71% respectively. GPT-3.5\nshowed moderate stability, with 81.8% coverage, 84.5% precision, and 80.6% F1-Score\nusing Graph-RAG Comprehensive and agents. Without agents, these values only slightly\ndeclined to 81.3%, 84.1%, and 80%. The Llama2 variants underperformed, particularly\nthe 13b version without agents. The Llama3 70b model improved in precision (92%)\nand F1-Score (91%) with RL agents but showed reduced performance without them. The\nBERT model achieved 79% precision and 76% F1-Score with general context and RL\nagent conditions, but still lagged behind the Llama3 models.\n\nMultilingual dataset: The coverage improvements were more modest for GPT-4\nand GPT-4o models, with only a 0.2% increase, but they retained high accuracy (around\n88-89%). Notably, Llama3 70b showed strong results in both contexts, with 90.1% cover-\nage in the multilingual setting and consistently high precision across datasets. However,\nin none of the scenarios did the multilingual dataset surpass the results achieved with\nthe English dataset, highlighting a clear performance gap. Smaller models like Llama2\n13b particularly struggled in both datasets, especially in multilingual tests where coverage\nremained low even with advanced techniques. The findings emphasize the superior adapt-\nability of larger models like Llama3 and GPT-4 across languages, while smaller models\nstruggle to maintain effectiveness without additional enhancements.\n\nDifference between Base and highest resultBase (No context \nand no RL agent)Graph-RAG\nComprehensiveRL AgentGraph-RAG\nComprehensive \n+ RL AgentGraph-RAG \nTargetedGraph-RAG \nTargeted \n+ RL Agent0.3%48.2%64.7%Coverage0255075100%92GPT-4oLlama3 8BLlama3 70BBERT\nFine-tuning\f4.2. Information Extraction from Backhoe Invoices\n\nFigure 5 shows that the Llama3 8b model, when operated with KGs and RL agents, dis-\nplays a remarkable improvement in accuracy. Specifically, the accuracy increased from\na baseline of 52.18% to 95.7% when using Graph-RAG Targeted and RL Agent, demon-\nstrating an enhancement of 43.52%.\n\nFigure 5. Information extraction from backhoe invoices.\n\nThe Llama3 8b model achieved the highest accuracy of 95.7% and the greatest\naccuracy improvement among the configurations, illustrating its strong synergy with KGs\nand RL agents. Conversely, without these tools, its accuracy substantially decreases to\nthe baseline of 52.18%. For the Llama3 70b model, the highest accuracy reached was\n97.21% with Graph-RAG Targeted and RL Agent, showing a slight accuracy increase\nfrom its baseline of 79.93%. This model also exhibited the highest consistency across\ndifferent configurations. The GPT-4o model showed improvements as well, reaching an\naccuracy of 86.48% with Graph-RAG Comprehensive and RL Agent, which is an in-\ncrease of approximately 7.25% over its baseline of 79.23%. These results highlight the\nsignificant impact of utilizing KGs and RL agents in enhancing the performance of ma-\nchine learning models, especially in tasks that involve complex document analysis such\nas information extraction from backhoe invoices.\n\n4.3. Discussion\n\nThis study aligns with the growing body of research exploring the potential of LLMs to\naddress NLP challenges. Although these models are capable of handling a wide range\nof tasks without the need for specialized data, in more specific cases, they show signifi-\ncant limitations due to the lack of fine-tuning, especially in smaller versions. LLMs face\nsubstantial limitations in their reasoning abilities, particularly when dealing with tasks\ninvolving multiple languages. In these scenarios, current LLMs still do not outperform\napproaches that utilize RL, whether through techniques like Proximal Policy Optimization\n(PPO), Trust Region Policy Optimization (TRPO), or Deep Deterministic Policy Gradient\n(DDPG), which require deep model adjustments, making their application considerably\ncostly, or through RL techniques that provide textual feedback, as explored in this work.\nConsequently, approaches like Graph-RAG or RL with textual feedback are more viable\nin terms of cost and complexity.\n\nThe combination of Graph-RAG and RL, or even just one of these techniques, is\nmore relevant for smaller models, which benefit from instructions with context and more\ndetailed guidance, while larger models tend to perform better with more concise data or, in\n\n7.3%17.3%43.5%Accuracy0GPT-4oLlama3 8BLlama3 70B255075100%97Difference between Base and highest resultBase (No context \nand no RL agent)Graph-RAG\nComprehensiveRL AgentGraph-RAG\nComprehensive \n+ RL AgentGraph-RAG \nTargetedGraph-RAG \nTargeted \n+ RL Agent\fsome cases, no additional data at all. Even with the application of techniques like Graph-\nRAG, larger models maintain high effectiveness in English but exhibit performance drops\nwhen applied to multilingual datasets.\n\n5. Related Works\n\nRecent studies combining LLMs with KGs have focused on models like OpenAI’s GPT-\n3.5 and Meta’s Llama. GPT-3.5 has been applied in areas such as engineering ed-\nucation [Yang et al. 2023], text classification [Shi et al. 2023], and node classification\nin graph structures [Li et al. 2024]. GPT-4 has been used in recommendation systems\nand biomedical studies [Xu et al. 2024, Guan et al. 2023]. Meta’s Llama2 models have\nshown effectiveness in processing complex graphs, with applications in vision sys-\ntems, academic databases, and digital news domains [Gouidis et al. 2024, Hu et al. 2024,\nWu et al. 2024]. Chain of Thought (CoT) prompting and GNN techniques have also\nbeen integrated with LLMs for improving model interpretability and processing struc-\ntured knowledge from KGs [Guan et al. 2023, Xu et al. 2023]. Techniques like PCA,\nUMAP, and prompt methods further integrate LLMs into the visual and structural do-\nmains of KGs, enhancing zero-shot learning [Gouidis et al. 2024, Alfasi et al. 2024]. In\nRL, approaches like RLHF and RLAIF have demonstrated improvements in summa-\nrization, negotiation dialogues, and domain knowledge applications [Roit et al. 2023,\nKwon et al. 2024, Mandi et al. 2023]. Although effective, RLHF and finetuning are ex-\npensive and nearly unfeasible for most experiments due to the significant computational\nand financial resources required [Ouyang et al. 2022, Nguyen et al. 2023]. Persistent is-\nsues like biases, toxicity, and hallucinations remain critical in both KGs and RL con-\ntexts [Gouidis et al. 2024, Xu et al. 2024, McKenna et al. 2023]. Differently from pre-\nvious works, our study addresses scalability high costs associated with the use of very\nlarge model and traditional techniques fine-tuning, by combineing RAG with RL. We\ndemonstrate the effectiveness of this approach for multi-label classification and informa-\ntion extraction using domain-specific KGs and datasets.\n\n6. Conclusions and Future Work\n\nThis study demonstrates the feasibility and effectiveness of integrating LLMs with Graph-\nRAG to enhance multi-label classification and information extraction. Experiments con-\nducted with variations of the GPT and Llama models, combined with the use of KGs and\nan RL agent, revealed significant improvements in the performance of smaller models,\nsuch as Llama3 8b, especially when combined with Graph-RAG. The combination of\nsmaller LLMs and Graph-RAG reduces the occurrence of “hallucinations”, contributing\nto superior accuracy and effectiveness, even in multilingual contexts. These outcomes\nsuggest a promising future for not so large LLM’s, especially in organizations facing data\nprivacy constraints and computational resource limitations. As future research directions,\nwe envision the exploitation of more diverse KGs and the investigation of RL techniques\nto further improve results of complex tasks. Furthermore, additional studies could apply\nour proposal to low resource languages, for expanding its accessibility and applicability.\n\nAcknowledgements: This work was supported by a 2022 CNPq Universal grant, FAPESC grant\n2021TR1510, the Print CAPES-UFSC Automation 4.0 Project, and indirectly by the Céos project,\nfinanced by the Public Ministry of Santa Catarina State (MPSC).\n\n\fReferences\n\n[Alfasi et al. 2024] Alfasi, D., Shapira, T., and Barr, A. B. (2024). Unveiling hidden links\n\nbetween unseen security entities. arXiv preprint arXiv:2403.02014.\n\n[Alizadeh et al. 2023] Alizadeh, K., Mirzadeh, I., Belenko, D., Khatamifard, K., Cho,\nM., Del Mundo, C. C., Rastegari, M., and Farajtabar, M. (2023). Llm in a flash:\narXiv preprint\nEfficient large language model inference with limited memory.\narXiv:2312.11514.\n\n[Beckhauser and Fileto 2024] Beckhauser, W. and Fileto, R. (2024). Can a simple customer\nreview outperform a feature set for predicting churn? In Anais do XXXIX Simpósio\nBrasileiro de Bancos de Dados, pages 117–128, Porto Alegre, RS, Brasil. SBC.\n\n[Bruno et al. 2023] Bruno, A., Mazzeo, P. L., Chetouani, A., Tliba, M., and Kerkouri,\nInsights into classifying and mitigating llms’ hallucinations. arXiv\n\nM. A. (2023).\narXiv:2311.08117.\n\n[Chen et al. 2024] Chen, J., Xiao, S., Zhang, P., Luo, K., Lian, D., and Liu, Z. (2024). Bge\nm3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings\nthrough self-knowledge distillation. ArXiv, abs/2402.03216.\n\n[Erickson 2018] Erickson, A. (2018). Comparative analysis of the eu’s gdpr and brazil’s\n\nlgpd: Enforcement challenges with the lgpd. Brook. J. Int’l L., 44:859.\n\n[Gao et al. 2023] Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P.,\nHe, C., Yue, X., et al. (2023). Llama-adapter v2: Parameter-efficient visual instruction\nmodel. arXiv preprint arXiv:2304.15010.\n\n[Gouidis et al. 2024] Gouidis, F., Papantoniou, K., Patkos, K. P. T., Argyros, A., and\nPlexousakis, D. (2024). Fusing domain-specific content from large language mod-\nels into knowledge graphs for enhanced zero shot object state classification. arXiv\narXiv:2403.12151.\n\n[Guan et al. 2023] Guan, Z., Wu, Z., Liu, Z., Wu, D., Ren, H., Li, Q., Li, X., and Liu, N.\n(2023). Cohortgpt: An enhanced gpt for participant recruitment in clinical study. arXiv\npreprint arXiv:2307.11346.\n\n[Hartmann et al. 2023] Hartmann, J., Heitmann, M., Siebert, C., and Schamp, C. (2023).\nMore than a feeling: Accuracy and application of sentiment analysis. International\nJournal of Research in Marketing, 40(1):75–87.\n\n[He et al. 2023] He, X., Bresson, X., Laurent, T., Perold, A., LeCun, Y., and Hooi, B.\n(2023). Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed\ngraph representation learning. In ICLR.\n\n[Hu et al. 2024] Hu, S., Zou, G., Yang, S., Zhang, B., and Chen, Y. (2024). Large lan-\nguage model meets graph neural network in knowledge distillation. arXiv preprint\narXiv:2402.05894.\n\n[Krugmann and Hartmann 2024] Krugmann, J. O. and Hartmann, J. (2024). Sentiment anal-\n\nysis in the age of generative ai. Customer Needs and Solutions, 11(1):1–19.\n\n[Kwon et al. 2024] Kwon, D., Weiss, E., Kulshrestha, T., Chawla, K., Lucas, G. M., and\nGratch, J. (2024). Are llms effective negotiators? systematic evaluation of the multi-\nfaceted capabilities of llms in negotiation dialogues. arXiv preprint arXiv:2402.13550.\n\n\f[Li et al. 2024] Li, R., Li, J., Han, J., and Wang, G. (2024). Similarity-based neighbor\n\nselection for graph llms. arXiv preprint arXiv:2402.03720.\n\n[Liu et al. 2024] Liu, X., Li, P., Huang, H., Li, Z., Cui, X., Liang, J., Qin, L., Deng, W., and\nHe, Z. (2024). Fakenewsgpt4: Advancing multimodal fake news detection through\nknowledge-augmented lvlms. arXiv preprint arXiv:2403.01988.\n\n[Liu et al. 2023] Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J.\n\n(2023). Gpt understands, too. AI Open.\n\n[Mandi et al. 2023] Mandi, Z., Jain, S., and Song, S. (2023). Roco: Dialectic multi-robot\n\ncollaboration with large language models. arXiv preprint arXiv:2307.04738.\n\n[McKenna et al. 2023] McKenna, N., Li, T., Cheng, L., Hosseini, M. J., Johnson, M., and\nSteedman, M. (2023). Sources of hallucination by large language models on inference\ntasks. arXiv preprint arXiv:2305.14552.\n\n[Nguyen et al. 2023] Nguyen, H. A., Stec, H., Hou, X., Di, S., and McLaren, B. M. (2023).\nEvaluating chatgpt’s decimal skills and feedback generation in a digital learning game.\nIn European Conference on Technology Enhanced Learning, pages 278–293. Springer.\n\n[Ouyang et al. 2022] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin,\nP., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models\nto follow instructions with human feedback. Advances in neural information process-\ning systems, 35:27730–27744.\n\n[Pan et al. 2023] Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J., and Wu, X. (2023). Unify-\ning large language models and knowledge graphs: A roadmap. ArXiv, abs/2306.08302.\n\n[Roit et al. 2023] Roit, P., Ferret, J., Shani, L., Aharoni, R., Cideron, G., Dadashi, R., Geist,\nM., Girgin, S., Hussenot, L., Keller, O., et al. (2023). Factually consistent summa-\nrization via reinforcement learning with textual entailment feedback. arXiv preprint\narXiv:2306.00186.\n\n[Shen et al. 2024] Shen, W., Li, C., Chen, H., Yan, M., Quan, X., Chen, H., Zhang, J., and\nHuang, F. (2024). Small llms are weak tool learners: A multi-llm agent. arXiv preprint\narXiv:2401.07324.\n\n[Shi et al. 2023] Shi, Y., Ma, H., Zhong, W., Tan, Q., Mai, G., Li, X., Liu, T., and Huang, J.\n(2023). Chatgraph: Interpretable text classification by converting chatgpt knowledge to\ngraphs. In 2023 IEEE International Conference on Data Mining Workshops (ICDMW),\npages 515–520. IEEE.\n\n[Shridhar et al. 2023] Shridhar, K., Sinha, K., Cohen, A., Wang, T., Yu, P., Pasunuru, R.,\nSachan, M., Weston, J., and Celikyilmaz, A. (2023). The art of llm refinement: Ask,\nrefine, and trust. arXiv preprint arXiv:2311.07961.\n\n[Team et al. 2023] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut,\nR., Schalkwyk, J., Dai, A. M., Hauth, A., et al. (2023). Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805.\n\n[Wang et al. 2023] Wang, Q., Gao, Z., and Xu, R. (2023). Graph agent: Explicit reasoning\n\nagent for graphs. arXiv preprint arXiv:2310.16421.\n\n\f[Wu et al. 2024] Wu, H., Zhang, Y., Han, Z., Hou, Y., Wang, L., Liu, S., Gong, Q., and Ge,\nY. (2024). Quartet logic: A four-step reasoning (qlfr) framework for advancing short\ntext classification. arXiv preprint arXiv:2401.03158.\n\n[Xu et al. 2024] Xu, D., Zhang, Z., Lin, Z., Wu, X., Zhu, Z., Xu, T., Zhao, X., Zheng, Y.,\nand Chen, E. (2024). Multi-perspective improvement of knowledge graph completion\nwith large language models. arXiv preprint arXiv:2403.01972.\n\n[Xu et al. 2023] Xu, H., Gao, Y., Hui, Z., Li, J., and Gao, X. (2023). Language knowledge-\nassisted representation learning for skeleton-based action recognition. arXiv preprint\narXiv:2305.12398.\n\n[Xue et al. 2024] Xue, F., Fu, Y., Zhou, W., Zheng, Z., and You, Y. (2024). To repeat or not\nto repeat: Insights from scaling llm under token-crisis. Advances in Neural Information\nProcessing Systems, 36.\n\n[Yang et al. 2023] Yang, Y., Chen, S., Zhu, Y., Zhu, H., and Chen, Z. (2023). Knowledge\ngraph empowerment from knowledge learning to graduation requirements achieve-\nment. Plos one, 18(10):e0292903.\n\n[Yao et al. 2024] Yao, Y., Duan, J., Xu, K., Cai, Y., Sun, Z., and Zhang, Y. (2024). A survey\non large language model (llm) security and privacy: The good, the bad, and the ugly.\nHigh-Confidence Computing, page 100211.\n\n\f"
        },
        {
            "titulo": "Identificação de aspectos explícitos e implícitos em críticas gastronômicas em português: avaliando o potencial dos LLMs",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31129",
            "idioma": "Português",
            "storage_key": "files/article_31129_30932.pdf",
            "autores": [
                {
                    "nome": "Luiz H. N. Silva",
                    "afiliacao": "IFSP",
                    "orcid": "http://orcid.org/0009-0007-2839-8259"
                },
                {
                    "nome": "Eloize R. M. Seno",
                    "afiliacao": "IFSP",
                    "orcid": "https://orcid.org/0000-0002-1549-9794"
                },
                {
                    "nome": "Rozane R. Rebechi",
                    "afiliacao": "UFRGS",
                    "orcid": "https://orcid.org/0000-0002-1878-7548"
                },
                {
                    "nome": "Helena M. Caseli",
                    "afiliacao": "UFSCar",
                    "orcid": "https://orcid.org/0000-0003-3996-8599"
                },
                {
                    "nome": "Fabiano M. Rocha Júnior",
                    "afiliacao": "IFSP",
                    "orcid": null
                },
                {
                    "nome": "Guilherme A. Faller",
                    "afiliacao": "UFRGS",
                    "orcid": null
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Análise de sentimentos baseada em aspectos",
                "aspectos explícitos e implícitos",
                "LLMs"
            ],
            "referencias": [
                "Almeida, T. S., Abonizio, H., Nogueira, R., and Pires, R. (2024). Sabiá-2: A new generation of portuguese large language models. ArXiv, abs/2403.09887.",
                "Assi, F. M., Candido, G. B., dos Santos Silva, L. N., Silva, D. F., and Caseli, H. M. (2022). Ufscar’s team at ABSAPT 2022: using syntax, semantics and context for solving the tasks. In Montes-y-Gomez, M. and et al., editors, Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022), volume 3202 of CEUR Workshop Proceedings. CEUR-WS.org.",
                "Balage Filho, P. P. (2017). Aspect extraction in sentiment analysis for portuguese language. PhD thesis, Sao Carlos - SP.",
                "Costa, R. and Pardo, T. (2020). Métodos baseados em léxico para extração de aspectos de opiniões em português. In Anais do IX Brazilian Workshop on Social Network Analysis and Mining, pages 61–72, Porto Alegre, RS, Brasil. SBC.",
                "Lopes, E., Correa, U., and Freitas, L. (2021). Exploring BERT for aspect extraction in portuguese language. The International FLAIRS Conference Proceedings, 34.",
                "Machado, M., Pardo, T., Ruiz, E., and Felippo, A. (2021). Learning rules for automatic identification of implicit aspects in portuguese. In Anais do XIII Simposio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 82–91, Porto Alegre, RS, Brasil. SBC.",
                "Machado, M. and Pardo, T. A. S. (2022). Evaluating methods for extraction of aspect terms in opinion texts in Portuguese - the challenges of implicit aspects. In Calzolari, N., Bechet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3819–3828, Marseille, France. European Language Resources Association.",
                "Machado, M. T. (2023). Methods for identifying aspects in opinion texts in Portuguese: the case of implicit aspects and their typological analysis. PhD thesis, Sao Carlos - SP.",
                "Oliveira, A., Cecote, T., Silva, P., Gertrudes, J., Freitas, V., and Luz, E. (2023). How good is ChatGPT for detecting hate speech in portuguese? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 94–103, Porto Alegre, RS, Brasil. SBC.",
                "Pereira, D. A. (2021). A survey of sentiment analysis in the portuguese language. Artificial Intelligence Review, 54(2):1087–1115.",
                "Rebechi, R. R., Nunes, R. R., Munhoz, L. R., and Marcon, N. O. (2021). Restaurant reviews in Brazil and the USA: A feast of cultural differences and their impact on translation. Mutatis Mutandis. Revista Latinoamericana de Traduccion, 14:372–396.",
                "Resplande, J., Garcia, E., Junior, A., Rodrigues, R., Silva, D., Maia, D., Da Silva, N., Filho, A., and Soares, A. (2022). Deep learning Brasil at ABSAPT 2022: Portuguese transformer ensemble approaches. In Montes-y-Gomez, M. and et al., editors, Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022), volume 3202 of CEUR Workshop Proceedings. CEUR-WS.org.",
                "Santos, W. and Paraboni, I. (2023). Predição de transtorno depressivo em redes sociais: BERT supervisionado ou ChatGPT zero-shot? In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 11–21, Porto Alegre, RS, Brasil. SBC.",
                "Schouten, K. and Frasincar, F. (2016). Survey on aspect-level sentiment analysis. IEEE Transactions on Knowledge and Data Engineering, 28(3):813–830.",
                "Seno, E., Silva, L., Anno, F., Rocha, F., and Caseli, H. (2024). Aspect-based sentiment analysis in comments on political debates in Portuguese: evaluating the potential of ChatGPT. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G., and Amaro, R., editors, Computational Processing of the Portuguese Language: 16th Conference, PROPOR 2024, pages 312–320, Santiago de Compostela, Galicia/Spain. Association for Computational Lingustics.",
                "Soni, P. K. and Rambola, R. (2022). A survey on implicit aspect detection for sentiment analysis: Terminology, issues, and scope. IEEE Access, 10:63932–63957.",
                "Vargas, F. A. and Pardo, T. A. S. (2018). Aspect clustering methods for sentiment analysis. In Computational Processing of the Portuguese Language: 13th International Conference, PROPOR 2018, Canela, Brazil, September 24–26, 2018, Proceedings, page 365–374, Berlin, Heidelberg. Springer-Verlag.",
                "Vargas, F. A. and Pardo, T. A. S. (2020). Linguistic rules for fine-grained opinion extraction. proceedings of the 14th International AAAI Conference on Web and Social Media, 2020. Zhang, L., Wang, S., and Liu, B. (2018). Deep learning for sentiment analysis : A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8",
                "Zhang, L., Wang, S., and Liu, B. (2018). Deep learning for sentiment analysis : A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8."
            ],
            "artigo_completo": "Identificac¸ ˜ao de aspectos expl´ıcitos e impl´ıcitos em cr´ıticas\ngastronˆomicas em portuguˆes: avaliando o potencial dos LLMs\n\nLuiz H. N. Silva1, Eloize R. M. Seno1, Rozane R. Rebechi2, Helena M. Caseli3\nFabiano M. Rocha J ´unior1, Guilherme A. Faller2\n\n1Instituto Federal de S˜ao Paulo (IFSP) – S˜ao Carlos, SP\n2 Depto de Letras – Universidade Federal do Rio Grande do SUL – Porto Alegre, RS\n3 Depto de Ciˆencia da Computac¸ ˜ao – Universidade Federal de S˜ao Carlos – S˜ao Carlos, SP\n{nascimento.henrique1, fabiano.j}@aluno.ifsp.edu.br, guilfaller@gmail.com\n\neloize@ifsp.edu.br, rozanereb@gmail.com, helenacaseli@ufscar.br\n\nAbstract. Aspect identification is a fundamental step in Aspect-Based Sentiment\nAnalysis (ABSA), which involves detecting the opinion target aspects in product\nor service reviews published on social media. Although there are many works\ndeveloped for detecting aspects in English, there are few studies in this area for\nPortuguese, and LLMs have been little explored. Given this context, this rese-\narch investigated the potential use of LLMs for aspect identification in culinary\nreviews in Portuguese.\n\nResumo. A identificac¸ ˜ao de aspectos ´e uma etapa fundamental da An´alise de\nSentimentos Baseada em Aspectos (ASBA) que consiste em detectar os aspectos\nalvos de opini˜ao em avaliac¸ ˜oes de produtos ou servic¸os publicadas nas m´ıdias\nsociais. Enquanto existem v´arios estudos focados na detecc¸ ˜ao de aspectos na\nl´ıngua inglesa, para o portuguˆes h´a poucos trabalhos na ´area e os LLMs pra-\nticamente n˜ao tˆem sido explorados. Dado esse contexto, esta pesquisa investi-\ngou o potencial de uso de LLMs na identificac¸ ˜ao de aspectos em cr´ıticas gas-\ntronˆomicas em portuguˆes.\n\n1. Introduc¸ ˜ao\n\nA an´alise de sentimentos baseada em aspectos (ASBA) ´e uma sub´area da An´alise de\nSentimentos (AS) que busca identificar e analisar opini˜oes e sentimentos relacionados a\naspectos ou atributos espec´ıficos de uma entidade, produto ou servic¸o. Em uma avaliac¸ ˜ao\nde um restaurante, por exemplo, aspectos como “comida”, “servic¸o” e “prec¸o’ podem ser\nanalisados individualmente, permitindo uma compreens˜ao mais detalhada das opini˜oes\ndos clientes sobre cada um deles.\n\nA ASBA representa o n´ıvel mais complexo da an´alise autom´atica, devido `a difi-\nculdade de se modelar as conex˜oes semˆanticas entre um determinado aspecto (termo) e as\npalavras que fazem parte do seu contexto [Zhang et al. 2018]. Uma etapa fundamental da\nASBA consiste na identificac¸ ˜ao de aspectos, os quais podem ser expl´ıcitos ou impl´ıcitos,\nde acordo com a literatura [Schouten and Frasincar 2016, Soni and Rambola 2022]. En-\nquanto o aspecto expl´ıcito ocorre diretamente no texto, o aspecto impl´ıcito n˜ao ´e men-\ncionado explicitamente, mas pode ser inferido pelo contexto. Por exemplo, na avaliac¸ ˜ao\nde um restaurante “A comida estava deliciosa, mas demorou muito para chegar.”, temos\num aspecto expl´ıcito “comida” com sentimento positivo e um aspecto impl´ıcito “servic¸o”\ncom sentimento negativo.\n\n\fEnquanto para o inglˆes h´a uma vasta literatura relacionada `a detecc¸ ˜ao de as-\npectos [Schouten and Frasincar 2016, Zhang et al. 2018, Soni and Rambola 2022], para\no portuguˆes as pesquisas ainda s˜ao emergentes [Pereira 2021]. Al´em disso, os traba-\nlhos existentes se baseiam principalmente no uso de regras, l´exicos e em algoritmos\nde aprendizado de m´aquina, sendo que o uso de modelos de linguagem em larga es-\n`A medida\ncala (Large Language Model – LLM, no inglˆes) tem sido pouco explorado.\nque o interesse p´ublico por modelos generativos pr´e-treinados, como os modelos da\nOpenAI, continua a crescer, espera-se que a utilidade desses modelos em resolver ta-\nrefas de PLN seja investigada. E nesse sentido, algumas iniciativas recentes tˆem surgido\n[Oliveira et al. 2023, Santos and Paraboni 2023].\n\nDado esse contexto, este estudo investigou a potencialidade de cinco LLMs na\nidentificac¸ ˜ao de aspectos expl´ıcitos e impl´ıcitos em cr´ıticas gastronˆomicas em portuguˆes.\nCr´ıticas gastronˆomicas s˜ao textos escritos por cr´ıticos profissionais da gastronomia com\nexperiˆencia em avaliar restaurantes, pratos e experiˆencias culin´arias. A escolha desse\ndom´ınio se justifica pelo fato de que as cr´ıticas gastronˆomicas, at´e onde se sabe, ainda\nn˜ao foram exploradas no contexto da ASBA em portuguˆes.\n\n2. Trabalhos Relacionados\n\nde\n\nl´exicos\n\nregras de\n\nno uso de\n\nem algoritmos\n\n[Costa and Pardo 2020],\n\nOs trabalhos de identificac¸ ˜ao de aspectos para o portuguˆes se baseiam, prin-\nlinguagem\ncipalmente,\n[Vargas and Pardo 2020, Machado et al. 2021],\naprendizado\nde m´aquina tradicionais\n[Balage Filho 2017, Vargas and Pardo 2018] e no uso\nde deep learning [Lopes et al. 2021, Assi et al. 2022, Machado and Pardo 2022,\nResplande et al. 2022]). Em [Resplande et al. 2022], por exemplo, os autores avaliaram\no uso de modelos baseados em Transformers na extrac¸ ˜ao de aspectos em avaliac¸ ˜oes\nde hot´eis. Os aspectos extra´ıdos foram classificados, posteriormente, como positivos,\nnegativos ou neutros usando o LLM GPT-3. Em um trabalho anterior [Seno et al. 2024],\no GPT-3.5 Turbo foi empregado na tarefa de detecc¸ ˜ao de aspectos e classificac¸ ˜ao\nde polaridade em coment´arios do dom´ınio pol´ıtico. Em [Machado 2023], os autores\ncompararam o uso de LLMs – GPT-3.5, Maritaca e Llama – com um modelo BERT\ne com v´arios classificadores tradicionais na identificac¸ ˜ao de aspectos em revis˜oes de\nprodutos eletrˆonicos, livros e hot´eis. Nos experimentos, os melhores resultados para os\naspectos expl´ıcitos foram obtidos pelo classificador CRF (o melhor F-score foi 81% para\nrevis˜oes de hot´eis)). Por´em, para os aspectos impl´ıcitos o melhor resultado, em termos\nde porcentagem de acerto, foi obtido com o Llama 7B (52%).\n\nDe forma similar, este estudo tamb´em explorou o uso do GPT-3.5 e dos modelos\nda fam´ılia Maritaca na detecc¸ ˜ao de aspectos em cr´ıticas gastronˆomicas. Por´em, os mode-\nlos investigados aqui s˜ao variac¸ ˜oes mais recentes das vers˜oes usadas por [Machado 2023].\n\n3. Identificac¸ ˜ao de aspectos em Cr´ıticas Gastronˆomicas usando LLMs\n\nPara a identificac¸ ˜ao de aspectos em cr´ıticas gastronˆomicas foram explorados alguns dos\nLLMs mais populares da atualidade como o GPT-3.5 Turbo, o GPT-4o e GPT-4o mini1.\nSegundo a OpenAI2, o GPT-4o ´e o seu modelo mais avanc¸ado e inteligente para tarefas\n\n1https://platform.openai.com/docs/api-reference/introduction\n2https://platform.openai.com/docs/models\n\n\fTabela 1. Prompts usados na anotac¸ ˜ao de aspectos expl´ıcitos e impl´ıcitos.\nAspectos expl´ıcitos: Dada a sentenc¸a EXEMPLO com os alvos de opini˜oes expl´ıcitos, identifique os\nalvos de opini˜ao expl´ıcitos na sentenc¸a (se houver) no formato [e - alvo1], se n˜ao houver nenhum alvo,\nindique com um ’-’. EXEMPLO: “A pizza estava gostosa. E a sobremesa tamb´em.”. Sa´ıda: [e - pizza]\n[e - sobremesa]\nAspectos impl´ıcitos: Dada a sentenc¸a EXEMPLO com os alvos de opini˜oes impl´ıcitos, identifique os\nalvos de opini˜ao impl´ıcitos (se houver) no formato [i - alvo], se n˜ao houver nenhum alvo, indique com\num ’-’. EXEMPLO: “A pizza estava gostosa, mas era muito cara. Al´em disso, estava fria”. Sa´ıda: [i -\nprec¸o] [i - temperatura]\n\nmais complexas. O GPT-4o mini ´e o modelo mais avanc¸ado na categoria de modelos\npequenos, que tamb´em inclui o GPT-3.5 Turbo. Al´em desses LLMs, tamb´em foram in-\nvestigados dois modelos monol´ıngues treinados para o portuguˆes, o Sabi´a-2-medium e\no Sabi´a-3 3. Em experimentos reportados por [Almeida et al. 2024], o Sabi´a-2-medium\n´e comparado a v´arios outros LLMs, alcanc¸ando desempenho igual ou melhor que GPT-\n3.5 Turbo em v´arias an´alises. O Sabi´a-3, por sua vez, lanc¸ado em julho de 2024, at´e o\nmomento da escrita deste artigo n˜ao se tinha informac¸ ˜oes sobre o seu desempenho.\n\nTodos os LLMs s˜ao modelos generativos baseados em prompt, que recebem como\nentrada um texto (prompt) contendo a descric¸ ˜ao da tarefa a ser realizada e geram as sa´ıdas\nconforme solicitado. O grande desafio em lidar com esses modelos consiste em definir um\nprompt que gere as sa´ıdas exatamente como se espera para a tarefa. V´arios prompts dife-\nrentes foram testados para a identificac¸ ˜ao de aspectos expl´ıcitos e impl´ıcitos no corpus.\nForam experimentados prompts espec´ıficos para cada tipo de aspecto usando exemplos\nde anotac¸ ˜ao humana (i.e. abordagem few-shot) e sem o uso de exemplos de anotac¸ ˜ao (i.e.\nabordagem zero-shot). Contudo, percebeu-se uma facilidade maior dos modelos ao usar a\nabordagem few-shot. Assim, na anotac¸ ˜ao do corpus foram adotados os prompts apresen-\ntados na Tabela 1. Em todos os LLMs investigados a temperatura foi ajustada em zero,\na fim de obter modelos mais determin´ısticos, conforme apontado por outros trabalhos da\nliteratura [Oliveira et al. 2023, Santos and Paraboni 2023].\n\n4. Corpus\n\nPara os experimentos foi usado um conjunto de 1005 sentenc¸as extra´ıdas do corpus de\ncr´ıticas gastronˆomicas de [Rebechi et al. 2021]. Cada sentenc¸a foi anotada por 5 ano-\ntadores humanos, todos pesquisadores da ´area de PLN, em duas etapas. Primeiramente\nos anotadores classificaram as sentenc¸as em opinativa ou factual. Em seguida, aspectos\nexpl´ıcitos e impl´ıcitos foram anotados, em dupla/trio, para as 374 (37,2%) sentenc¸as con-\nsideradas opinativas pelos anotadores. Para estas, 432 aspectos foram identificados, sendo\n88,6% expl´ıcitos e 11,4% impl´ıcitos. A Tabela 2 apresenta exemplos de sentenc¸as com\nanotac¸ ˜ao de aspectos expl´ıcitos (em negrito) e impl´ıcitos.\n\nDado o fato de que n˜ao ´e poss´ıvel determinar todos os aspectos poss´ıveis para o\ncorpus, n˜ao foi poss´ıvel calcular o coeficiente Kappa para estimar a concordˆancia entre\nos anotadores. Embora n˜ao se tenha obtido uma estimativa da concordˆancia na anotac¸ ˜ao\ndo corpus, a busca pelo consenso, seguida da clara convergˆencia dos anotadores, permite\nassegurar que os aspectos identificados reproduzem de forma bastante fiel os aspectos que\ngeralmente s˜ao considerados na avaliac¸ ˜ao de uma experiˆencia gastronˆomica.\n\n3Dispon´ıveis por meio da MariTalk API como um chatbot.\n\n\fTabela 2. Exemplos de anotac¸ ˜ao de aspectos expl´ıcitos (em negrito) e impl´ıcitos.\n\nSentenc¸a\nSe estiver sozinho, desista de tentar o omakassˆe (sequˆencia de iguarias decidi-\ndas e enviadas aos poucos pelo chef) — ele ´e gigante (para uma pessoa) e caro\n(42 itens, R$ 390).\nN˜ao ´e demais lembrar: a casa s´o aceita dinheiro ou cheque – costume fora de\nmoda, tamb´em trazido de outros tempos.\nCarta de vinhos: Excelente, com muitas opc¸ ˜oes argentinas para todos os bol-\nsos.\n\nImpl´ıcito\ntamanho; prec¸o\n\nforma de pagamento\n\nvariedade\nprec¸o\n\n(vinhos);\n\n5. Experimentos e Resultados\n\nA Tabela 3 apresenta os resultados obtidos por cada LLM na detecc¸ ˜ao de aspectos\nexpl´ıcitos e impl´ıcitos. O Sabi´a-medium-2 obteve o melhor F-score (48,37%) para os as-\npectos expl´ıcitos, alcanc¸ando tamb´em a maior cobertura (77,75%). Contudo, a maior pre-\ncis˜ao (40,90%) foi obtida pelo GPT-4o mini. J´a no que se refere aos aspectos impl´ıcitos,\nos resultados mostram uma grande dificuldade dos LLMs em identificar esse tipo de\naspecto. Vale mencionar que essa dificuldade tamb´em foi relatada pelos humanos na\nanotac¸ ˜ao do corpus. Como os aspectos impl´ıcitos s˜ao inferidos pelo contexto, nem sem-\npre ´e trivial perceber qual ´e o alvo de opini˜ao. Em alguns casos, essa inferˆencia exige um\nconhecimento mais especializado como no exemplo “Na boca, ´e equilibrado, com tani-\nnos firmes e boa estrutura.”, que se refere ao aspecto “vinho”. Para esse caso espec´ıfico,\napenas o modelo GPT-3.5 Turbo conseguiu identificar o aspecto impl´ıcito.\n\nTabela 3. Resultados obtidos para aspectos expl´ıcitos e impl´ıcitos.\n\nLLM\nSabi´a-2-medium 35,11%\n31,53%\nGPT-3.5 turbo\nSabi´a-3\n33,21%\nGPT-4o\n21,51%\n40,90%\nGPT-4o mini\n\nExpl´ıcitos\nPrecis˜ao Cobertura\n\nF-score\n77,75% 48,37% 1,60%\n1,95%\n44,60%\n76,18%\n2,21%\n44,58%\n67,80%\n3,90%\n33,54%\n76,18%\n2,23%\n22,81%\n15,82%\n\nImpl´ıcitos\nPrecis˜ao Cobertura F-score\n3,01%\n26,00%\n3,68%\n32,00%\n4,13%\n32,00%\n7,00%\n34,00%\n4,01%\n20,00%\n\n6. Conclus˜oes\n\nEste estudo investigou o uso de LLMs na detecc¸ ˜ao de aspectos em cr´ıticas gastronˆomicas.\nNos experimentos, o LLM monol´ıngue Sabi´a-2-medium mostrou um potencial maior na\ndetecc¸ ˜ao de aspectos expl´ıcitos, do que os modelos multil´ıngues analisados. Enquanto\nque o Sabi´a-3, tamb´em monol´ıngue, mostrou-se equivalente ao GPT-3.5 Turbo, supe-\nrando o GPT-4o e o GPT-4o mini. Al´em de apresentarem desempenho superior ou equi-\nvalente aos obtidos pelos modelos multil´ıngues, os modelos monol´ıngues s˜ao bem mais\nacess´ıveis4. Com relac¸ ˜ao aos aspectos impl´ıcitos, todos os LLMs tiveram bastante dificul-\ndade em identificar esse tipo de aspecto. O melhor desempenho foi obtido pelo GPT-4o\n(7% de F-score).\n\nComo trabalhos futuros, pretende-se investigar a combinac¸ ˜ao de LLMs para a\ntarefa de identificac¸ ˜ao de aspectos, bem como a utilizac¸ ˜ao de conhecimento do dom´ınio\nde cr´ıticas gastronˆomicas para enriquecer os prompts.\n\n4Os valores podem ser consultados em https://openai.com/api/pricing/ e https://\n\nwww.maritaca.ai/\n\n\fReferˆencias\n\nAlmeida, T. S., Abonizio, H., Nogueira, R., and Pires, R. (2024). Sabi´a-2: A new genera-\n\ntion of portuguese large language models. ArXiv, abs/2403.09887.\n\nAssi, F. M., Candido, G. B., dos Santos Silva, L. N., Silva, D. F., and Caseli, H. M. (2022).\nUfscar’s team at ABSAPT 2022: using syntax, semantics and context for solving the\ntasks. In Montes-y-G´omez, M. and et al., editors, Proceedings of the Iberian Langua-\nges Evaluation Forum (IberLEF 2022), volume 3202 of CEUR Workshop Proceedings.\nCEUR-WS.org.\n\nBalage Filho, P. P. (2017). Aspect extraction in sentiment analysis for portuguese lan-\n\nguage. PhD thesis, S˜ao Carlos - SP.\n\nCosta, R. and Pardo, T. (2020). M´etodos baseados em l´exico para extrac¸ ˜ao de aspectos de\nopini˜oes em portuguˆes. In Anais do IX Brazilian Workshop on Social Network Analysis\nand Mining, pages 61–72, Porto Alegre, RS, Brasil. SBC.\n\nLopes, E., Correa, U., and Freitas, L. (2021). Exploring BERT for aspect extraction in\n\nportuguese language. The International FLAIRS Conference Proceedings, 34.\n\nMachado, M., Pardo, T., Ruiz, E., and Felippo, A. (2021). Learning rules for automatic\nidentification of implicit aspects in portuguese. In Anais do XIII Simp´osio Brasileiro\nde Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 82–91, Porto Alegre,\nRS, Brasil. SBC.\n\nMachado, M. and Pardo, T. A. S. (2022). Evaluating methods for extraction of aspect\nterms in opinion texts in Portuguese - the challenges of implicit aspects. In Calzolari,\nN., B´echet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H.,\nMaegaard, B., Mariani, J., Mazo, H., Odijk, J., and Piperidis, S., editors, Proceedings\nof the Thirteenth Language Resources and Evaluation Conference, pages 3819–3828,\nMarseille, France. European Language Resources Association.\n\nMachado, M. T. (2023). Methods for identifying aspects in opinion texts in Portuguese:\nthe case of implicit aspects and their typological analysis. PhD thesis, S˜ao Carlos - SP.\n\nOliveira, A., Cecote, T., Silva, P., Gertrudes, J., Freitas, V., and Luz, E. (2023). How good\nis ChatGPT for detecting hate speech in portuguese? In Anais do XIV Simp´osio Bra-\nsileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 94–103, Porto\nAlegre, RS, Brasil. SBC.\n\nPereira, D. A. (2021). A survey of sentiment analysis in the portuguese language. Artifi-\n\ncial Intelligence Review, 54(2):1087–1115.\n\nRebechi, R. R., Nunes, R. R., Munhoz, L. R., and Marcon, N. O. (2021). Restaurant\nreviews in Brazil and the USA: A feast of cultural differences and their impact on\ntranslation. Mutatis Mutandis. Revista Latinoamericana de Traducci´on, 14:372–396.\n\nResplande, J., Garcia, E., Junior, A., Rodrigues, R., Silva, D., Maia, D., Da Silva, N.,\nFilho, A., and Soares, A. (2022). Deep learning Brasil at ABSAPT 2022: Portuguese\ntransformer ensemble approaches. In Montes-y-G´omez, M. and et al., editors, Proce-\nedings of the Iberian Languages Evaluation Forum (IberLEF 2022), volume 3202 of\nCEUR Workshop Proceedings. CEUR-WS.org.\n\n\fSantos, W. and Paraboni, I. (2023). Predic¸ ˜ao de transtorno depressivo em redes sociais:\nBert supervisionado ou ChatGPT zero-shot? In Anais do XIV Simp´osio Brasileiro de\nTecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 11–21, Porto Alegre, RS,\nBrasil. SBC.\n\nSchouten, K. and Frasincar, F. (2016). Survey on aspect-level sentiment analysis. IEEE\n\nTransactions on Knowledge and Data Engineering, 28(3):813–830.\n\nSeno, E., Silva, L., Anno, F., Rocha, F., and Caseli, H. (2024). Aspect-based sentiment\nanalysis in comments on political debates in Portuguese: evaluating the potential of\nChatGPT. In Gamallo, P., Claro, D., Teixeira, A., Real, L., Garcia, M., Oliveira, H. G.,\nand Amaro, R., editors, Computational Processing of the Portuguese Language: 16th\nConference, PROPOR 2024, pages 312–320, Santiago de Compostela, Galicia/Spain.\nAssociation for Computational Lingustics.\n\nSoni, P. K. and Rambola, R. (2022). A survey on implicit aspect detection for sentiment\n\nanalysis: Terminology, issues, and scope. IEEE Access, 10:63932–63957.\n\nVargas, F. A. and Pardo, T. A. S. (2018). Aspect clustering methods for sentiment analy-\nIn Computational Processing of the Portuguese Language: 13th International\nsis.\nConference, PROPOR 2018, Canela, Brazil, September 24–26, 2018, Proceedings,\npage 365–374, Berlin, Heidelberg. Springer-Verlag.\n\nVargas, F. A. and Pardo, T. A. S. (2020). Linguistic rules for fine-grained opinion ex-\ntraction. proceedings of the 14th International AAAI Conference on Web and Social\nMedia, 2020.\n\nZhang, L., Wang, S., and Liu, B. (2018). Deep learning for sentiment analysis : A survey.\n\nWiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8.\n\n\f"
        },
        {
            "titulo": "TableRAG: A Novel Approach for Augmenting LLMs with Information from Retrieved Tables",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31130",
            "idioma": "Inglês",
            "storage_key": "files/article_31130_30933.pdf",
            "autores": [
                {
                    "nome": "Elvis A. de Souza",
                    "afiliacao": "PUC-Rio",
                    "orcid": "http://orcid.org/0000-0001-9373-7412"
                },
                {
                    "nome": "Patricia F. da Silva",
                    "afiliacao": "Petrobras",
                    "orcid": "https://orcid.org/0009-0004-3415-1904"
                },
                {
                    "nome": "Diogo Gomes",
                    "afiliacao": "Petrobras",
                    "orcid": "https://orcid.org/0000-0001-9351-7647"
                },
                {
                    "nome": "Vitor Batista",
                    "afiliacao": "Petrobras",
                    "orcid": "https://orcid.org/0000-0002-9095-0266"
                },
                {
                    "nome": "Evelyn Batista",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0000-0001-6672-8318"
                },
                {
                    "nome": "Marco Pacheco",
                    "afiliacao": "PUC-Rio",
                    "orcid": "https://orcid.org/0000-0002-2381-0183"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "We present TableRAG, a novel pipeline designed to integrate tabular data into traditional Retrieval-Augmented Generation (RAG) systems. Our approach is composed of three main parts: (i) generating textual representations of tables; (ii) indexing table representations in vector databases for retrieval, and (iii) employing large language models to generate SQL or Python code for data manipulation over a given table. We assessed the effectiveness of TableRAG by comparing retrieval and re-ranking accuracies over the OTT-QA benchmark and by utilizing both open and closed-source LLMs to generate code for answering questions from the WikiTableQuestions benchmark. Our best results show 86.7% HITS@5 for retrieval and 74% accuracy for Q&A, demonstrating the feasibility of integrating tabular data into RAG systems with high accuracy.",
            "keywords": [
                "retrieval augmented generation",
                "tabular data",
                "information retrieval",
                "generative AI",
                "large language models"
            ],
            "referencias": [
                "Abraham, A. N., Rahman, F., and Kaur, D. (2022). Tablequery: Querying tabular data with natural language. arXiv preprint arXiv:2202.00454.",
                "Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., Noune, B., Pannier, B., and Penedo, G. (2023). Falcon-40B: an open large language model with state-of-the-art performance.",
                "Anand, Y., Nussbaum, Z., Duderstadt, B., Schmidt, B., and Mulyar, A. (2023). Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo.",
                ".",
                "Chen, W., Chang, M.-W., Schlinger, E., Wang, W., and Cohen, W. W. (2020a). Open question answering over tables and text. arXiv preprint arXiv:2010.10439.",
                "Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H., and Wang, W. (2020b). Hybridqa: A dataset of multi-hop question answering over tabular and textual data. arXiv preprint arXiv:2004.07347.",
                "Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V.,Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2019). Unsupervised cross-lingual representation learning at scale. CoRR, abs/1911.02116.",
                "Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Chang, B., Sun, X., Li, L., and Sui, Z. (2024). A survey on in-context learning.",
                "Dubey, A. et al. (2024). The llama 3 herd of models.",
                "Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H. (2023). Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.",
                "Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. (2023). Mistral 7b.",
                "Kandpal, N., Deng, H., Roberts, A., Wallace, E., and Raffel, C. (2023). Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pages 15696–15707. PMLR.",
                "Liang, C., Norouzi, M., Berant, J., Le, Q. V., and Lao, N. (2018). Memory augmented policy optimization for program synthesis and semantic parsing. Advances in Neural Information Processing Systems, 31.",
                "Lin, X. V., Chen, X., Chen, M., Shi,W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et al. (2023). Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint arXiv:2310.01352.",
                "Liu, T., Wang, F., and Chen, M. (2024). Rethinking tabular data understanding with large language models. In Duh, K., Gomez, H., and Bethard, S., editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 450–482, Mexico City, Mexico. Association for Computational Linguistics.",
                "Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H. (2022). When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511.",
                "OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., et al. (2024). Gpt-4 technical report.",
                "Pasupat, P. and Liang, P. (2015). Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305.",
                "Tonmoy, S., Zaman, S., Jain, V., Rani, A., Rawte, V., Chadha, A., and Das, A. (2024). A comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313.",
                "Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., and Wei, F. (2024). Multilingual e5 text embeddings: A technical report. arXiv preprint arXiv:2402.05672.",
                "Yin, P., Neubig, G., Yih,W.-t., and Riedel, S. (2020). TaBERT: Pretraining for joint understanding of textual and tabular data. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J., editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–8426, Online. Association for Computational Linguistics.",
                "Yu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q., Roman, S., et al. (2018). Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887."
            ],
            "artigo_completo": "TableRAG: A Novel Approach for Augmenting LLMs with\nInformation from Retrieved Tables\n\nElvis A. de Souza1, Patricia F. da Silva2, Diogo Gomes2,\nVitor Batista2, Evelyn Batista1, Marco Pacheco1\n\n1Applied Computational Intelligence Lab. (ICA/PUC-Rio)\n\n2Petrobras\n\nAbstract. We present TableRAG, a novel pipeline designed to integrate tabular\ndata into traditional Retrieval-Augmented Generation (RAG) systems. Our ap-\nproach is composed of three main parts: (i) generating textual representations\nof tables; (ii) indexing table representations in vector databases for retrieval,\nand (iii) employing large language models to generate SQL or Python code for\ndata manipulation over a given table. We assessed the effectiveness of TableRAG\nby comparing retrieval and re-ranking accuracies over the OTT-QA benchmark\nand by utilizing both open and closed-source LLMs to generate code for answer-\ning questions from the WikiTableQuestions benchmark. Our best results show\n86.7% HITS@5 for retrieval and 74% accuracy for Q&A, demonstrating the\nfeasibility of integrating tabular data into RAG systems with high accuracy.\n\n1. Introduction\n\nIn recent years, the application of generative models in Question Answering (Q&A) sys-\ntems has gained substantial traction, particularly in enterprise settings where accurate and\nefficient information retrieval is crucial for decision-making, customer support, and oper-\national efficiency. Large Language Models (LLMs) have shown remarkable capabilities\nin generating natural language responses based on vast amounts of textual data, however,\nthese models are not without their challenges—one of the most significant being the issue\nof hallucination, where the model generates plausible-sounding but incorrect or nonsensi-\ncal answers [Kandpal et al. 2023, Gao et al. 2023, Lin et al. 2023, Tonmoy et al. 2024].\nThis problem becomes particularly pronounced when the required information is not\npurely textual but embedded in semi-structured data, such as tables, which can be stored\nin large and diversified databases, where the ability to accurately retrieve and interpret\nthem is essential.\n\nTo mitigate hallucination and to retrieve relevant information to a particular do-\nmain, a widely adopted paradigm known as Retrieval-Augmented Generation (RAG)\nis employed. This technique is based on the premise that LLMs are more likely to\nprovide accurate responses when supplied with relevant context at runtime, i.e., within\nthe prompt that defines the task instructions, in a strategy called in-context learning\n[Dong et al. 2024]. Traditional Information Retrieval (IR) methods are utilized to fetch\npertinent documents, which are then fed into the language model for text generation.\n\nIn this work, we propose a RAG pipeline that integrates tabular data within textual\ndatabases for information retrieval, using retrieved tables to generate reliable responses\n(Figure 1). Building upon traditional RAG pipelines, we implement a table retrieval mod-\nule based on the same vector similarity strategy commonly used for texts, and another\n\n\fFigure 1. Overview of the TableRAG Pipeline\n\nmodule for Q&A based on generating code to manipulate tables. We evaluate the results\nusing two well-known Q&A benchmarks with tabular data, Open Table-and-Text Ques-\ntion Answering (OTT-QA) [Chen et al. 2020a] for the retrieval part of our pipeline, and\nWikiTableQuestions [Pasupat and Liang 2015] for the Q&A part.\n\nOur results show that it is possible to incorporate tabular data into Retrieval-\nAugmented Generation systems in an efficient way by representing tables as texts, sim-\nilarly to how traditional retrieval information systems work. Moreover, we show that\ncode generation yields good results for obtaining answers from retrieved tables, and we\nhighlight the considerable promise for improving the performance of open-source LLMs,\nwhich could lead to the development of more robust, adaptable, and affordable RAG sys-\ntems, capable of managing multimodal data sources.\n\n2. Related Work\n\nUnlike Q&A over texts, which involves reading document excerpts to answer questions\nusing extractive or generative models, Q&A over tables involves additional factors. To\nanswer complex questions using tables, one must interpret the arrangement of rows and\ncolumns, perform filtering, joins, mathematical operations, and various other forms of\ntable manipulation.\n\nThere are at least three tasks that have been tested in the literature for Q&A with\ntabular data. First, parsing semantically compositional questions in order to determine the\nmanipulation steps that are needed to obtain a given information from a table. The Wik-\niTableQuestions benchmark [Pasupat and Liang 2015] addresses this task. It comprises\nsemi-structured tables, which may contain textual data, and each question may require\noperations such as table lookup, aggregation (counting records, summing numerical val-\nues, etc.), superlatives (finding the maximum or minimum value), arithmetic operations,\namong others, which need to be identified in the question in order to determine the ma-\nnipulations steps that are needed to answer it correctly.\n\nSecond, manipulating relational database tables using their relationships with\nother tables to join information from different sources.\nThe Spider benchmark\n[Yu et al. 2018] introduced this task. Its specificity involves the use of foreign keys, join-\ning multiple tables, and constructing nested SQL queries.\n\n\fThird, answering multi-hop open-domain questions that need reasoning over both\ntextual passages and tabular information. The Open Table-and-Text Question Answering\n(OTT-QA) benchmark [Chen et al. 2020a] introduced this task. Its peculiarity is twofold:\nthe retriever model must find the table that best answers the question from a large collec-\ntion of tables, and the reader model must simultaneously examine data from two different\nmodalities, texts and tables.\n\nFor the Q&A part, our approach seeks to address the challenges proposed by the\nWikiTableQuestions benchmark, using LLMs to generate code that correctly manipulates\ntables in order to find answers to semantically compositional questions. In 2015, when\nthe WikiTableQuestions benchmark was proposed, the approach used to solve the task\ninvolved converting tables into knowledge graphs and parsing the questions into logical\nform, followed by selecting the most probable graph candidates to answer the question\n[Pasupat and Liang 2015]. The results achieved were 37.1% accuracy, considering all\ncandidate answers, and 76.6%, considering that at least one of the candidate answers was\ncorrect.\n\nMore recent works address the WikiTableQuestions task using large language\nmodels.\n[Yin et al. 2020] achieved a result of 52.3% accuracy by pre-training a new\nmodel, called TaBERT, with data from 26 million tables and their respective natural lan-\nguage contexts [Liang et al. 2018]. The work by [Liu et al. 2024] used GPT-3.5 to trans-\npose tables to normalize them and then used the same model to perform two types of in-\nference: with direct prompting (DP), asking the model to reason about the table in textual\nform, and as a Python agent, asking the model to interact with a Python shell in up to five\ninteractions. They achieve state-of-the-art accuracy of 73.6% on WikiTableQuestions.\n\n3. TableRAG Pipeline Components\n\nThe instruction in Figure 2 is an example of the prompt directed to the language models\nfor generating Python code. In this prompt, we can observe, besides the request for code\ngeneration, how the tables are represented textually and in consideration of their metadata.\nThese procedures will be explained step by step in the following subsections.\n\nObtaining Textual Representation of Tables This part of our approach is inspired by\nthe work of [Abraham et al. 2022]. Given that it is not always feasible to load entire\ntables into memory, the authors employ several strategies to represent the table through\nits schema, ensuring that the SQL query to be created is based on this indirect form of\nrepresentation. The idea works particularly well in the context of Retrieval-Augmented\nGeneration, where the initial step is to retrieve specific documents from a diverse doc-\nument repository. In our case, we aim to retrieve the tables that best answer the user’s\nquery, using the same strategy for text indexing and retrieval.\n\nThe metadata generated for a table consists of a textual description, generated by\nan LLM, along with additional information such as column names, unique values, and\ndata types (numeric, textual, dates) for each table column. Other pieces of information,\nsuch as title of the table, paragraphs that explain it, acronyms meanings and so on can\nbe inserted here to enrich the textual description of each table. When the table preview is\npassed to the language model, it is placed as the final element of the prompt, so to truncate\nit if exceeding the input token limit that the model accepts.\n\n\fFigure 2. Prompt for Python Code Generation\n\nColumn Renaming: A common error we observed in the construction of Python code\nand SQL queries by LLMs is the use of non-existent columns, a particular case of hallu-\ncination. To address this issue, we implement a strategy to rename columns to temporary\nplaceholders, such as Col1, Col2, Col3, . . . ColN. This forces language models to utilize\ncolumns based on the description of the data type they contain, rather than their names.\n\nTable Indexing: The table is then indexed in a vector database through its textual repre-\nsentation. The idea is that the summary generated by the LLM should be sufficient for the\ntable to be retrieved, leveraging the same strategy used in text indexing, by transforming\ntable descriptions into dense contextual vectors.\n\nRetrieval, Re-ranking and Filtering: Table retrieval is performed via similarity mea-\nsures, such as cosine similarity, comparing the vector representation of the user’s query\nwith the vector representations of table descriptions. After retrieval, we employ a re-\nranking step by asking an LLM to reorder the retrieved tables by relevance, and a filtering\nstep, by asking it to filter the results, eliminating retrieved tables that despite their simi-\nlarity to the user’s query cannot directly answer the question.\n\nCode Generation: Next, we prompt an LLM to generate code to obtain the answer\nfrom the table, as exemplified in Figure 2. The code generation prompt was meticulously\n\n\fadjusted to avoid common pitfalls these models tend to encounter, such as attempting\nto perform operations on dataframe columns with missing values without first handling\nthese cells. These adjustments were based on the outputs of GPT-4, for both SQL and\nPython code generation, and improved our accuracy in 16% for the same model, being\nthe previously mentioned column renaming procedure one of the most beneficial changes\nwe have made.\n\nResponse Generation: The generated code is then executed and the output is passed to\nanother LLM, which is responsible for providing a natural language response to the user’s\nquestion, considering the question, the data, and the output of the executed code.\n\n4. Methodology\n\nTo evaluate the quality of our pipeline, we utilized two traditional benchmarks,\ncalled OTT-QA [Chen et al. 2020a], for the retrieval part, and WikiTableQuestions\n[Pasupat and Liang 2015], for the Q&A part. The tables from both datasets were sourced\nfrom Wikipedia.\n\nWhile the WikiTableQuestions dataset is ideal for our purpose of testing code\ngeneration for table manipulation, it is not ideal for evaluating the efficiency of our table\nretrieval module. This is because the questions are constructed in a closed-domain fash-\nion, meaning they can only be answered if it is already known which table they refer to.1\nTherefore, when using the benchmark to evaluate our Q&A capabilities, we provide the\nlanguage model with the correct table. To assess the quality of our retrieval and re-ranking\nmodules, we will be testing them against the OTT-QA benchmark.\n\nThe OTT-QA benchmark is composed of questions based on those from another\ndataset, HybridQA [Chen et al. 2020b], but they were adapted to become “decontextual-\nized,” making them open-domain and therefore suitable for testing retrieval systems. It\ncontains 400K+ tables to retrieve from and 45K human-annotated questions. We use all\nthe 2,214 questions in the development partition of the benchmark to test retrieval and\nre-ranking. We use HITS@K to measure performance, where K ranges from 1 to 5, and\nmeans whether the correct table for each question is in the top-K retrieved or re-ranked ta-\nbles. Re-ranking is done after the retrieval step, and we compare results with and without\nre-ranking.\n\nThe WikiTableQuestions benchmark contains 2,108 semi-structured tables and\n22,033 complex question-answer pairs. The questions and answers were constructed by\nhumans through crowdsourcing, with instructions to create questions that involve various\ntypes of operations required to answer them, as shown in the distribution in Figure 3.\n\nDue to operational constraints involving LLMs costs, all tests in the WikiTable-\nQuestions were performed on a sample of 200 questions from the test dataset. As noted,\nseveral adjustments were made to the LLM instructions and to the table indexing method\nto facilitate the inference of correct answers and the inference of code that did not gener-\nate exceptions. All the adjustments were based solely on the results observed for the train\n\n1For example, in the question “what is the first city sorted alphabetically?”, there is no indication of\nwhich table should be found–cities of what country? in what state? in what period of time?–, and it can\nonly be correctly answered if the table to which the question refers has already been identified.\n\n\fFigure 3. Operations required to answer a sample of 200 questions from Wik-\niTableQuestions. Source: [Pasupat and Liang 2015].\n\ndataset questions, thereby preventing any information leakage from test to train.\n\nThe answers in WikiTableQuestions are provided as lists of size greater than or\nequal to 1, with sizes greater than 1 when more than one term is expected in the predicted\nanswer for it to be considered correct. Thus, we used accuracy to measure the perfor-\nmance of our Q&A module, normalizing the strings in the dataset and the strings inferred\nby the models, to check if all the expected terms are present in the predicted answer.2\n\nIn addition to accuracy, we also considered the number of generated codes that\nproduced any runtime exceptions and the time taken to answer each question. In other\nwords, we are also testing the ability of these models to generate functional and correct\ncode in the shortest possible time.\n\nThe scenarios in which we tested our pipeline are as follows:\n\n• For generating table descriptions, re-ranking, and the final natural language re-\nsponse, we consistently used the GPT 3.5 Turbo model from OpenAI, due to its\ngood performance and low cost.\n\n• For generating dense vector representations of each table descriptions, we use em-\nbeddings generated by a model based on XLM-RoBERTa [Conneau et al. 2019]\nfine-tuned for information retrieval [Wang et al. 2024], which are then indexed in\nthe Elasticsearch platform3 and are retrieved using the cosine similarity strategy.\n\n• For generating table manipulation code, we tested:\n\n– Closed-source models,\n\nnamely GPT 3.5 Turbo\n\nand GPT 4\n\n[OpenAI et al. 2024], accessed from OpenAI API;\n\n– Open-source models available from the GPT4All hub [Anand et al. 2023],\nnamely LLaMa 3 8B Instruct [Dubey et al. 2024], Nous Hermes 2 Mis-\ntral DPO 7B [Jiang et al. 2023], Falcon 7B [Almazrouei et al. 2023], and\nGPT4All Snoozy 13B [Anand et al. 2023]. We used a single V100 32 GB\nGPU to infer responses with these models.\n\n• All code generation models were tested by generating SQL queries (executed us-\ning the Python library sqldf) and code for dataframe manipulation (executed\nusing the Python library pandas).\n\n2We use accuracy as defined in [Mallen et al. 2022], since other metrics could be harder to be applied\ndue to the nature of generative models answers, which tend to have a lot more words than the needed terms.\n\n3https://www.elastic.co/\n\n\f5. Results\n\nFigure 4 shows the performance of the retrieval and the re-ranking modules in TableRAG\npipeline to find the most suitable tables to answer questions from OTT-QA. Re-ranking is\napplied on top of the retrieved tables, and increases the results of retrieval in 3.7 points in\nHITS@1, when the table ranked first is the correct one. Re-ranking is still better than just\nretrieval when looking at HITS@2, but after that, just retrieval becomes better than when\nre-ranking is applied. When looking at HITS@5, the results from retrieval are 1.4 point\nbetter than those with re-ranking. This decrease happens mainly because of lower-ranked\ntables that the LLM judged that needed to be filtered out of the list during re-ranking,\nwhen they were in fact the correct ones.\n\nFigure 4. Comparison of Retrieval and Re-ranking Results over OTT-QA\n\nFigure 5 shows the performance of the different models tested for generating\nPython code or SQL queries to answer the WikiTableQuestions. The yellow bars rep-\nresent the accuracy value, while the orange ones represent the number of codes that pro-\nduced exception at runtime, both in % of questions. The green line represents the average\ntime in seconds taken to answer each question.\n\nThe best results come from OpenAI’s closed-source models, both GPT-4 and GPT-\n3.5. The disparity between the best closed-source model and the best open-source model\ncan be easily explained, among other factors, by the size of the models: while Nous\nHermes 2 has 7 billion parameters in its neural architecture and achieved 40% accuracy,\nGPT-4 is speculated to have over 1 trillion4, reaching 74%.\n\nThe difference in results between generating SQL and Python code for closed-\nsource models is relatively small (74% in Python versus 72.5% in SQL for GPT-4). How-\never, the difference in time is striking: while GPT-4 took an average of 11.5 seconds to\ngenerate Python code per question, generating SQL code was much faster, averaging 2.8\nseconds. This is due to the fact that more preprocessing is required to construct functional\nPython code, and the larger the code, the longer the inference time. For open-source\nmodels, in general, better results are obtained when generating SQL code. Nous Hermes\n2 achieved 40% accuracy building SQL queries and only 20% building Python codes,\nwhile the better open-source Python code generator, LLaMA, obtained only 26%.\n\n4Sources suggest that GPT-4 could be a mixture of several models that, together, total 1.76 trillion\n\nparameters (https://en.wikipedia.org/wiki/GPT-4).\n\n\fFigure 5. Comparison of LLMs Results in Code Generation for Q&A over Wik-\niTableQuestions\n\n6. Conclusions\n\nWe presented TableRAG, a pipeline for integrating tabular data into traditional Retrieval-\nAugmented Generation systems. The pipeline consists of obtaining textual representa-\ntions of tables, indexing and retrieving them as dense contextual vectors, generating SQL\nor Python code for table manipulation, and generating a candidate answer in natural lan-\nguage. With the described pipeline, we achieved results of up to 86.7% HITS@5 in\nretrieval and 74% accuracy in Q&A using GPT-4.\n\nSome limitations of this work should be considered. Due to computational cost\nand processing time, the results for Q&A were obtained using a sample of 200 ques-\ntions from the WikiTableQuestions test set, making it difficult to compare our results with\nthose of other works using the same benchmark. Additionally, the instructions provided\nto the language models for code generation were adjusted based on the outputs of the\nGPT-4 model, as it was the best-performing model, but they could also have been ad-\njusted considering the most frequent errors of each of the other models individually. The\nopen-source models did not undergo any fine-tuning process, which could significantly\nimprove their results, and we did not test open-source models larger than 13B parameters.\nFinally, we did not perform any preprocessing on the tables to make them more easily\ninterpretable, such as the table transposition procedure performed by [Liu et al. 2024],\nwhich yielded the state-of-the-art with 73.6% accuracy using GPT-3.5.\n\nAcknowledgments\n\nThe work was carried out with assistance granted by the National Agency of Petroleum,\nNatural Gas and Biofuels (ANP), Brazil, associated with the investment of resources orig-\ninating from the R,D&I Clauses, through the Cooperation Agreement between Petrobras\nand PUC-Rio.\n\n\fReferences\n\n[Abraham et al. 2022] Abraham, A. N., Rahman, F., and Kaur, D. (2022). Tablequery:\nQuerying tabular data with natural language. arXiv preprint arXiv:2202.00454.\n\n[Almazrouei et al. 2023] Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojo-\ncaru, R., Debbah, M., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., Noune, B.,\nPannier, B., and Penedo, G. (2023). Falcon-40B: an open large language model with\nstate-of-the-art performance.\n\n[Anand et al. 2023] Anand, Y., Nussbaum, Z., Duderstadt, B., Schmidt, B., and Mulyar, A.\n(2023). Gpt4all: Training an assistant-style chatbot with large scale data distillation\nfrom gpt-3.5-turbo. https://github.com/nomic-ai/gpt4all.\n\n[Chen et al. 2020a] Chen, W., Chang, M.-W., Schlinger, E., Wang, W., and Cohen,\narXiv preprint\n\nW. W. (2020a). Open question answering over tables and text.\narXiv:2010.10439.\n\n[Chen et al. 2020b] Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H., and Wang, W.\n(2020b). Hybridqa: A dataset of multi-hop question answering over tabular and textual\ndata. arXiv preprint arXiv:2004.07347.\n\n[Conneau et al. 2019] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G.,\nGuzm´an, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2019). Unsuper-\nvised cross-lingual representation learning at scale. CoRR, abs/1911.02116.\n\n[Dong et al. 2024] Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu,\nZ., Chang, B., Sun, X., Li, L., and Sui, Z. (2024). A survey on in-context learning.\n\n[Dubey et al. 2024] Dubey, A. et al. (2024). The llama 3 herd of models.\n\n[Gao et al. 2023] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J.,\nand Wang, H. (2023). Retrieval-augmented generation for large language models: A\nsurvey. arXiv preprint arXiv:2312.10997.\n\n[Jiang et al. 2023] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S.,\nde las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R.,\nLachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed,\nW. E. (2023). Mistral 7b.\n\n[Kandpal et al. 2023] Kandpal, N., Deng, H., Roberts, A., Wallace, E., and Raffel, C.\n(2023). Large language models struggle to learn long-tail knowledge. In International\nConference on Machine Learning, pages 15696–15707. PMLR.\n\n[Liang et al. 2018] Liang, C., Norouzi, M., Berant, J., Le, Q. V., and Lao, N. (2018). Mem-\nory augmented policy optimization for program synthesis and semantic parsing. Ad-\nvances in Neural Information Processing Systems, 31.\n\n[Lin et al. 2023] Lin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez,\nP., Kahn, J., Szilvasy, G., Lewis, M., et al. (2023). Ra-dit: Retrieval-augmented dual\ninstruction tuning. arXiv preprint arXiv:2310.01352.\n\n[Liu et al. 2024] Liu, T., Wang, F., and Chen, M. (2024). Rethinking tabular data under-\nstanding with large language models. In Duh, K., Gomez, H., and Bethard, S., editors,\nProceedings of the 2024 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies (Volume 1: Long\n\n\fPapers), pages 450–482, Mexico City, Mexico. Association for Computational Lin-\nguistics.\n\n[Mallen et al. 2022] Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi,\nH. (2022). When not to trust language models: Investigating effectiveness of paramet-\nric and non-parametric memories. arXiv preprint arXiv:2212.10511.\n\n[OpenAI et al. 2024] OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\n\nAleman, F. L., et al. (2024). Gpt-4 technical report.\n\n[Pasupat and Liang 2015] Pasupat, P. and Liang, P. (2015). Compositional semantic parsing\n\non semi-structured tables. arXiv preprint arXiv:1508.00305.\n\n[Tonmoy et al. 2024] Tonmoy, S., Zaman, S., Jain, V., Rani, A., Rawte, V., Chadha, A., and\nDas, A. (2024). A comprehensive survey of hallucination mitigation techniques in\nlarge language models. arXiv preprint arXiv:2401.01313.\n\n[Wang et al. 2024] Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., and Wei,\nF. (2024). Multilingual e5 text embeddings: A technical report. arXiv preprint\narXiv:2402.05672.\n\n[Yin et al. 2020] Yin, P., Neubig, G., Yih, W.-t., and Riedel, S. (2020). TaBERT: Pretraining\nfor joint understanding of textual and tabular data. In Jurafsky, D., Chai, J., Schluter,\nN., and Tetreault, J., editors, Proceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 8413–8426, Online. Association for Com-\nputational Linguistics.\n\n[Yu et al. 2018] Yu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li,\nI., Yao, Q., Roman, S., et al. (2018). Spider: A large-scale human-labeled dataset\nfor complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint\narXiv:1809.08887.\n\n\f"
        },
        {
            "titulo": "A Dependency Treebank of Tweets in Brazilian Portuguese: Syntactic Annotation Issues and Approach",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31131",
            "idioma": "Inglês",
            "storage_key": "files/article_31131_30934.pdf",
            "autores": [
                {
                    "nome": "Ariani Di Felippo",
                    "afiliacao": "USP / UFSCar",
                    "orcid": "http://orcid.org/0000-0002-4566-9352"
                },
                {
                    "nome": "Maria das Graças V. Nunes",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-2776-6140"
                },
                {
                    "nome": "Bryan K. da Silva Barbosa",
                    "afiliacao": "UFSCar",
                    "orcid": "http://orcid.org/0000-0002-4637-6498"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Amplia-se a análise de dependência do português brasileiro (pt-br) para lidar com “conteúdo-gerado por usuários” ao desenvolver e anotar o primeiro treebank de tweets (atuais posts do X) em pt-br segundo o modelo Universal Dependencies. O DANTEStocks possui 4,048 tweets do mercado financeiro e anotação-UD de tags PoS e traços morfológicos. Neste artigo, descreve-se a estratégia de anotação sintática adotada para lidar com as idiossincrasias do Twitter e do domínio desse corpus. A versão do DANTEStocks enriquecida com as relações de dependência-UD e as diretrizes de anotação já estão publicamente disponíveis.",
            "keywords": [
                "corpus annotation",
                "tweet",
                "stock market",
                "Universal Dependencies"
            ],
            "referencias": [
                "Barbosa, B. K. S. (2024). Descrição sintático-semântica de nomes predicadores em tweets do mercado financeiro em português. Dissertação de Mestrado. Programa de Pós-graduação em Linguística, Universidade Federal de São Carlos, São Carlos/SP, 208p.",
                "Carletta, J. (1996). Assessing agreement on classification tasks: The kappa statistic. In Computational Linguistics, Volume 22, Number 2, pages 249–254. MIT Press.",
                "Cohen, J. (1960). A coefficient of agreement for nominal scales. In Educational and Psychological Measurement, Volume 20, Issue 1, pages 37-46.",
                "Di-Felippo, A.; Postali, C.; Ceregatto, G.; Gazana, L. S.; Roman, N. T. (2022). Diretrizes de anotação de PoS tags em tweets do mercado financeiro: orientações para anotação em língua portuguesa segundo a abordagem Universal Dependencies. Relatório Técnico do ICMC 438. Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo. São Carlos-SP, 24p.",
                "Di-Felippo, A., Nunes, M. G. V., Barbosa, B. K. S. (2024). Diretrizes de anotação de relações de dependência em tweets do mercado financeiro. Relatório Técnico do ICMC 446. Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo. São Carlos-SP, Abril, 70p.",
                "Duran, M.S. (2021). Manual de Anotação de PoS tags: orientações para anotação de etiquetas morfossintáticas em Língua Portuguesa, seguindo as diretrizes da abordagem Universal Dependencies (UD). Relatório Técnico do ICMC 434. ICMC, USP. São Carlos-SP, 55p.",
                "Duran, M.S. (2022). Manual de Anotação de Relações de Dependência - Versão Revisada e Estendida: Orientações para anotação de relações de dependência sintática em Língua Portuguesa, seguindo as diretrizes da abordagem Universal Dependencies (UD). Relatório Técnico do ICMC 440. ICMC, USP. São Carlos-SP, 166p.",
                "Duran, M. S., Lopes, L., Nunes, M.G.V., Pardo, T. A. S. (2023). The Dawn of the Porttinari Multigenre Treebank: Introducing its Journalistic Portion. In Proceedings of the 14th Symposium in Information and Human Language Technology, pages 115-124. Belo Horizonte/MG. SBC.",
                "Krumm, J., Davis, N. Narayanaswami, C. (2009). User-Generated Content. In IEEE Pervasive Computing, Volume 7, Issue 4, pages. 10 – 11, IEEE, 2009.",
                "Lopes, L., Duran, M. S.; Fernandes, P. H. L.; Pardo, T. A. S. (2022). PortiLexicon-UD: a Portuguese Lexical Resource according to Universal Dependencies Model. In Proceedings of the 13th International Conference on Language Resources and Evaluation (LREC), pages 6635 6643, Marseille, France. ELRA.",
                "Lopes, L.; Pardo, T. A. S. Towards Portparser - a highly accurate parsing system for Brazilian Portuguese following the Universal Dependencies framework. In Proceedings of the 16th International Conference on Computational Processing of Portuguese (PROPOR), pages 401-410, Santiago de Compostela, Galiza. ACL.",
                "Macqueen, J. (1967) Some methods for classification and analysis of multivariate observations. In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability. [S.l.], v. 1, n. 14, p. 281–297.",
                "Nivre, J., Fang, C.-T. (2017). Universal Dependency evaluation. In Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017), pages 86–95, Gothenburg, Sweden. ACL.",
                "Nivre, J., et al. (2016). Universal dependencies v1: A multilingual treebank collection. In Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC), pages 1659–1666, Portorož, Eslovênia. ELRA.",
                "Nivre, J. et al. (2020). Universal Dependencies v2: an evergrowing multilingual treebank collection. In Proceedings of the 12th International Conference on Language Resources and Evaluation Conference (LREC), pages 4034-4043. Marseille, França. ELRA.",
                "Qi, P., Zhang, Y., Zhang, Y, Bolton, J., Manning, C. D. (2020). Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) (System Demonstrations), pages 101-108. Online. ACL.",
                "Rademaker, A., Chalub, F., Real, L., Freitas, C., Bick, E., Paiva, V. de. (2017). Universal Dependencies for Portuguese. In Proceedings of the 4th International Conference on Dependency Linguistics (Depling), pages 197–206, Pisa, Italy. Linköping University Electronic Press.",
                "Sanguinetti, M. et al. (2023). Treebanking user-generated content: a UD based overview of guidelines, corpora and unified recommendations. In Lang Resources & Evaluation, Volume. 57, Issue 2, pages 493–544. Springer-Verlag, Berlin, Heidelberg.",
                "Silva, E.H.; Pardo, T.A.S.; Roman, N.T.; Di Felippo, A. (2021). Universal Dependencies for tweets in Brazilian Portuguese: tokenization and Part-of-Speech tagging. In Proceedings of the 18th National Meeting on Artificial and Computational Intelligence (ENIAC), pages. 434-445, Online. SBC.",
                "Scandarolli, C. L., Di-Felippo, A., Roman, N. T., Pardo, T. A. S. (2023). Tipologia de fenômenos ortográficos e lexicais em CGU: o caso dos tweets do mercado financeiro. In Anais da VIII Jornada de Descrição do Português (JDP) (Evento integrante do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana-STIL), p. 240-248, Belo Horizonte/MG, Brasil. SBC.",
                "Sobrevilla Cabezudo, M.A., Maziero, E.G., Souza, J.W.C., Dias, M.S., Cardoso, P.C.F., Balage Filho, P.P., Agostini, V., Nóbrega, F.A.A., Barros, C.D., Di Felippo, A., Pardo, T.A.S. (2015). Anotação de sentidos de verbos em textos jornalísticos do corpus CSTNews. In Revista de Estudos da Linguagem (RELIN), Volume 23, Número 3, p. 797-832.",
                "Straka, M. (2018). UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 197–207. Brussels, Belgium. ACL."
            ],
            "artigo_completo": "A Dependency Treebank of Tweets in Brazilian Portuguese: \n\nSyntactic Annotation Issues and Approach \n\nAriani Di Felippo1,2, Maria das Graças V. Nunes1, Bryan K. da Silva Barbosa1,3 \n\n1Núcleo Interinstitucional de Linguística Computacional (NILC) \n\n2Departamento de Letras, Universidade Federal de São Carlos (UFSCar) \nCaixa Postal 676 -- 13565-905 -- São Carlos -- SP -- Brazil \n\n3Programa de Pós-Grad. em Linguística, Universidade Federal de São Carlos (UFSCar) \nCaixa Postal 676 -- 13565-905 -- São Carlos -- SP -- Brazil \n\nariani@ufscar.br, gracan@icmc.usp.br, bryan42@estudante.ufscar.br \n\nAbstract. We broaden Brazilian Portuguese (BP) dependency parsing to handle \n“user-generated content” by developing and annotating the first BP treebank of \ntweets  (actual  X  posts)  within  the  Universal  Dependencies  framework. \nDANTEStocks has a size of 4,048 tweets from the stock market domain already \nannotated with PoS tags and morphological features from UD. In this paper, we \ndescribe our standards for dealing with Twitter- and domain-specific properties \nof  the  corpus  in  the  dependency  annotation  process.  The  enriched  version  of \nDANTEStocks with dependency relations from UD and the annotation guidelines \nare already publicly available. \n\nResumo.  Amplia-se  a  análise  de  dependência  do  português  brasileiro  (pt-br) \npara  lidar  com  “conteúdo-gerado  por  usuários”  ao  desenvolver  e  anotar  o \nprimeiro  treebank  de  tweets  (atuais  posts  do  X)  em  pt-br  segundo  o  modelo \nUniversal  Dependencies.  O  DANTEStocks  possui  4,048  tweets  do  mercado \nfinanceiro  e  anotação-UD  de  tags  PoS  e  traços  morfológicos.  Neste  artigo, \ndescreve-se  a  estratégia  de  anotação  sintática  adotada  para  lidar  com  as \nidiossincrasias do Twitter e do domínio desse corpus. A versão do DANTEStocks \nenriquecida com as relações de dependência-UD e as diretrizes de anotação já \nestão publicamente disponíveis. \n\n1. Introduction \n\nThe  Universal  Dependencies  (UD)  [Nivre  et  al.  2020]  project  specifies  a  complete \nmorphological and syntactic representation with the goal of facilitating multilingual tagger \nand parser development [Nivre 2016]. The morphology of a word consists of 3 levels of \ninformation:  PoS  tag,  lemma,  and  features.  Syntactic  annotation  consists  of  typed \ndependency relations (deprels) between words. Currently, the model has 17 PoS tags and \n37 deprels, plus a non-fixed set of morphological features. Figure 1 shows an example of \nan annotated tweet in DANTEStocks. In a dependency tree, one word is the head of the \nutterance  (root)  and  all  other  words  are  dependent  on  another  word.  The  labeled  arcs \nrepresent the deprels, pointing from heads to their dependents. The PoS tag and the lemma \nof each word are displayed below the text. The morphological features are not included in \nthis figure. However, the token “acordo” (“agreement”), for example, has the following \nfeatures and values according to UD: Number=Sing, Gender=Masc. \n\n  \n \n\fFigure 1. UD annotation of “#BR #BOVESPA #GOLL4 Gol assina acordo de \ncompartilhamento de voos com TAP - http://t.co/wHGukBg7qp”1. \n\nMotivated by UD, treebanks for new domains, genres and language varieties have \nbeen recently built. Among the treebanks featuring user-generated content (UGC) created \nfrom 2014 onwards, a significant number is either partially or entirely made up of Twitter \ndata,  whose  language  diverges  from  standard  written  texts  in  several  ways,  posing \nsignificant  challenges  for  building  UD-based  treebanks.  These  challenges  include  non-\nstandard spelling,  capitalization, punctuation, syntax, platform conventions, and creative \nlanguage  use,  which  often  introduce  many  unknown  words.  Promoting  cross-linguistic \nconsistency, UD guidelines for UGC annotation have been provided [e.g. Sanguinetti et al. \n2022], however, when it comes to a technical domain, specific strategies are required. Due \nto the variety and complexity of the language, adequate treatment of the phenomena by \nmeans of an already existing model, such as UD, is a non-trivial task. \n\nWe report the syntactic annotation of DANTEStocks within UD framework. First, \nwe briefly describe the segmentation, tokenization, and the previous PoS annotation of the \ncorpus (§2). Then, we present the annotation guidelines for the UD-deprels (§3). In (§4), \nwe detail the semi-automatic approach for annotating the dependency relations, including \ndata  organization,  creation  of  a  reference  subcorpus,  and  training  a  state-of-art  parsing \nmodel on tweets. In (§5), we report a small-scale evaluation of the syntactic annotation. \nFinally, we put our work into context and outline future work (§6). \n\n2. The DANTEStocks Corpus \n\nDANTEStocks  is  a  corpus comprising 4,048 tweets  (with  140-character limit) from the \nstock market domain. It was automatically collected by fetching posts containing a ticker2 \nof one of the 73 stocks that compose the Ibovespa3. Considering the entire tweet as a basic \nunit for syntactic analysis, the DANTEStocks’ tweets are not segmented into smaller units \n(sentences,  clauses  or  phrases).  This  decision  saved  the  effort  to  conduct  a  manual \nsegmentation  or  do  revision  of  an  automatic  process.  Additionally,  the  corpus  was  not \nnormalized to preserve its diversity, as the goal was to develop multigenre applications. \nAlthough focusing on syntax, we outline the previous segmentation and morphological UD-\nannotation because they contextualize some annotation decisions. \n\n1 “#BR #BOVESPA #GOLL4 Gol signs flight sharing agreement with TAP - http://t.co/wHGukBg7qp” \n2 It is a five or six-character alphanumerical string that represents a specific type of stock from a company, \nsuch as “PETR4” for Petrobras’ preferred stock. \n3 It is the benchmark indicator of B3 (“Brasil, Bolsa, Balcão”), which is the main financial exchange in Brazil. \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\fFollowing the lexicalist view of syntax of UD, the syntactic words4 (tokens) were \nautomatically  segmented  by  a  version  of  the  NLTK  TweetTokenizer5,  augmented  with \nspecific rules for UGC [Silva et al. 2021]. The tool preserves most white-space-delimited \ntokens,  including  phonetization  (e.g.  “d+”  >  “demais”),  hashtag,  cashtag6,  at-mention, \nemoticon, and URL, and splits off single orthographic tokens that correspond to multiple \n(syntactic) words, such as clitics, contractions (canonical and non-canonical), punctuation \nmarks  (except  for  abbreviations),  and  valuation  rates  and  monetary  values  with \nunconventional orthography. After the manual revision of the tool output, the corpus ends \nup with a total number of 81,037 tokens. \n\nThe morphological annotation was also conducted semi-automatically7 [Silva et al. \n2021]. The PoS tags generated by the UDPipe 2 parser [Straka 2018], trained incrementally \nover UD-Portuguese Bosque [Rademaker et al. 2017] and tweets, were manually analyzed \nby three annotators, and the cases of disagreement among them were adjudicated by a senior \nlinguist based on guidelines tailored for standard texts in BP [Duran 2021] and tweets [Di-\nFelippo et al. 2022]. All 17 UD-tags can be found in DANTEStocks. PUNCT, NOUN, and \nPROPN are the most frequent, with around 16%, 15% and 14% of all the tags, respectively. \nLemmas  and  grammatical  features  were  semi-automatically  obtained  by  using  the \nPortiLexicon-UD lexicon [Lopes et al. 2022]. Major manual adjustments were required for \nlemmatization  due  to  the  high  rate  of  out-of-vocabulary  words.  Regarding  grammatical \nfeatures, the scenario was quite different. The features extraction was guided by the already \nvalidated PoS tags and lemmas, which decreased the manual revision effort. Most of the \ncorrections  was  related  to  errors  arising  from  ambiguity  about  VERB  class  features \n(VerbForm, Mood, Tense, Gender, Number and Person). The manual revision also focused \non checking Typo, Abbr, and Foreign, which are features that can be associated to words \nbelonging to all PoS classes. \n\nWhile  many  syntactic  structures  of  tweets  could  be  quite  straightforwardly \nannotated using the general guidelines adapted for Portuguese [Duran 2022], many of them \nneeded specific  choices.  In the  next  section, we discuss the main  challenging issues  for \nannotation decisions related to dependency relations (deprels). \n\n3. Syntactic Annotation Issues \n\n3.1. Medium- and domain-dependent (lexical) phenomena \n\nMostly  following  the  recommendations  of  Sanguinetti  et  al.,  tokens  classified  as \northographic variation from standard norm by [Scandarolli et al. 2023] were annotated with \ntheir actual syntactic roles, since they are always syntactically integrated. These variations \ninclude user-generated content phenomena such as substitution, omission, insertion, and \ntransposition of characters (e.g., letters, spaces, hyphens, and diacritics). A good example \nis the token “nao” (instead of “não”) (“no”) in (1) “VALE5 nao passa de 29,9”8, which has \na case of diacritic omission.  In the  example,  “nao” was  related to the  root “passa” by \nadvmod, since it is an adverb that modifies a predicate. \n\n4 It is the basic annotation unit that plays a syntactic function in an utterance. \n5 https://www.nltk.org/api/nltk.tokenize.html \n6 It was specifically designed to track financial instruments (e.g., $PETR4). \n7 The version of the corpus containing PoS and features annotation is publicly available at: \nhttps://sites.google.com/icmc.usp.br/poetisa/resources-and-tools. \n8 “VALE5 does not exceed 29,9” \n\n  \n \n \n \n \n\fThe same strategy was adopted for treating most of the phenomena classified as \n“innovative  norm”9  by  [Scandarolli  et  al.  2023]  (i.e.,  abbreviation,  neologism,  mark  of \nexpressiveness and homophone writing), since they are also always syntactically integrated. \nPictogram (emoticon/emoji), which is a mark of expressiveness, is the only one that occurs \nnon-syntactically integrated (standalone), being attached to the root by discourse. The \nother  two  types  of  innovative  norm’s  phenomena  required  annotation  guidelines  when \nstandalone and syntactically integrated (Table 1). For the medium-dependent devices, the \ntreatment given to the at-mentions when preceded by the RT mark is only that differs from \nthe  recommendation  of  Sanguinetti  et  al.  Instead  of  considering  the  at-mention  as \nstandalone  and  attaching  it  to  the  main  predicate  with  vocative,  we  treat  it  as  a \nsyntactically  integrated  token  attached  to  the  RT  mark  by  nmod.  This  is  due  to  our \ninterpretation  of  an  elliptical  preposition  “de”  (“of”)  (“RT  de  @user”),  indicating  an \nattributive relationship between the RT/SYM and the @user/PROPN. Also differently, all \nthe cases of parataxis involving a UGC phenomenon in DANTEStocks are annotated \nwith a corresponding subrelation, not only for URL and hashtags. \n\nTable 1. UD-dependency guidelines for Twitter- and domain-specific issues. \n\nUGC issue  Subtype \n\nURL \n\nHashtag \n\nAt-mention \n\nSyntactic \nintegration \nNo \nYes \nNo \nYes \nNo \nYes \nNo \nYes \nYes \nTruncation \nCode-switching (intra)  Yes \nYes \nTicker \nNo \nYes \n\nCashtag \n\nRT \n\nStandard syntactic \nrole \n\nOther \n\n✓ \n\n✓ \n\n✓ \n\nparataxis:hashtag \n\nparataxis:mention \nnmod (of the RT) \nparataxis:url \n\nparataxis:rt \n\n✓ \n✓ (:wtrunc or :strunc)   \n✓ (if known) \n✓ \n\nflat:foreign (if unknown) \n\nparataxis:cashtag \n\n✓ \n\nMedium-\ndependent \ntoken \n\nDomain-\nspecific \ntoken \n\n3.2. Unconventional syntax \n\nBesides all the linguistic issues previously mentioned, the complexity of the UD-annotation \nalso rises from the highly contextual nature of Twitter, and the high level of fragmentation \nthat seems to be typical in UGC from stock market domain. This provides a rich context \nfor ambiguities and ellipses, resulting in unconventional syntactic structures whose most \nappropriate UD analysis depends on the interpretation of the tweet content. One example is \nnsbuj:pass  without  the  aux:pass.  To  recommend  attaching  “#cyre3”  to  the  root \n“postado” by nsbuj:pass in the tweet of Figure 2, we assumed that the auxiliary verb is \nelided. In Figure 2, we also assumed an elliptical preposition (“a”) preceding “+1,78” to \nconnect “1,78” by obl. Since the syntactic function of “(+)1,78” is ambiguous (i.e., obl \nof “postado” or nmod of “abertura”), the choice of “1,78” as dependent on the root by \nobl illustrates annotation decisions based on the interpretation of domain experts. \n\n9 They are lexical alternatives to existing standard words and frequent linguistic devices that are found in \nthe Twitter and/or stock market domain language [Scandarolli et al. 2023]. \n\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n\fFigure 2. Syntactic ellipsis in the fragment ““#cyre3 postado hj antes da abertura +1,78”10. \n\n3.3. Structural patterns \n\nBesides the UGC (lexical) phenomena and unconventional syntax issues, we also identified \n22  recurring  structural  patterns  among  the  tweets  in  DANTEStocks.  Such  patterns \ncorrespond to almost 1,000 instances of the corpus, i.e. unique tweets. For each pattern, we \ncreated a template for guiding the annotation of the pattern instances in the corpus. The 22 \ntemplates  also  compose  the  dependency  annotation  guidelines  for  the  DANTEStocks \ncorpus, as well  as the recommendations for the treatment of  the lexical phenomena and \nunconventional structures [Di-Felippo et al. 2024].  \n\nMore precisely, a template contains 3 fields: (i) pattern, i.e. a mnemonic description, \n(ii) elements, i.e. list of pattern elements and the corresponding annotation guideline within \nUD, and (iii) example, i.e., at least one attested instance of the pattern from the corpus with \nits UD-dependency annotation. It is important to mention that, since the patterns usually \nrefer  to  fragmented  and/or  full  of  syntactic  ellipsis  tweets,  the  template  specification  is \nbased on a possible interpretation of the tweets, which was done with the support of stock \nmarket´s experts. \n\nFor illustration, the Template 11 is shown in Table 2. It corresponds to 20 unique \ninstances in the corpus. Since the pattern of the template represents very fragmented tweets, \nthe domain experts helped us to interpret corpus utterance such as that in Table 2 as being \ncomposed by three blocks of information, resulting in the following pattern description: \n<hashtag-ticker><theme><url>. \n\nThe <theme> provides information about a specific stock, codified by the <hashtag-\nticker>, and it was considered the main information of the utterance. Since the <theme> is \nalways being introduced by the coordinate expression “support and resistance”11, the first \nelement of the expression (i.e. “suportes”) is the root¸ as indicated in the field “elements”. \nIn the “element” field, it is also indicated that the <hashtag-ticker> is dependent on the \nroot with the nmod tag, due to interpretation of “#VALE” as a nominal that functionally \ncorresponds to a modifier of another noun (“suportes”). Since the nmod relation is usually \nintroduced by a preposition (ADP tag) in Portuguese, we assume, to propose the template, \nthat there is an elliptical preposition “de(+a)” (i.e. “suportes e resistências  da VALE4”) \n(“support and resistance” of #VALE5). Finally, the <url> is dependent on the root with \nparataxis:url because it is a run-on segment. \n\n10 “#cyre3 posted today before opening +1,78”. \n11 Terms that indicate price levels where a specific stock tends to reject the current trend and reverse, i.e., \nthey indicate potential turning points in a stock’s price. \n\n  \n \n \n \n \n \n \n \n \n \n \n \n\fTable 2 Template for UD-dependency annotation of tweets with structural pattern. \n\nPattern \nElements  a. <hashtag-ticker> is dependent on the root with the nmod label \n\n<hashtag-ticker> <theme> <url>, where: \n\nb. <theme> contains the expression “suportes e resistências”; “suportes” is the root  \nc. <url> is dependent of the root with the parataxis:url tag \n\nExample  #VALE5 suportes e resistências http://t.co/c8OrWXrECN \n\n4. Syntactic Annotation Approach \n\nThe dependency-based annotation of DANTEStocks was held in two semi-automatic stages \n[Barbosa 2024]. The first one aimed at creating a reference subcorpus and the second stage \nof  the  annotation  focused  on  fine-tuning  a  pre-trained  parser  for  tweets  by  using  the \nreference subcorpus as part of its initial training set. To start the syntactic annotation, all \n4,048  tweets  were  grouped  into  three  major  sets,  capturing  tweets  with:  (i)  relatively \nstandard language, (ii) recurring structural patterns, and (iii) other (tweets that do not belong \nto any of the first two sets). Tweets were classified through k-means clustering [Macqueen \n1967] with tf-idf (“term frequency–inverse document frequency”) [Luhn 1957]. \n\n4.1. Creation of a Reference Subcorpus \n\nThe organization of tweets into sets as mentioned above allowed us to select a few instances \nfrom each set, covering all the lexical and structural diversity of DANTEStocks to compose \na reference subcorpus of 1,000 tweets. Furthermore, as an attempt to achieve annotation \nconsistency,  particularly  given  the  non-canonical  language  of  the  corpus,  the  semi-\nautomatic annotation of the subcorpus was also based on such classification. This means \nthat the data from each major set was manually reviewed separately. \n\nTo create a gold-standard subcorpus we also used the UDPipe 2 parser trained over \nUD-Portuguese Bosque to annotate  the 1.000 tweets. The  UD-annotated subcorpus  was \nlater manually revised by a single expert. Taking advantage of the previous experience of \nthe expert in UD-annotation of journalistic texts and the training of UDPipe 2 over Bosque, \nthe manual revision started with tweets that present relatively standard language. The next \ntweets were those with recurring structural patterns, and finally the tweets with a variety of \nlexical and structural characteristics. During the revision process, the challenging issues \ndescribed  in  Section  3  were  discussed,  and  the  annotation  decisions  gave  rise  to  the \nguidelines  for  the  treatment  of  tweets  from  the  stock  market  domain  within  the  UD \nframework  [Di  Felippo  et  al.  2024].  The  guidelines  were  used  to  support  the  manual \nrevision of the rest of the corpus, which was done when training a state-of-art parser on the \ntweets from DANTEStocks. After  the  revision  of  the  subcorpus,  we  ended  up  having  a \ngold-standard subset of 1,000 syntactically annotated tweets. \n\n \n \n \n \n \n \n \n \n \n \n \n\f4.2. Parsing model training \n\nThe  rest  of  the  corpus  was  annotated  by  customizing  Stanza  [Qi  et  al.  2020]  for \nDANTEStocks.  Stanza  is  a  well-known  pre-trained  model  for  Portuguese,  having  the \nadvantage of being a user-friendly pipeline for text analysis. The procedure began with the \nStanza  base  architecture,  fine-tuned  on  Porttinari-base  [Duran  et  al.  2023],  which  is  a \njournalistic corpus composed of 8,418 sentences (168,080 tokens) manually annotated with \nUD, and the reference subcorpus. For the first run of Stanza, comprising Porttinari-base and \nthe reference subcorpus as initial training dataset, was applied the same distribution of data \nfound in Porttinari-base12, resulting in a dataset of 9,893 samples, being 70% for training, \n10% for validation, and 20% for testing. The resulting parser was used to annotate a new \npackage  of  data  (out  the  remaining  3,048  tweets),  which  was  manually  revised  and \nincorporated to the previous data set, being then used to start a new training run of Stanza. \nThis cycle continued incrementally until the last package of tweets was annotated/revised.  \nBesides the first training iteration, we carried out five training runs, adding packages of \n203, 300, 400, 400, and 1233 tweets per iteration, respectively (totaling 2,536 tweets). The \nresulting model of the 6th (final) run was used to annotate the remaining 512 tweets. The \ntweet  packages  were  added  in  the  same  order  as  the  manual  revision  of  the  reference \nsubcorpus:  standard  language  tweets,  structural  pattern  tweets,  and  tweets  with  varied \nlexical/structural properties. \n\nFor each of the five runs, we kept, whenever possible, the same distribution of data \nfor  training,  validation  and  testing  used  in  the  first  iteration,  and  computed  Stanza’s \nperformance based on the Unlabeled Attachment Score13 (UAS) and Labeled Attachment \nScore14 (LAS). The UAS accuracy increased from 94.46% at the first run to 95,78% in the \nlast (6th) iteration, becoming 1,32% better. For LAS, the final accuracy (6th run) achieved \n94,62%,  increasing  0,76%  from  the  first  run  accuracy  of  93,86%.  The  increase  of  the \ndependency  relation  measures  indicates  that  the  model’s  ability  to  capture  the  syntactic \nstructures of the tweets has improved as we incorporate news tweet into the training sets. \nFor comparison purposes, the accuracy of the best model for journalist texts in Portuguese \nwas also around 96% (UAS) and 95% (LAS) [Lopes and Pardo 2024]. Figure 3 depicts the \noverall distribution of the dependency relations (without subrelations) in DANTEStocks. \n\nFigure 3. Frequency distribution of UD deprel tagset in DANTEStocks. \n\n12 The 8,418 sentences were split into training, development, and test sets, with 70% (5,893 sentences), 10% \n(842 sentences), and 20% (1,683 sentences) of the corpus, respectively. \n13 UAS indicates the accuracy of the head ignoring the relation’s name (deprel) [Nivre and Fang, 2017]. \n14 LAS evaluates the output of a parser by considering how many words have been assigned both the correct \nsyntactic head and the correct label, ignoring subrelations [Nivre and Fang 2017]. \n\n  \n \n \n \n \n \n \n \n \n\f5. Reliability of Annotation \n\nTo provide a reliability measure of the annotation of DANTEStocks, a second NLP expert \n(also with UD-annotation experience) manually reviewed the automatic annotation of 100 \nrandom tweets based on the same guidelines [Duran 2022; Di Felippo et al. 2024]. The \ndependency-trees  analyzed  by  the  additional  annotator  could  be  from  the  reference \nsubcorpus or generated by Stanza in one of its interactions. The Inter-Annotator Agreement \n(IAA) score was calculated by using the Kappa coefficient [Cohen, 1960; Carletta, 1996] \nin two different settings [Barbosa 2024]. In the first, the focus was to evaluate the annotation \nof head and deprel separately. The Kappa results for head and deprel were 0.96 and 0.97, \nrespectively. In the second setting, the evaluation aimed at the combination of head and \ndeprel, obtaining the Kappa score of 0.95. The IAA per deprel was measured by using the \ntotal agreement score [Sobrevilha Cabezudo 2015], since Kappa is not appropriate given \nthe unbalanced distribution of the relations. We obtained the total agreement of 100% for \nmore than half of the 46 different deprels (including subrelations) that occur in the sample \nof 100 tweets. Out of the 1.743 annotated relations, there are 42 cases of disagreement. The \nmost frequent conflict was between obl and nmod. Some of them were caused by different \nbut potential interpretations about the functional role of the prepositional phrase (in bold) \nin structure like “arrisque vd em #petr4” (“risk selling in #petr4”). While one annotator \nattached  “petr4”  to  the  verb  via  obl,  functioning  as  a  non-core  (oblique)  argument  or \nadjunct, the other assumed that “petr4” is a modifier of the noun “vd” (“venda”), being \nattached to it by nmod. It is also interesting that, among the 22 deprels with total agreement \ndifferent from 100%, 12 of them contain subrelations, indicating that the annotation is more \ncomplex when using language-specific relations. Even though a small-scale evaluation, the \nresults indicate that the overall IAA was otherwise quite high, especially for the challenging \ntask. This might be due to the large and detailed recommendations of our guidelines for the \nsyntactic annotation of the tweets. \n\n6. Final Remarks and Future Work \n\nWe described our effort on building the first BP treebank for Twitter microtext, annotated \nwithin the framework of UD. The contributions are the treebank itself, the instantiation of \nthe UD guidelines for stock market tweets in BP, and the customization of a current state-\nof-the-art  parser  for  tweets.  Our  main  difficult  was  interpreting  the  tweets,  due  to  the \nmedium- and domain-lexical phenomena and uncommon constructions. Thus, despite the \nconstant help of domain experts, we can say that the dependency annotation of many tweets \nin  DANTEStocks  (especially  those  with  fragmentation,  e.g.,  aborted  text)  represents \npotential  syntactic  analysis of the tweets.  Currently, the  two  annotators involved in this \nwork are analyzing the disagreements to assign a consensual deprel for each case and to \nmake the treebank available soon. The guidelines for the syntactic UD-based annotation of \nDANTEStocks and the treebank itself (beta version) are available at the POeTiSA project \nwebpage (https://sites.google.com/icmc.usp.br/poetisa/). \n\nAcknowledgements. This work was carried out at the Center for Artificial Intelligence of the \nUniversity of São Paulo (C4AI - http://c4ai.inova.usp.br/), with support by the São Paulo \nResearch  Foundation  (FAPESP  grant  #2019/07665-4)  and  by  the  IBM  Corporation.  The \nproject  was  also  supported  by  the  Ministry  of  Science,  Technology  and  Innovation,  with \nresources  of  Law  N.  8,248,  of  October  23,  1991,  within  the  scope  of  PPI-SOFTEX, \ncoordinated by Softex and published as Residence in TIC 13, DOU 01245.010222/2022-44. \n\n \n \n \n\fReferences \nBarbosa, B. K. S. (2024). Descrição sintático-semântica de nomes predicadores em tweets \ndo  mercado  financeiro  em  português.  Dissertação  de  Mestrado.  Programa  de  Pós-\ngraduação em Linguística, Universidade Federal de São Carlos, São Carlos/SP, 208p. \nCarletta,  J.  (1996).  Assessing  agreement  on  classification  tasks:  The  kappa  statistic.  In \n\nComputational Linguistics, Volume 22, Number 2, pages 249–254. MIT Press. \n\nCohen,  J.  (1960).  A  coefficient  of  agreement  for  nominal  scales.  In  Educational  and \n\nPsychological Measurement, Volume 20, Issue 1, pages 37-46. \n\nDi-Felippo, A.; Postali, C.; Ceregatto, G.; Gazana, L. S.; Roman, N. T. (2022). Diretrizes \nde anotação de PoS tags em tweets do mercado financeiro: orientações para anotação \nem língua portuguesa segundo a abordagem Universal Dependencies. Relatório Técnico \ndo ICMC 438. Instituto de Ciências Matemáticas e de Computação, Universidade de São \nPaulo. São Carlos-SP, 24p. \n\nDi-Felippo,  A.,  Nunes,  M.  G.  V.,  Barbosa,  B.  K.  S.  (2024).  Diretrizes  de  anotação  de \nrelações de dependência em tweets do mercado financeiro. Relatório Técnico do ICMC \n446. Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo. \nSão Carlos-SP, Abril, 70p. \n\nDuran,  M.S.  (2021).  Manual  de  Anotação  de  PoS  tags:  orientações  para  anotação  de \netiquetas morfossintáticas em Língua Portuguesa, seguindo as diretrizes da abordagem \nUniversal  Dependencies  (UD).  Relatório  Técnico  do  ICMC  434.  ICMC,  USP.  São \nCarlos-SP, 55p. \n\nDuran, M.S. (2022). Manual de Anotação de Relações de Dependência - Versão Revisada \ne Estendida: Orientações para anotação de relações de dependência sintática em Língua \nPortuguesa,  seguindo  as  diretrizes  da  abordagem  Universal  Dependencies  (UD). \nRelatório Técnico do ICMC 440. ICMC, USP. São Carlos-SP, 166p. \n\nDuran, M. S., Lopes, L., Nunes, M.G.V., Pardo, T. A. S. (2023). The Dawn of the Porttinari \nMultigenre  Treebank:  Introducing  its  Journalistic  Portion.  In  Proceedings  of  the  14th \nSymposium  in  Information  and  Human  Language  Technology,  pages  115-124.  Belo \nHorizonte/MG. SBC. \n\nKrumm,  J.,  Davis,  N.  Narayanaswami,  C.  (2009).  User-Generated  Content.  In  IEEE \n\nPervasive Computing, Volume 7, Issue 4, pages. 10 – 11, IEEE, 2009. \n\nLopes, L., Duran, M. S.; Fernandes, P. H. L.; Pardo, T. A. S. (2022). PortiLexicon-UD: a \nPortuguese  Lexical  Resource  according  to  Universal  Dependencies  Model.  In \nProceedings  of  the  13th  International  Conference  on  Language  Resources  and \nEvaluation (LREC), pages 6635 6643, Marseille, France. ELRA.  \n\nLopes,  L.;  Pardo,  T.  A.  S.  Towards  Portparser  -  a  highly  accurate  parsing  system  for \nBrazilian Portuguese following the Universal Dependencies framework. In Proceedings \nof  the  16th  International  Conference  on  Computational  Processing  of  Portuguese \n(PROPOR), pages 401-410, Santiago de Compostela, Galiza. ACL. \n\nLuhn, H.P. (1957). A statistical approach to mechanized encoding and searching of literary \ninformation. In IBM Journal of Research and Development, Volume 1, Issue 4, pages \n309-317. ISSN 0018-8646. doi:10.1147/rd.14.0309 \n\nMacqueen,  J.  (1967)  Some  methods  for  classification  and  analysis  of  multivariate \nobservations. In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics \nand Probability. [S.l.], v. 1, n. 14, p. 281–297. \n\n  \n\fNivre,  J.,  Fang,  C.-T.  (2017).  Universal  Dependency  evaluation.  In  Proceedings  of  the \nNoDaLiDa  2017  Workshop  on  Universal  Dependencies  (UDW  2017),  pages  86–95, \nGothenburg, Sweden. ACL. \n\nNivre, J., et al. (2016). Universal dependencies v1: A multilingual treebank collection. In \nProceedings  of  the  10th  International  Conference  on  Language  Resources  and \nEvaluation (LREC), pages 1659–1666, Portorož, Eslovênia. ELRA. \n\nNivre, J. et al. (2020). Universal Dependencies v2: an evergrowing multilingual treebank \ncollection. In Proceedings of the 12th International Conference on Language Resources \nand Evaluation Conference (LREC), pages 4034-4043. Marseille, França. ELRA. \n\nQi, P., Zhang, Y., Zhang, Y, Bolton, J., Manning, C. D. (2020). Stanza: A Python natural \nlanguage  processing  toolkit  for  many  human  languages.  In  Proceedings  of  the  58th \nAnnual  Meeting  of  the  Association  for  Computational  Linguistics  (ACL)  (System \nDemonstrations), pages 101-108. Online. ACL. \n\nRademaker, A., Chalub, F., Real, L., Freitas, C., Bick, E., Paiva, V. de. (2017). Universal \nDependencies  for  Portuguese.  In  Proceedings  of  the  4th  International  Conference  on \nDependency  Linguistics  (Depling),  pages  197–206,  Pisa,  Italy.  Linköping  University \nElectronic Press. \n\nSanguinetti, M. et al. (2023). Treebanking user-generated content: a UD based overview of \nguidelines,  corpora  and  unified  recommendations.  In  Lang  Resources  &  Evaluation, \nVolume. 57, Issue 2, pages 493–544. Springer-Verlag, Berlin, Heidelberg. \n\nSilva, E.H.; Pardo, T.A.S.; Roman, N.T.; Di Felippo, A. (2021). Universal Dependencies \nfor  tweets  in  Brazilian  Portuguese:  tokenization  and  Part-of-Speech  tagging.  In \nProceedings of the 18th National Meeting on Artificial and Computational Intelligence \n(ENIAC), pages. 434-445, Online. SBC. \n\nScandarolli,  C.  L.,  Di-Felippo,  A.,  Roman,  N.  T.,  Pardo,  T.  A.  S.  (2023).  Tipologia  de \nfenômenos ortográficos e lexicais em CGU: o caso dos tweets do mercado financeiro. \nIn Anais da VIII Jornada de Descrição do Português (JDP) (Evento integrante do XIV \nSimpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana -STIL), p. \n240-248, Belo Horizonte/MG, Brasil. SBC. \n\nSobrevilla Cabezudo, M.A., Maziero, E.G., Souza, J.W.C., Dias, M.S., Cardoso, P.C.F., \nBalage Filho, P.P., Agostini, V., Nóbrega, F.A.A., Barros, C.D., Di Felippo, A., Pardo, \nT.A.S.  (2015).  Anotação  de  sentidos  de  verbos  em  textos  jornalísticos  do  corpus \nCSTNews. In Revista de Estudos da Linguagem (RELIN), Volume 23, Número 3, p. \n797-832. \n\nStraka, M. (2018). UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings \nof  the  CoNLL  2018  Shared  Task:  Multilingual  Parsing  from  Raw  Text  to  Universal \nDependencies, pages 197–207. Brussels, Belgium. ACL. \n\n \n \n\f"
        },
        {
            "titulo": "Detection and Censorship of Offensive Language in Extended Texts in Portuguese",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31132",
            "idioma": "Inglês",
            "storage_key": "files/article_31132_30935.pdf",
            "autores": [
                {
                    "nome": "Lucas Lenoch de Souza",
                    "afiliacao": "UTFPR",
                    "orcid": "http://orcid.org/0009-0000-9084-7253"
                },
                {
                    "nome": "Franciele Beal",
                    "afiliacao": "UTFPR",
                    "orcid": "https://orcid.org/0009-0008-0752-0400"
                },
                {
                    "nome": "André Roberto Ortoncelli",
                    "afiliacao": "UTFPR",
                    "orcid": "https://orcid.org/0000-0001-9622-8525"
                },
                {
                    "nome": "Marlon Marcon",
                    "afiliacao": "UTFPR",
                    "orcid": "https://orcid.org/0000-0002-3698-8570"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "This article addresses the problem of detecting and censoring offensive language in extensive Brazilian Portuguese texts on the web. This paper proposes a pipeline for classifying and censoring extensive texts, focusing on comments, posts, and articles using NLP techniques. The results include an in-depth review of current methods for offensive content classification in Portuguese and the implementation of a BERTimbau-based pipeline for offense detection. This work represents a significant advancement in the state-of-the-art NLP in Portuguese, promoting safer and more respectful online environments for users, especially children.",
            "keywords": [
                "Hate-speech detection",
                "BERT model",
                "offensive content classification"
            ],
            "referencias": [
                "Cook, S. (2024). Cyberbullying facts and statistics for 2018 – 2024.",
                ".",
                "Devlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.",
                "Economist (2019). Increasing numbers of children have internet addiction – how worried should parents really be?",
                ". (Accessed on 10/10/2024).",
                "Hajibabaee, P., Malekzadeh, M., Ahmadi, M., Heidari, M., Esmaeilzadeh, A., Abdolazimi, R., and Jones, J. H. (2022). Offensive language detection on social media based on text classification. 2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC), pages 0092–0098.",
                "Honnibal, M., Montani, I., Van Landeghem, S., and Boyd, A. (2020). spaCy: Industrial-strength Natural Language Processing in Python.",
                "Husain, F. and Uzuner, O. (2021). A survey of offensive language detection for the arabic language. ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP), 20(1):1–44.",
                "Leite, J. A., Silva, D., Bontcheva, K., and Scarton, C. (2020). Toxic language detection in social media for brazilian portuguese: New dataset and multilingual analysis. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 914–924.",
                "Leray, W. (2023). Série de harry potter? envolvimento de j.k. rowling divide fãs.",
                ". (Accessed on 10/10/2024).",
                "Martins, T. (2022). Chico buarque dá comida aos censores - senso incomum.",
                ". (Accessed on 10/10/2024).",
                "Monteiro, E. (2023). Caso bruno e dom: justiça decide levar amarildo e outros dois réus a júri popular | amazonas | g1.",
                ". (Accessed on 10/10/2024).",
                "Pelle, R. P. and Moreira, V. P. (2017). Offensive comments in the brazilian web: a dataset and baseline results. In Anais do VI Brazilian Workshop on Social Network Analysis and Mining. SBC.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: pretrained bert models for brazilian portuguese. In Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I 9, pages 403–417. Springer.",
                "Trajano, D., Bordini, R. H., and Vieira, R. (2023). Olid-br: offensive language identification dataset for brazilian portuguese. Language Resources and Evaluation, pages 1–27.",
                "Trielli, L. (2021). Escócia: estupradores que se declararem mulher serão colocados em prisões femininas - senso incomum.",
                ". (Accessed on 10/10/2024).",
                "Vargas, F., Carvalho, I., Rodrigues de Góes, F., Pardo, T., and Benevenuto, F. (2022). HateBR: A large expert annotated corpus of Brazilian Instagram comments for offensive language and hate speech detection. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 7174–7183, Marseille, France. European Language Resources Association.",
                "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.",
                "Wagner Filho, J. A., Wilkens, R., Idiart, M., and Villavicencio, A. (2018). The brwac corpus: a new open resource for brazilian portuguese. In Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018)."
            ],
            "artigo_completo": "Detection and Censorship of Offensive Language in Extended\nTexts in Portuguese\n\nLucas Lenoch de Souza1,2, Franciele Beal1,3,\nAndré Roberto Ortoncelli 1,2,4, Marlon Marcon1,2,4\n\n1Federal University of Technology - Paraná - UTFPR\n\n2Software Engineering Coordination - Dois Vizinhos - Paraná - Brazil\n\n3Academic Department of Informatics - Pato Branco - Paraná - Brazil\n\n4Programa de Pós-graduação em Informática (PPGI)\n\nlucasouza191141@gmail.com\n\n{fbeal, ortoncelli, marlonmarcon}@utfpr.edu.br\n\nAbstract. This article addresses the problem of detecting and censoring offen-\nsive language in extensive Brazilian Portuguese texts on the web. This paper\nproposes a pipeline for classifying and censoring extensive texts, focusing on\ncomments, posts, and articles using NLP techniques. The results include an\nin-depth review of current methods for offensive content classification in Por-\ntuguese and the implementation of a BERTimbau-based pipeline for offense de-\ntection. This work represents a significant advancement in the state-of-the-art\nNLP in Portuguese, promoting safer and more respectful online environments\nfor users, especially children.\n\n1. Introduction\n\nIn recent years, the Internet has been growing at an impressive rate in terms of users and\nthe data generated by web page publications. This increasing Internet use often introduces\nmany children to virtual environments from a very early age. Consequently, offensive\nlanguage in online texts becomes a concern for ethical reasons [Economist 2019]. Issues\nsuch as cyberbullying, hate speech, and various forms of offensive content in social media\nposts are also relevant [Cook 2024].\n\nRegarding the intelligent interpretation of textual data from social networks, Nat-\nural Language Processing (NLP) is commonly used. To address the problem of offen-\nsive language in web posts, articles in the field of NLP, combined with Machine Learning\n(ML) and Deep Learning (DL) techniques, have been developed [Hajibabaee et al. 2022].\nThese efforts include creating research pipelines and developing high-quality annotated\ndatasets by professionals [Leite et al. 2020].\n\nGiven the differences in languages and cultures, it is only possible to formalize a\nmodel for some languages. However, efforts are being made to learn various languages\n[Husain and Uzuner 2021]. The HateBR [Vargas et al. 2022] corpus is an example for\nBrazilian Portuguese. Such an article highlights the lack of academic production related\nto offensive language in the national language, along with the dataset and its results.\nDespite existing academic contributions to addressing offensive language in Brazilian\n\n\fPortuguese, applying NLP and ML/DL techniques to classify longer texts (such as news\narticles or blog posts) remains challenging. These techniques are often limited to social\nmedia posts or news comments.\n\nBased on this information, the present work focuses on detecting and subsequently\ncensoring or filtering offensive language online. The goal is to ensure that web pages that\nare not explicitly focused on adult content can be suitable environments for children. Ad-\nditionally, efforts are made to reduce prejudice and offensiveness in online posts, benefit-\ning users’ emotional well-being and promoting mutual respect. Specifically, we propose\nidentifying offensive words using DL techniques, which can then be filtered or censored.\nWe define small texts as comments, posts, and sentences, while paragraphs and entire\npages constitute more extended texts, with the latter being the focus of this study.\n\nAs the main contributions of this work, we have: 1) An in-depth review of state-of-\nthe-art methods applied to offensive content classification for the Portuguese language; 2)\nAn update to the state-of-the-art results on the HateBR dataset [Vargas et al. 2022]; and 3)\nA Deep-Learning-based pipeline that effectively classifies and censors extensive Brazilian\nPortuguese texts based on their offensiveness.\n\n2. Related Works\n\nWhen considering works that address text processing with offensive language in the\nBrazilian context,\nincluding ToLD-BR\nthe range of existing works is very small,\n[Leite et al. 2020], OffComBr [Pelle and Moreira 2017], HateBR [Vargas et al. 2022],\nand OLID-BR [Trajano et al. 2023]. Such methods are explained in the following sec-\ntions.\n\n2.1. ToLD-BR\n\nThis dataset [Leite et al. 2020], presents an unspecified number of posts extracted from\nthe Twitter platform. A total of 42 individuals, chosen from 129 volunteers, were tasked\nwith annotating each post, classifying them into various categories of prejudice: homo-\nphobia, obscene language, misogyny, and xenophobia.\n\nThe posts were classified using BERT-style algorithms [Devlin 2018], which\nachieved optimal results for such a complex and subjective task. The researchers also\nexplored the possibility of building models for this task in multiple languages, but their\nresults indicated that monolingual data is still preferable for more accurate classifications.\n\n2.2. OffComBr\n\nThis paper presents the development of a dataset comprising comments on news articles\nderived from the website g1.globo.com, named “OffComBr” [Pelle and Moreira 2017].\nThe researchers obtained around 10,000 comments, but given the manual annotation pro-\ncess done by three experts in detecting offensive language, they included only 1,250 com-\nments in the final dataset.\n\nThe authors performed two classification algorithms (SMO and Naive Bayes) to\nevaluate the dataset, with different assessments depending on different data preprocessing\nmethods. Two versions of the dataset were developed, OffComBR-2 and OffComBR-3,\nwith the difference being the size, as the latter retained from the former only the annota-\ntions agreed upon by all three experts.\n\n\f2.3. HateBR\n\nThe work of [Vargas et al. 2022] presented the first large annotated corpus of offensive\nlanguage in Instagram comments in Brazilian Portuguese. Motivated by the presence\nof hate speech on social media and the lack of studies on the subject in Portuguese, the\nproject collected 7,000 Instagram comments, annotated by experts regarding the presence,\ndegree, and category of offensiveness. The process involved data collection, selection\nof accounts of Brazilian political figures (three left-wing and three right-wing), and the\nselection of 30 posts from which 15,000 comments were extracted, with 7,000 being\nbalanced between offensive and non-offensive.\n\nThe comments were labeled into three levels of offensiveness: whether they were\noffensive or not, the degree of offensiveness (mild, moderate, or high), and whether they\ncontained hate speech, categorized into nine types such as xenophobia, racism, and ho-\nmophobia. From the 7,000 comments, 3,500 were offensive, with 778 highly offensive,\n1,044 moderately offensive, and 1,678 mildly offensive. Among the offensive comments,\n727 contained some type of hate speech. Table 1 presents samples of data from HateBR.\n\nTable 1. Examples of comments extracted form the HateBR dataset.\n\nClass\n\nOffensive\n\nNon-Offensive\n\nWith hate speech\n\nWithout hate speech\n\nComments\n\nEssa besta humana é o câncer do País, tem que voltar para a jaula,\nurgentemente! E viva o Presidente Bolsonaro.\nQuem falou isso para você deputada? O Sergio Moro está aprovado pela maioria\ndos brasileiros.\nVagabunda. Comunista. Mentirosa. O povo chileno não merece uma desgraça\ndessa.\nPois é, deveria devolver o dinheiro aos cofres públicos do Brasil. Canalha.\n\nFinally, after a detailed explanation of their entire annotation system, as well as\nevaluations to judge the annotations of each of the three experts and decide the most\nappropriate annotations for each comment, the study presents the test results with some\nML models trained on the HateBR corpus, comparing the best result obtained with the\nbest results of two other reference works. In this work, we seek to replicate the results\nobtained by HateBR, following the same training procedure and using the same models\nfor comparison with our trained model.\n\n2.4. OLID-BR\n\nThe work of [Trajano et al. 2023] also developed an annotated dataset of offensive com-\nments in Portuguese, similar to HateBR. However, the main advantage of this dataset lies\nin its application to various NLP tasks, including binary classification of offensiveness,\nmulti-category prediction of the type of toxicity, identification of targeted toxic comments,\nprediction of the target of toxicity, and identification of toxicity spans in comments.\n\nThe primary focus of the work was on the task of identifying toxicity spans, which\ninvolves detecting sequences of characters containing offensive language. To collect data,\nOLID-BR used various sources such as Twitter, YouTube, and other datasets with differ-\nent annotation schemes.\n\nThe annotation was conducted in three stages: detection of offensive language,\ncategorization of offensive language, and identification of the target of the offense. Com-\n\n\fpared to HateBR, OLID-BR distinguishes between offensiveness against an individual, a\ngroup, or another type of target, while HateBR focuses on categorizing hate speech.\n\nData annotation in OLID-BR was not exclusively done by humans but also with\nthe assistance of the Perspective API1, allowing human annotators to correct the classifi-\ncations. The entire corpus was divided into three datasets for training and testing, with a\nsimilar distribution of classifications in each. This work replicated the part of OLID-BR\nrelated to the identification of toxicity spans, using the code available on GitHub to train\nthe model and apply it to the HateBR dataset for detecting offensive phrases and to the\nOLID-BR for identifying offensive spans.\n\n3. Main Technologies\nFor the development, training, and testing of techniques for offensive language detection\nand censorship, we primarily used two libraries available for the Python language for\ndeveloping ML algorithms: Transformers and spaCy.\n\n3.1. Transformers\n\nThe Transformers library is a Python tool that offers state-of-the-art architectures for NLP\ntasks, featuring over 32 pre-trained models in more than 100 languages. It provides deep\ninteroperability between TensorFlow 2.0 and PyTorch. The library is named after the\nTransformer architecture introduced by Google Brain in 2017, which is based on the\n\"attention mechanism.\" This mechanism allows the model to focus on important parts of\nthe input data, leading to superior performance in NLP tasks like sentence classification,\nnamed entity recognition (NER), and natural language generation compared to previous\nmodels like recurrent neural networks (RNNs) [Vaswani et al. 2017].\n\n3.2. BERT and BERTimbau\n\nThe algorithm used in the first stage of our pipeline was a fine-tuned version BERTim-\nbau [Souza et al. 2020], a Brazilian model based on BERT (Bidirectional Encoder Rep-\nresentations from Transformers) [Devlin 2018]. BERT, introduced by Google in 2018,\nis a language model that generates numerical representations for words based on their\nsurrounding context and is used for various NLP tasks. BERTimbau adapts BERT for\nBrazilian Portuguese using transfer learning, where a BERT model was trained on a Por-\ntuguese corpus (brWaC) [Wagner Filho et al. 2018] and evaluated on tasks like sentence\nsimilarity, textual entailment, and named entity recognition. In this work, BERTimbau\nwas specifically used to develop a model for detecting offensive language in sentences.\n\n3.3. SpaCy\n\nan\n\nfor Python, written\n\nopen-source NLP library\n\nin Cython\nSpaCy\nis\nthat facilitates tasks like part-of-speech tagging, named en-\n[Honnibal et al. 2020],\nIt provides pre-trained models and\ntity recognition (NER), and dependency parsing.\nallows users to train their own models for NER, where sentences are segmented into\nwords, each categorized (e.g., nouns, adverbs, or specific problem-related categories\nlike offensive and non-offensive). In this work, SpaCy was used in the second stage to\ndetect which words in an offensive sentence are offensive, following the methodology of\nOLID-BR, which also used SpaCy for tasks like detecting offensive spans in text.\n\n1https://www.perspectiveapi.com/\n\n\f4. Methodology\n\nThis study focused on developing NLP models to detect offensive language in extended\ntexts. Both the acquisition of training data and the actual censorship of words detected as\noffensive were carried out simplified due to them not being the primary focus. The de-\nvelopment process consisted of four stages: data collection, cleaning/tokenization, model\ntraining, and result evaluation explained in the following subsection.\n\n4.1. Data Collection\n\nFor data collection, we employ the HateBR [Vargas et al. 2022] and OLID-BR\n[Trajano et al. 2023] datasets to train the offensive content detection models, the for-\nmer to classify a text segment as potentially offensive or not and the latter to identify\nwords or expressions that contain offensiveness. Additionally, to evaluate qualitatively\nour solution, we selected news articles from the G1 portal [Monteiro 2023], Catraca Livre\n[Leray 2023], and two blog posts from Senso Incomum [Trielli 2021, Martins 2022].\n\n4.2. Data Cleaning and Feature Extraction\n\nWe used tools from the Transformers and spaCy libraries for data cleaning and feature ex-\ntraction. Specifically, for the model trained with the Transformers library, we employed a\ntokenizer ready to transform sentences into numerical representations used by the model\nfor calculations. For the model trained with spaCy, an embedded tokenization function-\nality was available. In summary, in this step, the trained models were capable of cleaning\nthe data and processing it without needing external code. For the training and evaluation\nof the first model (developed with the Transformers library), we compared it to the ML\nmodels presented by HateBR, which had the best results, using the same feature extrac-\ntion method they did: TF-IDF. TF-IDF (Term Frequency–Inverse Document Frequency)\ncalculates how relevant a word in a corpus is to a text, obtained by the ratio between\nthe number of times the term in question appears in one of the corpus texts (Term Fre-\nquency) and the frequency of appearances of this same term in the entire corpus (Inverse\nDocument Frequency). The frequency of the word in a text refers to the ratio between\nthe number of times it appears in the text and the number of words in the text, while the\nfrequency of the word in the corpus is the count of how many times it appears in all texts\nof the dataset. The reason for using such feature extraction is due to the empirical re-\nsults demonstrated by HateBR [Vargas et al. 2022], which show that, in general, models\ntrained with features extracted using TF-IDF outperformed other methods.\n\n4.3. Model Training\n\nThe model training process was split into two parts: the first was responsible for detecting\noffensiveness in a text (in this case, in a selected paragraph), and the second was respon-\nsible for identifying words or expressions that contain offensive language in segments\nclassified as offensive by the first model.\n\nWe employed the BERTimbau model from the HuggingFace platform for the\nfirst model. BERTimbau is a Brazilian language model developed by NeuralMind\n[Souza et al. 2020] through a technique called fine-tuning. In the case of this study, this\nprocess involved adding an extra layer of neurons at the end of the model, which was\nresponsible for classifying an input text as offensive or non-offensive.\n\n\fFor the second model,\n\nthe training process used by the OLID-BR study\n[Trajano et al. 2023] was utilized to detect offensive spans (i.e., sequences of characters\ncontaining offensive language, not limited to isolated words but also including expres-\nsions and punctuation).\n\n4.4. Model Demonstration\n\nWe evaluate the offensive language detection models using Precision, Recall, and F1-\nScore metrics. Precision measures how much we can trust a model when it predicts that\nan example belongs to a particular class by calculating the number of examples the model\ncorrectly predicted as belonging to that class divided by the total number of examples it\npredicted as belonging to that class. Recall is the number of samples the model correctly\nidentified as belonging to a class divided by the total number of samples that belong to\nthat class in the data. F1-Score is the harmonic mean between precision and recall, i.e., it\nis the average of both precision and recall values, giving more importance to low values,\nas a much lower precision or recall value indicates that the model is not balancing these\ntwo metrics well when we want to give equal importance to both.\n\nTo demonstrate the models’ effectiveness, we conducted qualitative tests on se-\nlected texts, as long as they were at least one page long or had more than one paragraph.\n\nFor the application process of the trained models, Figure 1 presents the follow-\ning steps graphically: (1) a large text is collected; (2) the text is divided into fragments\nbased on the characters of periods, commas, semicolons, exclamations, questions, and\nnew lines; (3) for each fragment, the BERTimbau-based model is used to check if it is\noffensive; (4) if not offensive, the fragment is returned as usual, but if it is, it proceeds\nto the next step; (5) the spaCy-trained model is used to identify the offensive spans; (6)\nreturning of the offensive censored spans, and the censored version of the fragment; (7) fi-\nnally, all fragments, censored or not, are reassembled using the same separators as before,\nthus returning the entire text now with the appropriate censorship.\n\nFigure 1. Censorship process of our proposal.\n\n\f5. Results\nThe results obtained from the training sessions, in terms of Precision, Recall, and F1-\nscore evaluated across the four different models tested for toxicity detection in sentences,\nare presented in Table 2. The parameters for conducting the training and testing followed\nthe proposal in the HateBR study [Vargas et al. 2022]. Since HateBR does not provide\nthe original pre-trained models and also the train/validation/test split configuration, it was\nnecessary to retrain each of the four models on this dataset to guarantee consistency be-\ntween the results: SVM, Naive Bayes, Logistic Regression, and MLP models. We also\nevaluated our fine-tuned version of the BERTimbau model [Souza et al. 2020], following\nthe TF-IDF method for feature extraction. For training the BERTimbau-based model, we\nutilized the tokenizer that comes pre-built with the model to perform feature extraction,\nas this is how BERT-based models operate.\n\nModel\nSVM\nNaiveBayes\nLogisticRegression\nMLP\nBERTimbau + fine-tuning\n\nPrecision Recall\n0.84\n0.87\n0.85\n0.82\n0.87\n\n0.87\n0.85\n0.86\n0.86\n0.92\n\nF1-score\n0.86\n0.86\n0.85\n0.84\n0.90\n\nTable 2.\n[Vargas et al. 2022].\n\nComparison of\n\ntrained models on the HateBR dataset\n\nThe data analysis shows that the BERTimbau + fine-tuning model employed in this\nstudy outperforms previous results in all comparison parameters, establishing this work\nas the state-of-the-art for the HateBR dataset. This result underscores the importance\nof using DL models in the NLP context, as they can yield significant benefits in text\nrecognition and classification processes.\n\nHaving completed the test of the BERTimbau model and trained the spaCy model\nfollowing the same methodology as OLID-BR [Trajano et al. 2023], we conducted the\nfinal evaluation of the complete pipeline. For this purpose, we executed the pipeline\nproposed in Figure 1, i.e., if a sentence is toxic, the offensive parts are detected and\ncensored. For presentation and qualitative evaluation purposes, censorship is performed\nby simply replacing the characters that constitute the offensive word or expression with\nasterisks (*).\n\nWe selected some news articles from specialized portals, such as G1, Estadão,\nand Catraca Livre, to perform a preliminary test, but no offensive language was detected.\nSubsequently, an opinion article about Chico Buarque from the Senso Incomum blog\n[Martins 2022] was used, where the presence of swear words, which would be appropri-\nately detected, was evident.\n\nTable 3 presents examples for qualitative analysis of the results of applying the\nproposed pipeline in this work. The Table shows the comparison between the original\ntext and the censored text. The demonstrated text is the only post from a collected blog,\nas the three obtained news articles contained no offensive language. These results are\nlimited to demonstrating the parts of the original text that our algorithm censored.\n\nOne consideration is that the model censored the word “censura” (in english cen-\nsorship), which is usually not considered offensive, as well as the expression “Rock das\n\n\fCensored Part\nobservador de *******\n\nOriginal Text Part\nobservador de bonobos\nA autocensura é o pior tipo de censura que existe A auto******* é o pior tipo de ******* que existe\nJoga pedra na Geni/Joga bosta na Geni/Ela é feita\nJoga pedra na Geni/Joga ***** na Geni/Ela é feita\npra apanhar/Ela é boa de cuspir/Ela dá pra qual-\npra ****************************Ela dá pra\nqualquer um/Maldita Geni\nquer um/Maldita Geni!\nseu lesbofóbico “Rock das Aranhas”!\nseu lesbofóbico ******************\nPaga-pau dos porcos estadunidenses, com toda\n**********************************,\ntoda certeza!\ncerteza!\nEspumando de ódio\nEspumando de ****\n\ncom\n\nTable 3. Examples of original text excerpts (on the left) and censored excerpts\n(on the right).\n\nAranhas” (Rock of the Spiders) instead of the previous word “lesbofóbico” (lesbopho-\nbic), which we judged to be far more offensive than an expression about rock and spiders.\nThe remaining censorships we considered appropriate.\n\n5.1. Source Code of the Experiments\n\nThe source code developed has been made publicly available in the form of Notebooks,\navailable on the GitHub Repository: https://github.com/ICDI/censorship-\noffensive-language:\n\n• The notebook for training and testing the model based on the BERTimbau model;\n• The notebook for training the spam detection model with spaCy, reusing the code\n\nfrom OLID-BR\n\n• The notebook with the tests using both models for text detection and censorship.\n\n6. Conclusion\n\nThis work developed an ML/DL-based program for detecting and censoring offensive\nlanguage in extended Portuguese texts extracted from the web. To this end, we present a\npipeline comprising two parts: one that detects the offensiveness in a portion of text (pre-\ncisely a sentence) and another that detects the offensive parts (spam) within the sentence.\nThis allows for censoring a large text part by part and returning the censored text.\n\nA positive aspect of this work is providing a program specifically focused on de-\ntecting and censoring large texts, presenting satisfactory results in quantitative and qual-\nitative analyses. As an evolution of this work, developing a specific dataset for large\ntext censorship can be envisaged, which, unlike the datasets used [Vargas et al. 2022] and\n[Trajano et al. 2023], would represent a significant advancement.\n\nThe results for detecting offensiveness in texts were superior to our original refer-\nence (the HateBR article), representing a new benchmark concerning the state-of-the-art,\nsurpassing the techniques used as a reference in the HateBR dataset [Vargas et al. 2022].\nThe censorship part was performed simply by replacing the characters in the offensive\nparts with asterisks, which can undoubtedly be improved by future work. The analysis\ncould have been more robust due to the lack of a specific dataset for large texts, even when\ndiscussing qualitative data. This fact demonstrates the need for creating a specific dataset\nbuilt from expert judgments on offensive language or by calculable metrics, which can\nevolve in future works.\n\n\fReferences\nCook, S.\nitech.\ncyberbullying-statistics/. (Accessed on 10/10/2024).\n\nfor 2024 | compar-\nhttps://www.comparitech.com/internet-providers/\n\nCyberbullying statistics and facts\n\n(2024).\n\nDevlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language\n\nunderstanding. arXiv preprint arXiv:1810.04805.\n\nEconomist\n\n(2019).\n\nIncreasing numbers of children have internet addiction –\nhttps://inews.co.uk/news/\n\nhow worried should parents really be?\nlong-reads/internet-addiction-children-increase-parents-\nguide-242434. (Accessed on 10/10/2024).\n\nHajibabaee, P., Malekzadeh, M., Ahmadi, M., Heidari, M., Esmaeilzadeh, A., Abdolaz-\nimi, R., and Jones, J. H. (2022). Offensive language detection on social media based on\ntext classification. 2022 IEEE 12th Annual Computing and Communication Workshop\nand Conference (CCWC), pages 0092–0098.\n\nHonnibal, M., Montani, I., Van Landeghem, S., and Boyd, A. (2020). spaCy: Industrial-\n\nstrength Natural Language Processing in Python.\n\nHusain, F. and Uzuner, O. (2021). A survey of offensive language detection for the ara-\nbic language. ACM Transactions on Asian and Low-Resource Language Information\nProcessing (TALLIP), 20(1):1–44.\n\nLeite, J. A., Silva, D., Bontcheva, K., and Scarton, C. (2020). Toxic language detection\nin social media for brazilian portuguese: New dataset and multilingual analysis. In\nProceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for\nComputational Linguistics and the 10th International Joint Conference on Natural\nLanguage Processing, pages 914–924.\n\nLeray, W. (2023). Série de harry potter?\n\nenvolvimento de j.k. rowling divide fãs.\n\nhttps://catracalivre.com.br/entretenimento/nova-serie-de-\nharry-potter-polemica-envolvendo-j-k-rowling-divide-fas/.\n(Accessed on 09/02/2024).\n\nMartins, T.\n\n(2022).\n\nChico buarque dá comida aos censores - senso inco-\nmum. https://sensoincomum.org/2022/01/28/chico-buarque-da-\ncomida-aos-censores/. (Accessed on 09/02/2024).\n\nMonteiro, E. (2023). Caso bruno e dom: justiça decide levar amarildo e outros dois réus\nhttps://g1.globo.com/am/amazonas/\n\na júri popular | amazonas | g1.\nnoticia/2023/10/03/caso-bruno-e-dom-justica-decide-\nlevar-amarildo-e-outros-dois-reus-a-juri-popular.ghtml.\n(Accessed on 09/02/2024).\n\nPelle, R. P. and Moreira, V. P. (2017). Offensive comments in the brazilian web: a dataset\nand baseline results. In Anais do VI Brazilian Workshop on Social Network Analysis\nand Mining. SBC.\n\nSouza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: pretrained bert models for\nbrazilian portuguese. In Intelligent Systems: 9th Brazilian Conference, BRACIS 2020,\nRio Grande, Brazil, October 20–23, 2020, Proceedings, Part I 9, pages 403–417.\nSpringer.\n\n\fTrajano, D., Bordini, R. H., and Vieira, R. (2023). Olid-br: offensive language identi-\nfication dataset for brazilian portuguese. Language Resources and Evaluation, pages\n1–27.\n\nTrielli, L. (2021). Escócia: estupradores que se declararem mulher serão colocados em\nprisões femininas - senso incomum. https://sensoincomum.org/2021/12/\n14/escocia-estupradores-que-se-declararem-mulher-serao-\ncolocados-em-prisoes-femininas/. (Accessed on 09/02/2024).\n\nVargas, F., Carvalho, I., Rodrigues de Góes, F., Pardo, T., and Benevenuto, F. (2022).\nHateBR: A large expert annotated corpus of Brazilian Instagram comments for offen-\nsive language and hate speech detection. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 7174–7183, Marseille, France. European\nLanguage Resources Association.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł.,\nand Polosukhin, I. (2017). Attention is all you need. Advances in neural information\nprocessing systems, 30.\n\nWagner Filho, J. A., Wilkens, R., Idiart, M., and Villavicencio, A. (2018). The brwac\ncorpus: a new open resource for brazilian portuguese. In Proceedings of the eleventh\ninternational conference on language resources and evaluation (LREC 2018).\n\n\f"
        },
        {
            "titulo": "Classificação de Notícias em Português Utilizando Modelos Baseados em Transferência de Aprendizagem e Transformers",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31133",
            "idioma": "Português",
            "storage_key": "files/article_31133_30936.pdf",
            "autores": [
                {
                    "nome": "Wagner Narde",
                    "afiliacao": "Grupo Energisa",
                    "orcid": "http://orcid.org/0009-0003-7896-5232"
                },
                {
                    "nome": "João Mendanha",
                    "afiliacao": "UFOP",
                    "orcid": null
                },
                {
                    "nome": "Henrique Barbosa",
                    "afiliacao": "UFMG",
                    "orcid": null
                },
                {
                    "nome": "Frederico Coelho",
                    "afiliacao": "UFMG",
                    "orcid": "https://orcid.org/0000-0002-7868-6968"
                },
                {
                    "nome": "Bruno Santos",
                    "afiliacao": "UFBA",
                    "orcid": "https://orcid.org/0000-0003-4501-2323"
                },
                {
                    "nome": "Luiz Torres",
                    "afiliacao": "UFOP",
                    "orcid": "https://orcid.org/0000-0002-4991-8395"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Fake news se espalha mais rápido em algumas redes sociais do que notícias regulares, o que pode ter diferentes consequências, desde influências nos resultados eleitorais até mortes devido a tratamentos incorretos de doenças. Este trabalho tem como objetivo empregar métodos baseados em aprendizado por transferência e modelos de aprendizado de máquina baseados em Transformers para classificar a veracidade de tweets na língua portuguesa (Brasil pt-BR). Para isso, foi criada uma base de dados confiável e rotulada, aberta para acesso gratuito. O conjunto de dados relaciona postagens extraídas do X (anteriormente conhecido como Twitter) e sua proximidade com fatos ou informações falsas. Subsequentemente, cinco modelos Transformer foram treinados em português. O modelo BERT ajustado, inicializado com pré-treinamento em textos em português, alcançou um desempenho superior, obtendo uma acurácia de 95.1%.",
            "keywords": [
                "Transformers",
                "Classificação",
                "Bert",
                "Aprendizado Supervisionado",
                "Notícias Falsas"
            ],
            "referencias": [
                "Data, P. (2024). Global social media users in 2024.",
                "DataReportal (2024). Social media users 2024 (global data & statistics).",
                "Gente, G. (2024). Pandemia e o consumo de notícias nas redes sociais.",
                "Henrique, J. (2018). Get old tweets programatically. Repository on GitHub.",
                "NegociosSC (2024). O uso da internet, redes sociais e mídia no brasil em 2024."
            ],
            "artigo_completo": "Classificac¸ ˜ao de Not´ıcias em Portuguˆes Utilizando Modelos\nBaseados em Transferˆencia de Aprendizagem e Transformers\n\nWagner Narde1, Jo˜ao Mendanha2, Henrique Barbosa4, Frederico Coelho4,\nBruno Santos3, Luiz Torres2\n\n1Grupo Energisa – Brasil\n\n2Dept. de Computac¸ ˜ao e Sistemas – Universidade Federal de Ouro Preto (UFOP)\n\n3Dept. de Ciˆencia da Computac¸ ˜ao – Universidade Federal da Bahia (UFBA)\n\n4Dept. de Engenharia Eletrˆonica – Universidade Federal de Minas Gerais (UFMG)\n\nwagner.b.n@hotmail.com, bruno.ps@ufba.br, luiz.torres@ufop.edu.br\n\nAbstract. Fake news spreads faster on some social networks than regular news,\nwhich can have different ramifications, from influences on election outcomes\nto deaths due to incorrect treatments of diseases. This work aims to employ\nmethods based on transfer learning and Transformer-based machine learning\nmodels to classify the veracity of tweets in the Portuguese language (Brazil pt-\nBR). To this aim, a reliably labeled database was created and opened to free\naccess. The dataset relates posts extracted from X (formerly Twitter) and their\nproximity with facts or fake information. Five Transformer models were sub-\nsequently trained in Portuguese. The fine-tuned BERT model, initialized with\npre-training on Portuguese text, achieved superior performance, yielding an ac-\ncuracy of 95.1%.\n\nResumo. Fake news se espalha mais r´apido em algumas redes sociais do que\nnot´ıcias regulares, o que pode ter diferentes consequˆencias, desde influˆencias\nnos resultados eleitorais at´e mortes devido a tratamentos incorretos de doenc¸as.\nEste trabalho tem como objetivo empregar m´etodos baseados em aprendizado\npor transferˆencia e modelos de aprendizado de m´aquina baseados em Transfor-\nmers para classificar a veracidade de tweets na l´ıngua portuguesa (Brasil pt-\nBR). Para isso, foi criada uma base de dados confi´avel e rotulada, aberta para\nacesso gratuito. O conjunto de dados relaciona postagens extra´ıdas do X (ante-\nriormente conhecido como Twitter) e sua proximidade com fatos ou informac¸ ˜oes\nfalsas. Subsequentemente, cinco modelos Transformer foram treinados em por-\ntuguˆes. O modelo BERT ajustado, inicializado com pr´e-treinamento em textos\nem portuguˆes, alcanc¸ou um desempenho superior, obtendo uma acur´acia de\n95.1%.\n\n1. Introduc¸ ˜ao\n\nOs ve´ıculos de comunicac¸ ˜ao de grande circulac¸ ˜ao por muito tempo foram jornais, revis-\ntas, r´adio e televis˜ao. Hoje, not´ıcias circulam atrav´es de v´ıdeos no YouTube, portais de\nnot´ıcia e em redes sociais como Facebook, X (Antigo Twitter) e WhatsApp. Tornando\na Internet, um dos principais meios de comunicac¸ ˜ao e consumo de not´ıcias. No Brasil,\n65% usam a Internet e suas aplicac¸ ˜oes como principais fontes de informac¸ ˜ao, nos Estados\n\n\fUnidos 53% e, no mundo, o n´umero estimado ´e de 62% [NegociosSC 2024, Gente 2024,\nDataReportal 2024, Data 2024]. O que, por um lado, demonstra que a Internet ampliou o\nacesso `a informac¸ ˜ao, mas, por outro lado, tamb´em transformou a forma como as not´ıcias\ns˜ao consumidas e compartilhadas.\n\nA ascens˜ao da Internet e, consequentemente, das redes sociais democratizaram\na produc¸ ˜ao de not´ıcias, permitindo que qualquer pessoa assuma o papel de produtor de\nconte´udo sem a supervis˜ao tradicional de jornalistas. Este fenˆomeno pode ter impactado\nnegativamente na qualidade das informac¸ ˜oes disseminadas, resultando em um aumento de\nnot´ıcias que propagam desinformac¸ ˜ao ou divulgam informac¸ ˜oes falsas [Reis et al. 2019,\nVargas et al. 2021].\n\n2. Proposta de Modelo\n\nNeste trabalho, propomos um modelo baseado em transferˆencia de aprendizagem, trans-\nformers e aprendizagem supervisionada para classificar textos em portuguˆes nas redes\nsociais, com foco na plataforma X. Tamb´em criamos uma base de dados em portuguˆes1\n(162 amostras e balanceada), que relaciona textos da plataforma X com sua veracidade,\nvisando melhorar a detecc¸ ˜ao de fake news e promover a qualidade da informac¸ ˜ao nas\nredes sociais.\n\n3. Metodologia\n\nO primeiro passo foi a construc¸ ˜ao de uma base de dados contendo postagens de usu´arios\nda rede social X (Antigo Twitter). Foi utilizada a ferramenta Get Old Tweets (GOT)\n[Henrique 2018] para coletar tweets hist´oricos, incluindo not´ıcias falsas. Dessa forma,\ncada tweet foi analisado e classificado manualmente para determinar sua proximidade\ncom o fato, assegurando que o conjunto de dados fosse rigoroso e preciso. Esse processo\npermitiu a criac¸ ˜ao de um conjunto de dados robusto e rotulado, essencial para o treina-\nmento e validac¸ ˜ao eficazes do modelo de classificac¸ ˜ao de textos em portuguˆes proposto.\nEm seguida, os textos coletados passam por uma fase de preparac¸ ˜ao, onde s˜ao inicial-\nmente pr´e-processados e, posteriormente, rotulados. Ap´os esse processo, os dados s˜ao\najustados para servirem como entradas adequadas para os modelos de aprendizagem de\nm´aquina.\n\n3.1. Conjunto de dados: Coleta e Processamento\n\nEste trabalho utiliza dados textuais em portuguˆes extra´ıdos da plataforma de rede social\nX. A plataforma permite a extrac¸ ˜ao de informac¸ ˜oes atrav´es de sua Application Program-\nming Interface (API). Com a rede social selecionada, iniciou-se a coleta de dados para\ncompor a base de dados. O processo de obtenc¸ ˜ao dos tweets consistiu em buscar no site\nde checagem de not´ıcias verdadeiras ou falsas (LUPA2) e pesquisar por elas utilizando\na ferramenta GOT [Henrique 2018]. Para isso, foram realizadas filtragens de not´ıcias e\nalinhamento temporal aproximado para obtenc¸ ˜ao das postagens realizadas sobre a not´ıcia\nverificada.\n\n1https://github.com/WagnerNarde/ML-Transformers-Tweets-falsos\n2https://lupa.uol.com.br/\n\n\f3.2. Selec¸ ˜ao dos Modelos de Aprendizagem\n\nNeste trabalho, foram adotados modelos de aprendizagem baseados em Transformers.\nOriginalmente desenvolvido para traduc¸ ˜ao autom´atica, o Transformer se destacou por\nsua capacidade de capturar relac¸ ˜oes de dependˆencia de longo alcance de forma efi-\ncaz. Buscou-se por modelos Transformers que receberam pr´e-treinamento em por-\ntuguˆes, visando aproveitar a transferˆencia de aprendizagem. Como resultado, optou-se\npor ajustar os seguintes modelos: BERT base pr´e-treinado em portuguˆes brasileiro por\n[Souza et al. 2020]; BERT base pr´e-treinado em 104 idiomas, incluindo portuguˆes, por\n[Devlin et al. 2019]; RoBERTa pr´e-treinado por [Liu et al. 2019] com um corpus de 6,9\nmilh˜oes de frases em portuguˆes; XLM-R base, pr´e-treinado por [Conneau et al. 2020]\nincluindo portuguˆes; e, por fim, o modelo ELECTRA uncased\nem 100 idiomas,\n[Clark et al. 2020], pr´e-treinado especificamente em portuguˆes.\n\n3.3. Treinamento\n\nPara avaliar a capacidade de generalizac¸ ˜ao do modelo, foi utilizado o m´etodo de validac¸ ˜ao\ncruzada com 10 partic¸ ˜oes (10-fold cross-validation). Este m´etodo divide o conjunto de\ndados em 10 sub-conjuntos. Cada subconjunto ´e usado uma vez como conjunto de teste,\nenquanto os restantes s˜ao usados como conjunto de treinamento. Esse processo ´e repetido\n10 vezes, garantindo que cada amostra do conjunto de dados seja utilizada para testes ao\nmenos uma vez. Esse procedimento n˜ao apenas melhora a capacidade de generalizac¸ ˜ao\ndo modelo, mas tamb´em fornece uma estimativa mais robusta do desempenho do modelo\nem dados n˜ao vistos.\n\n4. Resultados\n\nAs m´etricas de avaliac¸ ˜ao incluem Acur´acia, F1-Score, Precis˜ao, Sensibilidade (Recall) e\nMCC.\n\nTabela 1. Resultados obtidos em cada modelo\n´Epocas Acur´acia\n\nModelo\nELECTRA (uncased)\nRoBERTa pr´e-treinado em Portuguˆes\nXLM-R pr´e-treinado em multi-idiomas\nBERT com Pr´e-treinamento em Portuguˆes\nBERT com Pr´e-treinamento Multi-idioma\n\n10\n7\n9\n6\n10\n\n0.864\n0.901\n0.903\n0.944\n0.914\n\nF1\n0.848\n0.897\n0.898\n0.955\n0.918\n\nPrecis˜ao Sensibilidade MCC\n0.720\n0.812\n0.804\n0.887\n0.825\n\n0.883\n0.852\n0.883\n0.944\n0.900\n\n0.824\n0.962\n0.922\n0.971\n0.944\n\nOs resultados deste trabalho, apresentados na Tabela 1, mostram que cada mo-\ndelo de aprendizado de m´aquina treinado para a classificac¸ ˜ao de not´ıcias em portuguˆes\nteve um desempenho variado, dependendo do n´umero de ´epocas e das caracter´ısticas do\npr´oprio modelo. O ELECTRA (uncased), treinado com 10 ´epocas, apresentou o desem-\npenho mais baixo, o que pode ser atribu´ıdo `a falta de diferenciac¸ ˜ao entre letras mai´usculas\ne min´usculas, bem como `a qualidade dos pesos de pr´e-treinamento dispon´ıveis. O mo-\ndelo RoBERTa pr´e-treinado em Portuguˆes, configurado com 7 ´epocas, superou o ELEC-\nTRA, beneficiando-se de uma arquitetura que captura melhor as nuances lingu´ısticas do\nportuguˆes e diferencia entre mai´usculas e min´usculas. O modelo XLM-R pr´e-treinado\nem multi-idiomas, com 9 ´epocas, demonstrou uma leve superioridade em relac¸ ˜ao ao Ro-\nBERTa em termos de acur´acia e F1, aproveitando o conhecimento adquirido em m´ultiplos\nJ´a o BERT com Pr´e-treinamento\nidiomas para melhorar a compreens˜ao semˆantica.\n\n\fMulti-idioma, utilizando 10 ´epocas, mostrou robustez com acur´acia e F1 acima de 0.9,\ndestacando-se pela capacidade de transferir conhecimento lingu´ıstico de um corpus mul-\ntilingue para o portuguˆes. Por fim, o BERT pr´e-treinado em Portuguˆes foi o modelo com\nmelhor desempenho geral, utilizando apenas 6 ´epocas de treinamento. Este modelo se\ndestacou na classificac¸ ˜ao correta das not´ıcias, com acur´acia, F1 e MCC superiores, evi-\ndenciando a efic´acia do pr´e-treinamento espec´ıfico em portuguˆes e a importˆancia do ajuste\nfino dos hiperparˆametros para maximizar a efic´acia do modelo em tarefas espec´ıficas de\nclassificac¸ ˜ao de texto.\n\n5. Discuss˜ao\n\nOs modelos ELECTRA uncased pr´e-treinado em Portuguˆes e RoBERTa pr´e-treinado em\nPortuguˆes apresentaram resultados abaixo do esperado, pode-se levantar a quest˜ao de que\nse tais modelos passaram pelo mesmo processo de pr´e-treinamento dos outros m´etodos.\nO modelo RoBERTa exige mais recursos computacionais comparado com o Bert, al´em\nde ser um aprimoramento do mesmo, portanto, melhores resultados eram esperados desse\nmodelo. O modelo ELECTRA sendo um modelo uncase, esperava-se um desempenho\nabaixo dos outros classificadores pr´e-treinados exclusivamente em portuguˆes. Ainda as-\nsim, acredita-se que o modelo n˜ao conseguiu generalizar bem o problema.\n\nO XLM-R foi um modelo originalmente proposto para a traduc¸ ˜ao de idiomas, por\nisso ele est´a dispon´ıvel em vers˜ao multi-idiomas pr´e-treinado em v´arios idiomas, inclusive\nportuguˆes. Apesar do XLM-R n˜ao ter sido originalmente proposto para classificac¸ ˜ao de\ntexto, ele obteve resultados melhores que o ELECTRA.\n\nO Modelo BERT com Pr´e-treinamento em Portuguˆes obteve acur´acia e F1 superi-\nores a todos os outros modelos, mostrando que o pr´e-treinamento em portuguˆes feito por\n[Souza et al. 2020] foi muito eficiente e contribuiu positivamente para o bom desempenho\ndo modelo. Os resultados preliminares mostraram que o modelo foi capaz de classificar\nnoticias de uma base de dados relativamente pequena, bases de dados com poucas amos-\ntras ´e um desafio em algumas ´areas, como na sa´ude.\n\n6. Conclus˜oes\n\nEste trabalho apresentou uma abordagem para detecc¸ ˜ao de tweets falsos em portuguˆes\natrav´es de NLP. Al´em disso, foi criada e disponibilizada uma base de dados balanceada\ncom tweets classificados de forma confi´avel. A base possibilitou o treinamento de mo-\ndelos para detecc¸ ˜ao de not´ıcias falsas. Sendo que o modelo BERT com 6 ´epocas foi o\nmelhor comparado aos outros modelos testados.\n\n7. Trabalhos Futuros\n\nNa continuac¸ ˜ao do trabalho, pretendemos estender a avaliac¸ ˜ao comparativa com outros\nmodelos estado da arte da literatura de classificac¸ ˜ao de texto baseados em aprendizado\nprofundo. Pretende-se aumentar a base de dados com mais dados rotulados, mantendo a\nconfiabilidade, e tamb´em buscar dados de outras fontes. Al´em de mostrar os resultados\nda classificac¸ ˜ao de not´ıcias verdadeiras, planeja-se apresentar tamb´em os resultados de\nclassificac¸ ˜ao das not´ıcias falsas, assim como utilizar outras estrat´egias para o treinamento,\ncomo a validac¸ ˜ao cruzada com 5 partic¸ ˜oes.\n\n\fReferˆencias\n\n[Clark et al. 2020] Clark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. (2020). ELEC-\nTRA: Pre-training text encoders as discriminators rather than generators. In ICLR.\n\n[Conneau et al. 2020] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G.,\nGuzm´an, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). Unsupervi-\nsed cross-lingual representation learning at scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 8440–8451, Online.\nAssociation for Computational Linguistics.\n\n[Data 2024] Data, P. (2024). Global social media users in 2024. Accessed: 2024-06-28.\n\n[DataReportal 2024] DataReportal (2024). Social media users 2024 (global data & statis-\n\ntics). Accessed: 2024-06-28.\n\n[Devlin et al. 2019] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT:\nIn Pro-\nPre-training of deep bidirectional transformers for language understanding.\nceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computa-\ntional Linguistics.\n\nde\n[Gente 2024] Gente,\nhttps://gente.globo.com/\nnot´ıcias\nnas\npandemia-e-o-consumo-de-noticias-nas-redes-sociais/. Aces-\nsado em 28 de junho de 2024.\n\nG.\nredes\n\nPandemia\n\nconsumo\n\nsociais.\n\n(2024).\n\no\n\ne\n\n[Henrique 2018] Henrique, J. (2018). Get old tweets programatically. Repository on\n\nGitHub.\n\n[Liu et al. 2019] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,\nM., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly optimized bert\npretraining approach. arXiv preprint arXiv:1907.11692.\n\n[NegociosSC 2024] NegociosSC (2024).\n\nbrasil\n\nem 2024.\n\nno\no-uso-da-internet-redes-sociais-e-midia-no-brasil-em-2024/.\nAcessado em 28 de junho de 2024.\n\nO uso da internet,\n\nredes sociais e m´ıdia\nhttps://www.negociossc.com.br/blog/\n\n[Reis et al. 2019] Reis, J. C. S., Correia, A., Murai, F., Veloso, A., and Benevenuto, F.\nIEEE Intelligent Systems,\n\n(2019). Supervised learning for fake news detection.\n34(2):76–81.\n\n[Souza et al. 2020] Souza, F., Nogueira, R., and Lotufo, R. (2020). BERTimbau: pretrained\nIn 9th Brazilian Conference on Intelligent\n\nBERT models for Brazilian Portuguese.\nSystems, BRACIS, Rio Grande do Sul, Brazil, October 20-23 (to appear).\n\n[Vargas et al. 2021] Vargas, F., Benevenuto, F., and Pardo, T. (2021). Toward discourse-\nIn Proceedings of the Student\n\naware models for multilingual fake news detection.\nResearch Workshop Associated with RANLP 2021, pages 210–218.\n\n\f"
        },
        {
            "titulo": "Automatic Annotation of Enhanced Universal Dependencies for Brazilian Portuguese",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31134",
            "idioma": "Inglês",
            "storage_key": "files/article_31134_30937.pdf",
            "autores": [
                {
                    "nome": "Elvis A. de Souza",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0000-0001-9373-7412"
                },
                {
                    "nome": "Magali S. Duran",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-3843-4600"
                },
                {
                    "nome": "Maria das Graças V. Nunes",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-2776-6140"
                },
                {
                    "nome": "Gustavo Sampaio",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0009-0008-9067-8400"
                },
                {
                    "nome": "Giovanna Belasco",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0009-0007-1737-3312"
                },
                {
                    "nome": "Thiago A. S. Pardo",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2111-1319"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "This paper presents the first attempt to automatically annotate Enhanced Universal Dependencies for Brazilian Portuguese. We use a symbolic annotation system, based on graph rewriting rules, and modify its original rules to better suit the linguistic characteristics of Portuguese using a manually annotated sample from the journalistic portion of Porttinari treebank as ground truth. Our objective is to assess the performance of the automatic annotation for a novel language and to determine the extent of possible improvements through rule modifications. Results demonstrate significant performance enhancements, where linguistic-driven rule adjustments improved the annotation accuracy 11.38 points, achieving 96.05% F1-score.",
            "keywords": [
                "Enhanced Dependencies",
                "Universal Dependencies",
                "syntactic annotation",
                "corpus annotation",
                "graph rewriting"
            ],
            "referencias": [
                "Bai, J., Wang, Y., Chen, Y., Yang, Y., Bai, J., Yu, J., and Tong, Y. (2021). Syntax-BERT: Improving pre-trained transformers with syntax trees. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3011–3020.",
                "Bölücü, N., Rybinski, M., and Wan, S. (2023). Investigating the impact of syntax-enriched transformers on quantity extraction in scientific texts. In Proceedings of the Second Workshop on Information Extraction from Scientific Publications, pages 1–13, Bali, Indonesia.",
                "Bouma, G., Seddah, D., and Zeman, D. (2020). Overview of the iwpt 2020 shared task on parsing into enhanced universal dependencies. In 58th Annual Meeting of the Association for Computational Linguistics.",
                "Bouma, G., Seddah, D., and Zeman, D. (2021). From raw text to enhanced universal dependencies: The parsing shared task at iwpt 2021. In Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021), pages 146–157.",
                "Candido, A., Maziero, E., Specia, L., Gasperin, C., Pardo, T., and Aluisio, S. (2009). Supporting the adaptation of texts for poor literacy readers: a text simplification editor for Brazilian Portuguese. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 34–42, Boulder, Colorado.",
                "De Marneffe, M.-C., Dozat, T., Silveira, N., Haverinen, K., Ginter, F., Nivre, J., and Manning, C. D. (2014). Universal stanford dependencies: A cross-linguistic typology. In LREC, volume 14, pages 4585–4592.",
                "Duran, M., Lopes, L., Nunes, M. G. V., and Pardo, T. (2023). The dawn of the porttinari multigenre treebank: Introducing its journalistic portion. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 115–124, Porto Alegre, RS, Brasil. SBC.",
                "Duran, M. S. (2024). Anotação de enhanced dependencies. Disponível em:",
                ". Acesso em: 10 out. 2024.",
                "Guillaume, B. and Perrier, G. (2021). Graph rewriting for enhanced universal dependencies. In IWPT 2021-17th International Conference on Parsing Technologies.",
                "Lin, Y., Wang, C., Song, H., and Li, Y. (2021). Multi-head self-attention transformation networks for aspect-based sentiment analysis. IEEE Access, 9:8762– 8770.",
                "Nivre, J., De Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajic, J., Manning, C. D., McDonald, R., Petrov, S., Pyysalo, S., and Silveira, N. (2016). Universal dependencies v1: A multilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 1659–1666.",
                "Nivre, J., de Marneffe, M.-C., Ginter, F., Hajic, J., Manning, C. D., Pyysalo, S., Schuster, S., Tyers, F., and Zeman, D. (2020). Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4034–4043.",
                "Oliveira, L., Claro, D. B., and Souza, M. (2023). Dptoie: a portuguese open information extraction based on dependency analysis. Artificial Intelligence Review, 56(2):7015–7046.",
                "Pagano, A. S., Duran, M. S., and Pardo, T. A. S. (2023). Enhanced dependencies para o português brasileiro. In Proceedings of the 2nd Edition of the Universal Dependencies Brazilian Festival, pages 461–470.",
                "Schuster, S. and Manning, C. D. (2016). Enhanced english universal dependencies: An improved representation for natural language understanding tasks. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 2371–2378.",
                "Shi, T. and Lee, L. (2021). TGIF: Tree-graph integrated-format parser for enhanced UD with two-stage generic- to individual-language finetuning. In Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021), pages 213–224.",
                "Zhou, J., Zhang, Z., Zhao, H., and Zhang, S. (2020). LIMIT-BERT: Linguistics informed multi-task BERT. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4450–4461."
            ],
            "artigo_completo": "Automatic Annotation of Enhanced Universal Dependencies\nfor Brazilian Portuguese\n\nElvis A. de Souza, Magali S. Duran, Maria das Grac¸as V. Nunes,\nGustavo Sampaio, Giovanna Belasco, Thiago A. S. Pardo\n\n1N´ucleo Interinstitucional de Lingu´ıstica Computacional (NILC)\nInstituto de Ciˆencias Matem´aticas e de Computac¸ ˜ao, Universidade de S˜ao Paulo\n\n{elvis.desouza99,magali.duran}@gmail.com, gracan@icmc.usp.br,\n\n{gustavo.sampaio,giovannabelasco}@usp.br, taspardo@icmc.usp.br\n\nAbstract. This paper presents the first attempt to automatically annotate En-\nhanced Universal Dependencies for Brazilian Portuguese. We use a symbolic\nannotation system, based on graph rewriting rules, and modify its original rules\nto better suit the linguistic characteristics of Portuguese using a manually an-\nnotated sample from the journalistic portion of Porttinari treebank as ground\ntruth. Our objective is to assess the performance of the automatic annota-\ntion for a novel language and to determine the extent of possible improvements\nthrough rule modifications. Results demonstrate significant performance en-\nhancements, where linguistic-driven rule adjustments improved the annotation\naccuracy 11.38 points, achieving 96.05% F1-score.\n\n1. Introduction\nMorphological and syntactic annotation have shown to be relevant for several Natural\nLanguage Processing (NLP) initiatives. For instance, tasks of open information extrac-\ntion (Oliveira et al. 2023) and text simplification (Candido et al. 2009) may directly base\ntheir decisions on syntax. Considering the more recent trends of Large Language Models,\nseveral works have demonstrated improvements in results when linguistic knowledge is\nprovided (Zhou et al. 2020; Bai et al. 2021; Lin et al. 2021; B¨ol¨uc¨u et al. 2023). On the\nlinguistic perspective, linguistic annotation may help describing varied language phenom-\nena, possibly supporting the validation and/or proposal of new theories.\n\nUniversal Dependencies (UD) is a framework for the morphological, morphosyn-\ntactic and syntactic annotation of human languages. UD provides standardized guidelines\nand has been used to annotate over 283 treebanks for 161 languages, being widely adopted\nas it proposes consensual annotation decisions and allows comparative and multilingual\nefforts. Concerning the syntactic annotation, the UD framework supports two levels of\ndepth: basic dependency trees and enhanced graphs. Basic dependency trees provide in-\nformation on syntactic dependencies, where each token is connected to a governing (head)\ntoken through a relation (e.g., in the sentence The boy cried, “boy” is connected as subject\nto the head “cried” by a nsubj relation). Enhanced Universal Dependencies (EUD) gener-\nally build upon the basic dependencies by adding relations and nodes (or tokens) to make\nexplicit the implicit relationships between tokens (Nivre et al. 2020) (e.g., in Figure 1,\n“boy” is also connected to “left” by a nsubj enhanced relation, as it is shared by the verbs\n“cried” and “left”). This enhancement can facilitate NLP tasks by providing additional\ninformation.\n\n\fFigure 1. EUD annotation – the red nsubj dependency is a new EUD dependency.\n\nFigure 2. EUD annotation – relation extended with the lexical item “with”.\n\nThis paper investigates the issue of EUD annotation for Brazilian Portuguese. To\nthe best of our knowledge, this is the first evaluation of EUD annotation for this lan-\nguage. Following two previous shared tasks on EUD annotation (Bouma et al. 2020;\nBouma et al. 2021), which did not include Portuguese, we build upon one of the systems\nthat participated in the 2021 task, namely Grew (Guillaume and Perrier 2021), based on\ngraph rewriting rules for annotated syntactic trees. This symbolic system comes with\na set of original (and universal) rules, and we made a series of modifications based on\ncorpus investigation, generating an improved set of rules. The two sets of rules were ap-\nplied to a sample dataset from the journalistic portion of Porttinari (Duran et al. 2023), a\nPortuguese treebank available in the Universal Dependencies project catalog, which we\nmanually enriched with EUD annotation to assess the quality of the automatic annotation.\nTherefore, our objective is to verify the performance of the program’s original rules for\nPortuguese and how much we can improve it with modified rules.\n\nIn the end, we discuss persistent annotation errors and future perspectives on EUD\nautomatic annotation. As an additional contribution, the rules and the annotated data are\nalso made available to the interested reader.\n\n2. Related Work\nEUDs present significant challenges compared to traditional UD annotation.\nIn addi-\ntion to the UD website, where the guidelines are updated as needed, there is a series of\nworks discussing the relevance and explaining the application of this type of annotation\nin treebanks (De Marneffe et al. 2014; Nivre et al. 2016; Schuster and Manning 2016;\nNivre et al. 2020). The instantiation of these relations for Portuguese was introduced and\ndetailed in (Pagano et al. 2023). Overall, EUDs may include 6 annotation situations:\n\n1. Inclusion of the prepositions, coordinating conjunctions, and subordinating con-\njunctions lemmas in the label of the relations they introduce (as in Figure 2);\n2. Identification of the controlling subject of the null subject in xcomp clauses (as in\n\nFigure 3);\n\nFigure 3. EUD annotation – nsubj relation for a verb dependent of xcomp.\n\n\fFigure 4. EUD annotation – obj relation propagated to the dependent of conj.\n\nFigure 5. EUD annotation – “book” is the object of “read” and “that” is ref of\n“book”.\n\n3. Propagation, to the dependent of conj, of the relation that reaches the head of conj\n\n(as in Figure 4);\n\n4. Propagation, to the dependent of conj, of some relations that depart from the head\n\nof conj (as in Figure 1);\n\n5. Replacement of the relative pronoun in relative clauses with its antecedent, mark-\ning the relationship of the relative pronoun with its antecedent with a label exclu-\nsive to the EUD: ref (as in Figure 5);\n\n6. Insertion of an empty token to take the place of an elliptical predicate and estab-\nlishment of relationships of this empty token with the participants of the orphan\nrelation (as in Figure 6).\n\nWhile UD trees are simple hierarchical structures with a root, EUD graphs are\nconnected and can contain cycles. For example, in Figure 5, the node “book” is depen-\ndent of “read” in a obj relation, however, it is also governor of “read” in a relative clause\nrelation (acl:relcl), a basic syntactic annotation that is kept in the enhanced graph, estab-\nlishing a cycle between two nodes. Another challenge is that some relations are lexical-\nized (as in Figure 2), considerably increasing the set of labels to be predicted and making\nthem language-dependent. Additionally, a token can have more than one enhanced rela-\ntion, having multiple governors, and there may be additional empty tokens to represent\nelliptical predicates (Bouma et al. 2020). In Figure 1, the node “boy” has two governors:\nthe verbs “cried” and “left”, which are coordinated, while in the basic annotation only the\nfirst verb would be its governor. In Figure 6, an empty token, [has], has been added to the\nEUD graph to solve the elliptical predicate issue, and several dependencies were changed\nto fit this new token.\n\nThe shared tasks held at IWPT in 2020 (Bouma et al. 2020) and in 2021\n(Bouma et al. 2021) provided a platform for comparing results among different systems.\nTo date, there is no treebank annotated with EUD for the Portuguese language, meaning\n\nFigure 6. EUD annotation – an empty token [has] was inserted to account for an\nelliptical predicate.\n\n\fthat the language has never been subjected to any attempt of automatic annotation. To\nparticipate in the competition, a treebank did not need to have all six types of EUD; here,\nwe are testing a rule-based approach on a fully annotated Portuguese dataset with all six\ntypes of EUD, produced for the purpose of this work.\n\nThe system we chose to use, Grew, ranked seventh in the 2021 competition, with\n81.58% ELAS (a F1-score over EUD relations), being the best ranked symbolic-based\nsystem1. Our goal is to test the possibilities and limitations of a linguistically-driven\nrule-based approach, which can be constructed with linguistic supervision, being easily\napplied for other languages as well, without training, and with high interpretability.\n\n3. Methodology\n\nWe use two small gold-standard EUD sets: one for testing (gold-test) and one for devel-\nopment (gold-dev). The gold-dev set was drawn from Porttinari-base, the main portion of\nPorttinari, while gold-test was sourced from Porttinari-test, designed for evaluating auto-\nmatic annotation systems (Duran et al. 2023). Gold-dev comprises 100 manually selected\nsentences, chosen by a linguist to represent challenging EUD phenomena. In contrast, the\n100 test sentences were randomly selected to reflect the natural frequency of phenomena\nin Porttinari. Due to (intentional) differing selection methods, dev and test sets show dis-\nparities, e.g., the dev set contains 23 sentences with the orphan label (predicate ellipsis),\nwhereas the test set includes only two.\n\nWe begin our work analyzing Grew (Guillaume and Perrier 2021) original rules\nfor EUD, referred to as “original rules”, which are universal and ideally applicable to any\nlanguage. We observed the annotation results on the development set and, as errors were\nidentified, we created new rules and modified existing ones to address these deficiencies.\nNotably, none of the sentences from the test set influenced rule modifications. As a result\nof the process, we have the rule set named “modified rules.”\n\nOur evaluation focuses on the program’s overall F1-score (also named ELAS, i.e.,\nlabeled-attachment score over enhanced dependencies), as well as F1-score for each of\nthe 6 EUD types. To achieve this, we automatically classified each enhanced relation\ninto one of the 6 categories using linguistic rules. For example, we know, from Figure\n3, that nsubj relations from verbs that are xcomp dependents towards nominals, when the\nnominal also has a nsubj relation coming from the verb that is the xcomp governor, are\nrelations of the type “assignment of xcomp subjects”.\n\nGrew rules consist of patterns (that may involve any UD annotation information)\nto be identified in sentences and a set of commands to be executed when these patterns\nare found. These rules are incorporated into a mechanism known as a “strategy,” which\nallows for the control of which rules are applied for each language and in which order.\nFor instance, the resolution of predicate ellipses should be done first, as other rules related\nto the propagation of dependents of coordinated elements can be applied considering the\nempty token inserted in the sentence.\n\nIn Table 1, we find the number of rules for each type of EUD relation (1-6) in\nGrew rule set, according to our automatic identification of EUD types, plus our new rules\n(7). There are also “unclassified” rules, as they do not produce any visible changes to\n\n1The system is 7.66 points below the system that ranked first, TGIF (Shi and Lee 2021).\n\n\fa sentence, but rather implicit changes that are going to be used for other rules inside a\nGrew strategy. Besides the new rules, some of the original rules were modified, and they\nwill be seen in the Results section, where we consider how many times the rules for each\nEUD type have been applied before and after our modifications.\n\nEUD Types\n\nNumber of Rules\n\n1 - Addition of prepositions and conjunctions\n2 - Assignment of xcomp subjects\n3 - Propagation of conj head\n4 - Propagation of conj dependents\n5 - Annotation of relative pronoun referent\n6 - Inclusion of elliptical predicate\n7 - New rules\n\nUnclassified rules\n\n3\n11\n18\n8\n13\n23\n15\n\n64\n\nTable 1. Number of rules related to each EUD type\n\n4. Results\n\nBoth the test and dev samples were manually annotated for EUD. Table 2 presents a\ndescription of these corpora, as well as the distribution of each of the EUD types. The\nnumber of EUD relations in this section ignores relations that are simple replicas of basic\nrelations without any modifications, as well as punctuation relations. “More than one\nclassification” refers to relations that were classified as result of more than one EUD type\nin action; “Unclassified” refers to the few relations that could not be correctly classified\nas one of the six EUD types using our automatic type identification rules.\n\nSentences\nTokens\nEUD Relations\nSentences with elliptical predicates\n\n1 - Addition of prepositions and conjunctions\n2 - Assignment of xcomp subjects\n3 - Propagation of conj head\n4 - Propagation of conj dependents\n5 - Annotation of relative pronoun referent\n6 - Inclusion of elliptical predicate\n\nRelations with more than one classification\nUnclassified relations\n\ngold-dev % dev\n\ngold-test % test\n\n100\n2,213\n776\n23\n\n397\n44\n67\n45\n72\n113\n\n37\n1\n\n-\n-\n-\n23.0%\n\n51.16%\n5.67%\n8.63%\n5.8%\n9.28%\n14.56%\n\n4.77%\n0.13%\n\n100\n2,012\n587\n2\n\n-\n-\n-\n2.0%\n\n362\n34\n56\n30\n55\n11\n\n31\n8\n\n61.67%\n5.79%\n9.54%\n5.11%\n9.37%\n1.87%\n\n5.28%\n1.36%\n\nTable 2. Distribution of phenomena in the gold-standard samples of EUD\n\nRegarding the distribution of EUD types per sample, we see a reasonably large\ndifference between the two, with the frequency of phenomena always being higher in the\n\n\fdev sample. Particularly in class 6, the difference (14.56% of relations in gold-dev versus\n1.87% of relations in gold-test) is due to the fact that the orphan relation, indicative of\npredicate ellipsis, is infrequent in the corpus, as commented before.\n\nTable 3 shows how many times the rules for each EUD types were applied to\nannotate the gold-standard samples. The difference in applications from “Original” to\n“Modif.” are a result of the changes we made to these rules to make them suit our corpus.\nThe increase from 0 to 60 and 31 in “4 - Propagation of conj dependents” is due to the\nremoval of constraints in the original rules to better suit the Portuguese data. New rules,\nsuch as the one in Figure 7, could be classified into one of each EUD types, but were left\nas a new type to highlight that they are completely new.\n\nEUD Types\n\ngold-dev\n\ngold-test\n\nOriginal Modif. Original Modif.\n\n1 - Addition of prepositions and conjunctions\n2 - Assignment of xcomp subjects\n3 - Propagation of conj head\n4 - Propagation of conj dependents\n5 - Annotation of relative pronoun referent\n6 - Inclusion of elliptical predicate\n7 - New rules\n\n415\n36\n84\n0\n108\n71\n0\n\n448\n40\n87\n60\n117\n75\n75\n\n355\n27\n57\n0\n95\n6\n0\n\n374\n29\n57\n31\n95\n7\n8\n\nTable 3. Number of rule applications for each EUD type\n\nFigure 7. A new rule, created to annotate sentences such as “Essa lei permitiu-\nlhes ganhar um aumento de sal ´ario” (This law allowed them to earn a salary\nraise), where “lhes” is a pronominal indirect object (IOBJ) of a governor of xcomp\nrelation (HEADXCOMP), “permitiu”, thus it should gain a new enhanced relation\nas nsubj of the xcomp dependent (DEPXCOMP), “ganhar”.\n\nTable 4 shows the program’s performance considering both samples (test and dev)\nand both sets of rules (original and modified). ELAS indicates the overall performance of\nthe program. Items 1 to 6 represent the performance, according to the F1-score metric,\nfor each of the six types of EUD. The last line shows the number of sentences where an\nempty token insertion was made to resolve an ellipsis, but the insertion was incorrectly\nmade. Considering that sentences with ellipses are more challenging to annotate, as they\nrequire the empty token inserted into the sentence to be placed in the correct position,\nand considering that various relations in the sentence may suffer negative impact due\n\n\fgold-dev\n\ngold-test\n\nOriginal Modif. Original Modif.\n\nELAS\nELAS (excluding sentences w/ ellipses)\n\n61.36% 78.97% 84.67% 96.05%\n88.50% 99.07% 88.97% 96.05%\n\n1 - Addition of prepositions and conjunctions 93.35% 98.99% 95.17% 98.90%\n85.39% 89.89% 92.54% 97.06%\n2 - Assignment of xcomp subjects\n72.00% 87.94% 84.13% 96.43%\n3 - Propagation of conj head\n84.11% 92.47% 96.67% 96.67%\n4 - Propagation of conj dependents\n88.28% 100.0% 94.23% 94.23%\n5 - Annotation of relative pronoun referent\n100.0%\n9.05%\n6 - Inclusion of elliptical predicate\n\n40.71% 0%\n\nSentences with misplaced empty token\n\n21\n\n8\n\n2\n\n0\n\nTable 4. Overall ELAS and by EUD type\n\nto the incorrect placement of this empty token, we calculated two types of ELAS: one\nconsidering the entire sample, and another excluding the sentences with predicate ellipses.\n\nOverall, we observe that the numbers are lower in the development sample, re-\nflecting the fact that it contains many more sentences with ellipses than the test sample\nand that the phenomena were selected for their complexity. The results are superior using\nthe modified rule set, reaching up to 99.07% ELAS for the development sample, exclud-\ning sentences with ellipses. For sentences with predicate ellipses, we reduced the number\nof errors in empty token insertion. In the test sample, errors dropped from 2 to 0, and in\nthe development sample, from 21 to 8. Consequently, in the test sample, the results for\nrelations related to the inclusion of the elliptical predicate reach 100%, but in the devel-\nopment sample, where the sentences are more complex, we only achieve 40.71% ELAS,\nindicating that there is still room for improvement in particularly difficult sentences.\n\nComparing the modified and the original rule numbers, the obtained performance\nimprovement is evident. When using the regular data distribution of the treebank as\nbenchmark (test data), where predicate ellipsis is not very frequent, we perform 11.38\nabsolute ELAS better using the modified rule set in comparison to the original set.\n\nby\n\nAs\n\nthe\n\nGrew\n\nnoted\n\n2021\n(Guillaume and Perrier 2021),\nthe parser’s performance heavily relies on the accu-\nracy of the basic syntactic parser. Working with gold UD annotation, the EUD annotation\nis above 92% ELAS for all languages, being English the one with the highest performance\n(99.0% ELAS) and Lithuanian the lowest one (92.1%). Our result for Portuguese, in\ncomparison, would be of 96.05% ELAS using the modified rule set.\n\nteam submission\n\nIWPT\n\nto\n\nWe observed that labeling the dependency relations between the empty token and\nthe former participants of the orphan relation remains particularly challenging. Clues to\nthis can be found in the head clause of conj: the available dependency relations are those\nthat exist in the head clause and do not exist in the dependent clause. However, semanti-\ncally equivalent arguments often have different syntactic forms (for example, a temporal\nmodifier may occur as advmod, obl, or advcl), which makes labeling the dependency re-\nlations difficult. The task is computationally complex, and, since the occurrence of this\n\n\fFigure 8.\ncriminals, free” (loose translation).\n\nIncorrect EUD annotation of the sentence “We get arrested and the\n\nphenomenon is infrequent, we recommend manually reviewing all relations after insertion\nof the empty token until we advance in the solutions to improve accuracy.\n\nWe noticed that the enhanced dependencies of elliptical token insertion and coref-\nerent annotation (ref ), because they present an alternative annotation to that of the basic\ndependencies, constitute a new basis for the other enhanced dependencies. This has two\nimplications: (1) since they constitute a new basis, these two enhanced types must be\nannotated before the others, and (2) errors in these two enhanced types can generate cas-\ncading errors in the other enhanced annotations. For example, in the sentence of Figure\n8, when the program does not identify that “bandidos” is the subject of the empty token,\nthe subject slot is empty and the conj subject propagation rules annotate “gente” as the\nsubject of the empty token, which is incorrect.2\n\n5. Final Remarks\nWe have addressed the issue of automatic enhanced dependencies annotation for Por-\ntuguese, which, to the best of our knowledge, consists in the first attempt for this lan-\nguage. The presented system along with our modified rules has shown its effectiveness\nin automatically generating complete annotations, which serve as a valuable resource for\nfurther linguistic analysis and model training, achieving an overall ELAS of 96.05% over\ngold basic syntactic annotation.\n\nThe next step is to use this system and rules to fully annotate Porttinari, creating\nthe first UD treebank with EUD annotations for Brazilian Portuguese. By leveraging\nthe capabilities of Grew, we aim to provide comprehensive and accurate annotations that\ninclude all 6 types of enhanced dependencies, which will be done in batches with human\nsupervision to ensure the dataset quality3.\n\nMore information about this work may be found at the POeTiSA project web\n\nportal: https://sites.google.com/icmc.usp.br/poetisa\n\nAcknowledgments\nThis work was carried out at the Center for Artificial Intelligence of the University of\nS˜ao Paulo (C4AI - http://c4ai.inova.usp.br/), with support by the S˜ao Paulo Research\nFoundation (FAPESP grant #2019/07665-4) and by the IBM Corporation. The project\nwas also supported by the Ministry of Science, Technology and Innovation, with resources\nof Law N. 8,248, of October 23, 1991, within the scope of PPI-SOFTEX, coordinated by\nSoftex and published as Residence in TIC 13, DOU 01245.010222/2022-44.\n\n2Further discussion of specific enhancements for Portuguese can be found in the annotation technical\n\nreport (Duran 2024).\n\n3The rules and the data that we used are publicly available at https://github.com/alvelvis/\n\ngrew-ed-portuguese.\n\n\fReferences\n\n[Bai et al. 2021] Bai, J., Wang, Y., Chen, Y., Yang, Y., Bai, J., Yu, J., and Tong, Y. (2021).\nSyntax-BERT: Improving pre-trained transformers with syntax trees. In Proceedings\nof the 16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume, pages 3011–3020.\n\n[B¨ol¨uc¨u et al. 2023] B¨ol¨uc¨u, N., Rybinski, M., and Wan, S. (2023).\n\nInvestigating the\nimpact of syntax-enriched transformers on quantity extraction in scientific texts.\nIn\nProceedings of the Second Workshop on Information Extraction from Scientific Publi-\ncations, pages 1–13, Bali, Indonesia.\n\n[Bouma et al. 2020] Bouma, G., Seddah, D., and Zeman, D. (2020). Overview of the\niwpt 2020 shared task on parsing into enhanced universal dependencies. In 58th Annual\nMeeting of the Association for Computational Linguistics.\n\n[Bouma et al. 2021] Bouma, G., Seddah, D., and Zeman, D. (2021). From raw text to\nenhanced universal dependencies: The parsing shared task at iwpt 2021. In Proceed-\nings of the 17th International Conference on Parsing Technologies and the IWPT 2021\nShared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021), pages\n146–157.\n\n[Candido et al. 2009] Candido, A., Maziero, E., Specia, L., Gasperin, C., Pardo, T., and\nAluisio, S. (2009). Supporting the adaptation of texts for poor literacy readers: a text\nsimplification editor for Brazilian Portuguese. In Proceedings of the Fourth Workshop\non Innovative Use of NLP for Building Educational Applications, pages 34–42, Boul-\nder, Colorado.\n\n[De Marneffe et al. 2014] De Marneffe, M.-C., Dozat, T., Silveira, N., Haverinen, K.,\nGinter, F., Nivre, J., and Manning, C. D. (2014). Universal stanford dependencies: A\ncross-linguistic typology. In LREC, volume 14, pages 4585–4592.\n\n[Duran et al. 2023] Duran, M., Lopes, L., Nunes, M. G. V., and Pardo, T. (2023). The\nIn\ndawn of the porttinari multigenre treebank: Introducing its journalistic portion.\nAnais do XIV Simp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Hu-\nmana, pages 115–124, Porto Alegre, RS, Brasil. SBC.\n\n[Duran 2024] Duran, M. S. (2024). Anotac¸ ˜ao de enhanced dependencies. Dispon´ıvel\nem: https://repositorio.usp.br/item/003209188. Acesso em: 10 out.\n2024.\n\n[Guillaume and Perrier 2021] Guillaume, B. and Perrier, G. (2021). Graph rewriting for\nIn IWPT 2021-17th International Conference on\n\nenhanced universal dependencies.\nParsing Technologies.\n\n[Lin et al. 2021] Lin, Y., Wang, C., Song, H., and Li, Y. (2021). Multi-head self-attention\ntransformation networks for aspect-based sentiment analysis. IEEE Access, 9:8762–\n8770.\n\n[Nivre et al. 2016] Nivre, J., De Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajic, J., Man-\nning, C. D., McDonald, R., Petrov, S., Pyysalo, S., and Silveira, N. (2016). Universal\nIn Proceedings of the Tenth\ndependencies v1: A multilingual treebank collection.\nInternational Conference on Language Resources and Evaluation (LREC’16), pages\n1659–1666.\n\n[Nivre et al. 2020] Nivre, J., de Marneffe, M.-C., Ginter, F., Hajic, J., Manning, C. D.,\nPyysalo, S., Schuster, S., Tyers, F., and Zeman, D. (2020). Universal Dependencies\nv2: An evergrowing multilingual treebank collection. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference, pages 4034–4043.\n\n\f[Oliveira et al. 2023] Oliveira, L., Claro, D. B., and Souza, M. (2023). Dptoie: a por-\ntuguese open information extraction based on dependency analysis. Artificial Intelli-\ngence Review, 56(2):7015–7046.\n\n[Pagano et al. 2023] Pagano, A. S., Duran, M. S., and Pardo, T. A. S. (2023). Enhanced\nIn Proceedings of the 2nd Edition of the\n\ndependencies para o portuguˆes brasileiro.\nUniversal Dependencies Brazilian Festival, pages 461–470.\n\n[Schuster and Manning 2016] Schuster, S. and Manning, C. D. (2016). Enhanced en-\nglish universal dependencies: An improved representation for natural language under-\nIn Proceedings of the Tenth International Conference on Language\nstanding tasks.\nResources and Evaluation (LREC’16), pages 2371–2378.\n\n[Shi and Lee 2021] Shi, T. and Lee, L. (2021). TGIF: Tree-graph integrated-format\nparser for enhanced UD with two-stage generic- to individual-language finetuning.\nIn Proceedings of the 17th International Conference on Parsing Technologies and the\nIWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT\n2021), pages 213–224.\n\n[Zhou et al. 2020] Zhou, J., Zhang, Z., Zhao, H., and Zhang, S. (2020). LIMIT-BERT:\nLinguistics informed multi-task BERT. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 4450–4461.\n\n\f"
        },
        {
            "titulo": "Disfluency Detection and Removal in Speech Transcriptions via Large Language Models",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31135",
            "idioma": "Inglês",
            "storage_key": "files/article_31135_30938.pdf",
            "autores": [
                {
                    "nome": "Pedro L. S. de Lima",
                    "afiliacao": "UFCG",
                    "orcid": "http://orcid.org/0009-0003-3924-2808"
                },
                {
                    "nome": "Cláudio E. C. Campelo",
                    "afiliacao": "UFCG",
                    "orcid": "https://orcid.org/0000-0003-4404-2344"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "The field of Automatic Speech Recognition (ASR) has significantly expanded within the technological landscape due to its extensive use in sectors such as education, healthcare, and customer service. Many modern applications depend on analyzing spoken content through Speech-to-Text (STT) conversion models. However, transcriptions produced by these systems often contain undesirable elements, such as word repetitions and the prolongation of certain sounds, known as disfluencies or linguistic crutches. These elements can negatively affect the quality of automatic content analysis by Natural Language Processing (NLP) models, including those for named entity recognition, emotion detection, or sentiment analysis. Therefore, this study aims to evaluate the feasibility of identifying and eliminating linguistic disfluencies using Large Language Models (LLMs), such as GPT-4, LLaMA, Claude, and Gemini, through Prompt Engineering techniques. The approach was tested using a corpus of debate transcriptions with manually annotated disfluency occurrences, yielding promising results.",
            "keywords": [
                "Automatic Speech Recognition",
                "Linguistic Disfluencies",
                "Large Language Models",
                "Prompt Engineering",
                "Speech-to-Text"
            ],
            "referencias": [
                "Anthropic. (2024). Claude 3.5 Sonnet.",
                "Baevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.",
                "Bassi, S., Duregon, G., Jalagam, S., & Roth, D. (2023). End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining.",
                "Corley, M., & Stewart, O. W. (2008). Hesitation disfluencies in spontaneous speech: The meaning of um. Language and Linguistics Compass, 2(4), 589-602.",
                "Ferguson, J., Durrett, G., & Klein, D. (2015). Disfluency detection with a semi-markov model and prosodic features. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 257-262).",
                "Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., & Mohamed, A. (2021). HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.",
                "Meta. (2024). Introducing LLaMA 3: Advancements in Large Language Models.",
                "OpenAI, Achiam, J., Adler, S., et al. (2024). GPT-4 Technical Report.",
                "OpenAI. (2024). OpenAI Tokenizer.",
                "Romana, A., Koishida, K., & Provost, E. M. (2023). Automatic Disfluency Detection from Untranscribed Speech.",
                "Snover, M., Dorr, B., & Schwartz, R. (2004). A lexically-driven algorithm for disfluency detection. In Proceedings of HLT-NAACL 2004: Short Papers (pp. 157-160).",
                "Team, G., Georgiev, P., and et al., V. I. L. (2024). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
                "Zayats, V., Ostendorf, M., & Hajishirzi, H. (2016). Disfluency detection using a bidirectional LSTM."
            ],
            "artigo_completo": "Disfluency Detection and Removal in Speech Transcriptions\nvia Large Language Models\n\nPedro L. S. de Lima1 , Cl´audio E. C. Campelo1\n\n1Departamento de Sistemas e Computac¸ ˜ao\nUniversidade Federal de Campina Grande\nCampina Grande, Para´ıba - Brasil\n\npedro.lima@ccc.ufcg.edu.br\n\ncampelo@dsc.ufcg.edu.br\n\nAbstract. The field of Automatic Speech Recognition (ASR) has significantly ex-\npanded within the technological landscape due to its extensive use in sectors\nsuch as education, healthcare, and customer service. Many modern applica-\ntions depend on analyzing spoken content through Speech-to-Text (STT) conver-\nsion models. However, transcriptions produced by these systems often contain\nundesirable elements, such as word repetitions and the prolongation of certain\nsounds, known as disfluencies or linguistic crutches. These elements can neg-\natively affect the quality of automatic content analysis by Natural Language\nProcessing (NLP) models, including those for named entity recognition, emo-\ntion detection, or sentiment analysis. Therefore, this study aims to evaluate the\nfeasibility of identifying and eliminating linguistic disfluencies using Large Lan-\nguage Models (LLMs), such as GPT-4, LLaMA, Claude, and Gemini, through\nPrompt Engineering techniques. The approach was tested using a corpus of\ndebate transcriptions with manually annotated disfluency occurrences, yielding\npromising results.\n\n1. Introduction\n\nAutomatic Speech Recognition (ASR) has become essential in modern society, enabling\nthe conversion of human speech into written text. This technology facilitates a range of\napplications through Speech-to-Text (STT) models, including virtual assistants, meeting\ntranscription, automatic captioning, and customer service. Despite significant advances\nin speech recognition accuracy, a constant feature in transcriptions generated by these\nsystems is the presence of linguistic disfluencies. During human speech production, it is\ncommon to generate various sounds within speech, known as disfluencies.\n\nDisfluencies have been extensively studied and are primarily classified into three\ntypes: hesitations, repetitions, and corrections [Corley and Stewart 2008]. When a speech\nmodel transcribes voice into text, it often overlooks the context of the spoken words, fo-\ncusing instead on achieving an accurate transcription. As a result, these disfluencies are\ncommon and appear in caption transcriptions, meeting notes, and any text derived from\nspontaneous human speech. Various studies explore different techniques for disfluency\n\n\fdetection, ranging from unimodal to multimodal approaches, some even use Transformer-\nbased methods, but none thoroughly investigate the utility of modern and widely accessi-\nble Large Language Models (LLMs) for the detection and removal of linguistic crutches.\n\nLLMs based on Transformers present a promising alternative. Due to their ability\nto capture complex contexts and understand linguistic nuances, such as differentiating\nbetween disfluent and fluent text, they present a promising alternative. LLMs can be easily\nmanipulated using Prompt Engineering techniques, which involve creating instructions to\nguide their behavior toward a specific goal. This work aims to fill a gap in the study of\ndisfluency detection and removal in text transcriptions by evaluating the capabilities of the\nmost advanced LLMs available today, such as OpenAI’s GPT-4o [OpenAI et al. 2024],\nGemini 1.5 Pro Experimental 0827 [Team et al. 2024], Claude 3.5 [Anthropic 2024] and\nLLaMMa 3 (70B parameters) [Meta 2024] to assess the extent of their applicability to\nthis task.\n\nThe main contributions of this paper include:\n\n• An analysis of LLMs’ ability to remove particular text excerpts while preserving\n\nother relevant information.\n\n• A comparative analysis of available models and their effectiveness in handling\n\ntranscribed spontaneous human speech.\n\n• An assessment of the feasibility, in terms of computational cost, of cleaning tran-\n\nscriptions of natural human speech.\n\n• A dataset with annotated disfluencies in Brazilian Portuguese.\n\nThe following sections of this paper are organized as follows: Section 2 presents a\nliterature review, covering foundational and relevant research on disfluency detection and\nremoval, leading up to the current state-of-the-art. Section 3 details the research method-\nology, explaining data collection and handling processes, as well as the construction of\nprompts and an exploratory data analysis, followed by Section 4, which presents the re-\nsults. Finally, Section 5 offers the conclusion.\n\n2. Related Work\n\nResearch on the detection and removal of disfluencies in speech encompasses a variety\nof techniques, each contributing to the advancement of the state-of-the-art in this field.\nStudies in this domain typically utilize one of three types of input: textual transcription,\naudio signal, or a combination of the two. Unimodal solutions rely on a single source of\ninformation, whereas multimodal solutions integrate multiple sources, such as audio and\ntext, to perform the task of disfluency detection/removal. The next subsections present\nresearch carried out using the unimodal text approach, followed by the unimodal audio\napproach, a comparison between the two approaches, and finally the conclusion of this\nsection.\n\n2.1. Text-Based Approaches\n\nIn this context, [Snover et al. 2004] proposed a Transformation-Based Learning (TBL)\nalgorithm for disfluency detection in speech transcriptions, employing lexical features\n(word usage and sentence structure). The system, referred to as System A, achieved\nresults comparable to those employing prosodic features (variations in intonation, rhythm,\n\n\fduration, and intensity/volume of speech), demonstrating that satisfactory performance\ncan be achieved without heavily relying on prosodic cues. The study underscores the\nimportance of features such as the lexeme itself, Part-of-Speech (POS) tags, and word\nfrequency for the speaker in identifying disfluencies. System A showed promising results\nin detecting various types of disfluencies and paved the way for future research focused\non natural language processing techniques.\n\n[Ferguson et al. 2015] proposed a conditional semi-Markovian method (semi-\nCRF) for disfluency detection in speech transcriptions, focusing on repairs such as repe-\ntitions and false starts. This technique utilizes lexical, structural, and prosodic features,\nsuch as pauses and word duration, extracted from alignment with the speech signal. This\napproach achieved an F-score of 85.4% on the Switchboard corpus (a dataset consisting of\nEnglish telephone conversations collected in the United States during the 1990s), surpass-\ning the performance of previous studies. Concurrently, [Zayats et al. 2016] introduced\na novel method for disfluency detection in speech transcriptions using a Bidirectional\nLSTM (BLSTM) neural network. Their solution employs word embeddings (numerical\nrepresentations of words), POS tags, and lexical pattern features as input. Additionally,\nthe model incorporates an explicit repair mechanism and uses Integer Linear Program-\nming (ILP) to enforce structural constraints on the disfluency sequence. This approach\nachieved an F-score of 85.9% on the Switchboard corpus. Analysis of the results indicates\nthat this approach performs better in detecting complex disfluencies that do not involve\nmere repetitions of words. Despite its effectiveness, the model’s reliance on predefined\nresources limits its adaptability to different types of disfluencies, contexts, and speaking\nstyles.\n\n[Bach and Huang 2019] also explored the BiLSTM technique with self-attention\nfor disfluency detection in speech transcriptions. The authors demonstrated competitive\nresults with BERT on the Switchboard corpus, outperforming it in terms of robustness and\nefficiency on out-of-domain datasets. The artificial addition of extra and incorrect words\nduring model training proved highly effective in enhancing its robustness to various data\ntypes and transcription errors, making it a compelling alternative for disfluency detection\nin real-world scenarios. Furthermore, the proposed models are smaller than BERT, which\nresults in reduced computational resource requirements overall.\n\n2.2. Audio-Based Approaches\n\n[Bassi et al. 2023] propose an end-to-end approach for speech transcription with disflu-\nency removal using a large-scale pre-trained HuBERT acoustic model. The traditional\ntwo-step method, which first transcribes the audio into text and then removes disfluen-\ncies, neglects the prosodic cues present in the original audio. The proposed approach\nprocesses the audio directly and uses acoustic representations learned during pre-training\nto identify and remove disfluencies during transcription. The authors demonstrate that\nthe end-to-end solution surpasses the two-step approach in terms of Word Error Rate\n(WER) and Character Error Rate (CER) on the Switchboard test set, achieving 12.2%\nWER and 7.3% CER. The study also highlights the significance of the pre-training objec-\ntive: HuBERT, pre-trained with a clustering objective that groups audio representations\nbased on similarities, significantly outperformed Wav2Vec2, which was pre-trained with\na contrastive objective that maximizes similarity among similar samples and minimizes\nsimilarity among different samples. These results suggest that end-to-end models with\n\n\flarge-scale acoustic pre-training with clustering objectives are a promising approach for\naccurate disfluent speech transcription.\n\n2.3. Comparison Between Unimodal and Multimodal Models\n\n[Romana et al. 2023] investigated the automatic detection of disfluencies in speech by\ncomparing language-based, acoustic, and multimodal methods. Their results demon-\nstrated that while language models such as BERT exhibited high accuracy with manual\ntranscriptions, performance significantly declined with the use of transcriptions gener-\nated by Automatic Speech Recognition (ASR). Acoustic approaches utilizing models like\nWav2Vec 2.0, HuBERT, and WavLM proved promising by avoiding reliance on tran-\nscriptions. However, the authors found that multimodal solutions combining acoustic and\nlinguistic information through a BLSTM fusion network achieved the best results, out-\nperforming unimodal techniques in disfluency detection and categorization. This study\nhighlights the potential of multimodal methods for creating more robust disfluency detec-\ntion systems.\n\nThe academic works presented in this section illustrate the progress made in the\nfield, with advanced techniques in artificial intelligence, transformers, and robust mul-\ntimodal methods applicable to various data types and transcription errors. These solu-\ntions have proven effective in detecting and removing disfluencies across diverse con-\ntexts. However, the use of widely available Large Language Models (LLMs) for cleaning\nautomatic transcriptions has been insufficiently studied. Therefore, there is a need to in-\nvestigate how LLMs can be leveraged for this purpose, complementing the advancements\nachieved in the reviewed academic works and democratizing access to these technologies.\n\n3. Methodology\n\nThis section contains information about the methodology used in the research, including\nhow the data was obtained and organized, and the construction of the prompts.\n\n3.1. The Dataset\n\nThe dataset for this study consists of text extracted from four debate sessions held at\nFederal University of Campina Grande to analyze debater performance. It includes 114\nminutes of transcribed audio in Portuguese, providing insights into the dynamics and\neffectiveness of various debating techniques. The debates were moderated, with each ses-\nsion involving 4 to 5 debaters discussing topics related to Artificial Intelligence. Each\ndebater had a chance to speak following questions posed by the moderator, and interrup-\ntions were not allowed, resulting in a free-flowing and spontaneous discourse. After the\ndebates, the audio recordings were transcribed using Microsoft’s Azure model, with the\ntranscripts stored in a JSON file. This file was then converted into Excel tables containing\nall the transcribed data. The data underwent human review to correct major transcription\nerrors, such as non-existent or meaningless words. Additionally, each table was annotated\nfor disfluencies. Disfluencies were categorized into three types: hesitations, repetitions,\nand corrections. Four HTML-style tags were created to mark these disfluencies in the\ntext:\n\n• <hes {content}/>, which marks hesitations\n• <rep {content}/>, which marks repetitions\n\n\f• <erro {content}/>, which marks errors\n• <corr {content}/>, which marks corrections\n\nThis marking and correction process resulted in four Excel files with the tran-\nscriptions of the respective debates. These files were then subjected to an exploratory\ndata analysis.\n\n3.2. The Prompts\n\nTo perform the task of disfluency detection and removal, four different prompts were\ndeveloped. To determine which prompt technique is most effective, three types of prompt\nengineering methods were tested:\n\n• Zero-Shot Prompting\n• Few-Shot Prompting\n• Chain-of-Thought Prompting\n\nThese three types of prompts differ significantly in how they present information\nto the language model (LLM), and the study aims to understand the extent of the LLMs’\nknowledge about disfluencies. In the Zero-Shot case, the prompt provides little or no\ncontext about the task, so it was divided into two prompts. The first prompt is a direct\ncommand to the LLM to remove repetitions, hesitations, and corrections from the text,\nwhile keeping it otherwise unchanged. The second prompt adds a description of what\ndisfluencies are and how the three targeted types are characterized. The Few-Shot prompt\nincludes all the information from the first two prompts, as well as an example of disflu-\nent text in three stages: the original disfluent text, the text with disfluency tags, and the\ncleaned text. Finally, the Chain-of-Thought prompt is designed to help the LLM adopt a\nstep-by-step approach to detecting and removing disfluencies from the text. These four\nprompts were executed with each of the LLMs. The average number of tokens processed\nby the LLMs in Group 14, the smallest group, ranged from 4,273 tokens (with the smallest\nprompt) to 5,041 tokens (with the largest prompt). In contrast, Group 1, the largest group,\nprocessed between 5,250 tokens (smallest prompt) and 6,004 tokens (largest prompt).\nThis calculation was estimated using the Tokenizer from the OpenAI Platform.\n\nPrompting Technique Context\nZero-Shot Prompting\nZero-Shot Prompting\nFew-Shot Prompting\n\nChain-of-Thought\nPrompting\n\nNone\nDefinition of disfluencies\nDefinition of disfluencies and a three-stage snapshot\nof the text during the disfluency cleaning process\nDefinition of disfluencies plus a guide on how to rec-\nognize and remove each type of disfluencies\n\nTable 1. Prompts Created For the Task\n\n3.3. Exploratory Data Analysis\n\nData from the tagged transcriptions in Excel files were analyzed to gain an overview of\nhow each disfluent text is characterized. The initial analysis focused on the quantity of\ndisfluencies per group. For this purpose, disfluencies were tallied in each file employing\nthe markers described in the Dataset section. These counts were aggregated for each\n\n\fFigure 1. Total Relative Disflu-\nencies per 100 Words\n\nFigure 2. Disfluencies by Type\nand Group\n\ngroup, and the totals were visualized using graphs to aid interpretation. Figure 1 displays\nthe comparison of disfluency rates across four groups, labeled 1, 14, 8, and 7 on the\nX-axis. This figure presents the proportion of disfluencies calculated per 100 words for\neach group, facilitating a comparison of the relative frequency of disfluencies between\nthe groups. Figure 2, using the same group labels (1, 14, 8, and 7) on the X-axis, depicts\nthe number of disfluencies broken down by type (hesitation, error, repetition). This figure\nillustrates the distribution of different disfluency types within each group.\n\n3.4. Configuration and Execution of LLMs\n\nThe execution of data in Large Language Models (LLMs) was carried out through specific\nApplication Programming Interfaces (APIs). The Google Gemini 1.5 Pro Experimental\n0827, Anthropic’s Claude 3.5 Sonnet, and OpenAI’s ChatGPT-4o were accessed via the\nofficial APIs provided by their respective companies. The LLaMa 3 72B was used through\nthe Groq platform. The implementation was structured into 16 Python notebooks in the\nGoogle Colaboratory environment, with four notebooks assigned to each LLM, corre-\nsponding to debate groups. Each notebook was initialized with the configuration of the\ncorresponding LLM, followed by the result extraction codes detailed in this methodolog-\nical section, and then executed using the pre-established prompts. The results obtained\nwere recorded at the end of each notebook, later compiled into tables for this work, and\nanalyzed for the research objectives. In a separate notebook, an exploratory data anal-\nysis was conducted using Excel files from the groups, with the procedures and results\ndescribed in detail in this work.\n\n4. Results\n\nModel\nGemini\nGPT-4o\nLLaMa\nClaude\n\nTable 2. Zero-Shot (No Context) - Group 14\n\nTotal Removal Rate Levenshtein Similarity Time (seconds)\n\n14.06%\n62.50%\n53.12%\n57.81%\n\n97.95%\n96.83%\n95.24%\n32.98%\n\n85.69\n49.77\n21.86\n18.47\n\nThe data presented in Tables 2 and 3 clearly show that when using Zero-Shot\nprompts in Group 14 (the most disfluent group), GPT-4o, Gemini, and LLaMa main-\ntained a relatively good textual structure, as indicated by the Levenshtein similarity value.\nNone of the LLMs successfully balanced the removal of disfluencies while preserving the\n\n\fModel\nGemini\nGPT-4o\nLLaMa\nClaude\n\nTable 3. Zero-Shot (With Context) - Group 14\n\nTotal Removal Rate Levenshtein Similarity Time (seconds)\n\n10.94%\n60.94%\n62.50%\n60.94%\n\n97.96%\n74.96%\n51.16%\n33.37%\n\n86.23\n49.77\n8.79\n18.65\n\noriginal text’s quality with these two prompts, but text maintenance results for Claude\nand LLaMa fell significantly below expectations for most tested prompts, making them\ncurrently unreliable for this type of task. Therefore, the following analysis focuses solely\non GPT-4o and Gemini 1.5.\n\nTable 4. Test Results for GPT-4o and Gemini - Few Shot - Group 14\n\nModel\nGemini\nGPT-4o\n\nTotal Removal Rate Levenshtein Similarity Time (seconds)\n\n51.56%\n67.19%\n\n98.18%\n96.83%\n\n82.57\n74.73\n\nWith Few-Shot prompting (Table 4), GPT-4o achieved a 67.19% disfluency re-\nmoval rate while maintaining 96.83% of the original text. It also surpassed Gemini in\nresponse time. Although Gemini had a slightly higher text maintenance rate, it performed\npoorly in removing disfluencies.\n\nTable 5. Test Results for GPT-4o and Gemini - Chain of Thought - Group 14\nTotal Removal Rate Levenshtein Similarity Time (seconds)\n\n26.56%\n68.75%\n\n98.09%\n97.85%\n\n84.61\n45.76\n\nModel\nGemini\nGPT-4o\n\nUsing Chain-of-Thought prompting (Table 5), GPT-4o was the only one among\nthe four LLMs to produce a minimally favorable result. When compared to Gemini, GPT-\n4o achieved a 68.75% total disfluency removal rate, despite a similar text maintenance\nrate, while Gemini, though maintaining text quality, failed in removing disfluencies.\n\nTable 6. Test Results for GPT-4o and Gemini - Few Shot - Group 8\n\nModel\nGemini\nGPT-4o\n\nTotal Removal Rate Levenshtein Similarity Time (seconds)\n\n62.96%\n48.15%\n\n95.80%\n88.22%\n\n102.58\n47.05\n\nTable 7. Test Results for GPT-4o and Gemini - Chain of Thought - Group 8\n\nModel\nGemini\nGPT-4o\n\nTotal Removal Rate % Levenshtein Similarity % Time (seconds)\n\n33.33%\n40.74%\n\n98.19%\n88.51%\n\n104.26\n45.25\n\nIn Group 8, one of the least disfluent groups, GPT-4o’s effectiveness declined\nin both disfluency removal and maintaining text fluency, as shown in Tables 6 and 7.\n\n\fTable 8. Test Results for GPT-4o and Gemini - Few Shot - Group 1\n\nModel\nGemini\nGPT-4o\n\nModel\nGemini\nGPT-4o\n\nTotal Removal Rate Levenshtein Similarity Time (seconds)\n\n63.89%\n68.06%\n\n96.68%\n78.18%\n\n126.40\n52.48\n\nTable 9. GPT-4o and Gemini - Chain of Thought - Group 1\nTotal Removal Rate Levenshtein Similarity Time (seconds)\n\n25.00%\n68.06%\n\n97.85%\n77.55%\n\n127.22\n53.69\n\nGemini achieved a 62.96% removal rate with good text maintenance, albeit taking more\nthan twice as long. This trend, where GPT-4o did not match Gemini in text maintenance,\nwas also observed in Group 1, as shown in Tables 8 and 9, which is the largest group but\nnot as disfluent as Group 14. The models demonstrated high effectiveness in removing\nrepetitions, achieving 91.30% removal in Group 14 for GPT-4o, compared to Gemini’s\n56.52% in the Few-Shot prompt. In the Chain-of-Thought prompt, GPT-4o maintained\na consistent removal rate of 91.30% while also outpacing Gemini in processing time.\nAlthough GPT-4o showed strong performance in Group 14, it struggled in Group 1, where\nGemini achieved 87.18% removal with superior text preservation (96.68%). These results\nsuggest that while GPT-4o excels in specific contexts, Gemini may be more robust when\nhandling larger, more complex texts. 1.\n\n5. Conclusion and Future Work\n\nThis study explored the efficacy of Large Language Models (LLMs) in detecting and\neliminating linguistic disfluencies from transcriptions of academic debates. By leverag-\ning advanced prompt engineering techniques, such as Zero-Shot, Few-Shot, and Chain-\nof-Thought prompting, we assessed the performance of leading LLMs — GPT-4, Gemini\n1.5, Claude 3.5, and LLaMa 3 — in this task. The results revealed several key insights into\nthe capabilities and limitations of these models. GPT-4o demonstrated the highest over-\nall performance in disfluency removal, achieving an optimal balance between removing\ndisfluencies and maintaining text coherence, particularly under Few-Shot and Chain-of-\nThought prompting conditions. Gemini 1.5 also performed well but showed variability\ndepending on the prompt type and the specific debate group analyzed. It excelled in text\nmaintenance but had lower removal rates compared to GPT-4o in some cases. Claude\n3.5 and LLaMa 3 produced weaker results, struggling to maintain text coherence while\nremoving disfluencies. GPT-4o demonstrated more efficient processing times compared\nto the other models, which is crucial for practical, real-world applications. In conclu-\nsion, while LLMs like GPT-4o and Gemini 1.5 show promise for improving transcription\nquality by removing disfluencies, further advancements—such as fine-tuning, employ-\ning more advanced prompt engineering techniques, integrating widely used LLMs with\nmultimodal systems, or developing future models—are necessary to fully enhance their\ncapabilities.\n\n1Repository: https://github.com/pedrosqra/STIL\n\n\fReferences\n\nAnthropic (2024). Claude 3.5 sonnet. https://www.anthropic.com/news/\n\nclaude-3-5-sonnet. Accessed: 2024-08-27.\n\nBach, N. and Huang, F. (2019). Noisy bilstm-based models for disfluency detection. In\n\nInterspeech, pages 4230–4234.\n\nBaevski, A., Zhou, H., Mohamed, A., and Auli, M. (2020). wav2vec 2.0: A framework\n\nfor self-supervised learning of speech representations. CoRR, abs/2006.11477.\n\nBassi, S., Duregon, G., Jalagam, S., and Roth, D. (2023). End-to-end speech recogni-\ntion and disfluency removal with acoustic language model pretraining. arXiv preprint\narXiv:2309.04516.\n\nCorley, M. and Stewart, O. W. (2008). Hesitation disfluencies in spontaneous speech: The\n\nmeaning of um. Language and Linguistics Compass, 2(4):589–602.\n\nFerguson, J., Durrett, G., and Klein, D. (2015). Disfluency detection with a semi-markov\nIn Proceedings of the 2015 Conference of the North\nmodel and prosodic features.\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pages 257–262.\n\nHsu, W., Bolte, B., Tsai, Y. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. (2021).\nHubert: Self-supervised speech representation learning by masked prediction of hidden\nunits. CoRR, abs/2106.07447.\n\nMeta, A. (2024).\n\nIntroducing llama 3: Advancements in large language models. Ac-\n\ncessed: 2024-08-27.\n\nOpenAI (2024). Openai tokenizer. Accessed: 2024-10-08.\n\nOpenAI, Achiam, J., and et al., S. A. (2024). Gpt-4 technical report.\n\nRomana, A., Koishida, K., and Provost, E. M. (2023). Automatic disfluency detection\n\nfrom untranscribed speech. arXiv preprint arXiv:2311.00867.\n\nSnover, M., Dorr, B., and Schwartz, R. (2004). A lexically-driven algorithm for disfluency\n\ndetection. In Proceedings of HLT-NAACL 2004: Short Papers, pages 157–160.\n\nTeam, G., Georgiev, P., and et al., V. I. L. (2024). Gemini 1.5: Unlocking multimodal\n\nunderstanding across millions of tokens of context.\n\nZayats, V., Ostendorf, M., and Hajishirzi, H. (2016). Disfluency detection using a bidi-\n\nrectional lstm. arXiv preprint arXiv:1604.03209.\n\n\f"
        },
        {
            "titulo": "Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31136",
            "idioma": "Inglês",
            "storage_key": "files/article_31136_30939.pdf",
            "autores": [
                {
                    "nome": "Mirelle Bueno",
                    "afiliacao": "UNICAMP",
                    "orcid": "http://orcid.org/0000-0003-2374-6123"
                },
                {
                    "nome": "E. Seiti de Oliveira",
                    "afiliacao": "UNICAMP",
                    "orcid": "https://orcid.org/0000-0002-7882-6203"
                },
                {
                    "nome": "Rodrigo Nogueira",
                    "afiliacao": "UNICAMP / Maritaca AI",
                    "orcid": "https://orcid.org/0000-0002-2600-6035"
                },
                {
                    "nome": "Roberto Lotufo",
                    "afiliacao": "UNICAMP / NeuralMind.ai",
                    "orcid": "https://orcid.org/0000-0002-5652-0852"
                },
                {
                    "nome": "Jayr Pereira",
                    "afiliacao": "UFCA",
                    "orcid": "https://orcid.org/0000-0001-5478-438X"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "We present Quati, a dataset specifically designed for evaluating Information Retrieval (IR) systems for the Brazilian Portuguese language. It comprises a collection of queries formulated by native speakers and a curated set of documents sourced from a selection of frequently accessed Brazilian Portuguese websites, which ensures a representative and relevant corpus. To label the query–document pairs, we use a state-of-the-art LLM, which shows inter-annotator agreement levels comparable to human performance in our assessments. Our annotation methodology is described, enabling the cost-effective creation of similar datasets for other languages, with an arbitrary number of labeled documents per query. As a baseline, we evaluate a diverse range of open-source and commercial retrievers. Quati is publicly available at",
            "keywords": [
                "Information Retrieval",
                "Brazilian Portuguese Dataset"
            ],
            "referencias": [
                "Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara, A., Mitra, B., Nguyen, T., et al. (2016). Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268.",
                "Bonifacio, L., Jeronymo, V., Abonizio, H. Q., Campiotti, I., Fadaee, M., Lotufo, R., and Nogueira, R. (2021). mmarco: A multilingual version of the ms marco passage ranking dataset. arXiv preprint arXiv:2108.13897.",
                "Clark, J. H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., and Palomaki, J. (2020). Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages. Transactions of the Association for Computational Linguistics, 8:454–470.",
                "Cormack, G. V., Clarke, C. L., and Buettcher, S. (2009). Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 758–759.",
                "Craswell, N., Mitra, B., Yilmaz, E., Campos, D., Voorhees, E. M., and Soboroff, I. (2021). Trec deep learning track: Reusable test collections in the large data regime. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 2369–2375.",
                "Damessie, T. T., Nghiem, T. P., Scholer, F., and Culpepper, J. S. (2017). Gauging the quality of relevance assessments using inter-rater agreement. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1089–1092.",
                "Faggioli, G., Dietz, L., Clarke, C., Demartini, G., Hagen, M., Hauff, C., Kando, N., Kanoulas, E., Potthast, M., Stein, B., et al. (2023). Perspectives on large language models for relevance judgment. arXiv preprint arXiv:2304.09161.",
                "Farzi, N. and Dietz, L. (2024). An exam-based evaluation approach beyond traditional relevance judgments. arXiv preprint arXiv:2402.00309.",
                "Formal, T., Lassance, C., Piwowarski, B., and Clinchant, S. (2021). Splade v2: Sparse lexical and expansion model for information retrieval. arXiv preprint arXiv:2109.10086.",
                "Jeronymo, V., Nascimento, M., Lotufo, R., and Nogueira, R. (2022). mrobust04: A multilingual version of the trec robust 2004 benchmark. arXiv preprint arXiv:2209.13738.",
                "Johnson, J., Douze, M., and Jégou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535–547.",
                "Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., and Mikolov, T. (2016a). Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651.",
                "Joulin, A., Grave, E., Bojanowski, P., and Mikolov, T. (2016b). Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759.",
                "Kusupati, A., Bhatt, G., Rege, A., Wallingford, M., Sinha, A., Ramanujan, V., Howard-Snyder, W., Chen, K., Kakade, S., Jain, P., et al. (2022). Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:30233–30249.",
                "Lawrie, D., Mayfield, J., Oard, D. W., and Yang, E. (2022). Hc4: A new suite of test collections for ad hoc clir. In European Conference on Information Retrieval, pages 351–366. Springer.",
                "Lewis, D. D., Yang, Y., Russell-Rose, T., and Li, F. (2004). Rcv1: A new benchmark collection for text categorization research. Journal of machine learning research, 5(Apr):361–397.",
                "Lima de Oliveira, L., Romeu, R. K., and Moreira, V. P. (2021). Regis: A test collection for geoscientific documents in portuguese. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2363–2368.",
                "Nair, S., Yang, E., Lawrie, D., Duh, K., McNamee, P., Murray, K., Mayfield, J., and Oard, D. W. (2022). Transfer learning approaches for building cross-language dense retrieval models. In European Conference on Information Retrieval, pages 382–396. Springer.",
                "Nakayama, H., Kubo, T., Kamura, J., Taniguchi, Y., and Liang, X. (2018). doccano: Text annotation tool for human. Software available from",
                ".",
                "Overwijk, A., Xiong, C., and Callan, J. (2022). Clueweb22: 10 billion web documents with rich information. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3360–3362.",
                "Peters, C. and Braschler, M. (2002). The importance of evaluation for cross-language system development: the clef experience. In LREC. Citeseer.",
                "Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. (2021). Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.",
                "Sakai, T., Oard, D. W., and Kando, N. (2021). Evaluating Information Retrieval and Access Tasks: NTCIR’s Legacy of Research Impact. Springer Nature.",
                "Schäuble, P. and Sheridan, P. (1998). Cross-language information retrieval (clir) track overview. NIST SPECIAL PUBLICATION SP, pages 31–44.",
                "Thomas, P., Spielman, S., Craswell, N., and Mitra, B. (2023). Large language models can accurately predict searcher preferences. arXiv preprint arXiv:2309.10621.",
                "Vitório, D., Souza, E., Martins, L., da Silva, N. F., de Carvalho, A. C. P. d. L., Oliveira, A. L., and de Andrade, F. E. (2024). Building a relevance feedback corpus for legal information retrieval in the real-case scenario of the brazilian chamber of deputies. Language Resources and Evaluation, pages 1–21.",
                "Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. (2022). Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533.",
                "Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V., and Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information Processing Systems, volume 35, pages 24824–24837. Curran Associates, Inc.",
                "Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. (2020). mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.",
                "Zendel, O., Culpepper, J. S., Scholer, F., and Thomas, P. (2024). Enhancing human annotation: Leveraging large language models and efficient batch processing. In Proceedings of the 2024 Conference on Human Information Interaction and Retrieval, pages 340–345.",
                "Zhang, X., Thakur, N., Ogundepo, O., Kamalloo, E., Alfonso-Hermelo, D., Li, X., Liu, Q., Rezagholizadeh, M., and Lin, J. (2023). Miracl: A multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:1114–1131."
            ],
            "artigo_completo": "Quati: A Brazilian Portuguese Information Retrieval\n\nDataset from Native Speakers\n\nMirelle Bueno*1 , E. Seiti de Oliveira*1 , Rodrigo Nogueira1,2 , Roberto Lotufo1,3 ,\nJayr Pereira4\n\n1Departamento de Engenharia de Computac¸ ˜ao e Automac¸ ˜ao (DCA)\nUniversidade Estadual de Campinas – UNICAMP\n13083-872 – Campinas – S˜ao Paulo, Brasil\n\n2Maritaca AI\n\n3NeuralMind.ai\n\n4Universidade Federal do Cariri\nJuazeiro do Norte-CE, Brasil.\n\nm174909@dac.unicamp.br, eduseiti@dca.fee.unicamp.br\n\n{rfn,lotufo}@unicamp.br, jayr.pereira@ufca.edu.br\n\nAbstract. We present Quati,1 a dataset specifically designed for evaluating In-\nformation Retrieval (IR) systems for the Brazilian Portuguese language. It com-\nprises a collection of queries formulated by native speakers and a curated set\nof documents sourced from a selection of frequently accessed Brazilian Por-\ntuguese websites, which ensures a representative and relevant corpus. To label\nthe query–document pairs, we use a state-of-the-art LLM, which shows inter-\nannotator agreement levels comparable to human performance in our assess-\nments. Our annotation methodology is described, enabling the cost-effective\ncreation of similar datasets for other languages, with an arbitrary number of\nlabeled documents per query. As a baseline, we evaluate a diverse range of\nopen-source and commercial retrievers. Quati is publicly available at https:\n//huggingface.co/datasets/unicamp-dl/quati, and all scripts\nat https://github.com/unicamp-dl/quati.\n\n1. Introduction\n\nThe development of Information Retrieval (IR) systems depends on high-quality evalua-\ntion datasets, which should contain queries and documents ideally in the same target lan-\nguage of those systems, in order to capture specific information needs and social-cultural\naspects. That, contrasts with translated datasets, which potentially represent the infor-\nmation needs and knowledge of a different culture or society. Hence, translated datasets\nmay not effectively measure a retrieval system’s ability in real-world scenarios involving\nnative users.\n\n*Equal contribution.\n1We named our dataset after this South American mammal, whose foraging behavior represents the\n\nresolute search for resources.\n\n\fDespite being one of\n\nIR datasets in Portuguese.\n\nthe most widely spoken languages in the world,\nExisting datasets such as\nthere is a scarcity of\nREGIS [Lima de Oliveira et al. 2021] and RCV1 [Lewis et al. 2004]2,\nthough valu-\nable, fall short due to their limited size and specialized domains (geoscience and\nnews). While translated datasets such as mMARCO [Bonifacio et al. 2021] and mRo-\nbust04 [Jeronymo et al. 2022] have helped to alleviate this issue, the use of automatic\ntranslations often represents the loss of socio-cultural characteristics of the target lan-\nguages, and the evaluations may become biased by the source language.\n\nTo address those issues, we created Quati, a Brazilian Portuguese evaluation\ndataset, comprising human-written queries and a high-quality native corpus. Quati is\ncreated using a semi-automated pipeline, aiming to reduce the labeling cost barrier. We\nuse a Large Language Model (LLM) to judge a passage’s relevance for a given query,\npublishing a cost-effective pipeline to create an IR evaluation dataset with an arbitrary\nnumber of annotated passages per query.3 In this context, our work aims to answer the\nfollowing research question: Can LLMs be used to compose a semi-automated pipeline\nfor annotating query–passages relevance for Brazilian Portuguese IR systems?\n\nTo evaluate the quality of the LLM annotations, we compare them with human\nannotations on a sample of query–passage pairs and confirmed a Cohen’s Kappa coef-\nficient of 0.31. While this figure is below the 0.41 seen in human-human annotation\nagreement, it is consistent with the findings reported in the literature [Faggioli et al. 2023,\nThomas et al. 2023, Farzi and Dietz 2024] and it will likely increase as LLMs improve in\nquality. The usage of a modular semi-automated pipeline, allows the dataset construction\nmethod to be replicated to create high-quality IR datasets for other languages.\n\n2. Related Work\n\n[Jeronymo et al. 2022], Mr.Tydi\n\nEvaluation datasets are an important variable in the IR context as they expose the\nlimitations of search systems and guide their development. However, most of the avail-\nable datasets are in English, as is the case with MS MARCO [Bajaj et al. 2016].\nsuch MIRACL [Zhang et al. 2023], mMARCO [Bonifacio et al. 2021],\nWorks\nTREC CLIR\nmRobust\n[Sch¨auble and Sheridan 1998],\nNT-\nto develop\nCIR [Sakai et al. 2021] and HC4\ndatasets for other languages, but most are based on language translation to adapt\nEnglish to the target\nOngoing ef-\nforts [Lima de Oliveira et al. 2021, Vit´orio et al. 2024] are starting to change that\nscenario creating IR datasets for Brazilian Portuguese, but so far focusing on specific\ndomains.\n\n[Lawrie et al. 2022] are efforts\n\n[Peters and Braschler 2002],\n\nlanguages, or do not\n\ninclude Portuguese.\n\n[Clark et al. 2020],\n\nCLEF\n\nThe creation of datasets for IR is a resource-intensive task, particularly in the pro-\ncess of judging the relevance of documents. Recent endeavors have witnessed a shift\ntowards leveraging LLMs to assess query–passage relevance [Zendel et al. 2024]. Faggi-\noli et al. [Faggioli et al. 2023] further underscored the potential of employing LLMs for\nautomating the judgment of document relevance, thereby opening up promising avenues\n\n2https://trec.nist.gov/data/reuters/reuters.html\n3The total cost for this dataset was U$140.19 (0.03 per query–passage) for an average of 97.78 annotated\n\npassages per query.\n\n\ffor exploration in this domain. Complementary evaluations conducted by Thomas et al.\n[Thomas et al. 2023] demonstrated a significant correlation between human judgments\nand those made by the GPT-3.5-turbo model.\n\n3. Methodology\n\nWe used a semi-automatic method to create Quati, as depicted in Figure 1. The required\ninputs are: 1) A large corpus, originally written in the target language, from which we\nextract the passages to compose our IR dataset; 2) A set of test queries, manually created\nto represent the information needs of native speakers. In the following sections, we detail\nthe steps of the pipeline.\n\nFigure 1. Proposed IR dataset creation methodology.\n\n3.1. Passages preparation\n\nThe passages preparation step is composed by the following substeps:\n\nData collection: We used the Portuguese subset of ClueWeb22 [Overwijk et al. 2022]\ncategory B, which includes 4.1 million web pages more likely to be visited according to\nBing search algorithms during the first half of 2022 [Overwijk et al. 2022].\n\nURL filtering: We excluded any documents from our dataset whose URLs’ domain\nended with “.pt”, which refer to Portuguese from Portugal, as the language style in those\ndocuments might differ significantly from that used in Brazilian Portuguese web pages.\nAdditionally, we used FastText [Joulin et al. 2016b, Joulin et al. 2016a] as an additional\nlanguage verification method to ensure that only Portuguese documents were included in\nour corpus.\n\nDocument segmentation into passages: Following language verification, we segmented\nthe documents into approximately 1,000-character segments and assessed the percentage\nof line breaks (\\n) occurrences within each segment, removing those with more than 20%.\n\n\fThis criterion was used to increase the probability of retaining segments predominantly\ncomposed of natural language text.\n\nSeparation into large and small versions: With the process described in the previous\nsteps, we collected a total of 20 million segments. From this set, we randomly selected\n10 million segments (hereinafter referred to as 10M corpus) to be the passages in our\ncorpus, creating a large, but still manageable, dataset of more than 11 GB of size. A\nsecond dataset was built from the first one applying additional filtering rules taken from\nthe MassiveWeb Corpus [Rae et al. 2021] — e.g.\nremoving passages with more than\n10% of symbols, or with mean word length outside the 3 to 10 interval — and sampling\nonly 1 million segments (hereinafter referred to as 1M corpus) from the resulting 7M\nfiltered documents — the goal was to create a smaller and higher-quality dataset that\nwould facilitate experimentation with embedding models, as encoding the original 10\nmillion segments can be computationally expensive.\n\n3.2. Manual queries creation\n\nWe employed human-created queries for the evaluation dataset, aiming high-quality ques-\ntions to capture common information needs from a diverse corpus, created by native\nspeakers of the target language. We created a total of 200 test queries.\n\n3.3. Passages retrieval\n\nThe next step is the passage retrieval to build a list of query–passages to annotate. As\nit would be prohibitive to have the relevance scores each query for the entire corpus, we\nselect to annotate the top-k passages returned by multiple IR systems. It is assumed that\nthe diversity of their results will enable the collection of a variety of passages, creating a\nrobust evaluation dataset.\n\nWe selected a mix of strong and weak IR systems, to include a variety of pas-\nsages: BM25: a strong baseline for retrieval; BM25 + mT5-XL: two-stage pipeline\nwith BM25 followed by mT5-XL (3.7 billion parameters) [Xue et al. 2020]; BM25 + E5-\nlarge: two-stage pipeline with BM25 and E5-large [Wang et al. 2022]4; E5-large and E5-\nbase: E5 variants as dense retrievers, using FAISS [Johnson et al. 2019] with inner prod-\nuct for search; ColBERT-X [Nair et al. 2022]: a multilingual ColBERT-v1 fine-tuned\nin Brazilian Portuguese subset of mMARCO; SPLADE v2: a learned sparse retriever\n[Formal et al. 2021] fine-tuned on Brazilian Portuguese subset of mMARCO; SPLADE\nv2 + mT5-XL: two-stage pipeline using SPLADE v2 followed by mT5.\n\nWe also use Reciprocal Ranking Fusion (RRF) [Cormack et al. 2009] to in-\ncrease the retrieved documents diversity, using the following combinations: E5-large\n+ ColBERT-X; E5-large + SPLADE v2; and E5-large + BM25 + mT5-XL.\n\nWe also include commercial embedding models:\n\ntext-embedding-ada-0025,\ntext-embedding-3-small6 and as it employs the Matryoshka Representation Learning\ntechnique [Kusupati et al. 2022], we performed the retrieval using only the first half di-\nmensions (identified as text-embedding-3-small half). FAISS [Johnson et al. 2019] using\ninner product was applied for dense vectors search for all of them.\n\n4https://huggingface.co/intfloat/multilingual-e5-large\n5https://openai.com/blog/new-and-improved-embedding-model\n6https://openai.com/blog/new-embedding-models-and-api-updates\n\n\fTo evaluate the diversity of retrieved passages, we counted the query–passage\ncombinations exclusively returned by each IR system, which should be a number from 0 to\n500, 0 meaning the query–passages returned by a particular IR system were also returned\nby another IR system. Although we look for diversity, there should be a balance: we could\nhave reached 5,000 different query–passage combinations (10 IR systems, 50 queries,\n10 passage/query) if all systems returned exclusive passages, but that would indicate no\nagreement on the most relevant passages per query.\n\n3.4. Query–passages annotation\n\nThe final step of the query annotation is to use an LLM to label the retrieved pas-\nsages’ relevance for each query. We selected the top-k=10 passages for a sample of 50\nqueries using all the retrieval systems considered on both the 10M and 1M corpora and\nsent them for LLM evaluation. We applied a few-shot Chain-of-Thought (CoT) prompt\n[Wei et al. 2022], and we adopted the TREC 2021 Deep Learning track 4-score relevance\nannotation scale [Craswell et al. 2021]: (1) Irrelevant: the passage is outside the scope\nof the question; (2) Relevant: the passage pertains to the question’s topic but does not\nprovide a direct answer; (3) Highly relevant: the passage answers the question, but lacks\nin clarity or has unrelated information. (4) Perfectly relevant: the passage answers the\nquestion with clarity and precision.\n\nWe selected OpenAI GPT-4 model7 as the annotator. Due to cost limitations, we\nused a 50-sample from the 200 queries. We asked the LLM to label only the top-10\nretrieved passages of each IR system for each query. We used a CoT prompt with two\nin-context examples selected from the mMARCO pt-BR dataset [Bonifacio et al. 2021].\nThe prompt written in Brazilian Portuguese includes the task explanation and the CoT\nexamples to produce the 4-score passage relevance value for a given query. The final\nevaluation was requested in JSON format to simplify the LLM response parsing process.\nThe prompt was built and refined using a limited set of questions sampled from the same\nmMARCO pt-BR dataset. The final prompt version can be found online.8\n\n4. Experiments\n\n4.1. LLM annotation quality assessment\n\nWe assess the quality of our LLM-based annotator by comparing its query–passage rel-\nevance scores with those provided by human annotators. This process was conducted\non a 24-sample of the 50 annotated queries. Using the Doccano [Nakayama et al. 2018]\nsystem, three researchers annotated the top-10 passages returned by the BM25 + mT5\nIR system applying the same TREC-DL 2021 4-score grading system. The agreement\namong the query–passage relevance annotations generated by the LLM and humans was\nmeasured using Cohen Kappa, Pearson, and Spearman correlation coefficients.\n\n4.2. Retrieval systems evaluation\n\nWe used the LLM annotated query–passages to evaluate the IR systems effectiveness in\nthe 10M and 1M Quati datasets. As we already have the IR runs for the passages retrieval\nby all the systems (see Section 3.3), we simply compute the nDCG@10 metric over those\n\n7We used gpt-4-1106-preview, available at the OpenAI API.\n8https://github.com/unicamp-dl/quati/blob/main/prompt.md\n\n\fTable 1. The single–system query–passages column indicates the ones re-\nturned only by that system, either for the 10M or the 1M sets; the percentage\nrefers to 500 query–passages. For the single system total, the percentage refers\nto the union of evaluated passages. “Others” are results with data preparation\nissues, but valid annotations.\n\nRetrieval System\n\nSingle–system query–passages\n\n10M dataset\n\n1M dataset\n\nE5-base\nBM25\nSPLADE v2 pt-BR\nE5-large\nColBERT-X mMARCO pt-BR\nBM25 + E5-large\nSPLADE v2 pt-BR + mT5-XL\nBM25 + mT5-XL\nE5-large + ColBERT-X mMARCO pt-BR\nRRF\nE5-large + SPLADE v2 pt-BR RRF\ntext-embedding-ada-002\ntext-embedding-3-large\ntext-embedding-3-small half\ntext-embedding-3-small\n\nOthers\n\nSingle system query–passages total\nUnion of all systems query–passages\n\n262 (52.4%)\n248 (49.6%)\n151 (30.2%)\n122 (24.4%)\n115 (23.0%)\n115 (23.0%)\n86 (17.2%)\n60 (12.0%)\n32 (6.4%)\n\n29 (5.8%)\n\n253 (50.6%)\n\n195 (39.0%)\n120 (24.0%)\n\n93 (18.6%)\n\n137 (27.4%)\n121 (24.2%)\n45 (9.0%)\n31 (6.2%)\n\n814 (54.27%)\n\n3029 (61.96%)\n4889\n\nresults. Besides establishing a baseline for a variety of IR systems, this experiments also\nindirectly assess the overall quality of Quati validation dataset: by verifying different ef-\nfectiveness for already published IR systems, we validate Quati potential to indeed assess\nsuch systems.\n\n5. Results and Discussion\n\n5.1. Annotated passages variability\n\nTable 1 indicates a range from 29 to 262 query–passage combinations exclusively returned\nby a single IR system. On average, each system returned 28.85% of new passages, and\nfrom the total 4,889 evaluated query–passages, 61.96% (3029) were returned by a single\nsystem, suggesting our pool of systems is diverse. As shown in Table 2, the IR systems\nwere able to retrieve a diversity set of query–passages, including “perfectly relevant”\n(score=3) ones; also, the diversity increased for less relevant passages, indicating the\nsystems agreed more as the passage relevance increased.\n\n5.2. LLM annotations quality is aligned with crowd workers\n\nTable 3 shows the Cohen’s Kappa and the Spearman’s Rho correlation coefficients\nfor the human and LLM annotations, computed for the 240 query–passage combina-\ntions. The average Cohen’s Kappa of 0.31 is aligned with the literature. For example,\n\n\fTable 2. Query–passage relevance score counts. The systems agreed more,\nreturning the same passages per query, as the relevance score increases. “Rel-\nevant” includes passages from scores 1 to 3.\n\nScore\n\nAll\nquery–passages\n\nSingle-system\nquery–passages\n\n0\n1\n2\n3\n\nRelevant\nTotal\n\n2489\n985\n759\n656\n\n2400\n4889\n\n1839\n586\n375\n229\n\n1190\n3029\n\n%\n\n73.89\n59.49\n49.41\n34.91\n\n49.58\n61.96\n\nTable 3. Cohen’s Kappa and Spearman’s Rho correlations among Human An-\nnotators (HA) and the GPT-4, for the query–passage 4-score evaluations. For\neach annotator, 4th row holds the average of the correlation against the others.\nWe then compute the mean of that value only for the Human Annotators (”Mean\nHA” row), to characterize their overall correlation.\n\nHA1\nHA2\nHA3\n\nMean\nStd\n\nMean HA\n\nCohen’s Kappa\nHA3\nHA2\n\n0.4369\n—\n0.4105\n\n0.4237\n0.0132\n\n0.4294\n0.4105\n—\n\n0.4199\n0.0095\n\nHA1\n\n—\n0.4369\n0.4294\n\n0.4331\n0.0037\n\n0.4256±0.0055\n-\n0.0019\n\n-\n0.0057\n\nGPT-4\n\n0.3234\n0.2593\n0.3498\n\n0.3108\n0.0380\n\n—\n-\n0.1096\n\nSpearman’s Rho\nHA2\n\nHA3\n\n0.6931\n—\n0.6985\n\n0.6958\n0.0027\n\n0.6924\n0.6985\n—\n\n0.6954\n0.0031\n\nHA1\n\n—\n0.6931\n0.6924\n\n0.6927\n0.0004\n\n0.6946±0.0014\n0.0011\n\n0.0008\n\n-\n0.0019\n\nGPT-4\n\n0.6073\n0.6174\n0.6296\n\n0.6181\n0.0091\n\n—\n-\n0.0765\n\nDiff. Mean HA 0.0076\n\n[Faggioli et al. 2023] reported 0.26 for GPT-3.5, and [Thomas et al. 2023] reported Co-\nhen’s Kappa ranging from 0.20 to 0.64, depending on the prompt used on GPT-4. Our\nhuman annotators’ mean Cohen’s Kappa of 0.4256 falls within crowd workers interval of\na 0.24 to 0.52, according to [Damessie et al. 2017].\n\nAs query–passage relevance annotation is a subjective task, we argue a non-\ncategorical metric such as the Spearman’s Rho would be more appropriate to measure\nthe annotators’ correlation, as errors by a single score level should be considered “less\ncritical”, or within the subjectivity intrinsic for the task. Although human annotators’\ncorrelation is still above their correlations with the LLM, Spearman metrics are within a\nhigher value, better capturing the current LLM effectiveness on the query–passage rele-\nvance evaluation.\n\n5.3. Retrieval systems evaluation results\n\nWe evaluated the retrievers effectiveness using the LLM annotated query–passages\n(qrels); table 4 present the results for both the 10M and the 1M datasets. The ranking\n\n\fTable 4. The nDCG@10 effectiveness on the 50 test queries. The results follows\nthe IR literature and suggests the dataset can effectively evaluate a range of\ndifferent IR systems.\n\nRetrieval system\n\nBM25\nE5-large\nSPLADE v2 pt-BR\nE5-large + SPLADE v2 pt-BR RRF\nColBERT-X mMARCO pt-BR\nBM25 + E5-large\ntext-embedding-ada-002\ntext-embedding-3-small\ntext-embedding-3-large\nE5-large + ColBERT-X mMARCO pt-BR RRF\nSPLADE v2 pt-BR + mT5-XL\nBM25 + mT5-XL\n\nnDCG@10\n\n10M dataset\n\n1M dataset\n\n0.4467\n0.5563\n0.5806\n0.6272\n0.6279\n0.6364\n—\n—\n—\n0.6377\n0.6966\n0.7109\n\n0.3991\n—\n—\n—\n0.4927\n0.5423\n0.5630\n0.5688\n0.6319\n—\n—\n0.6593\n\nof retrievers with respect to effectiveness matched our expectations, following the litera-\nture. We consider that an additional indication of the overall datasets quality as, despite\nbeing created in a semi-automated cost-effective way, they are able to evaluate a diversity\nof retrievers.\n\n6. Conclusion\nThis paper introduced the Quati, a dataset for supporting the development of IR systems\nfor Brazilian Portuguese retrieval tasks. Quati is publicly available in two sizes, 10M a 1M\npassages, with 50-query qrels with respectively an average of 97.78 and 38.66 annotated\npassages per query. Through comparisons with human annotators we answer our research\nquestion, showing that state-of-the-art LLM can be used in a semi-automated and cost-\neffective way to create IR datasets for a specific target language, in the query–passage\nannotation role, with equivalent performance of humans: LLM annotations correlate with\nhumans’ in similar way human crowd workers annotations do, for a fraction of the cost.\n\nAcknowledgements\nWe thank Leod´ecio Braz da Silva Segundo for the valuable support during the human\nannotation task. We also thank Leonardo Benardi de Avila and Monique Monteiro for the\nSPLADE v2 retrievals, using the model they trained for Brazilian Portuguese. This re-\nsearch was partially funded by grant 2022/01640-2 from Fundac¸ ˜ao de Amparo `a Pesquisa\ndo Estado de S˜ao Paulo (FAPESP).\n\nReferences\nBajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara,\nA., Mitra, B., Nguyen, T., et al. (2016). Ms marco: A human generated machine\nreading comprehension dataset. arXiv preprint arXiv:1611.09268.\n\n\fBonifacio, L., Jeronymo, V., Abonizio, H. Q., Campiotti, I., Fadaee, M., Lotufo, R., and\nNogueira, R. (2021). mmarco: A multilingual version of the ms marco passage ranking\ndataset. arXiv preprint arXiv:2108.13897.\n\nClark, J. H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., and Palo-\nmaki, J. (2020). Tydi qa: A benchmark for information-seeking question answering in\nty pologically di verse languages. Transactions of the Association for Computational\nLinguistics, 8:454–470.\n\nCormack, G. V., Clarke, C. L., and Buettcher, S. (2009). Reciprocal rank fusion outper-\nIn Proceedings of the 32nd\nforms condorcet and individual rank learning methods.\ninternational ACM SIGIR conference on Research and development in information re-\ntrieval, pages 758–759.\n\nCraswell, N., Mitra, B., Yilmaz, E., Campos, D., Voorhees, E. M., and Soboroff, I. (2021).\nTrec deep learning track: Reusable test collections in the large data regime. In Proceed-\nings of the 44th international ACM SIGIR conference on research and development in\ninformation retrieval, pages 2369–2375.\n\nDamessie, T. T., Nghiem, T. P., Scholer, F., and Culpepper, J. S. (2017). Gauging the qual-\nity of relevance assessments using inter-rater agreement. In Proceedings of the 40th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, pages 1089–1092.\n\nFaggioli, G., Dietz, L., Clarke, C., Demartini, G., Hagen, M., Hauff, C., Kando, N.,\nKanoulas, E., Potthast, M., Stein, B., et al. (2023). Perspectives on large language\nmodels for relevance judgment. arXiv preprint arXiv:2304.09161.\n\nFarzi, N. and Dietz, L. (2024). An exam-based evaluation approach beyond traditional\n\nrelevance judgments. arXiv preprint arXiv:2402.00309.\n\nFormal, T., Lassance, C., Piwowarski, B., and Clinchant, S. (2021). Splade v2: Sparse lex-\nical and expansion model for information retrieval. arXiv preprint arXiv:2109.10086.\n\nJeronymo, V., Nascimento, M., Lotufo, R., and Nogueira, R. (2022). mrobust04: A mul-\ntilingual version of the trec robust 2004 benchmark. arXiv preprint arXiv:2209.13738.\n\nJohnson, J., Douze, M., and J´egou, H. (2019). Billion-scale similarity search with GPUs.\n\nIEEE Transactions on Big Data, 7(3):535–547.\n\nJoulin, A., Grave, E., Bojanowski, P., Douze, M., J´egou, H., and Mikolov, T.\narXiv preprint\n\nFasttext.zip: Compressing text classification models.\n\n(2016a).\narXiv:1612.03651.\n\nJoulin, A., Grave, E., Bojanowski, P., and Mikolov, T. (2016b). Bag of tricks for efficient\n\ntext classification. arXiv preprint arXiv:1607.01759.\n\nKusupati, A., Bhatt, G., Rege, A., Wallingford, M., Sinha, A., Ramanujan, V., Howard-\nSnyder, W., Chen, K., Kakade, S., Jain, P., et al. (2022). Matryoshka representation\nlearning. Advances in Neural Information Processing Systems, 35:30233–30249.\n\nLawrie, D., Mayfield, J., Oard, D. W., and Yang, E. (2022). Hc4: A new suite of test\ncollections for ad hoc clir. In European Conference on Information Retrieval, pages\n351–366. Springer.\n\n\fLewis, D. D., Yang, Y., Russell-Rose, T., and Li, F. (2004). Rcv1: A new benchmark\nJournal of machine learning research,\n\ncollection for text categorization research.\n5(Apr):361–397.\n\nLima de Oliveira, L., Romeu, R. K., and Moreira, V. P. (2021). Regis: A test collection\nfor geoscientific documents in portuguese. In Proceedings of the 44th International\nACM SIGIR Conference on Research and Development in Information Retrieval, pages\n2363–2368.\n\nNair, S., Yang, E., Lawrie, D., Duh, K., McNamee, P., Murray, K., Mayfield, J., and Oard,\nD. W. (2022). Transfer learning approaches for building cross-language dense retrieval\nmodels. In European Conference on Information Retrieval, pages 382–396. Springer.\n\nNakayama, H., Kubo, T., Kamura,\nannotation tool\ndoccano:\nhttps://github.com/doccano/doccano.\n\nText\n\nJ., Taniguchi, Y., and Liang, X.\nfor human.\n\nSoftware\n\navailable\n\n(2018).\nfrom\n\nOverwijk, A., Xiong, C., and Callan, J. (2022). Clueweb22: 10 billion web documents\nwith rich information. In Proceedings of the 45th International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval, pages 3360–3362.\n\nPeters, C. and Braschler, M. (2002). The importance of evaluation for cross-language\n\nsystem development: the clef experience. In LREC. Citeseer.\n\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J.,\nHenderson, S., Ring, R., Young, S., et al. (2021). Scaling language models: Methods,\nanalysis & insights from training gopher. arXiv preprint arXiv:2112.11446.\n\nSakai, T., Oard, D. W., and Kando, N. (2021). Evaluating Information Retrieval and\n\nAccess Tasks: NTCIR’s Legacy of Research Impact. Springer Nature.\n\nSch¨auble, P. and Sheridan, P. (1998). Cross-language information retrieval (clir) track\n\noverview. NIST SPECIAL PUBLICATION SP, pages 31–44.\n\nThomas, P., Spielman, S., Craswell, N., and Mitra, B. (2023). Large language models can\n\naccurately predict searcher preferences. arXiv preprint arXiv:2309.10621.\n\nVit´orio, D., Souza, E., Martins, L., da Silva, N. F., de Carvalho, A. C. P. d. L., Oliveira,\nA. L., and de Andrade, F. E. (2024). Building a relevance feedback corpus for legal\ninformation retrieval in the real-case scenario of the brazilian chamber of deputies.\nLanguage Resources and Evaluation, pages 1–21.\n\nWang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F.\n(2022). Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V.,\nand Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language\nmodels. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh,\nA., editors, Advances in Neural Information Processing Systems, volume 35, pages\n24824–24837. Curran Associates, Inc.\n\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and\nRaffel, C. (2020). mt5: A massively multilingual pre-trained text-to-text transformer.\narXiv preprint arXiv:2010.11934.\n\n\fZendel, O., Culpepper, J. S., Scholer, F., and Thomas, P. (2024). Enhancing human anno-\ntation: Leveraging large language models and efficient batch processing. In Proceed-\nings of the 2024 Conference on Human Information Interaction and Retrieval, pages\n340–345.\n\nZhang, X., Thakur, N., Ogundepo, O., Kamalloo, E., Alfonso-Hermelo, D., Li, X., Liu,\nQ., Rezagholizadeh, M., and Lin, J. (2023). Miracl: A multilingual retrieval dataset\ncovering 18 diverse languages. Transactions of the Association for Computational\nLinguistics, 11:1114–1131.\n\n\f"
        },
        {
            "titulo": "A Linguagem em Foco: Anotação de Sinalizadores Discursivos em Textos Jornalísticos",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31137",
            "idioma": "Português",
            "storage_key": "files/article_31137_30940.pdf",
            "autores": [
                {
                    "nome": "Paula Cardoso",
                    "afiliacao": "UFPA",
                    "orcid": "http://orcid.org/0000-0003-3621-8960"
                },
                {
                    "nome": "Jackson Souza",
                    "afiliacao": "UFBA",
                    "orcid": "https://orcid.org/0000-0003-1881-6780"
                },
                {
                    "nome": "Roana Rodrigues",
                    "afiliacao": "UFS",
                    "orcid": "https://orcid.org/0000-0002-7748-8716"
                },
                {
                    "nome": "Ewerson Dantas",
                    "afiliacao": "UFS",
                    "orcid": "https://orcid.org/0009-0005-1381-7372"
                },
                {
                    "nome": "Larissa Santa Bárbara",
                    "afiliacao": "UFS",
                    "orcid": "https://orcid.org/0009-0004-1549-8435"
                },
                {
                    "nome": "Mateus Araújo",
                    "afiliacao": "UFBA",
                    "orcid": "https://orcid.org/0009-0001-8356-7322"
                },
                {
                    "nome": "Naira Gama",
                    "afiliacao": "UFBA",
                    "orcid": "http://orcid.org/0009-0000-7944-7515"
                },
                {
                    "nome": "Tobias Almeida",
                    "afiliacao": "UFLA",
                    "orcid": "https://orcid.org/0009-0008-3347-2713"
                },
                {
                    "nome": "Gabriel Cruz",
                    "afiliacao": "UFBA",
                    "orcid": "https://orcid.org/0009-0002-1916-8906"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Rhetorical structure theory",
                "marcadores discursivos",
                "sinalizadores discursivos"
            ],
            "referencias": [
                "Cardoso, P. C., Maziero, E. G., Jorge, M. L. C., Seno, E. M., Di Felippo, A., Rino, L. H. M., Nunes, M. d. G. V., e Pardo, T. A. (2011). CSTNews - A discourse-annotated corpus for single and multi-document summarization of news texts in Brazilian Portuguese. In Proceedings of the 3rd RST Brazilian Meeting, pages 88–105",
                "Dantas, E., Bárbara, L. d. J. S., Pereira, M. A., Gama, N. S., Almeida, T. J. A., Souza, J. W. d. C., Cardoso, P. C. F., e Rodrigues, R. (2024). Manual de anotação de sinalizadores discursivos em textos jornalísticos. Série de Relatórios Técnicos do Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo.",
                "Hovy, E. e Lavid, J. (2010). Towards a ‘science’of corpus annotation: a new methodological challenge for corpus linguistics. International Journal of Translation, 22(1):13–36.",
                "Krippendorff, K. (2011). Computing krippendorff’s alpha-reliability.",
                "Mann, W. C. e Thompson, S. A. (1987). Rhetorical Structure Theory: A theory of text organization. University of Southern California, Information Sciences Institute Los Angeles",
                "Marcu, D. (2000). The rhetorical parsing of unrestricted texts: A surface-based approach. Computational linguistics, 26(3):395–448.",
                "Pustejovsky, J. e Stubbs, A. (2012). Natural Language Annotation for Machine Learning: A guide to corpus-building for applications. O’Reilly Media, Inc.",
                "Zeldes, A. (2016). rstWeb-a browser-based annotation interface for Rhetorical Structure Theory and discourse relations. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 1–5."
            ],
            "artigo_completo": "A Linguagem em Foco: Anotac¸ ˜ao de Sinalizadores Discursivos\nem Textos Jornal´ısticos\n\nPaula Cardoso1, Jackson Souza2, Roana Rodrigues3, Ewerson Dantas3,\nLarissa Santa B´arbara3, Mateus Ara ´ujo2, Naira Gama2,\nTobias Almeida4, Gabriel Cruz2\n\n1Universidade Federal do Par´a - Bel´em/PA\n\n2Universidade Federal da Bahia - Salvador/BA\n\n3Universidade Federal de Sergipe - Aracaj´u/SE\n\n4Universidade Federal de Lavras - Lavras/MG\n\npcardoso@ufpa.br,{jacksoncruz,pereiramateus,gabrielsizinio,nairagama}@ufba.br,\n\n{roana,larissajesus}@academico.ufs.br, tobias.almeida@estudante.ufla.br\n\nAbstract. Due to their capability to enable the observation of linguistic and\nsocial behaviors, annotated corpora have become of interest to various fields of\nstudy. In the context of Rhetorical Structure Theory (RST), this paper presents\nthe methodological and practical processes involved in annotating discourse\nmarkers within a Brazilian Portuguese journalistic corpus. Additionally, we\nprovide initial quantitative and qualitative assessments of the decisions made\nby the annotation team.\n\nResumo. Por serem recursos que permitem a observac¸ ˜ao de comportamentos e\nusos lingu´ısticos e sociais, os corpora anotados passaram a ser de interesse de\ndiferentes ´areas do conhecimento. No contexto da Rhetorical Structure Theory\n(RST) apresentamos neste trabalho os processos metodol´ogicos e pr´aticos de\nanotac¸ ˜ao de sinalizadores discursivos em um corpus jornal´ıstico do portuguˆes\ndo Brasil. Ainda, apresentamos as primeiras avaliac¸ ˜oes (quanti e qualitativa)\nsobre as decis˜oes tomadas pelo grupo de anotadores.\n\n1. Introduc¸ ˜ao\nA Lingu´ıstica de corpus (LC), enquanto ´area, instiga a utilizac¸ ˜ao de t´ecnicas e meto-\ndologias que nos levam a reunir grandes conjuntos de dados textuais (escritos, orais ou\nmultimodais), a fim de descrever fenˆomenos lingu´ısticos. Em interface ao Processamento\nde Linguagem Natural (PLN), uma das tarefas que a LC se prop˜oe a realizar ´e a anotac¸ ˜ao\ndesse conjunto de dados, tida como “o processo de enriquecer um corpus, adicionando\ninformac¸ ˜oes lingu´ısticas inseridas por humanos ou m´aquinas com um objetivo te´orico ou\npr´atico” [Pedro e Vale 2018].\n\nPor serem recursos que permitem a observac¸ ˜ao de comportamentos e usos\nlingu´ısticos e sociais, os corpora anotados passaram a ser de interesse de diver-\nsas ´areas do conhecimento, como Humanidades digitais, Lingu´ıstica e Computac¸ ˜ao.\n\n\f[Pustejovsky e Stubbs 2012] apontam que a an´alise dos corpora permite desvendar a na-\ntureza da linguagem e, consequentemente, capturar poss´ıveis propriedades que possam\nser modeladas computacionalmente.\n\nPor´em, esse processo de anotac¸ ˜ao tende a ser custoso, j´a que grande parte ´e re-\nalizada de forma semiautom´atica e requer intervenc¸ ˜ao humana.\n[Hovy e Lavid 2010]\napresentam uma metodologia gen´erica sobre esse processo, que engloba etapas como\npreparac¸ ˜ao do conjunto de dados, instanciac¸ ˜ao da base te´orica, anotac¸ ˜ao de fragmentos\ndo corpus, medic¸ ˜ao das decis˜oes de anotac¸ ˜ao e escalabilidade do processo de maneira\nautom´atica. No entanto, essa tarefa pode ser ajustada conforme o tipo de anotac¸ ˜ao a ser\nrealizada, o que pode levar `a omiss˜ao de algumas das etapas sugeridas pelos autores.\n\n[Taboada e Das 2013] e [Liu e Zeldes 2019], a partir de corpora pr´e-anotados\ncom o modelo Rhetorical Structure Theory (RST) [Mann e Thompson 1987] identifica-\nram uma s´erie de pistas lingu´ısticas e estruturais que serviam de sinalizadores para as\nrelac¸ ˜oes discursivas previamente identificadas. Ambos os trabalhos organizaram os Si-\nnalizadores Discursivos (SD) em func¸ ˜ao de suas caracter´ısticas (semˆanticas ou sint´aticas,\npor exemplo), pondo em xeque a ideia de que as relac¸ ˜oes RST deveriam ser identificadas\nmajoritariamente por meio de Marcadores Discursivos (MD), tidos como preposic¸ ˜oes e\nconjunc¸ ˜oes.\n\nCom base nessa metodologia, [Rodrigues et al. 2023] descreveram SDs para al´em\ndos MDs a partir do corpus CSTNews [Cardoso et al. 2011]. Tal recurso lingu´ıstico-\ncomputacional consiste em um conjunto de textos jornal´ısticos em Portuguˆes que j´a havia\nsido anotado segundo o modelo RST. A RST prop˜oe que um texto coerente ´e formado por\nunidades m´ınimas de discurso (Elementary Discourse Units - EDU ou proposic¸ ˜oes) que\ndesempenham func¸ ˜oes ret´oricas para que o objetivo comunicacional do autor seja atin-\ngido. Partindo dessa anotac¸ ˜ao pr´evia, os anotadores deste trabalho, por sua vez, identifi-\ncaram apenas os sinalizadores que consideraram relevantes para caracterizar e/ou indicar\ndeterminadas relac¸ ˜oes, como em (1) - extra´ıdo do corpus CSTNews.\n\n(1) [A selec¸ ˜ao brasileira masculina de vˆolei,]A [que ´e treinada por Bernardinho,]B\n[venceu a Finlˆandia por 3 sets a 0.]C\n\nAs porc¸ ˜oes (1a) e (1c) foram conectadas por meio da relac¸ ˜ao RST Same-Unit,\nj´a que est˜ao separadas por haver deta-\nindicando que se trata da mesma unidade,\nlhamento informacional em 1B em relac¸ ˜ao `a (1a) por meio da relac¸ ˜ao Elaboration.\n[Rodrigues et al. 2023] indicaram que a pontuac¸ ˜ao (no caso, v´ırgula), a concordˆancia ver-\nbal e o encaixamento de outra relac¸ ˜ao RST poderiam ser utilizadas como pistas para a\nidentificac¸ ˜ao da relac¸ ˜ao Same-unit.\n\nEsse estudo preliminar resultou em um manual de anotac¸ ˜ao de SDs em textos\njornal´ısticos [Dantas et al. 2024], em que h´a, al´em de instruc¸ ˜oes, a proposta da primeira\ntaxonomia de sinalizadores de relac¸ ˜oes RST para o PB. Destaca-se que esse tipo de re-\ncurso com explicac¸ ˜oes, exemplos e instruc¸ ˜oes objetivas subsidia a decis˜ao dos anotadores\ndiante de fatos novos e/ou j´a conhecidos [Duran et al. 2022].\n\nAssim, objetivamos neste trabalho relatar as etapas metodol´ogicas e pr´aticas de\nanotac¸ ˜ao de SDs no corpus CSTNews, al´em de apontar as primeiras avaliac¸ ˜oes sobre as\ndecis˜oes tomadas pelo grupo de anotadores. Para tanto, este trabalho est´a organizado\nem 5 sec¸ ˜oes, al´em desta Introduc¸ ˜ao. Na Sec¸ ˜ao 2, destacamos trabalhos relacionados ao\n\n\fprocesso de anotac¸ ˜ao e an´alise em RST, sobretudo para o PB. Na Sec¸ ˜ao 3, detalhamos a\nmetodologia de anotac¸ ˜ao empreendida neste estudo. Na Sec¸ ˜ao 4 apresentamos os resulta-\ndos e as discuss˜oes correspondentes. Por fim, na Sec¸ ˜ao 5, tecemos algumas considerac¸ ˜oes\nfinais.\n\n2. Trabalhos Relacionados\n\nIdentificar relac¸ ˜oes RST por meio de marcas expl´ıcitas no texto n˜ao ´e uma tarefa nova,\nespecialmente em PLN para an´alise de discurso. Os MDs s˜ao tidos como conectivos en-\ntre porc¸ ˜oes textuais, sinalizando determinadas relac¸ ˜oes discursivas, como o “mas” para\noposic¸ ˜ao, por exemplo. A an´alise das relac¸ ˜oes discursivas (ou de coerˆencia) est´a intima-\nmente ligada a descobrir a intenc¸ ˜ao do autor ao apresentar partes do texto em uma ordem\ne combinac¸ ˜ao espec´ıficas. Portanto, trata-se de uma tarefa que vai al´em de identificar os\nMDs.\n\nA literatura [Marcu 2000, Pardo 2005, Taboada e Das 2013]\n\nindica que a\nidentificac¸ ˜ao de MD, em func¸ ˜ao da relac¸ ˜ao RST a que ocorrem, facilita o processamento\ndo texto. Estudos recentes [Das e Taboada 2018, Liu e Zeldes 2019] afirmam que os MDs\nsinalizam apenas um n´umero restrito de relac¸ ˜oes dentro de um texto, e sugerem que as\nrelac¸ ˜oes RST podem ser identificadas por sinais que v˜ao al´em deles. Como os MDs n˜ao\nmarcam explicitamente as relac¸ ˜oes e n˜ao s˜ao exclusivos, a noc¸ ˜ao de SD parece ser mais\napropriada do que a de MD nesse contexto.\n\n[Das e Taboada 2018] argumentam que para uma comunicac¸ ˜ao ser eficaz, ´e funda-\nmental que as relac¸ ˜oes sejam interpretadas de maneira relativamente clara, o que requer\nsinalizadores precisos. Os autores acreditam que as relac¸ ˜oes de coerˆencia s˜ao entida-\ndes cognitivas, e, portanto, ´e poss´ıvel descobrir como ouvintes e leitores as identificam\nusando indicadores que auxiliem o processo interpretativo. Utilizando o RST Discourse\nTreebank, os autores realizaram uma anotac¸ ˜ao detalhada dos SD, resultando no RST Sig-\nnalling Corpus (RST-SC). Eles observaram que pode haver relac¸ ˜oes sinalizadas por um\n´unico sinalizador (como MD, referˆencias pessoais, orac¸ ˜oes relativas ou dois pontos) ou\npor combinac¸ ˜oes de SD (como v´ırgula + orac¸ ˜ao no partic´ıpio passado, ou construc¸ ˜ao\nsint´atica paralela + cadeia lexical). Quando surgia uma nova instˆancia de um tipo es-\npec´ıfico de relac¸ ˜ao, os anotadores consultavam a taxonomia para encontrar o(s) sinaliza-\ndor(es) mais adequado para aquela instˆancia. Durante o processo de anotac¸ ˜ao, os autores\nobservaram casos em que n˜ao foi poss´ıvel determinar com precis˜ao o SD que representava\numa determinada relac¸ ˜ao.\n\nEm [Liu e Zeldes 2019] descreve-se um esforc¸o de anotac¸ ˜ao para ancorar SD a\npartir de diversas categorias tais como sint´atica, semˆantica, gr´afico and morfol´ogica. Seus\nresultados mostraram que, com 11 documentos e 4.732 tokens, 923 foram instˆancias de\nSD, o que representou mais de 92% dos sinais discursivos. O tipo semˆantico representou\na maioria dos casos, enquanto as relac¸ ˜oes discursivas ancoradas por DM corresponderam\na apenas cerca de 8,5% dos tokens ancorados.\n\nQuanto `a l´ıngua portuguesa, [Pardo 2005] foi o precursor em investigar a\nconstruc¸ ˜ao de analisadores discursivos. A partir de um corpus de textos cient´ıficos e\nanotado com RST, o autor identificou diversos padr˜oes de an´alise que especificam os re-\nlacionamentos entre as relac¸ ˜oes ret´oricas e seus marcadores textuais. Apesar de muitos\npadr˜oes serem baseados em MD, o autor ressalta que n˜ao existe uma relac¸ ˜ao sine qua non\n\n\fentre MD e as relac¸ ˜oes que sinalizam, pois uma mesma relac¸ ˜ao pode ser sinalizada por\nv´arios marcadores (por exemplo, a relac¸ ˜ao Concession pode ser sinalizada pelos marcado-\nres “entretanto”, “no entanto”, entre outros) e um mesmo marcador pode sinalizar v´arias\noutras relac¸ ˜oes (por exemplo, o marcador “porque” pode sinalizar as relac¸ ˜oes Cause e\nResult (volitivas ou n˜ao), Justify, Explanation, entre outras).\n\n[Maziero 2016]\n\nAinda com relac¸ ˜ao ao portuguˆes,\n\ninvestigou atributos de\norganizac¸ ˜ao textual, da morfossintaxe, da sintaxe, da semˆantica e discurso para construir\num analisador discursivo baseado na RST. A partir da an´alise de corpora anotados com\nRST, o autor aponta que: a) existem relac¸ ˜oes que apresentam grande subjetividade, tais\ncomo as relac¸ ˜oes Evidence, Justify e Explanation; b) a relac¸ ˜ao Same-unit ocorre apenas no\nn´ıvel intrassentencial, o que ´e esperado, pois ´e respons´avel por ligar proposic¸ ˜oes quebra-\ndas por uma relac¸ ˜ao de Parenthetical ou Elaboration, por exemplo; c) algumas relac¸ ˜oes\ns˜ao mais frequentes no n´ıvel intrassentencial do que no inter-sentencial. Ap´os v´arios\nexperimentos com aprendizado de m´aquina para identificac¸ ˜ao das relac¸ ˜oes discursivas\nno n´ıvel intrassentencial, o autor concluiu que atributos morfossint´aticos proporcionaram\nmelhores resultados do que os atributos semˆanticos e discursivos.\n\n3. Metodologia\n\nA anotac¸ ˜ao de SDs foi feita a partir do corpus CSTNews1. Os textos do corpus est˜ao\norganizados em 50 conjuntos, com dois ou trˆes documentos que noticiam o mesmo evento.\nPor essa caracter´ıstica multidocumento e de redundˆancia, a anotac¸ ˜ao foi feita apenas no\nmaior texto do conjunto, pois acreditamos que quanto maior for o texto, maior ´e a chance\nde encontrarmos mais relac¸ ˜oes RST e, possivelmente, essas relac¸ ˜oes ocorram nos outros\noutros textos da mesma colec¸ ˜ao. Nesse caso, foram separados 50 textos para esta tarefa\nde anotac¸ ˜ao.\n\nA anotac¸ ˜ao de SDs foi realizada por meio da ferramenta rstWeb [Zeldes 2016],\nque ´e uma plataforma desenvolvida para facilitar a an´alise e a anotac¸ ˜ao de textos com\nbase na RST. Essa ferramenta permite aos usu´arios realizar an´alises estruturais detalhadas\ndos textos, identificando proposic¸ ˜oes e suas relac¸ ˜oes de coerˆencia conforme proposto pela\nteoria. Neste trabalho, a taxonomia de SDs, na Figura 1, foi implementada.\n\nOs textos escolhidos foram pr´e-processados e distribu´ıdos a um grupo de oito\npesquisadores. A anotac¸ ˜ao aconteceu de maneira ass´ıncrona, em que cada anotador re-\ncebia semanalmente 3 ou 4 textos. Cada texto foi anotado por trˆes anotadores para que\npud´essemos ter uma vers˜ao do corpus com a decis˜ao sobre a indicac¸ ˜ao dos SDs por mai-\noria simples.\n\nPara promover discuss˜ao e resoluc¸ ˜ao de d´uvidas, especialmente sobre casos n˜ao\nprevistos pelo manual, foram conduzidas reuni˜oes semanais com o grupo. Al´em disso,\ndois dos anotadores, por terem mais experiˆencia com tarefas nesse sentido, nunca ficavam\njuntos no trio, para que pudessem auxiliar na resoluc¸ ˜ao de d´uvidas de maneira ass´ıncrona.\nRessalta-se que os anotadores possu´ıam diferentes formac¸ ˜oes acadˆemicas (linguistas ou\ncientistas da computac¸ ˜ao) e com experiˆencias distintas em tarefas de anotac¸ ˜ao de corpus.\nPor conta disso, foi necess´aria uma etapa de treinamento para que o grupo se familiari-\nzasse com a taxonomia de SDs e com o modelo RST, al´em de ter acesso ao manual de\n\n1Dispon´ıvel em: http://nilc.icmc.usp.br/CSTNews/login/?next=/CSTNews/\n\n\fFigura 1. Taxonomia de sinalizadores discursivos proposta por Autores (2023).\n\nFigura 2. Taxonomia de sinalizadores discursivos proposta por Autores (2023).\n\nanotac¸ ˜ao para auxiliar em suas decis˜oes.\n\nNa fase de treinamento, foi realizada a anotac¸ ˜ao de trˆes textos do corpus. Em\nreuni˜oes s´ıncronas, os anotadores puderam corrigir poss´ıveis equ´ıvocos e identificar quais\nunidades do discurso deveriam ser devidamente anotadas, ou seja EDUs que estivessem\npresentes em um mesmo per´ıodo sint´atico.\n\nNa Figura 2, tˆem-se um exemplo de anotac¸ ˜ao para a relac¸ ˜ao Circumstance. Essa\nrelac¸ ˜ao RST deve apresentar uma situac¸ ˜ao realiz´avel, em que o sat´elite (EDU 1, provˆe\na situac¸ ˜ao que ´e apresentada no n´ucleo - EDU 2). No exemplo, tem-se que essa relac¸ ˜ao\nest´a sendo sinalizada por meio de Adv´erbio (vermelho), Orac¸ ˜ao circunstancial (azul) e\nPontuac¸ ˜ao (lil´as).\n\nTrˆes textos de diferentes tamanhos foram anotados por todos os anotadores em\ndistintas fases do processo, com o objetivo de medir periodicamente a concordˆancia do\ngrupo. Esse processo se repetiu a cada 10 conjuntos de textos anotados. Ao final de 2\nmeses, 47 textos foram anotados.\n\n\fTrecho anotado com a relac¸ ˜ao RST\nConcession\n“Tal capacidade de mutac¸ ˜ao fez escola,\nmas dificilmente as criaturas saber˜ao\nsuperar o criador.”\n\nAnotadores Sinalizadoes indicados\n\nA\nB\nC\n\n“mas” + “,”\n“mas” + “,”\n“mas”\n\nTabela 1. Comparac¸ ˜ao de anotac¸ ˜oes.\n\nA concordˆancia foi medida automaticamente a partir de duas abordagens. Na\nabordagem gold observa-se estritamente o que o grupo de anotadores apontaram como\nsinalizador, sendo, portanto, mais restrita. J´a na abordagem silver definiu-se um intervalo\nde cinco janelas (`a esquerda e `a direita) em relac¸ ˜ao ao sinalizador-alvo, como demons-\ntrado na Tabela 1.\n\nNa Tabela 1, tem-se um exemplo de como as duas abordagens da concordˆancia\nforam aplicadas. Os anotadores A e B indicaram os mesmos sinalizadores, ao passo que\no sinalizador C indicou apenas um em comum com o grupo. Caso fosse considerada\napenas uma an´alise mais restritiva sobre a concordˆancia, a decis˜ao do anotador C prejudi-\ncaria o c´alculo, ao passo que numa abordagem mais ampla, sua decis˜ao n˜ao traria tantos\npreju´ızos.\n\nApesar de todos os esforc¸os metodol´ogicos de distribuic¸ ˜ao de textos, e de todos os\nanotadores estarem alinhados junto ao modelo te´orico e `a ferramenta utilizados, ´e poss´ıvel\nque fatores externos `a tarefa influenciam na disposic¸ ˜ao dos anotadores, fazendo-os even-\ntualmente n˜ao apenas discordarem sobre um sinalizador, mas tamb´em n˜ao se atentarem\na realizar a indicac¸ ˜ao adequada. Por conta disso, escolheu-se neste trabalho n˜ao ape-\nnas realizar uma an´alise mais restrita sobre a concordˆancia, mas tamb´em mais ampla,\nadmitindo-se nesta, sobretudo, a dimens˜ao mais subjetiva da tarefa.\n\nas\n\nutiliza-se\n\nabordagens\n\nEm ambas\n\na medida Krippendorff Alpha\n[Krippendorff 2011]. Trata-se de uma medida que avalia a concordˆancia entre dois\nou mais anotadores, o que se encaixa melhor no contexto deste trabalho, j´a que cada texto\nfoi anotado por trˆes pessoas. O resultado da concordˆancia ´e medido num intervalo que\nvaria entre -1 e 1, em que valores mais pr´oximos a 1 indicam alta concordˆancia; valores\npr´oximos a 0 indicam baixa concordˆancia; e valores pr´oximos a -1 indicam discordˆancia\ntotal.\n\n4. Resultados e Discuss˜ao\n\nNa Tabela 2, tem-se a m´edia dos resultados das concordˆancias gold e silver da anotac¸ ˜ao\nem diferentes etapas do processo. Como dito, na fase de treinamento (clusters 1, 2 e 3),\nos anotadores realizaram uma primeira anotac¸ ˜ao e, ap´os reuni˜ao de alinhamento, fizeram\ncorrec¸ ˜oes. O c´alculo da concordˆancia geral (clusters 16, 31 e 39) foi feito sobre o mesmo\ntexto anotado por todo o grupo.\n\nDado que o processo de anotac¸ ˜ao pode ser longo e complexo, fatores externos\naos aspectos lingu´ısticos (como cansac¸o e diminuic¸ ˜ao da atenc¸ ˜ao, por exemplo) pode ter\ninfluenciado os anotadores. ´E poss´ıvel perceber isso ao comparar as fases de treinamento\ncom as demais, em que as demais sofreram decr´escimos discretos. Al´em disso, outro\nposs´ıvel aspecto que pode ter influenciado nesse resultado ´e a distribuic¸ ˜ao das relac¸ ˜oes\n\n\fFase do trabalho\nTreinamento\nConcordˆanca geral\nRodadas de anotac¸ ˜ao\n\nConcordˆancia\nGold Silver\n0,693\n0,581\n0,596\n0,460\n0,691\n0,496\n\nTabela 2. Resultado da concord ˆancia.\n\nRST no corpus CSTNews. [Cardoso et al. 2011] apontam que h´a relac¸ ˜oes RST que ocor-\nrem apenas uma vez, como Otherwise, por exemplo, e outras que aconteceram de maneira\npredominante, como Elaboration, que ocorreu 1,514 vezes. Nesse caso, ´e poss´ıvel que,\nao se deparar com uma relac¸ ˜ao RST n˜ao prevista na fase de treinamento e, portanto, au-\nsente no manual de anotac¸ ˜ao, os anotadores enfrentaram dificuldades em indicar poss´ıveis\nsinalizadores das relac¸ ˜oes em quest˜ao.\n\nAl´em de uma an´alise quantitativa, foram feitas observac¸ ˜oes qualitativas prelimi-\nnares. Para tanto, durante a anotac¸ ˜ao, os anotadores realizaram indicac¸ ˜oes de d´uvidas,\ninconsistˆencias e/ou outras quest˜oes em um formul´ario eletrˆonico. Ao final de cada se-\nmana, todos os apontamentos eram compilados e discutidos entre o grupo para aprimorar\no processo. A partir disso, ´e poss´ıvel destacar alguns pontos:\n\na) Considerac¸ ˜oes sobre o processo de anotac¸ ˜ao\n\nEm caso de n˜ao encontrar uma etiqueta para representar o fenˆomeno observado, o\nanotador poderia registrar os tokens envolvidos e marcar como CPD (Casos Para Discutir\ndepois). Em discuss˜oes e an´alises preliminares, os anotadores destacaram a intenc¸ ˜ao\nde marcar as entidades mencionadas no segmento textual. [Das e Taboada 2018], por sua\nvez, descrevem que os anotadores discordavam bastante entre entidade e tipos semˆanticos,\nou seja, enquanto um anotador seleciona entidade como o sinal relevante para uma certa\nrelac¸ ˜ao, o outro anotador a anota como sendo semˆantica. Os autores observaram que\nmuitos dos atributos de entidade e caracter´ısticas semˆanticas na verdade se sobrep˜oem.\nDessa forma, essa dificuldade acontece tamb´em para a l´ıngua inglesa.\n\nAssim como [Liu 2019, Das e Taboada 2018] relatam, tamb´em observamos no\ncorpus de estudo v´arias relac¸ ˜oes que n˜ao tinham um token expl´ıcito para servir de si-\nnalizador. Esses casos foram registrados como CPD. Por outro lado, as primeiras an´alises\nrevelaram que alguns SD s˜ao altamente indicativos, enquanto outros s˜ao gen´ericos ou\namb´ıguos. Assim, para obter uma compreens˜ao mais precisa, ´e necess´ario considerar os\ncontextos ao redor dos SD para desambigu´a-los.\n\nb) Considerac¸ ˜oes sobre dificuldades e limitac¸ ˜oes encontradas\n\nA anotac¸ ˜ao das relac¸ ˜oes RST ´e um processo que se baseia na interpretac¸ ˜ao do\nanalista. Assim, a depender dessa interpretac¸ ˜ao ser˜ao indicadas determinadas relac¸ ˜oes\nRST em detrimento de outras, resultando, ent˜ao, em diferentes sinalizadores para essas\nrelac¸ ˜oes. Neste estudo, a identificac¸ ˜ao de SDs foi feita por um grupo majoritariamente\ndiferente de quem fez a anotac¸ ˜ao RST, com uma distˆancia temporal consider´avel entre\nas duas tarefas. Esse fato, portanto, pode ter sido um dificultador para o grupo que fez a\nindicac¸ ˜ao dos sinalizadores.\n\nAl´em disso, os anotadores destacaram que algumas relac¸ ˜oes RST utilizadas no\n\n\fCSTNews s˜ao mais dif´ıceis de interpretar, e consequentemente, torna-se um desafio apon-\ntar SD espec´ıficos, como exemplificado em (2).\n\n(2) (...)\n[com o Programa Fome Zero, conseguiu atingir o primeiro ponto\ndas Metas do Milˆenio - erradicar a fome -, com dez anos de antecedˆencia,]A\n[reduzindo em mais da metade a pobreza extrema.]B\n\nO trecho (2b) em relac¸ ˜ao ao trecho (2a) apresenta a relac¸ ˜ao Volitional result, ou\nseja, o resultado ocasionado foi n˜ao intencional. Nesse caso em espec´ıfico, os anotadores\nindicaram que o sentido do verbo “reduzindo” seria o indicativo do resultado, por´em sem\nmenc¸ ˜ao ao aspecto volitivo. Destaca-se que a maioria dos rols de relac¸ ˜oes para outras\nl´ınguas n˜ao preveem diferenc¸a nesse aspecto.\n\nOutro aspecto que parece ter apresentado dificuldade aos anotadores foi o fato de o\nmanual de anotac¸ ˜ao ter sido desenvolvido com base no estudo de [Rodrigues et al. 2023] e\nos resultados da fase de treinamento. Como citado, relac¸ ˜oes e sinalizadores que n˜ao esta-\nvam previstos e que ocorreram ao longo do corpus podem ter ocasionado certos equ´ıvocos\nentre os anotadores. Ademais, o fato de o manual indicar certa correlac¸ ˜ao entre SDs e\nrelac¸ ˜oes pode ter condicionado o olhar dos anotadores, como demonstrado em (3).\n\n(3) [nesta terc¸a deve se encontrar com o relator do caso na Cˆamara, deputado Jos´e\nCarlos Ara´ujo (PR-BA)]A [para tratar do assunto.]B\n\nDe acordo com [Cardoso et al. 2011], a sentenc¸a entre (3a) e (3b) ´e de Purpose.\nO manual de anotac¸ ˜ao de SDs utilizou esse exemplo e indicou que a preposic¸ ˜ao “para”\npode ser utilizada para identificar essa relac¸ ˜ao. Entretanto, o objetivo entre os segmentos\npode tamb´em ser evidenciado por meio de “orac¸ ˜ao final” presente em (3b). Nesse caso,\n´e poss´ıvel que os anotadores tenham sido condicionados a partir de determinados pressu-\npostos sobre as relac¸ ˜oes, ainda que tenham sido estimulados a indicarem em formul´ario\neletrˆonico outros poss´ıveis SDs e definic¸ ˜oes n˜ao previstos no manual.\n\nPor fim, cabe pontuar que no reposit´orio online do projeto de pesquisa “RST al´em\ndos marcadores discursivos”2 disponibilizamos para consulta o corpus com a vers˜ao uni-\nficada entre os anotadores, a anotac¸ ˜ao de SDs e a planilha completa da concordˆancia dos\nanotadores.\n\n5. Considerac¸ ˜oes Finais\n\nNeste trabalho buscamos detalhar a metodologia empregada na identificac¸ ˜ao de SDs em\ntextos jornal´ısticos a partir da taxonomia proposta por [Dantas et al. 2024]. Destacamos\nque um estudo com essa abordagem em PB ainda n˜ao havia sido realizado, ao contr´ario\ndo que j´a ocorre em outros idiomas, especialmente o inglˆes.\n\nOs resultados relatados podem subsidiar outras an´alises em estudos futuros. Um\ndesses estudos se concentra na investigac¸ ˜ao quali e quantitativa da correlac¸ ˜ao entre SDs e\nas relac¸ ˜oes RST, algo j´a iniciado por [Rodrigues et al. 2023] e tal como outros trabalhos\nfizeram [Liu 2019, Das e Taboada 2018, Pardo 2005]. Outro estudo ser´a em relac¸ ˜ao `a\nconcordˆancia de aspectos da anotac¸ ˜ao, como tipos (sint´atico e semˆantico, por exemplo)\ne subtipos (pronome relativo e conhecimento de mundo, por exemplo) dos sinalizadores.\nAo final desses estudos ser´a poss´ıvel fazer o levantamento da distribuic¸ ˜ao dos SDs no\n\n2Dispon´ıvel em https://sites.google.com/view/rst-poetisa/\n\n\fcorpus, bem como observar quais s˜ao mais ou menos consensuais entre os anotadores.\n\nDados os apontamentos cr´ıticos realizados sobre as limitac¸ ˜oes identificadas,\ndestaca-se que este trabalho apresenta potencial de servir de diretriz de investigac¸ ˜oes de\nan´alises sobre as relac¸ ˜oes RST e seus SDs e aprimoramento de ferramentas e recursos\npara anotac¸ ˜ao de corpus. Tais aspectos s˜ao de extrema importˆancia ao alargar a anotac¸ ˜ao\na escalas maiores buscando n˜ao apenas ampliar a quantidade de textos, mas tamb´em di-\nversificar os gˆeneros textuais a serem considerados.\n\n6. Agradecimentos\n\nEste trabalho foi realizado no ˆambito do Centro de Inteligˆencia Artificial da Universidade\nde S˜ao Paulo (C4AI - http://c4ai.inova.usp.br/), com o apoio da Fundac¸ ˜ao de Amparo\n`a Pesquisa do Estado de S˜ao Paulo (processo FAPESP #2019/07665-4) e da IBM. Este\nprojeto tamb´em foi apoiado pelo Minist´erio da Ciˆencia, Tecnologia e Inovac¸ ˜oes, com\nrecursos da Lei N. 8.248, de 23 de outubro de 1991, no ˆambito do PPI-Softex, coordenado\npela Softex e publicado como Residˆencia em TIC 13, DOU 01245.010222/2022-44.\n\n.\n\nReferˆencias\n\nCardoso, P. C., Maziero, E. G., Jorge, M. L. C., Seno, E. M., Di Felippo, A., Rino, L.\nH. M., Nunes, M. d. G. V., e Pardo, T. A. (2011). CSTNews - A discourse-annotated\ncorpus for single and multi-document summarization of news texts in Brazilian Portu-\nguese. In Proceedings of the 3rd RST Brazilian Meeting, pages 88–105.\n\nDantas, E., B´arbara, L. d. J. S., Pereira, M. A., Gama, N. S., Almeida, T. J. A., Souza, J.\nW. d. C., Cardoso, P. C. F., e Rodrigues, R. (2024). Manual de anotac¸ ˜ao de sinaliza-\ndores discursivos em textos jornal´ısticos. S´erie de Relat´orios T´ecnicos do Instituto de\nCiˆencias Matem´aticas e de Computac¸ ˜ao, Universidade de S˜ao Paulo.\n\nDas, D. e Taboada, M. (2018). RST signalling corpus: A corpus of signals of coherence\n\nrelations. Language Resources and Evaluation, 52:149–184.\n\nDuran, M. S., Nunes, M. d. G. V., Lopes, L., e Pardo, T. A. S. (2022). Manual de anotac¸ ˜ao\ncomo recurso de processamento de linguagem natural: o modelo universal dependen-\ncies em l´ıngua portuguesa. Dom´ınios de Lingu@ gem, 16(4):1608–1643.\n\nHovy, E. e Lavid, J. (2010). Towards a ‘science’of corpus annotation: a new methodologi-\ncal challenge for corpus linguistics. International Journal of Translation, 22(1):13–36.\n\nKrippendorff, K. (2011). Computing krippendorff’s alpha-reliability.\n\nLiu, Y. (2019). Beyond the Wall Street Journal: Anchoring and comparing discourse\n\nsignals across genres. arXiv preprint arXiv:1909.00516.\n\nLiu, Y. e Zeldes, A. (2019). Discourse relations and signaling information: Anchoring\n\ndiscourse signals in RST-DT. Society for Computation in Linguistics, 2(1).\n\nMann, W. C. e Thompson, S. A. (1987). Rhetorical Structure Theory: A theory of text\norganization. University of Southern California, Information Sciences Institute Los\nAngeles.\n\n\fMarcu, D. (2000). The rhetorical parsing of unrestricted texts: A surface-based approach.\n\nComputational linguistics, 26(3):395–448.\n\nMaziero, E. G. (2016). An´alise ret´orica com base em grande quantidade de dados. PhD\n\nthesis, Universidade de S˜ao Paulo.\n\nPardo, T. A. S. (2005). M´etodos para an´alise discursiva autom´atica. PhD thesis, Univer-\n\nsidade de S˜ao Paulo.\n\nPedro, W. e Vale, O. (2018). Comentcorpus: o uso de mecanismos lingu´ısticos na\ndetecc¸ ˜ao de ironia e sarcasmo para o portuguˆes do Brasil em um corpus opinativo.\nLingu´ıstica de corpus: perspectivas. Porto Alegre: Instituto de Letras da Universidade\nFederal do Rio Grande do Sul, pages 19–40.\n\nPustejovsky, J. e Stubbs, A. (2012). Natural Language Annotation for Machine Learning:\n\nA guide to corpus-building for applications. O’Reilly Media, Inc.\n\nRodrigues, R., Souza, J. W., e Cardoso, P. C. F. (2023). Sinalizadores ret´orico-discursivos:\nrevisitando a anotac¸ ˜ao RST no c´orpus CSTnews. In Anais do XIV Simp´osio Brasileiro\nde Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages 249–257. SBC.\n\nTaboada, M. e Das, D. (2013). Annotation upon annotation: Adding signalling informa-\n\ntion to a corpus of discourse relations. Dialogue & Discourse, 4(2):249–281.\n\nZeldes, A. (2016). rstWeb-a browser-based annotation interface for Rhetorical Structure\nTheory and discourse relations. In Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Demonstrations,\npages 1–5.\n\n\f"
        },
        {
            "titulo": "Genipapo - A Multigenre Dependency Parser for Brazilian Portuguese",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31138",
            "idioma": "Inglês",
            "storage_key": "files/article_31138_30941.pdf",
            "autores": [
                {
                    "nome": "Ariani Di Felippo",
                    "afiliacao": "USP / UFSCar",
                    "orcid": "http://orcid.org/0000-0002-4566-9352"
                },
                {
                    "nome": "Norton T. Roman",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0002-0563-2045"
                },
                {
                    "nome": "Bryan K. S. Barbosa",
                    "afiliacao": "USP / UFSCar",
                    "orcid": "https://orcid.org/0000-0002-4637-6498"
                },
                {
                    "nome": "Thiago A. S. Pardo",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0003-2111-1319"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Neste artigo, apresenta-se um esforço pioneiro para o desenvolvimento de um modelo de parsing multigênero para o português brasileiro. Seguindo o projeto Universal Dependencies, treinou-se um dos modelos do estado-da-arte em três corpora gold-standard de diferentes gêneros textuais (jornalístico, acadêmico e conteúdo gerado por usuário – postagens do X). Os experimentos revelam que nosso modelo multigênero de parsing produz resultados melhores ou competitivos em relação aos modelos de gênero único.",
            "keywords": [
                "dependency parser",
                "multigenre",
                "Universal Dependencies",
                "Brazilian Portuguese"
            ],
            "referencias": [
                "Bai, J., Wang, Y., Chen, Y., Yang, Y., Bai, J., Yu, J., and Tong, Y. (2021). Syntax-BERT: Improving pre-trained transformers with syntax trees. In Proceedings of the 16th Conference of the EACL, p. 3011–3020.",
                "Barbosa, B. K. d. S. (2024). Descrição sintático-semântica de nomes predicadores em tweets do mercado financeiro em português. Master’s thesis, Programa de Pós-Gradução em Linguísica, Universidade Federal de São Carlos.",
                "Bick, E. (2000). The Parsing System “Palavras”. Automatic Grammatical Analysis of Portuguese in a Constraint Grammar Framework. University of Arhus, Arhus.",
                "Bölücü, N., Rybinski, M., and Wan, S. (2023). Investigating the impact of syntax-enriched transformers on quantity extraction in scientific texts. In Proceedings of the 2nd Workshop on Information Extraction from Scientific Publications, p. 1–13, Bali.",
                "Candido, A., Maziero, E., Specia, L., Gasperin, C., Pardo, T., and Aluisio, S. (2009). Supporting the adaptation of texts for poor literacy readers: a text simplification editor for Brazilian Portuguese. In Proceedings of the 4th Workshop on Innovative Use of NLP for Building Educational Applications, p. 34–42, Boulder, Colorado.",
                "da Silva, F. J. V., Roman, N. T., and Carvalho, A. M. B. R. (2020). Stock market tweets annotated with emotions. Corpora, 15(3):343–354.",
                "de Marneffe, M.C., Manning, C. D., Nivre, J., and Zeman, D. (2021). Universal Dependencies. Computational Linguistics, 47(2):255–308.",
                "Duran, M. S. (2022). Manual de anotação de relações de dependência - versão revisada e estendida: orientações para anotação de relações de dependência sintática em língua portuguesa, seguindo as diretrizes da abordagem Universal Dependencies (UD).",
                "Jurafsky, D. and Martin, J. H. (2024). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models. Online manuscript released August 20, 2024.",
                "Lin, Y., Wang, C., Song, H., and Li, Y. (2021). Multi-head self-attention transformation networks for aspect-based sentiment analysis. IEEE Access, 9:8762–8770.",
                "Lopes, L. and Pardo, T. (2024). Towards portparser - a highly accurate parsing system for Brazilian Portuguese following the Universal Dependencies framework. In Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1, p. 401–410, Santiago de Compostela, Galicia/Spain. ACL.",
                "Nivre, J., de Marneffe, M.-C., Ginter, F., Hajič, J., Manning, C. D., Pyysalo, S., Schuster, S., Tyers, F., and Zeman, D. (2020). Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, p. 4034–4043, Marseille, France. ELRA.",
                "Rademaker, A., Chalub, F., Real, L., Freitas, C., Bick, E., and de Paiva, V. (2017). Universal Dependencies for Portuguese. In Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017), p. 197–206, Pisa, Italy. Linköping University Electronic Press.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained bert models for brazilian portuguese. In Intelligent Systems, p. 403–417, Cham. Springer International Publishing.",
                "Zhou, J., Zhang, Z., Zhao, H., and Zhang, S. (2020). LIMIT-BERT: Linguistics informed multi-task BERT. In Findings of the Association for Computational Linguistics: EMNLP 2020, p. 4450–4461.",
                "Zilio, L., Wilkens, R., and Fairon, C. (2018). Passport: A dependency parsing model for portuguese. In Computational Processing of the Portuguese Language, p. 479–489, Cham. Springer International Publishing."
            ],
            "artigo_completo": "Genipapo – a Multigenre Dependency Parser\nfor Brazilian Portuguese\n\nAriani Di Felippo1,2, Norton T. Roman3, Bryan K. S. Barbosa1,2,\nThiago A. S. Pardo1\n\n1 N´ucleo Interinstitucional de Lingu´ıstica Computacional (NILC)\nInstituto de Ciˆencias Matem´aticas e de Computac¸ ˜ao, Universidade de S˜ao Paulo (USP)\nS˜ao Carlos/SP – Brazil\n\n2Departamento de Letras, Universidade Federal de S˜ao Carlos (UFSCar)\nS˜ao Carlos/SP – Brazil\n\n3Escola de Artes, Ciˆencias e Humanidades, Universidade de S˜ao Paulo (USP)\nS˜ao Paulo/SP – Brazil\n\nariani@ufscar.br, norton@usp.br, bryankhelven@ieee.org\ntaspardo@icmc.usp.br\n\nAbstract. In this article, we present a pioneer effort on building a multigenre\nparsing model for Brazilian Portuguese. Following the Universal Dependen-\ncies framework, we trained a current state-of-the-art model in three corpora\nfrom different text genres (journalistic, academic and user-generated content –\nX posts). Our experiments show that our multigenre parsing model achieves\nbetter or competitive results in relation to single-genre trained parsers.\n\nResumo. Neste artigo, apresenta-se um esforc¸o pioneiro para o desenvolvi-\nmento de um modelo de parsing multigˆenero para o portuguˆes brasileiro.\nSeguindo o projeto Universal Dependencies, treinou-se um dos modelos do\nestado-da-arte em trˆes corpora gold-standard de diferentes gˆeneros textuais\n(jornal´ıstico, acadˆemico e conte´udo gerado por usu´ario – postagens do X). Os\nexperimentos revelam que nosso modelo multigˆenero de parsing produz resulta-\ndos melhores ou competitivos em relac¸ ˜ao aos modelos de gˆenero ´unico.\n\n1. Introduction\n\nSyntactic parsing is the task of automatically uncovering the syntactic relations among\nthe words of a sentence, resulting in syntactic trees, which correspond to one of the first\nanalysis levels in Natural Language Processing (NLP) [Jurafsky and Martin 2024]. This\ntask has proved useful for several different applications, such as text simplification, infor-\nmation extraction, automatic summarization, and sentiment analysis, among many others.\n\nIn the beginning,\n\nAs time goes by, parsing takes different importance degrees and attend dif-\nferent desires.\nit was common to have parsing as a step in\nNLP applications (e.g. grammar checking [Martins et al. 1998] and text simplification\n[Candido et al. 2009]). Recent advances in deep learning, distributional models, and\nlanguage modeling have allowed many applications to forgo deeper linguistic analysis,\nbut current research efforts have indicated that the inclusion of linguistic knowledge\nduring model training or in post-processing steps (e.g.\nin neuro-symbolic approaches)\n\n\fmay be relevant for improving results [Zhou et al. 2020, Bai et al. 2021, Lin et al. 2021,\nB¨ol¨uc¨u et al. 2023]. Moreover, given the expensive computational requirements for train-\ning the above models and the search for explicability and interpretability, linguistic anal-\nysis systems have reemerged as relevant alternatives in several research situations.\n\nThere are some well known parsers for Portuguese, including those considered\nclassic, such as PALAVRAS [Bick 2000] and PassPort [Zilio et al. 2018], and more re-\ncent ones aligned to the Universal Dependencies (UD) project [de Marneffe et al. 2021],\nas UDPipe\nstate-of-the-art Portparser\nthe\n[Lopes and Pardo 2024] (with accuracy near 95% for news texts).\n\n[Straka 2018]\n\ncurrent\n\nand\n\n2\n\nWe propose here to move a step further in parsing for Brazilian Portuguese (BP).\nUsing the different annotated corpora that are available in the UD initiative, and adopting\na widely known parsing framework (the Stanza pipeline [Qi et al. 2020]), we investigate\nthe issue of multigenre parsing, aiming at producing a parser that works well for differ-\nent language writing styles, including short and usually syntactically fragmented X posts\n(formerly known as tweets), “daily language” of news texts and (supposedly) more re-\nfined writing of academic texts. The resulting system, named Genipapo1 (an acronym for\n“multiGENre PArser for POrtuguese”), achieves better or competitive results in relation to\nthe single-genre trained parsers, consisting in a step to unleash the potential of Portuguese\ntext analysis tools to work on a wide variety of texts.\n\nThe rest of this article is organized as follows. Section 2 introduces the UD frame-\nwork. Section 3 briefly presents the main related work in the area. The adopted resources\nand methodology are reported in Section 4, whereas Section 5 presents the results of our\nexperiments. We conclude this article in Section 6.\n\n2. The Universal Dependencies framework\n\nUD [Nivre et al. 2020] is currently the most used dependency-based framework of mor-\nphological and syntactic analysis in NLP [Sanguinetti et al. 2023].\nIt is an attempt to\nstandardize the annotation of morphology and syntax, proposing a “universal” annotation\nstrategy for all languages, facilitating the development of multilingual taggers and parsers.\nAt the time of this writing, there are already over 240 treebanks available for more than\n150 languages, dealing with a variety of textual genres.\n\nIn UD, the following morphology information is considered: (i) Part-of-Speech\n(PoS) tags, (ii) lemmas, and (iii) features. The syntactic annotation consists of typed de-\npendency relations (deprels) between words. Currently, the model has 17 PoS tags and 37\ndeprels, plus a non-fixed set of morphological features. Figure 1 shows an example of an\nannotated post from the DANTEStocks corpus [Di-Felippo et al. 2021]. The basic depen-\ndency representation is a tree, where exactly one word is the head of the utterance (root)\n(e.g. “assina” – “sign”), and all the remaining words depend on some other word. The\nlabeled arcs represent the dependency relations, pointing from heads to their dependents.\nPoS tags, lemmas, and morphological features are displayed below the words in Figure 1.\n\n1The corresponding fruit, “Jenipapo” (with ‘J’ instead of ‘G’), is a tropical fruit, appreciated in several\nstates of Brazil and used for different purposes, from painting to eating and preparing beverages. By adopt-\ning this inspiration for the name of our parser, we sought this symbolic connection with something rooted\nin the Brazilian culture and language.\n\n\fFigure 1. Example of UD morphological and syntactic annotation.\n\n3. Related work\n\nAbout the linguistic resources for training UD-parsers, there are some available datasets\nin BP. One of the first corpora with UD annotation for texts in standard (or canonical)\nPortuguese is the UD-Portuguese-Bosque treebank [Rademaker et al. 2017], which con-\ntains 210,958 tokens across 9,357 sentences. The Brazilian portion of this corpus consists\nof 4,213 well-written sentences extracted from journalistic texts. There is also Petro-\nGold [Souza et al. 2021], which is a fully revised treebank that consists of academic texts\nfrom the oil and gas domain, in a total of 8,946 sentences (and 232,333 tokens). Differ-\nently from UD-Portuguese-Bosque, PetroGold is a specialised or domain-specific corpus.\nBesides, the UD project makes available the UD-Portuguese-GSD corpus [Zeman 2017].\nTotaling 12,020 sentences (296,169 tokens) from news texts and blogs, it features two\ndifferent textual genres, with different degree of canonicalness.\n\nSpecifically aiming at growing syntax-based resources for BP, another treebank\n(with genres beyond newswire texts) has been created. Porttinari\n[Pardo et al. 2021]\ncurrently includes two main genres (with others under construction): (i) news texts, rep-\nresenting standard written language, and (ii) user-generated content (UGC), representing\ninformal non-canonical web language (in particular, tweets/X posts).\n\nConcerning parsing models, some dependency UD-parsers are available for BP,\nspecially for news texts. UDPipe 2 [Straka 2018] is probably the most used model. Using\na graph-based biaffine attention architecture, it achieves a Labelled Attachment Score2\n(LAS) of 87.04% for news texts. Stanza [Qi et al. 2020] is another well-known sys-\ntem, which uses a feature-enriched Bi-LSTM-based deep biaffine neural method. Ac-\ncording to the results for the UD version 2.123, Stanza achieves 87.75% of LAS for\nnews texts. UDify [Kondratyuk and Straka 2019] is another important system.\nIt is a\nsemi-supervised multitask self-attention model. There is also the recently released Port-\nParser [Lopes and Pardo 2024], which was built by training UDPipe 2 with BERTim-\nbau [Souza et al. 2020] on the Porttinari-base corpus [Duran et al. 2023a], which is part\nof the journalistic portion of the larger Porttinari4 [Pardo et al. 2021] treebank. The model\n\n2This score evaluates the output of a parser by considering how many words have been assigned both\n\nthe correct syntactic head and the correct label of the relation [Nivre and Fang 2017].\n\n3https://stanfordnlp.github.io/stanza/ performance.html\n4https://sites.google.com/icmc.usp.br/poetisa/porttinari\n\n\fachieved LAS around 95%. This LAS value brings an improvement of around 7% over\nsome well-known existing baselines for standard written Portuguese language.\n\nAs a final example, it is important to cite the work of [Zilio et al. 2018]. However\ndelivering lower results than those by more recent works, the authors compared some\nprevious and classical parsing methods for BP. The authors reported that the best model\n(called PassPort) achieved LAS of 85.21% in the UD corpus. In an additional small scale\nevaluation, the PassPort was manually compared to PALAVRAS, using a single corpus\nof 90 sentences (1,295 tokens), randomly selected from three different genres, to wit,\nliterature, news texts and subtitles. The systems achieved similar results for dependency\nparsing, with a LAS of 85.02% for PassPort against 84.36% for PALAVRAS.\n\n4. Materials and methods\n\nGiven the objective of building a multigenre UD parser for BP, three corpora, belonging\nto three different genres, build our materials.\n\nOur first corpus, DANTEStocks [Di-Felippo et al. 2021], comprises 4,048 tweets\n(with 81,048 tokens) from the stock market domain automatically collected during 2014\n(which limits each post to 140 characters). The corpus was built by fetching mes-\nsages containing a ticker5 of one of the 73 stocks that composed Ibovespa at that time\n[da Silva et al. 2020]. DANTEStocks presents a combination of standard and non-standard\nwritten language, as well as speech marks, domain specific vocabulary and medium (Twit-\nter) features. The dependency relations of the corpus were annotated in two semiauto-\nmatic stages [Barbosa 2024]. First, a reference subcorpus of 1,000 tweets was annotated\nusing UDPipe 2, which had been trained on UD-Portuguese-Bosque and was chosen be-\ncause it is easily available for use online and offers reliable performance. This subcorpus\nwas then manually revised before being designated as a gold standard. The rest of the\ncorpus was then annotated by customizing Stanza for DANTEStocks. We used the com-\nbined Porttinari-base and reference subcorpus as the initial training set for Stanza. The\nresulting parsing model was used to automatically annotate a new (first) package of data\n(out of the remaining 3,048 tweets). The first package was manually revised and incorpo-\nrated to the previous dataset, being used to start a new training run of Stanza. This cycle\nof training iteration continued incrementally until the last (in a total of 6) package was\nannotated/revised. Regarding LAS, the final score (6th run) achieved 94.62%, increasing\n0.76% from the first run score of 93.86%.\n\nIt\n\nThe second corpus, PetroGold [de Souza and Freitas 2023], is a gold-standard\nintegrates the Petrolˆes corpus,\ntreebank for the oil and gas (O&G) domain.\nwhich is a collection of academic and technical documents from public agencies such\nas Petrobras and “Agˆencia Nacional do Petr´oleo, G´as Natural e Biocombust´ıveis”\n(ANP) [Gomes et al. 2021]. PetroGold is composed of 19 academic texts (theses and\ndissertations), with a total of 9,127 sentences and 253,640 tokens. The syntactic anno-\ntation of PetroGold also followed a semiautomatic approach. Especifically, four experts\nwere responsable for reviewing the output of a customized version of Stanza, trained on\nthe combination of UD-Portuguese-Bosque (v.2.6) and a small collection of data from\nthe O&G domain. Through an intrinsic evaluation using a model created by the UDPipe\n\n5A five or six-character alphanumerical string that represents a type of stock from a company, such as\n\n“PETR4” for Petrobras’ preferred stock.\n\n\ftool, the corpus achieves 88.53% of LAS. For NLP purposes, the corpus is subdivided\ninto three subsets. The subsets have 7,170, 737 and 1,039 sentences for training (80%),\nvalidation (8%) and test (12%).\n\nOur final corpus, Porttinari-base [Duran et al. 2023a], is the gold-standard (i.e.\nfully manually annotated and revised) journalistic subcorpus of Porttinari, which is com-\nposed of 8,418 sentences (168,080 tokens) selected from Folha de S˜ao Paulo newspaper.\nThe Porttinari-base annotation process started with an automatic annotation by UDPipe\n2 using the UD-Portuguese-Bosque corpus, which achieved 87% accuracy (in terms of\nLAS). Next, the dependency relations were manually revised in detail following an an-\nnotation manual containing specific guidelines for BP [Duran 2022]. Porttinari-base is\nalso subdivided into training, validation and test subsets. The subsets have 5,893, 842 and\n1,683 sentences in the train (70%), dev (10%) and test (20%) files, respectively.\n\nFor developing our parser, we employed the Stanza pipeline, which was trained\nand evaluated across different corpora. Since both PetroGold and Porttinari-base corpora\nalready come subdivided in train, validation and test sets, we first set apart their test sets\nto ensure they would only be used for final evaluation purposes. After this, we unified\neach corpus’ train and validation sets to build a larger training set for each, which was\nthen used in our experiments. Next, we randomly split (from a uniform distribution),\nDANTEStocks in training and test sets, following the same principle of keeping the test\nset strictly for final testing. Table 1 details each set size across the corpora.\n\nTable 1. Size and proportion of train and test sets across corpora.\n\nTrain\n\nTest\n\nCorpus\n\nDANTEStocks\n\nUnits Proportion Units Proportion\n3,643\nUP-Portuguese-PetroGold 7,907\n6,735\n\n405\n1,039\n1,683\n\n10%\n12%\n20%\n\n90%\n88%\n80%\n\nPorttinari-base\n\nUnit\n\ntweet\nsentence\nsentence\n\nTo assess the model’s performance across different genres, we combined the train-\ning sets from all three corpora to create a fourth, unified training set, along with a corre-\nsponding test set. A grid search was conducted for hyperparameter optimization, focusing\non batch size (2000, 3000, 4000, and 5000) and dropout rate (0.2, 0.3, and 0.4), since\nStanza does not natively support learning rate adjustments. Next, we ran 5-fold cross-\nvalidation with grid search (using the above mentioned grid) at each of the four training\nsets6, whereby each set was further split in five subsets, with four being used to train the\nmodel, and the fifth one being held for validation purposes. This subdivision procedure\nis repeated five times. We then selected, for each training set, the hyperparameters that\nproduced the highest LAS value across the validation sets during cross-validation.\n\nHaving the best set of hyperparameters for each of the four corpora (DANTE-\nStocks, PetroGold, Porttinari-base and their union), we retrained the model at each corpus\ntraining set, varying its random seed (42, 123, 456, 789 and 101,112), thereby changing\nthe model’s innitial configurations. To do so, PetroGold’s and Porttinari-base’s training\nsets were split back into their original training and validation sets, whereas DANTEStocks’\ntraining set was randomly split into training and validation sets, so that the entire DAN-\nTEStocks corpus would contain 10% of the data for test, 10% for validation and 80% for\n\n6I.e. each corpus’ individual training set and the largest set built from the union of these training sets.\n\n\ftraining purposes. The best performance model, across all seeds, was then selected for\neach corpus. As a final step, all four models were tested and compared using the previ-\nously separated test sets, which had been reserved exclusively for this final evaluation.\n\n5. Results and discussion\n\nTables 2 and 3 present the results of our model, when trained in each corpus’ training set\n(rows in the tables), and tested at the different test sets of the experiment. Table 2 refers to\nmodel results in terms of LAS, whereas Table 3 presents the results in terms of Unlabelled\nAttachment Score7 (UAS). In the tables, the “Genipapo” lines refer to the model trained\nin all of the available training sets, i.e. our multigenre model, while the “All together”\ncolumns refer to the union of all test sets.\n\nTable 2. Model’s LAS (%) at each corpus’ test set.\n\nTraining set\n\nPorttinari-base\nDANTEStocks\nPetroGold\nDANTEStocks + Port.-base\nDANTEStocks + PetroGold\nPorttinari-base + PetroGold\nGenipapo\n\nTest set\nPorttinari-base DANTEStocks PetroGold All together\n\n94.82\n87.61\n86.74\n94.91\n87.66\n94.92\n94.94\n\n66.10\n91.95\n61.30\n92.67\n91.85\n66.75\n92.69\n\n87.47\n83.68\n95.33\n87.94\n84.10\n95.29\n95.11\n\n88.48\n86.48\n87.30\n91.90\n86.66\n91.84\n94.75\n\nTable 3. Model’s UAS (%) at each corpus’ test set.\n\nTraining set\n\nPorttinari-base\nDANTEStocks\nPetroGold\nDANTEStocks + Port.-base\nDANTEStocks + PetroGold\nPorttinari-base + PetroGold\nGenipapo\n\nTest set\nPorttinari-base DANTEStocks PetroGold All together\n\n95.88\n90.36\n89.69\n95.95\n90.26\n95.91\n95.97\n\n75.55\n93.98\n71.45\n94.39\n93.97\n76.03\n94.42\n\n90.38\n87.51\n95.84\n90.97\n88.04\n96.00\n95.81\n\n91.27\n89.63\n90.15\n93.86\n89.76\n93.67\n95.73\n\nWe see that each model trained in isolation produces the best results for its corre-\nsponding genre. For instance, considering LAS, training with Porttinari-base produced\nthe best results for the test set of Porttinari-base (94.82%) and worse results for DAN-\nTEStocks (66.10%) and PetroGold (87.47%). This pattern holds across the genres, where\nthe isolated models consistently perform best when tested on the same genre they were\ntrained on. More interestingly, Genipapo, our multigenre parser, outperforms the single-\ngenre trained parsers for 2 of the genres (news texts and X posts), but not for the academic\ngenre. The differences, however, are minimal (less than 1%), suggesting that they could\nbe due to random fluctuation rather than statistically significant differences.\n\nWhen combining all test sets (“All together” columns in the tables), Genipapo\ndelivers the best results, achieving a 7% improvement in LAS and nearly 5% in UAS\n\n7UAS indicates the accuracy of the head ignoring the relation’s name (deprel) [Nivre and Fang 2017].\n\n\fcompared to the second-best results from single-genre parsers, and a 3% LAS and 2%\nUAS improvement over parsers trained on pairs. This suggests that Genipapo may be\nthe more suitable choice for processing texts from varied sources, such as diverse web\ncontent.\n\nBy looking at the results produced by Genipapo, when tested on each corpus sepa-\nrately, we see some common mistakes between pairs of deprels. One of the most common\nerrors across the three corpora was the confusion between obl and nmod. This result does\nnot come as a total surprise, since the classification of a nominal as an adverbial adjunct\n(obl) or as a nominal modifier (nmod) was already reported in the literature as a chal-\nlenge for parsing standard Portuguese (and also for humans in some situations), such as\nin journalistic and academic texts [Duran et al. 2023b, Souza et al. 2021]. Once this phe-\nnomenon is also observed in DANTEStocks’ UGC, this difficulty seems to be unrelated\nto the degree of “canonicalness” of the corpus. The pairs acl (adnominal clause) and\nadvcl (adverbial clause) and obj (the second argument of a verb) and nsubj (a nominal\nsubject) show a relevant confusion only in the standard language corpora. The confusion\nbetween acl and advcl seems to be a case of ambiguity that requires semantic knowledge\nto be solved, and the confusion between obj and nsubj occurs when the candidate to the\nsubject is at the right of the verb, since noun phrases at the right can be either object or\nsubject in Portuguese [Duran et al. 2023b].\n\nWhen comparing the errors of Genipapo at each deprel, we see the model making\na higher number of wrong root predictions in DANTEStocks, given its error rate of 7.7%\nagainst 2.0% in Porttinari-base and 0.9% in PetroGold. This might be due to the lin-\nguistic phenomena of tweets that bring some difficulty to the syntactic annotation of the\nroot. Another interesting observation relates to parataxis, which is one the the most fre-\nquent tag in our UGC corpus (708 cases), but not in the remaining corpora. The relatively\nlow error rate in DANTEStocks (9.3%) indicates that this deprel has been well learned by\nGenipapo in UGC. Moreover, we could see that the deprel tags most wrongfully predicted\ndue to under representation in Porttinari-base and DANTEStocks are the same: reparan-\ndum, dislocated and orphan. The first two tags do not occur in PetroGold, and the only\ntwo occurrences of orphan in this corpus were wrongly predicted.\n\nAs a way to compare Genipapo’s performance with that by a state-of-the-art\nmodel, we also run Portparser in the same testing sets as Genipapo (Table 4). We note\nthat the training, validation, and test splits of the Porttinari-base used by Portparser differ\nfrom those publicly available and used in our experiments with Genipapo. This discrep-\nancy means that some sentences present in the test sets of Porttinari-base and the unified\nset (All together) may have been included in the training or validation sets of Portparser,\nartificially boosting its LAS and UAS scores. Despite this, Genipapo outperformed Port-\nparser across all testing sets except for the Porttinari-base test set. In terms of LAS,\nGenipapo showed significant improvements over Portparser on the DANTEStocks test set\n(92.69% vs. 64.45%), the PetroGold test set (95.11% vs. 86.74%), and the combined\ntest set (94.75% vs. 89.51%). However, Portparser performed better on the Porttinari-\nbase test set (98.06% vs. 94.94%). The same pattern is observed in UAS scores, where\nGenipapo outperformed Portparser on DANTEStocks (94.42% vs. 75.81%), PetroGold\n(95.81% vs. 90.50%), and the combined test set (95.73% vs. 92.62%). Nevertheless,\nPortparser achieved higher UAS on Porttinari-base (98.58% vs. 95.97%).\n\n\fTable 4. Portparser’s LAS and UAS at each corpus’ test set.\n\nTest set\nPorttinari-base\nDANTEStocks\nPetroGold\nAll together\n\nLAS (%) UAS (%)\n\n98.06\n64.45\n86.74\n89.51\n\n98.58\n75.81\n90.50\n92.62\n\n6. Final remarks\n\nIn this paper, we introduced Genipapo, a multigenre UD-parser for Brazilian Portuguese,\nand showed that it had better or competitive performance in relation to genre specific\ntrained parsers. Future work includes (i) to extend Genipapo’s training to other genres and\ndomains, such as audio transcriptions, literary texts, and tweets related to the COVID-19\npandemic, whose corpora are still under construction, and (ii) to explore different parsing\nstrategies and pipelines.\n\nMore details about this work may be found at the POeTiSA project web portal:\n\nhttps://sites.google.com/icmc.usp.br/poetisa/.\n\nAcknowledgements\n\nThis work was carried out at the Center for Artificial Intelligence of the University of S˜ao\nPaulo (C4AI-http://c4ai.inova.usp.br/), with support by the S˜ao Paulo Research Founda-\ntion (FAPESP 2019/07665-4) and by the IBM Corporation. The project was also sup-\nported by the Ministry of Science, Technology and Innovation, with resources of Law\nN.8.248, of October 23, 1991, with in the scope of PPI-SOFTEX, coordinated by Softex\nand published as Residence in TIC13, DOU01245.010222/2022-44.\n\nReferences\n\nBai, J., Wang, Y., Chen, Y., Yang, Y., Bai, J., Yu, J., and Tong, Y. (2021). Syntax-\nBERT: Improving pre-trained transformers with syntax trees. In Proceedings of the\n16th Conference of the EACL, p. 3011–3020.\n\nBarbosa, B. K. d. S. (2024). Descric¸ ˜ao sint´atico-semˆantica de nomes predicadores\nem tweets do mercado financeiro em portuguˆes. Master’s thesis, Programa de P´os-\nGraduac¸ ˜ao em Lingu´ısica, Universidade Federal de S˜ao Carlos.\n\nBick, E. (2000). The Parsing System “Palavras”. Automatic Grammatical Analysis of\n\nPortuguese in a Constraint Grammar Framework. University of Arhus, ˚Arhus.\n\nB¨ol¨uc¨u, N., Rybinski, M., and Wan, S. (2023). Investigating the impact of syntax-enriched\ntransformers on quantity extraction in scientific texts. In Proceedings of the 2nd Work-\nshop on Information Extraction from Scientific Publications, p. 1–13, Bali.\n\nCandido, A., Maziero, E., Specia, L., Gasperin, C., Pardo, T., and Aluisio, S. (2009).\nSupporting the adaptation of texts for poor literacy readers: a text simplification editor\nfor Brazilian Portuguese. In Proceedings of the 4th Workshop on Innovative Use of\nNLP for Building Educational Applications, p. 34–42, Boulder, Colorado.\n\nda Silva, F. J. V., Roman, N. T., and Carvalho, A. M. B. R. (2020). Stock market tweets\n\nannotated with emotions. Corpora, 15(3):343–354.\n\n\fde Marneffe, M.-C., Manning, C. D., Nivre, J., and Zeman, D. (2021). Universal Depen-\n\ndencies. Computational Linguistics, 47(2):255–308.\n\nde Souza, E. and Freitas, C. (2023). Explorando variac¸ ˜oes no tagset e na anotac¸ ˜ao uni-\nversal dependencies (ud) para portuguˆes: Possibilidades e resultados com base no tree-\nbank petrogold. In Anais do XIV Simp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e\nda Linguagem Humana, p. 125–134, Porto Alegre, RS, Brasil. SBC.\n\nDi-Felippo, A., Postali, C., Ceregatto, G., Gazana, L., Silva, E., Roman, N., and Pardo,\nT. (2021). Descric¸ ˜ao preliminar do corpus DANTEStocks: diretrizes de segmentac¸ ˜ao\npara anotac¸ ˜ao segundo Universal Dependencies. In Anais do XIII Simp´osio Brasileiro\nde Tecnologia da Informac¸ ˜ao e da Linguagem Humana, p. 335–343, Porto Alegre, RS,\nBrasil. SBC.\n\nDuran, M., Lopes, L., Nunes, M. d. G. V., and Pardo, T. A. S. (2023a). The dawn of the\nPorttinari multigenre treebank: introducing its journalistic portion. In Proceedings of\nthe XIV Brazilian Symposium in Information and Human Language Technology (STIL),\np. 115–124, Porto Alegre, RS, Brasil. SBC.\n\nDuran, M., Nunes, M. d. G. V., and Pardo, T. A. S. (2023b). Construc¸ ˜oes sint´aticas do\nportuguˆes que desafiam a tarefa de parsing: uma an´alise qualitativa. In Proceedings\nof the 2nd Universal Dependencies Brazilian Festival (UDFest-BR), p. 424–433, Porto\nAlegre, RS, Brasil. SBC.\n\nDuran, M. S. (2022). Manual de anotac¸ ˜ao de relac¸ ˜oes de dependˆencia - vers˜ao revisada\ne estendida: orientac¸ ˜oes para anotac¸ ˜ao de relac¸ ˜oes de dependˆencia sint´atica em l´ıngua\nportuguesa, seguindo as diretrizes da abordagem Universal Dependencies (UD).\n\nGomes, D. S. M., Cordeiro, F. C., Consoli, B. S., Santos, N. L., Moreira, V. P., Vieira, R.,\nMoraes, S., and Evsukoff, A. G. (2021). Portuguese word embeddings for the oil and\ngas industry: Development and evaluation. Computers in Industry, 124:103347.\n\nJurafsky, D. and Martin, J. H. (2024). Speech and Language Processing: An Introduction\nto Natural Language Processing, Computational Linguistics, and Speech Recognition\nwith Language Models. 3rd edition. Online manuscript released August 20, 2024.\n\nKondratyuk, D. and Straka, M. (2019). 75 languages, 1 model: Parsing Universal Depen-\ndencies universally. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), p. 2779–2795, Hong Kong, China. ACL.\n\nLin, Y., Wang, C., Song, H., and Li, Y. (2021). Multi-head self-attention transformation\n\nnetworks for aspect-based sentiment analysis. IEEE Access, 9:8762–8770.\n\nLopes, L. and Pardo, T. (2024). Towards portparser - a highly accurate parsing system for\nBrazilian Portuguese following the Universal Dependencies framework. In Proceed-\nings of the 16th International Conference on Computational Processing of Portuguese\n- Vol. 1, p. 401–410, Santiago de Compostela, Galicia/Spain. ACL.\n\nMartins, R. T., Hasegawa, R., Nunes, M. d. G. V., Montilha, G., and Oliveira, O. N.\n(1998). Linguistic issues in the development of regra: A grammar checker for brazilian\nportuguese. Natural Language Engineering, 4(4):287–307.\n\n\fNivre, J., de Marneffe, M.-C., Ginter, F., Hajiˇc, J., Manning, C. D., Pyysalo, S., Schuster,\nS., Tyers, F., and Zeman, D. (2020). Universal Dependencies v2: An evergrowing\nmultilingual treebank collection. In Proceedings of the Twelfth Language Resources\nand Evaluation Conference, p. 4034–4043, Marseille, France. ELRA.\n\nNivre, J. and Fang, C.-T. (2017). Universal Dependency evaluation. In Proceedings of\nthe NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017), p. 86–95,\nGothenburg, Sweden. ACL.\n\nPardo, T., Duran, M., Lopes, L., Felippo, A., Roman, N., and Nunes, M. (2021). Porttinari\nIn Proceedings of the XIII\n- a large multi-genre treebank for brazilian portuguese.\nBrazilian Symposium in Information and Human Language Technology, p. 1–10, Porto\nAlegre, RS, Brasil. SBC.\n\nQi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. (2020). Stanza: A python\nIn Proceedings of\nnatural language processing toolkit for many human languages.\nthe 58th Annual Meeting of the Association for Computational Linguistics: System\nDemonstrations, p. 101–108, Online. ACL.\n\nRademaker, A., Chalub, F., Real, L., Freitas, C., Bick, E., and de Paiva, V. (2017). Uni-\nversal Dependencies for Portuguese. In Proceedings of the Fourth International Con-\nference on Dependency Linguistics (Depling 2017), p. 197–206, Pisa, Italy. Link¨oping\nUniversity Electronic Press.\n\nSanguinetti, M., Bosco, C., Cassidy, L., and et al. (2023). Treebanking user-generated\ncontent: a ud based overview of guidelines, corpora and unified recommendations.\nLanguage Resources Evaluation, 57:493–544.\n\nSouza, E., Silveira, A., Cavalcanti, T., Castro, M., and Freitas, C. (2021). Petrogold –\ncorpus padr˜ao ouro para o dom´ınio do petr´oleo. In Anais do XIII Simp´osio Brasileiro\nde Tecnologia da Informac¸ ˜ao e da Linguagem Humana, p. 29–38, Porto Alegre, RS,\nBrasil. SBC.\n\nSouza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained bert models for\nbrazilian portuguese. In Intelligent Systems, p. 403–417, Cham. Springer International\nPublishing.\n\nStraka, M. (2018). UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings\nof the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal\nDependencies, p. 197–207, Brussels, Belgium. ACL.\n\nZeman, D. e. a. (2017). CoNLL 2017 shared task: Multilingual parsing from raw text to\nUniversal Dependencies. In Proceedings of the CoNLL 2017 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies, p. 1–19, Vancouver, Canada. ACL.\n\nZhou, J., Zhang, Z., Zhao, H., and Zhang, S. (2020). LIMIT-BERT: Linguistics in-\nformed multi-task BERT. In Findings of the Association for Computational Linguis-\ntics: EMNLP 2020, p. 4450–4461.\n\nZilio, L., Wilkens, R., and Fairon, C. (2018). Passport: A dependency parsing model for\nportuguese. In Computational Processing of the Portuguese Language, p. 479–489,\nCham. Springer International Publishing.\n\n\f"
        },
        {
            "titulo": "Adapting LLMs to New Domains: A Comparative Study of Fine-Tuning and RAG strategies for Portuguese QA Tasks",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31139",
            "idioma": "Inglês",
            "storage_key": "files/article_31139_30942.pdf",
            "autores": [
                {
                    "nome": "Leandro Yamachita da Costa",
                    "afiliacao": "UFRJ",
                    "orcid": "http://orcid.org/0009-0001-7932-6163"
                },
                {
                    "nome": "João Baptista de Oliveira e Souza Filho",
                    "afiliacao": "UFRJ",
                    "orcid": "https://orcid.org/0000-0001-6005-8480"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": null,
            "keywords": [
                "Retrieval Augmented Generation",
                "RAG",
                "Fine-Tuning",
                "LLMs",
                "Portuguese Question Answering"
            ],
            "referencias": [
                "Sparck Jones, Karen. \"A statistical interpretation of term specificity and its application in retrieval.\" Journal of documentation 28.1 (1972): 11-21.",
                "Robertson, Stephen E. and Hugo Zaragoza. “The Probabilistic Relevance Framework: BM25 and Beyond.” Found. Trends Inf. Retr. 3 (2009): 333-389.",
                "“Introducing Meta Llama 3: The most capable openly available LLM to date.” AI at Meta. (2024).",
                "Wagner Filho, Jorge A., et al. \"The brWaC corpus: A new open resource for Brazilian Portuguese.\" Proceedings of the eleventh international conference on language resources and evaluation LREC (2018).",
                "Lin, Chin-Yew. “ROUGE: A Package for Automatic Evaluation of Summaries.” Annual Meeting of the Association for Computational Linguistics (2004).",
                "Conover, Mike, et al. \"Free Dolly: Introducing the world’s first truly open instruction-tuned LLM.\" Company Blog of Databricks (2023)."
            ],
            "artigo_completo": "Adapting LLMs to New Domains: A Comparative Study of \nFine-Tuning and RAG strategies for Portuguese QA Tasks \n\nLeandro Yamachita da Costa1, João Baptista de Oliveira e Souza Filho1 \n\n1Programa de Engenharia Elétrica \nUniversidade Federal do Rio de Janeiro (UFRJ) – Rio de Janeiro, RJ – Brazil \n\nleandro.yamachita@coppe.ufrj.br, jbfilho@poli.ufrj.br \n\nAbstract. The rise of Large Language Models (LLMs) represented a \nsignificant advance in text generation applications.  However, LLMs face \nchallenges in domains outside the scope of their original training. This \nstudy investigates the following two approaches to adapt LLMs to new \ndomains in the context of generative question-answering (QA) with data in \nPortuguese: fine-tuning and Retrieval-Augmented Generation (RAG).  The \nexperiments carried out in this study demonstrate the effectiveness of \nincorporating external data sources, even in models that had not been \nadjusted  for  the  specific  domain.  Furthermore,  the  combination  of \nsupervised fine-tuning with RAG proved to be the most effective approach.  \n\n1. Introduction \n\nThe rise of Large Language Models (LLMs) marked a significant advance in Natural \nLanguage Processing (NLP), especially in text generation tasks [Brown et al., 2020; \nAchiam  et  al.,  2023].  These  models,  which  are  trained  with  large  volumes  of  data, \ncan  retain  vast  amounts  of  knowledge  implicitly  in  their  parameters.  However, \nLLMs  face  challenges  in  domains  outside  the  scope  of  their  original  training  data, \nsuch as areas of specialized knowledge or current affairs [Kandpal et al., 2023, Kasai \net  al.,  2024].  This  issue  accentuates  the  need  to  adapt  LLMs  to  specific  contexts, \nespecially for smaller models with limited memory capacity.   \n\nThis study explores the adaptation of LLMs to new domains in the generative \nquestion-answer (QA) task, a scenario where the model generates answers based on \nquestions provided to it. Fine-tuning is a common approach to adjust LLMs to new \ndomains  by  modifying  the  model  parameters  with  training  on  application-specific \ndata.  In  the  QA  task,  fine-tuning  can  be  done  with  question-answers  pairs  in  a \n\"closed-book\" scenario [Zhang et al., 2024], where the model does not have access to \nexternal \ninformation.  Nonetheless,  this  approach  may  require  considerable \ncomputational power and extensive data annotation work [Guo et al., 2023].   \n\nA  widely  adopted  alternative  is  the  integration  of  external  knowledge \nsources, such as documents and books, in a setting known as \"open-book\" [Zhang et \nal., 2024]. A typical strategy of this approach is the Retrieval-Augmented Generation \n\n  \n \n \n\f(RAG)  [Lewis  et  al.,  2020],  which  combines  an  information  retriever  (IR)  model, \naimed at searching relevant data on an external data source, with a language model \nto generate answers based on the information retrieved.  This strategy allows LLMs \nto adapt to new domains without the need for fine-tuning. \n\nThis work analyzed these two approaches to adapt LLMs to specific domains \nin  the  QA  task  with  data  in  Portuguese.  We  analyzed  both  fine-tuning  and  RAG \nconfigurations,  in  addition  to  the  integration  of  the  two.  The  experiments \ndemonstrated the effectiveness of incorporating external data sources for improved \nresults.  Moreover,  different  fine-tuning  strategies  have  shown  to  be  particularly \neffective  when  combined  with  the  inclusion  of  external  data,  even  with  a  reduced \nvolume  of  training  data.  For  all  analysis,  we  considered  a  scenario  with  limited \ncomputational  resources,  where  we  only  used  a  general-purpose  GPU  (Nvidia \nGeForce RTX 4090). The fine-tuning of the models was performed using the QLoRA \ntechnique [Dettmers et al., 2023] with quantized models. \n\n2. Background and Related Work \n\nIn this session we briefly discuss RAG and LLMs fine-tuning. \n\n2.1. Retrieval-augmented Language Models \n\nConditioning  LLM  responses  to  information  from  external  sources  has  proven \neffective  in  adapting  these  models  to  specific  domains  in  several  NLP  tasks.    The \nRAG approach has been successfully applied in areas such as agriculture [Balaguer et \nal.,  2024],  scientific  literature  [Lála  et  al.,  2023],  and  medical  data  [Zakka  et  al., \n2024];  attesting  its  utility  in  improving  the  accuracy  and  relevance  of  answers. \nAdditionally, it can reduce the occurrence of hallucinations [Borgeaud et al., 2022; \nShuster et al., 2021], improve the model's ability to manage the gradual decline in its \nknowledge  over  time  [Vu  et  al.,  2023],  and  enhance  the  interpretability  of  the \nanswers [Lewis et al., 2020; Izacard et al., 2023].   \n\nThe effectiveness of RAG depends on the quality of the retrieval mechanism \nused, which impacts the relevance of the contextual information obtained.  Among \ntraditional  retrieval  mechanisms,  those  based  on  term  frequency  stand  out,  which \nemploy sparse representations of text passages, such as TF-IDF [Sparck Jones, 1972] \nand  BM25  [Robertson  and  Zaragoza,  2009].  Alternatively,  more  recent  approaches \nemploy  dense  representations  of  texts,  such  as  Dense  Passage  Retriever  (DPR) \n[Karpukhin et al., 2020] and ColBERT [Khattab and Zaharia, 2020].  \n\n2.2. Fine-tuning \n\nTo  adapt  QA  models  to  a  new  domain,  fine-tuning  seeks  to  adjust  the  model  to \nrespond  according  to  the  pattern  observed  in  the  training  data.    Furthermore,  it  is \nexpected  that  with  fine-tuning  the  model  will  acquire  domain-specific  knowledge, \nenhancing its capacity to provide more accurate answers. \n\n \n \n \n \n\fFine-tuning  large  language  models  (LLMs)  typically  requires  significant \ncomputational  resources.  Parameter  Efficient  Fine  Tuning  (PEFT)  [Xu  et  al.,  2023] \naddresses  this  by  freezing  the  model’s  parameters  and  adjusting  only  the  newly \nadded ones. Among PEFT methods, Low-Rank Adaptation (LoRA) [Hu et al., 2021] \nreduces the number of trainable parameters using low-rank matrices. The Quantized \nLow-Rank  Adaptation  (QLoRA)  [Dettmers  et  al.,  2023]  takes  this  approach  a  step \nfurther  by  applying  LoRA  to  quantized  models.  Studies  indicate  that  PEFT-tuned \nmodels often perform comparably to those fully fine-tuned [Li et al., 2023]. \n\n2.3. RAG and Fine-tuning \n\nThe  effectiveness  of  RAG  and  fine-tuning  strategies  has  been  extensively  studied. \n[Balaguer et al., 2024] compare these methods in a QA model for agriculture, while \n[Ovadia et al., 2023] extend the comparison to various topics with a multiple-choice \nQA  model.  Many  RAG  models  undergo  fine-tuning,  such  as  [Lewis  et  al.,  2020], \nwhere  both  the  model  and  retriever  are  adjusted  together.  [Zhang  et  al.,  2024] \nintroduce RAFT (Retrieval-Augmented Fine-Tuning), which helps the model ignore \nirrelevant documents. Overall, pre-trained models require additional fine-tuning to \nlearn specific reading comprehension tasks, which is essential for the effectiveness of \nRAG.  This  instruction  fine-tuning  does  not  always  need  to  be  done  with  domain-\nspecific data. \n\n3. Methodology  \n\n3.1. Language Models \n\nFor  the  experiments  in  this  study,  we  used  a  Portuguese-adapted  version  of  the \nmodel T5 [Raffel et al., 2020], called PTT5 [Carmo et al., 2020], and two versions of \nthe Llama-3 8B model [AI at Meta, 2024]. \n\nPTT5  was  pre-trained  on  the  BrWac  [Wagner  et  al.,  2018],  a  dataset \ncomposed of millions of Internet pages in Brazilian Portuguese. This study used the \nbase version of the model, which contains 220M parameters. This choice was due to \nits relatively small size, Portuguese pre-training, and encoder-decoder architecture, \ndistinguishing  it  from  Llama  3.  Llama  3,  which  was  developed  by  Meta,  uses  a \ndecoder-only architecture and is available in both pre-trained and instruction-tuned \nversions. Despite being trained mainly on English data, Llama 3 is multilingual and \nwas  evaluated  exclusively  with  Portuguese  data  in  this  study.  The  8-billion-\nparameter  version  of  Llama  was  chosen  for  its  suitability  to  limited  computational \nresources and its popularity as a widely used open-source model. \n\n3.2. Fine-tuning Approach \n\nFor fine-tuning the models, we used two different techniques: full parameter fine-\ntuning  and  the  QLoRa  technique  [Dettmers  et  al.,  2023].    Full  fine-tuning  was \n\n  \n \n \n\fapplied only to the PTT5 model, due to its reduced size and the limited availability \nof  computational  resources.  For  the  Llama  3-8B  models,  we  chose  the  QLoRa \ntechnique due to the large number of parameters in these models.  \n\n3.3. RAG Setup \n\nWe adopted a basic RAG setup, in which we used the retriever and language models \nwith  no  changes  to  their  original  architectures.  Three  retriever  models  were \nevaluated:  BM25  [Robertson  and  Zaragoza,  2009],  Dense  Passage  Retriever  (DPR) \n[Karpukhin et al., 2020], and ColBERT [Khattab and Zaharia, 2020]. With DPR, the \nembeddings were generated by a BERT-based [Devlin et al., 2019] model known as \nSentence-BERT [Reimers and Iryna, 2019].  In the case of ColBERT, we specifically \nused its second version - ColBERTv2 [Santhanam et al., 2021]. All IR models were \nused  without  any  type  of  training  on  the  data  under  study.  The  texts  retrieved  for \neach  query  were  concatenated  and  inserted  into  the  input  prompt  of  the  LLMs  as \nsupport texts.  \n\n3.4. Evaluation Setup \n\nWe considered four metrics to evaluate the results of the experiments: (i) Rouge-1 \nand  (ii)  Rouge-L  [Lin,  2004],  which  are  based  on  word  overlap;  (iii)  BERTScore \n[Zhang  et  al.,  2019],  which  employs  embeddings  generated  by  a  BERT  model;  and \n(iv) a specific metric developed with the use of GPT (GPT 4o mini). \n\nGPT was used to verify the accuracy of the answers generated by the models. \nTo  achieve  this,  we  developed  the  GPTScore  metric,  which  evaluates  whether  the \nmodels’  answers  align  with  the  content  of  the  reference  answers,  even  when  they \ndiffer in wording, length, or style. This evaluation used the prompt shown in Figure \n1.  The  metric  was  computed  by  tallying  the  number  of  “yes”  or  “no”  answers \nprovided by GPT. \n\nFigure 1. Prompt used to obtain the GPTScore. \n\n4. Experiments and Results \n\nThis section outlines the experimental setup and presents the results obtained using \nthe RAG and fine-tuning strategies.  \n\n \n \n \n \n\f4.1. Experimental Setup \n\nWe utilized two datasets in Portuguese. The first one, Pirá 2.0 [Paschoal et al., 2021; \nPirozelli  et  al.,  2024],  focuses  on  topics  related  to  the  Brazilian  coast,  oceans  and \nclimate change. This dataset contains questions, answers, and support texts derived \nfrom the abstracts of scientific papers and specialized reports on the aforementioned \ntopics,  all  of  which  have  a  version  in  Portuguese.    Comprising  2258  samples,  this \ndataset  was  split  as  follows:  80%  for  training,  10%  for  validation,  and  10%  for \ntesting. We selected this dataset because it includes texts in Portuguese and focuses \non  a  specific  domain.    The  second  dataset  is  a  Portuguese  translation  of  the \nDatabricks-Dolly  dataset  [Conover  et  al.,  2023],  which  consists  of  pairs  of \ninstructions  and  answers  across  various  task  categories  generated  by  Databricks \nemployees.  For  this  study,  we  only  kept  the  records  classified  as  \"closed  QA\"  task, \nwhich  involves  questions  and  answers  based  on  excerpts  from  Wikipedia.  This \ndataset contains 1766 records, distributed as follows: 70% for the training, 15% for \nvalidation,  and  15%  for  test.  It  was  included  in  this  study  because  it  contains \nquestions  from  a  broader  domain  that  still  require  supporting  texts  for  answer \nformulation.   \n\nThe experiments conducted in this study aimed to explore different strategies \nfor adapting LLMs to new domains, particularly focusing on fine-tuning and RAG-\nbased approaches. We investigated various settings regarding the use of supporting \ntexts, both in the fine-tuning process and during the validation stage. For the fine-\ntuning  experiments,  we  evaluated  two  approaches:  one  that  includes  supporting \ntexts and question-answer pairs in the input prompt, termed \"RAG FT\"; and another \nthat utilizes only question-answer pairs, referred to as \"QA FT\". For validation, the \nscenarios  in  which  support  texts  were  included  in  the  input  prompt  were  called \n\"RAG\".  For  settings  in  which  only  the  question  was  used  as  input,  the  following \nprompt  was  utilizes:  “Responda  à  pergunta  de  forma  sucinta.\\n\\nPergunta: \n{question}”.  In  cases  where  support  texts  were  also  included,  the  prompt  was \nmodified  to:  “Responda  à  pergunta  de  forma  sucinta  e  com  base  no  contexto  dado. \nContexto: {context}\\n\\n Pergunta: {question}”. \n\nIn our experiments, we employed a \"greedy\" decoding strategy, selecting the \ntoken  with  the  highest  probability  during  generation.  We  established  a  maximum \noutput  limit  of  100  tokens,  while  the  input  limit  was  set  at  1024  tokens  to \naccommodate  most  of  the  supporting  texts  without  truncation.  Fine-tuning  of  the \nLlama models employed the QLoRa method with 4-bit quantization over 10 epochs, \nutilizing a batch size and gradient accumulation of 4 to optimize hardware capacity. \nThe  model  generated  answers  at  16-bit  precision.  Meanwhile,  the  PTT5  model \nunderwent  full  fine-tuning  for  60  epochs,  using  a  batch  size  of  8  and  gradient \naccumulation  of  4.  All  training  was  conducted  using  Hugging  Face  libraries  on  an \nNVIDIA GeForce RTX 4090 GPU. \n\n  \n \n \n\fTo select the best retriever for the RAG experiments, we used GPT 4o mini \nto  answer  the  questions  in  each  dataset  based  on  the  context  provided  by  each \nretriever.  For each question, the texts retrieved by each model were concatenated \nand added to the prompt used by GPT to generate the answer. For the Pirá dataset, \nwe used the four most relevant passages identified by each retriever, while for the \nDolly dataset, we used the three most relevant passages. ColBERT outperformed all \nother models across the evaluated metrics and was, therefore, chosen for this study.  \nTable  1  summarizes  these  results  and  includes  a  hypothetically  ideal  retriever, \nsimulated by using context texts that are always correct for each question. \n\nTable 1. Evaluation of the retriever methods (see text). \n\n4.2. Models without fine-tuning \n\nThe experiments with models without fine-tuning, considering only the question in \nthe  input  prompt,  may  reveal  their  level  of  prior  knowledge  about  the  datasets’ \ndomain.    The  results  for  this  setting,  referred  to  as  \"No  FT,  No  RAG\"  in  Table  2, \nindicate  that  these  models  have  low  prior  knowledge  of  the  Pirá  dataset’s  domain \nand  moderate  knowledge  of  the  Dolly  dataset.  In  this  analysis,  results  for  PTT5 \nmodels are not reported, as we were unable to obtain satisfactory answers from this \nmodel without fine-tuning.  \n\nIn the case of RAG experiments with models without fine-tuning, referred to \nas \"No FT, RAG\", we observed an increase in GPTScore and a more modest rise in \nRouge  metrics.  This  behavior  is  expected,  as  Rouge  metrics,  which  assess  term \nmatching, are more influenced by the style of the answers – particularly their length \nand vocabulary.  Since the models were not fine-tuned to the datasets of interest, the \ngenerated answers may not align with the answer patterns from the dataset. It was \nobserved  that  the  pre-trained  model  often  answered  the  questions  and  then \ncontinued  generating  question-answer  pairs  indefinitely  until  it  reached  the \nmaximum  number  of  output  tokens.  The  Llama  Instruct  model  performed \nsignificantly  better  on  both  the  GPTScore  and  BERTScore  due  to  its  prior  fine-\ntuning,  which  enhanced  its  reading  comprehension  abilities.  This  suggests  that \n\n \n \n \n \n \n \n \n\fmodels  with  advanced  comprehension  skills,  even  if  trained  in  domains  different \nfrom  those  being  tested,  can  substantially  benefit  from  the  use  of  supporting \ncontexts  to  leverage  their  performance.  We  can  also  observe  that  models  without \nprior domain knowledge and that did not explore RAG were the worst performers. \n\n4.3. Models fine-tuned solely with question-answer pairs  \n\nThis setting aims to evaluate whether the models can internalize knowledge about \nthe  domains  through  the  fine-tuning  process,  specifically  based  on  questions  and \nanswers from the datasets, referred to as \"QA FT, No RAG\" in Table 2. The results \nshow that for both datasets, fine-tuning does not provide a significant improvement \nwhen the model is tested without RAG. However, when the model includes RAG, \nreferred  to  as  \"QA  FT,  RAG\"  in  Table  2,  we  observed  a  meaningful  gain  in  some \nevaluation  scenarios.  This  suggests  that  the  fine-tuning  process  helps  the  model \nlearn the style of the answers - the length of the answers becomes more similar to \nthat  observed  in  the  dataset  -  but  does  not  necessarily  enable  it  to  retain  domain \nknowledge. It is worth noting that the limited amount of training data may hinder \nthe model's ability to learn effectively through the fine-tuning process.  \n\n4.4. Models fine-tuned with question, answer and context \n\nIn  this  scenario,  the  models  were  fine-tuned  with  the  addition  of  contexts  in  the \ntraining prompts. In the validation setting without RAG, referred to as \"RAG FT, No \nRAG\", all models performed poorly.  This result is expected, as the primary purpose \nof fine-tuning with added contexts is to train the model to generate answers based on \nthe context itself.  Since this setting does not include the support texts in the input \nprompts, fine-tuning did not appear to achieve the desired outcome. \n\nIn the setting that includes RAG, referred to as \"RAG FT, RAG\", the models \nachieved the best results across all metrics for the two datasets analyzed. It is worth \nnoting  that  for  the  Llama  Instruct  model,  which  was  already  fine-tuned  for  the \nreading comprehension task, all settings that utilized RAG performed well according \nto GTPScore.  However, for Rouge metrics, the models with fine-tuning on domain-\nspecific data showed superior performance. This experiment suggests that even if the \nmodel  is  capable  of  extracting  answers  from  the  context,  fine-tuning  on  problem-\nspecific  data  may  be  beneficial  for  generating  answers  in  a  format  more  closely \naligned with that found in the dataset.  We also observed that fine-tuning the pre-\ntrained Llama model allowed it to achieve results comparable to those of the Llama \nInstruct  model,  despite  the  latter  being  previously  fine-tuned  with  a  significantly \nlarger amount of data. This result indicates that fine-tuning with context texts, even \nwhen performed with a reduced dataset, can enhance the model’s  ability  to extract \nrelevant  information  from  context.  In  this  setting,  we  also  observed  a  significant \nimprovement  in  the  PTT5  results,  which  were  clearly  surpassed  by  those  obtained \nwith the Llama models, likely to their much larger number of parameters. \n\n  \n \n\fTable 2. Experimental results (see text). \n\n5. Conclusion \n\nThis work analyzed various methods for adapting LLMs to specific domains in QA \ntasks, including fine-tuning the model and integrating external data through RAG. \nThe experiments demonstrated that incorporating external data generally improves \nthe  models’  performance,  regardless  of  whether  fine-tuning  is  applied.  The  results \nalso  showed  that  fine-tuning,  even  when  conducted  with  a  reduced  dataset,  can \nenhance  the  models’  performance.  Additionally,  we  observed  that  while  the  best \nresults  were  achieved  by  models  specifically  tuned  to  domain  data,  a  model  with \npreviously  fine-tuned  instructions  produced  similar  outcomes,  with  the  clear \nadvantage of not requiring any additional fine-tuning. \n\nThe  experiments  presented  here  were  conducted  using  a  basic  RAG  architecture, \nwithout any additional training of the retrievers on the datasets of interest. Future \nwork could explore the same settings with adjustments to the retrievers as well. \n\n6. Acknowledgments \n\nTo CNPq, FAPERJ, and CAPES. This study was financed in part by the Coordenação \nde Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) – Finance Code \n001. \n\n \n \n \n \n \n\f7. References \n\nBrown, Tom B. \"Language models are few-shot learners.\" In Proceedings of the 34th \nInternational Conference on Neural Information Processing Systems (NIPS \n'20) (2020). \n\nAchiam, \n\nJosh,  OpenAI  et  al. \n\n\"GPT-4 \n\ntechnical \n\nreport.\" arXiv  preprint \n\narXiv:2303.08774 (2023). \n\nKandpal,  Nikhil,  et  al.  \"Large  language  models  struggle  to  learn  long-tail \n\nknowledge.\" International Conference on Machine Learning. PMLR, (2023). \n\nKasai, Jungo, et al. \"REALTIME QA: What's the Answer Right Now?\" Advances in \n\nNeural Information Processing Systems 36 (2024). \n\nZhang,  Tianjun,  et  al.  \"RAFT:  Adapting  Language  Model  to  Domain  Specific \n\nRAG.\" arXiv preprint arXiv:2403.10131, (2024). \n\nGuo,  Kunpeng,  et  al.  \"Fine-tuning  Strategies  for  Domain  Specific  Question \nAnswering under Low Annotation Budget Constraints.\" IEEE 35th International \nConference on Tools with Artificial Intelligence (ICTAI). IEEE, (2023). \n\nLewis,  Patrick,  et  al.  \"Retrieval-Augmented  Generation  for  Knowledge-Intensive \nNLP  Tasks.\" Advances in Neural Information Processing Systems 33:  9459-9474, \n(2020). \n\nDettmers,  Tim \n\net \nLLMs.” ArXiv abs/2305.14314, (2023). \n\nal. \n\n“QLoRA:  Efficient \n\nFinetuning \n\nof  Quantized \n\nBalaguer, Angels, et al. \"RAG vs fine-tuning: Pipelines, tradeoffs, and a case study on \n\nagriculture.\" arXiv e-prints (2024): arXiv-2401. \n\nLála,  Jakub,  et  al.  \"PaperQA:  Retrieval-augmented  generative  agent  for  scientific \n\nresearch.\" arXiv preprint arXiv:2312.07559, (2023). \n\nZakka,  Cyril,  et  al.  \"Almanac—retrieval-augmented  language  models  for  clinical \n\nmedicine.\" NEJM AI 1.2 (2024): AIoa2300068. \n\nBorgeaud, Sebastian, et al. \"Improving language models by retrieving from trillions \n\nof tokens.\" International conference on machine learning. PMLR, (2022). \n\nShuster,  Kurt,  et  al. \n\n\"Retrieval  augmentation \n\nreduces  hallucination \n\nin \n\nconversation.\" arXiv preprint arXiv:2104.07567, (2021). \n\nVu,  Tu,  et  al.  \"FreshLLMs:  Refreshing  large  language  models  with  search  engine \n\naugmentation.\" arXiv preprint arXiv:2310.03214, (2023). \n\nIzacard, Gautier, et al. \"Atlas: Few-shot learning with retrieval augmented language \n\nmodels.\" Journal of Machine Learning Research 24.251 (2023): 1-43. \n\nSparck  Jones,  Karen.  \"A  statistical  interpretation  of  term  specificity  and  its \n\napplication in retrieval.\" Journal of documentation 28.1 (1972): 11-21. \n\n  \n\fRobertson,  Stephen  E.  and  Hugo  Zaragoza.  “The  Probabilistic  Relevance \n\nFramework: BM25 and Beyond.” Found. Trends Inf. Retr. 3 (2009): 333-389. \n\nKarpukhin,  Vladimir,  et  al.  \"Dense  passage  retrieval  for  open-domain  question \n\nanswering.\" arXiv preprint arXiv:2004.04906 (2020). \n\nKhattab, Omar, and Matei Zaharia. \"ColBERT: Efficient and effective passage search \nvia  contextualized  late  interaction  over  BERT.\" Proceedings  of  the  43rd \nInternational  ACM  SIGIR  conference  on  research  and  development  in \nInformation Retrieval (2020). \n\nXu, Lingling, et al. \"Parameter-efficient fine-tuning methods for pretrained language \nassessment.\" arXiv  preprint \n\nreview \n\nand \n\nmodels: \ncritical \nA \narXiv:2312.12148 (2023). \n\nHu,  Edward  J.,  et  al.  \"LoRA:  Low-rank  adaptation  of  large  language  models.\" arXiv \n\npreprint arXiv:2106.09685 (2021). \n\nLi,  Yixiao,  et  al.  \"LoftQ:  Lora-fine-tuning-aware  quantization  for  large  language \n\nmodels.\" arXiv preprint arXiv:2310.08659 (2023). \n\nOvadia,  Oded,  et  al.  \"Fine-tuning  or  retrieval?  comparing  knowledge  injection  in \n\nLLMs.\" arXiv preprint arXiv:2312.05934 (2023). \n\nRaffel, Colin, et al. \"Exploring the limits of transfer learning with a unified text-to-\n\ntext transformer.\" Journal of machine learning research 21.140 (2020): 1-67. \n\nCarmo,  Diedre,  et  al.  \"PTT5:  Pretraining  and  validating  the  T5  model  on  Brazilian \n\nPortuguese data.\" arXiv preprint arXiv:2008.09144 (2020). \n\n“Introducing Meta Llama 3: The most capable openly available LLM to date.” AI at \n\nMeta. (2024). https://ai.meta.com/blog/meta-llama-3/ \n\nWagner  Filho,  Jorge  A.,  et  al.  \"The  brWaC  corpus:  A  new  open  resource  for \nBrazilian  Portuguese.\" Proceedings of the eleventh international conference on \nlanguage resources and evaluation LREC (2018). \n\nDevlin,  Jacob  et  al.  “BERT:  Pre-training  of  Deep  Bidirectional  Transformers  for \nLanguage  Understanding.” North  American  Chapter  of  the  Association  for \nComputational Linguistics (2019). \n\nReimers,  Nils  and  Iryna  Gurevych.  “Sentence-BERT:  Sentence  Embeddings  using \nSiamese  BERT-Networks.” Conference  on  Empirical  Methods  in  Natural \nLanguage Processing (2019). \n\nSanthanam,  Keshav  et  al.  “ColBERTv2:  Effective  and  Efficient  Retrieval  via \nLightweight  Late  Interaction.” North American Chapter of the Association for \nComputational Linguistics (2021). \n\n \n \n\fLin,  Chin-Yew. \n\n“ROUGE:  A  Package \n\nfor  Automatic  Evaluation  of \nSummaries.” Annual  Meeting  of  the  Association  for  Computational \nLinguistics (2004). \n\nZhang,  Tianyi,  et  al.  \"BERTScore:  Evaluating  text  generation  with  BERT.\" arXiv \n\npreprint arXiv:1904.09675 (2019). \n\nPaschoal, André FA, et al. \"Pirá: A bilingual portuguese-english dataset for question-\nanswering  about  the  ocean.\" Proceedings  of  the  30th  ACM  International \nConference on Information & Knowledge Management (2021). \n\nPirozelli, Paulo, et al. \"Benchmarks for Pirá 2.0, a Reading Comprehension Dataset \nabout the Ocean, the Brazilian Coast, and Climate Change.\" Data Intelligence 6.1 \n(2024): 29-63. \n\nConover,  Mike,  et  al.  \"Free  Dolly:  Introducing  the  world’s  first  truly  open \n(2023). \n\ninstruction-tuned \nhttps://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-\nviable-instruction-tuned-llm \n\nLLM.\" Company  Blog  of  Databricks \n\n  \n\f"
        },
        {
            "titulo": "LLMs as Tools for Evaluating Textual Coherence: A Comparative Analysis",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31140",
            "idioma": "Inglês",
            "storage_key": "files/article_31140_30943.pdf",
            "autores": [
                {
                    "nome": "Bryan K. S. Barbosa",
                    "afiliacao": "UFCG",
                    "orcid": "http://orcid.org/0000-0002-4637-6498"
                },
                {
                    "nome": "Cláudio E. C. Campelo",
                    "afiliacao": "UFCG",
                    "orcid": "https://orcid.org/0000-0003-4404-2344"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Este estudo avalia o desempenho de Grandes Modelos de Língua (LLMs) recentes, como GPT-4o, GPT-3.5, Claude Opus e LLaMA 2, na análise automática de coerência textual. A pesquisa foca em três aspectos: coerência local, onde GPT-4o e o Claude Opus se destacam; coerência global, na qual Claude Opus e o mais eficaz; e detecção de incoerências, onde GPT-4o apresenta melhor desempenho. Esses resultados revelam as capacidades e limitações dos modelos atuais, contribuindo para o entendimento de suas aplicações no âmbito do Processamento de Línguas Naturais e trazendo avanços contínuos à área.",
            "keywords": [
                "textual coherence",
                "incoherence",
                "comparison",
                "NLP"
            ],
            "referencias": [
                "Aleixo, P. and Pardo, T. A. S. (2008). Cstnews: Um córpus de textos jornalísticos anotados segundo a teoria discursiva multidocumento cst (cross-document structure theory). Technical Report 326, Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos-SP. 12p.",
                "Barzilay, R. and Lapata, M. (2008). Modeling local coherence: An entity-based approach. In Knight, K., Ng, H. T., and Oflazer, K., editors, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 141–148, Ann Arbor, Michigan. Association for Computational Linguistics.",
                "Braz Junior, G. and Fileto, R. (2021). Investigating coherence in posts from a doubts forum in a virtual learning environment with bert. Conference Paper.",
                "Charolles, M. (1978). Introdução aos problemas da coerência dos textos: abordagem teórica e estudo das práticas pedagógicas. Editora Pontes.",
                "Davies, M. (2008). The corpus of contemporary american english (coca). Available online at",
                ".",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 4171–4186. Association for Computational Linguistics.",
                "Dias, M. (2016). Investigação de modelos de coerência local para sumários multi-documento. PhD thesis, Universidade de São Paulo.",
                "Elsner, M., Austerweil, J., and Charniak, E. (2007). A unified local and global model for discourse coherence. In Sidner, C., Schultz, T., Stone, M., and Zhai, C., editors, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 436–443, Rochester, New York. Association for Computational Linguistics.",
                "Freitas, A. R. P. (2013). Análise automática de coerência usando o modelo grade de entidades para o português. PhD thesis.",
                "Grosz, B. J., Joshi, A. K., and Weinstein, S. (1995). Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.",
                "Halliday, M. A. K. and Hasan, R. (1976). Cohesion in English. Longman.",
                "Hoey, M. (2013). Textual interaction: An introduction to written discourse analysis. Routledge.",
                "Jurafsky, D. and Martin, J. H. (2024). Speech and Language Processing, chapter 23. Draft, 3 edition. Accessed: 2024-10-10.",
                "Koch, I. and Travaglia, L. (2003). A coerência textual. Editora Contexto.",
                "Lai, A. and Tetreault, J. (2018). Discourse coherence in the wild: A dataset evaluation and methods. In Proceedings of SIGdial, pages 214–223.",
                "Lapata, M. and Barzilay, R. (2005). Automatic evaluation of text coherence: models and representations. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, IJCAI’05, page 1085–1090, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.",
                "Lin, Z., Ng, H. T., and Kan, M.Y. (2011). Automatically evaluating text coherence using discourse relations. In Lin, D., Matsumoto, Y., and Mihalcea, R., editors, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 997–1006, Portland, Oregon, USA. Association for Computational Linguistics.",
                "Mann, W. C. and Thompson, S. A. (1987). Rhetorical structure theory: Description and construction of text structures. In Natural Language Generation, pages 85–95. Springer Netherlands.",
                "Mikkelsen, L. F., Kinch, O., Pedersen, A. J., and Lacroix, O. (2022). Ddisco: A discourse coherence dataset for danish. In Proceedings of the 13th Language Resources and Evaluation Conference (LREC), pages 1234–1243.",
                "Naismith, B., Mulcaire, P., and Burstein, J. (2023). Automated evaluation of written discourse coherence using gpt-4. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 394–403, Online. Association for Computational Linguistics.",
                "Sagi, E. (2010). Discourse structure effects on the global coherence of texts.",
                "Seno, E. R. M. and Rino, L. H. M. (2005). Co-referential chaining for coherent summaries through rhetorical and linguistic modeling. In Proceedings of the Workshop on Crossing Barriers in Text Summarization Research/RANLP, Borovets, Bulgaria. Núcleo Interinstitucional de Linguística Computacional – NILC/USFCAR.",
                "Thompson, I. (1986). Readability beyond the sentence: Global coherence and ease of comprehension. Journal of Technical Writing and Communication, 16(1):131–140.",
                "Van Dijk, T. A. (1977). Text and context: Explorations in the semantics and pragmatics of discourse."
            ],
            "artigo_completo": "LLMs as Tools for Evaluating Textual Coherence:\nA Comparative Analysis\n\nBryan K. S. Barbosa1, Claudio E. C. Campelo1\n\n1Systems and Computing Department\nFederal University Campina Grande (UFCG) – Campina Grande, PB – Brazil\n\nbryankhelven@ieee.org, campelo@dsc.ufcg.edu.br\n\nAbstract. This study evaluate recent Large Language Models (LLMs), such as\nGPT-4o, GPT-3.5, Claude Opus, and LLaMA 2, for their ability to analyze tex-\ntual coherence. The research focuses on three areas: local coherence, where\nmodels like GPT-4o and Claude Opus excel; global coherence, where Claude\nOpus is most effective; and incoherence detection, where GPT-4o demonstrates\nstrong performance. These findings reveal both the capabilities and areas for\nimprovement in current models, shedding light on their potential applications in\nnatural language processing, paving the way for improvements in the field.\n\nResumo. Este estudo avalia o desempenho de Grandes Modelos de L´ıngua\n(LLMs) recentes, como GPT-4o, GPT-3.5, Claude Opus e LLaMA 2, na\nan´alise autom´atica de coerˆencia textual. A pesquisa foca em trˆes aspec-\ntos: coerˆencia local, onde GPT-4o e o Claude Opus se destacam; coerˆencia\nglobal, na qual Claude Opus ´e o mais eficaz; e detecc¸ ˜ao de incoerˆencias, onde\nGPT-4o apresenta melhor desempenho. Esses resultados revelam as capaci-\ndades e limitac¸ ˜oes dos modelos atuais, contribuindo para o entendimento de\nsuas aplicac¸ ˜oes no ˆambito do Processamento de L´ınguas Naturais e trazendo\navanc¸os cont´ınuos `a ´area.\n\n1. Introduction\n\nThe concept of coherence lies at the very heart of effective communication, serving as a\nkeystone element that determines the clarity, understandability, and overall quality of tex-\ntual content [Koch and Travaglia 2003]. Coherence transcends the boundaries of syntax\nor grammar; it embodies the logical flow of ideas, ensuring that a text is not just a col-\nlection of sentences but a unified whole that conveys meaning with precision and subtlety\n[Freitas 2013]. As we move deeper into the digital age, where written text interactions\nare increasingly prevalent [Hoey 2013], the capacity to automatically analyze textual co-\nherence became a crucial task within the realm of Natural Language Processing (NLP).\n\nThe advent of Large Language Models (LLMs) such as GPT-3, Llama, and Gem-\nini has revolutionized our approach to generating text that mirrors the nuance and depth of\nhuman-written content. These models, trained on extensive corpora, have demonstrated\nan impressive ability to produce coherent and contextually relevant text across a wide\nrange of topics. This proficiency in text generation naturally extends to the potential for\nthese models to excel in tasks related to textual analysis. The underlying hypothesis is\nsimple yet profound: if an LLM can generate coherent text, it should, by extension, pos-\nsess a refined ability to discern coherence – or the lack thereof – in existing texts.\n\n\fIn the field of computational linguistics, textual coherence is defined by the logical\nand orderly sequence in which ideas are presented within a text, ensuring that information\nand arguments are conveyed in a comprehensible and fluid manner [Seno and Rino 2005].\nThis involves not only the superficial connection between sentences through discourse\nmarkers or transition words but also a deeper harmony in terms of theme, purpose, and\nshared knowledge between the author and the reader [Charolles 1978]. For NLP systems,\nassessing the coherence of a text implies understanding how its constituent parts – whether\nat the sentence, paragraph, or document level – come together to form a unified whole\nthat is logically consistent and aesthetically pleasing [Jurafsky and Martin 2024]. This\ndefinition highlights the complexity of the textual coherence analysis task, underscoring\nit as a significant challenge within the field.\n\nHistorically, coherence has been conceptualized through various theoretical\nframeworks. Rhetorical Structure Theory (RST) [Mann and Thompson 1987] posits that\ntext coherence is derived from the hierarchical organization of text units, while Centering\nTheory [Grosz et al. 1995] emphasizes the role of discourse entities and their continu-\nity across sentences. Over time, computational approaches to coherence have evolved\nfrom rule-based systems, relying on explicit coherence markers and structural patterns,\nto sophisticated machine learning algorithms that infer coherence implicitly from large\ndatasets [Jurafsky and Martin 2024]. The development of neural network-based models,\nparticularly those employing attention mechanisms such as BERT [Devlin et al. 2018],\nhas marked a significant advancement, enabling a deeper understanding of contextual re-\nlationships within texts.\n\nGiven this context, the primary objective of this study is to evaluate the capabilities\nof various LLMs in the analysis of textual coherence. Specifically, the study assesses\nhow the models GPT 3.5, GPT 4, GPT 4o, Claude 3 Opus, Claude 3.5 Sonnet, Claude\n3 Haiku, Gemini, LLaMA 2 13b, LLaMA 2 7b, and Bard perform in three key tasks:\nclassifying texts as (i) locally or (ii) globally coherent or incoherent, and (iii) identifying\nspecific incoherent segments within texts. By examining these aspects, the study aims\nto contribute to the ongoing dialogue on improving NLP technologies and advancing our\nunderstanding of how machines process and understand the subtleties of human language.\n\nThe remainder of this article is structured as follows: Section 2 reviews key the-\nories and models in textual coherence analysis. Section 3 presents the relevant literature,\nwhile Section 4 details the methodology, including the models evaluated and the metrics\nused. Section 5 discusses the results and their implications, and Section 6 concludes with\na summary of findings and suggestions for future research.\n\n2. Theoretical Background\n\nTextual cohesion and coherence are fundamental to discourse analysis and NLP, as they\nexplain how texts are structured and interpreted. Cohesion refers to the connections within\na text created through various linguistic relations, such as pronouns, conjunctions, and\nlexical ties, ensuring that the text is perceived as a unified whole rather than a random\ncollection of sentences [Halliday and Hasan 1976]. Coherence, on the other hand, is a\nmore abstract concept, referring to the logical and meaningful organization of ideas within\na text, allowing readers to follow the flow of information and understand the intended\nmessage [Van Dijk 1977].\n\n\fCohesion can be achieved through grammatical and lexical means. Grammatical\ncohesion includes the use of pronouns, ellipses, and conjunctions to link sentences, while\nlexical cohesion involves the repetition of words or the use of synonyms to maintain\nthe continuity of ideas. However, a text can be cohesive without being coherent if the\nsentences do not contribute to a meaningful whole [Koch and Travaglia 2003].\n\nCoherence can be examined at two levels:\n\nlocal and global. Local coherence\nrefers to the logical connections between adjacent sentences and paragraphs, ensuring\nthat each idea flows smoothly into the next. This is often achieved through cohesive\ndevices, such as pronouns and conjunctions, which help maintain continuity in meaning.\nGlobal coherence, on the other hand, concerns the overall structure and unity of the text,\nwhere all parts contribute to a consistent and meaningful whole [Charolles 1978]. Both\nlevels of coherence are essential for a text to be understood as a cohesive and logically\norganized entity.\n\nTheoretical frameworks like RST and Centering Theory have been foundational\nin the study of coherence. For instance, RST [Mann and Thompson 1987] analyzes\nthe hierarchical organization of text by examining the relationships between different\nsegments, which help to structure the text in a coherent manner. Centering Theory\n[Grosz et al. 1995] focuses on how discourse entities are managed across sentences, en-\nsuring that the reader can follow the progression of ideas smoothly. These models have\nsignificantly influenced NLP research, particularly in the analysis and generation of co-\nherent texts, offering insights into the mechanisms that make a text understandable and\nlogically connected [Jurafsky and Martin 2024].\n\n3. Related Work\n\nAs a central area of investigation in NLP, textual coherence, particularly in the con-\ntext of local coherence, which focuses on the logical and sequential flow between\nadjacent sentences or paragraphs, has been extensively studied through models like\nthe entity grid.\nIntroduced by [Lapata and Barzilay 2005] and further developed by\n[Barzilay and Lapata 2008], the entity grid model abstracts a text into a grid that cap-\ntures the distribution and transitions of discourse entities across sentences. By analyzing\nthese patterns, the model can effectively infer the level of local coherence within a text.\nThis approach has been widely adopted and has inspired numerous subsequent studies.\nFor instance, [Elsner et al. 2007] enhanced coherence assessment by integrating the en-\ntity grid with a content model, while [Lin et al. 2011] refined the method by incorporating\ndiscourse relations, further advancing the field’s understanding of how sentences connect\nand maintain coherence.\n\nThe shuffle test, introduced by [Barzilay and Lapata 2008], has become a stan-\ndard method for evaluating local coherence models. This test involves comparing the\ncoherence of a text in its original order versus a shuffled version, challenging models to\nrecognize the coherent sequence. Studies like those by [Lin et al. 2011] and [Dias 2016]\nhave used this test to validate the effectiveness of their models, highlighting its importance\nas a benchmark in coherence evaluation.\n\nIn contrast, global coherence, which concerns the overall unity and thematic con-\nsistency of a text, has received less attention but remains a key aspect for understanding\nhow texts function as a whole. Early work by [Thompson 1986] emphasized the role of\n\n\fglobal coherence in enhancing readability and comprehension, arguing that a coherent\ntext allows readers to follow the central theme or argument effortlessly. More recent con-\ntributions by [Sagi 2010] have explored the hierarchical structure of texts, demonstrating\nhow well-organized discourse contributes to global coherence.\n\nRecent advancements in NLP have introduced BERT and LLMs like GPT-3,\nwhich have significantly expanded the possibilities for coherence analysis. These models,\ntrained on extensive datasets, exhibit a remarkable ability to capture both local and global\ncoherence, leading to more refined and human-like assessments of textual structure. For\nexample, [Braz Junior and Fileto 2021] applied BERT, specifically BERTimbau, in edu-\ncational forums to measure coherence. By analyzing sentence embeddings, the model\neffectively assessed sentence order, accurately distinguishing between coherent and per-\nmuted texts, thus demonstrating its capability to capture nuanced textual relationships.\nSimilarly, [Naismith et al. 2023] utilized GPT-4 for coherence assessment in educational\ncontexts, where the model not only rated coherence but also provided explanatory ra-\ntionales that closely aligned with human evaluations. This study demonstrated GPT-4’s\neffectiveness in replicating human judgments and even surpassing traditional NLP metrics\nby offering rationale-supported evaluations, thereby highlighting its potential to enhance\nautomated discourse coherence assessment and its applications in educational settings.\n\n4. Methodology\n\nThe primary aim of this study is to analyze and compare the performance of various LLMs\nin evaluating textual coherence across different aspects using two distinct approaches: (i)\nthrough LLMs APIs and (ii) through LLMs chat interfaces To achieve this, we selected\na diverse set of corpora for their relevance and variety in text types, which allows for a\nthorough assessment of the models’ capabilities across different linguistic contexts. These\ndatasets were preprocessed and, annotated as necessary to ensure consistency across the\ntasks.\n\nOne of the four corpora utilized in this study is the Corpus of Contemporary Amer-\nican English (COCA) [Davies 2008], which offers a balanced compilation of over one bil-\nlion words across genres such as spoken language, fiction, academic texts, and web pages.\nFor our analysis, we focused on the free portions of blog and academic sections of COCA,\nwhich together comprise a diverse range of coherence levels. The blog section, with 991\ntexts, is characterized by its informal and subjective nature, often exhibiting lower co-\nherence, while the academic section, consisting of 256 texts, is known for its structured\nand precise language, typically demonstrating higher coherence. These sections were\nemployed in both local and global coherence tasks, with specific subsets annotated for\ndetailed global coherence analysis and incoherence identification.\n\nThe study also incorporates the CST News Corpus [Aleixo and Pardo 2008],\nwhich consists of 50 collections of Brazilian Portuguese news articles, each centered on\na specific event or topic. Originally developed to support research on multi-document\nsummarization, the corpus includes approximately 150 news articles and 300 human-\ngenerated summaries from various newspapers, such as Folha de S˜ao Paulo, Estad˜ao, and\nO Globo. This diversity in sources makes the corpus particularly well-suited for coher-\nence studies, as it allows for evaluating both local and global coherence in a multilingual\ncontext. The CST News Corpus was especially valuable for assessing model performance\n\n\fin Brazilian Portuguese, adding a multilingual dimension to our evaluations.\n\nAnother key corpus in this research is the Grammarly Corpus of Discourse Coher-\nence (GCDC) [Lai and Tetreault 2018], which contains 4,800 texts from four real-world\nsources: Yahoo Answers, Clinton Emails, Enron Emails, and Yelp Reviews. Due to its\ncontext-dependent structure, the Yahoo Answers portion (1,200 texts) was excluded from\nour study. The Clinton Emails provide a mix of professional and personal correspon-\ndence, Enron Emails focus on formal business communication, and Yelp Reviews feature\nuser-generated feedback on businesses. Each text is annotated for global coherence on\na 3-point scale (low, medium, high), with 8,000 ratings from both expert and non-expert\nannotators via Amazon Mechanical Turk. These pre-existing annotations were used for\ncomparing the models’ performance against human judgments, enhancing our evaluation\nof global coherence tasks.\n\nLastly, the DDisCo corpus [Mikkelsen et al. 2022] was developed to fill a gap in\nresources for studying discourse coherence in Danish. It comprises 1,002 texts from two\nmain sources: Reddit and Danish Wikipedia. The Reddit texts, totaling 501, consist of\ninformal user-generated content, while the 501 Danish Wikipedia texts offer more formal,\nstructured information. Each text is annotated for global coherence on a 3-point scale\n(low, medium, high) by linguistics experts. This corpus introduces linguistic diversity into\nour research, allowing us to evaluate model performance in another non-English context.\nIt was particularly useful for assessing how well the models generalize across different\nlanguages and discourse structures.\n\n4.1. Local Coherence Analysis\n\nThe local coherence analysis in this study employed the shuffle test, which evaluates text\ncoherence by comparing the original order of sentences within each text to randomly\nshuffled version. This test was applied to texts from four corpora: COCA, CST News,\nGCDC, and DDisCo. A total of 2,318 texts were selected, comprising 991 blog texts\nand 256 academic texts from COCA, 251 news articles from CST News, 842 texts from\nGCDC, and 991 texts from DDisCo. Each text was segmented into sentences, and those\ncontaining fewer than four sentences were excluded as they would not allow for the 20\nrequired permutations. The remaining texts were shuffled 20 times, generating 46,360\nincoherent versions, resulting in a dataset of 48,678 texts for analysis.\n\nThe models’ performance was evaluated using two distinct methods: (i) via LLMs\nAPIs and (ii) through LLMs chat interfaces.\nIn the API-based evaluation, texts were\nprocessed directly through automated API calls, streamlining the evaluation process. In\ncontrast, the chat interface evaluation simulated real-world usage by submitting the texts\nthrough interactive prompts.\n\nFor both approaches, the models were provided with a standardized prompt\nfor Local Coherence Analysis1 to guide them in distinguishing between coherent and\nincoherent texts. Performance was measured using accuracy, precision, recall, and\nF1-score, comparing the models’ classifications against the original text labels.\n\n1https://github.com/bryankhelven/coherence-findings\n\n\f4.2. Global Coherence Analysis\nThe global coherence analysis in this study aimed to evaluate the ability of various LLMs\nto assess the overall logical consistency and thematic organization of texts across a to-\ntal of 2,142 texts. This analysis included 1,200 texts from the DDisCo Corpus and 842\ntexts from the GCDC Corpus, both of which already contained human annotations. Ad-\nditionally, a new annotation phase was conducted for a subset of 100 texts from the\nCOCA and CST News corpora, as these lacked pre-existing coherence labels. Three lan-\nguages/linguistics experts evaluated this subset, which consisted of 10 academic texts and\n60 blog texts from COCA, along with 30 news articles from CST News. For consistency,\neach text in this study was assigned a coherence score on a Likert scale ranging from low\nto high coherence (1 to 3), using the same scale previously adopted for assigning scores\nby the works of [Lai and Tetreault 2018] and [Mikkelsen et al. 2022]. This ensured that\nthe evaluation of global coherence was standardized across the various corpora used in\nthis analysis.\n\nFollowing the annotation process, the study assessed the models’ performance in\nglobal coherence tasks using two methods: (i) LLMs APIs for an automated process, and\n(ii) LLMs chat interfaces to simulate real-world, user-driven interactions. A total of 2,142\ntexts were used for this analysis, comprising the 100 manually annotated texts, 1,200 texts\nfrom the DDisCo corpus, and 842 texts from the GCDC corpus. Both methods utilized\na standardized prompt for Global Coherence Analysis2 to guide the models in assessing\nglobal coherence. The evaluation metrics were consistent with those used in the local\ncoherence analysis, but in this case, the models’ classifications were compared directly to\nthe original human annotations (scores of 1, 2, or 3).\n\n4.3. Incoherence Identification\nThe incoherence identification task evaluated the ability of various LLMs to detect seg-\nments within texts that disrupt logical flow. We used 130 texts for this task, includ-\ning 100 texts previously annotated for global coherence (10 academic texts and 60 blog\ntexts from COCA, 30 news articles from CST News) and an additional 30 texts from\nthe GCDC corpus (10 each from Yelp, Clinton, and Enron). The same three annotators\nfrom the global coherence task identified incoherent segments, focusing on the categories\nof Incorrect Use of Logical Connectors, Unnecessary Repetition, Irrelevant Information,\nContradictions, Sequence of Events, and Inconsistent Verb Tenses. Fleiss’ Kappa, which\nscored 0.8326 and indicated excellent agreement, was chosen for its capacity to account\nfor chance agreement among multiple annotators across various incoherence types. Each\nannotated segment was treated as a unit, ensuring robust reliability.\n\nThe annotators, familiar with each other, communicated freely to resolve difficul-\nties, following a shared understanding of coherence from [Koch and Travaglia 2003]. The\nmodels’ performance was evaluated using the same two methods as before – LLMs APIs\nand chat interfaces. However, in this task, each model was treated as an additional anno-\ntator. The agreement between model-generated annotations and human annotations was\nmeasured using Fleiss’ Kappa to determine how closely the models aligned with human\njudgment. The prompt for incoherence identification3 was also standardized and used\nacross all models in this task.\n\n2Available on GitHub (see first footnote).\n3Ibid.\n\n\f5. Results and Discussion\n\nThe results obtained during the execution of the analysis are summarized in Tables 1,\n2, and 3, highlighting the performance of various models in both API and chat-based\ninteractions.\n\nTable 1. Performance Metrics for Local Coherence Classification\n\nAPI\n\nChat\n\nModel\nBard\nClaude 3 Haiku\nClaude 3 Opus\nClaude 3.5 Sonnet\nGemini\nGPT 3.5\nGPT 4\nGPT 4o\nLLaMA 2 13b\nLLaMA 2 7b\n\nAcc\n0.756\n0.914\n0.979\n0.973\n0.978\n0.918\n0.970\n0.982\n0.831\n0.817\n\nPr\n0.755\n0.906\n0.991\n0.986\n0.989\n0.908\n0.982\n0.990\n0.825\n0.804\n\nRe\n0.740\n0.898\n0.983\n0.981\n0.980\n0.901\n0.980\n0.988\n0.816\n0.797\n\nF1\n0.748\n0.902\n0.987\n0.983\n0.985\n0.905\n0.981\n0.989\n0.820\n0.800\n\nAcc\n0.739\n0.949\n0.974\n0.972\n0.971\n0.962\n0.969\n0.977\n0.888\n0.805\n\nPr\n0.742\n0.902\n0.971\n0.969\n0.971\n0.905\n0.966\n0.975\n0.821\n0.801\n\nRe\n0.739\n0.899\n0.973\n0.968\n0.970\n0.902\n0.965\n0.973\n0.818\n0.798\n\nF1\n0.740\n0.900\n0.972\n0.968\n0.970\n0.903\n0.965\n0.974\n0.819\n0.799\n\nTable 2. Performance Metrics for Global Coherence Classification\n\nAPI\n\nChat\n\nModel\nClaude 3 Haiku\nClaude 3 Opus\nClaude 3.5 Sonnet\nGemini\nGPT 3.5\nGPT 4\nGPT 4o\nLLaMA 2 13b\nLLaMA 2 7b\n\nAcc\n0.959\n0.982\n0.980\n0.976\n0.960\n0.974\n0.978\n0.970\n0.968\n\nPr\n0.918\n0.986\n0.984\n0.963\n0.920\n0.961\n0.965\n0.930\n0.928\n\nRe\n0.921\n0.987\n0.982\n0.966\n0.923\n0.964\n0.968\n0.933\n0.931\n\nF1\n0.920\n0.986\n0.983\n0.965\n0.921\n0.963\n0.967\n0.932\n0.930\n\nAcc\n0.911\n0.933\n0.930\n0.928\n0.912\n0.926\n0.930\n0.922\n0.920\n\nPr\n0.871\n0.936\n0.934\n0.915\n0.873\n0.914\n0.918\n0.887\n0.881\n\nRe\n0.875\n0.939\n0.931\n0.918\n0.879\n0.919\n0.920\n0.883\n0.884\n\nF1\n0.875\n0.937\n0.932\n0.916\n0.877\n0.917\n0.919\n0.888\n0.883\n\nTable 3. Fleiss’ Kappa for Incoherence Identification\n\nModel\nAnnotators only (baseline)\nClaude 3 Haiku\nClaude 3 Opus\nClaude 3.5 Sonnet\nGemini\nGPT 3.5\nGPT 4\nGPT 4o\nLLaMA 2 13b\nLLaMA 2 7b\n\nAPI\n0.8326\n0.7995\n0.8166\n0.8279\n0.8119\n0.8038\n0.8152\n0.8316\n0.6787\n0.5823\n\nChat\n0.8326\n0.7653\n0.7987\n0.8082\n0.7858\n0.7716\n0.8093\n0.8234\n0.6492\n0.5418\n\nTable 1 shows the performance metrics for Local Coherence Classification, with\nGPT 4o achieving the highest scores in both API and chat interactions. Claude 3 Opus and\nClaude 3.5 Sonnet also performed well, especially in the API interaction, which demon-\nstrates their effectiveness in accurately identifying coherent texts. In contrast, LLaMA 2\n13b and LLaMA 2 7b had similar lower performance on both scenarios, suggesting lim-\nitations in processing and classifying local coherence. Similarty, for Global Coherence\nClassification, GPT 4o and Claude 3 Opus stood out with the highest performance in both\ninteraction modes, while Claude 3 Haiku had the lowest as shown in Table 2.\n\nThe results for the Incoherence Identification task are summarized in Table 3,\nwhere GPT 4o again demonstrated the highest agreement with human annotators, with\na Fleiss’ Kappa of 0.8316 in API interaction and 0.8234 in chat. Claude 3.5 Sonnet\nfollowed closely, with Kappa values of 0.8279 in API and 0.8082 in chat, while LLaMA\n\n\fmodels, particularly LLaMA 2 7b, showed significantly lower Kappa values, indicating\nthat these models struggle more with identifying incoherent segments.\n\nThe difference in performance between API and chat interactions is notable, with\nall models generally performing better in the API-based tests across all scenarios. This\nmay indicate that API interactions allow for more precise and structured processing, lead-\ning to higher accuracy and consistency.\n\n6. Conclusions and Future Work\n\nThis study assessed the performance of LLMs in evaluating textual coherence at both lo-\ncal and global levels and identifying incoherences within various corpora. Models such as\nGPT 4o and Claude 3 consistently outperformed others, particularly in API-based evalu-\nations, where they achieved high accuracy and reliability. In local coherence tasks, GPT\n4o demonstrated an F1 score of 0.989 in API-based tests, while in global coherence tasks,\nClaude 3 Opus led with an F1 score of 0.986. However, chat-based interactions revealed\na performance decline, with GPT 4o’s F1 score dropping to 0.974 in local coherence and\nClaude 3 Opus to 0.937 in global coherence. This suggests that the mode of interaction\nimpacts model effectiveness, with API-based methods being more stable.\n\nDespite the strong performance of top models, the Incoherence Identification task\nproved challenging across the board. GPT 4o showed the highest agreement with hu-\nman annotators (Fleiss’ Kappa of 0.8316), but all models exhibited lower performance\nin chat-based settings. These findings underscore the need for improvement in this area,\nespecially as lower-tier models like LLaMA 2 struggled significantly, with Fleiss’ Kappa\ndropping as low as 0.5418 in chat-based evaluations.\n\nThese findings have practical implications for NLP as models like GPT 4o and\nClaude 3 can be integrated into proofreading tools, content generators, and educational\nsoftware to improve textual coherence. Their ability to assess and enhance coherence\nbenefits machine-generated content and helps users create cohesive texts. Recognizing\nthe impact of interaction modes on performance guides developers in choosing effective\ndeployment strategies, favoring API integrations for consistency and accuracy.\n\nThe study acknowledges threats to validity, particularly the risk that some of the\nevaluation corpora may have been part of the training data for the LLMs, potentially\ninflating performance. This overlap introduces biases that could compromise objectivity,\nas models may recall patterns from training instead of genuinely evaluating coherence.\nThe assumption of coherence in original texts and the limited size and diversity of the\nannotated datasets also pose risks to the generalizability of the findings.\n\nFuture work should address these limitations by expanding the range of evaluated\ntext types and incorporating larger, more diverse annotator groups, as well as utilizing new\nand manually collected corpus to ensure that the models have not had prior access to it.\nAdditionally, exploring fine-tuning techniques and evaluating newer model architectures\nwill be essential. The development of improved evaluation metrics and the exploration of\ncross-linguistic and multimodal coherence analysis are also recommended to enhance the\nrobustness and applicability of LLMs in complex language tasks.\n\n\fReferences\nAleixo, P. and Pardo, T. A. S. (2008). Cstnews: Um c´orpus de textos jornal´ısticos anota-\ndos segundo a teoria discursiva multidocumento cst (cross-document structure theory).\nTechnical Report 326, Instituto de Ciˆencias Matem´aticas e de Computac¸ ˜ao, Universi-\ndade de S˜ao Paulo, S˜ao Carlos-SP. 12p.\n\nBarzilay, R. and Lapata, M. (2008). Modeling local coherence: An entity-based approach.\nIn Knight, K., Ng, H. T., and Oflazer, K., editors, Proceedings of the 43rd Annual\nMeeting of the Association for Computational Linguistics (ACL’05), pages 141–148,\nAnn Arbor, Michigan. Association for Computational Linguistics.\n\nBraz Junior, G. and Fileto, R. (2021).\n\nInvestigating coherence in posts from a doubts\n\nforum in a virtual learning environment with bert. Conference Paper.\n\nCharolles, M. (1978).\n\nIntroduc¸ ˜ao aos problemas da coerˆencia dos textos: abordagem\n\nte´orica e estudo das pr´aticas pedag´ogicas. Editora Pontes.\n\nDavies, M. (2008). The corpus of contemporary american english (coca). Available online\n\nat https://www.english-corpora.org/coca/.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep\nIn Proceedings of the 2018\nbidirectional transformers for language understanding.\nConference of the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, volume 1, pages 4171–4186. Association for\nComputational Linguistics.\n\nDias, M. (2016).\n\nInvestigac¸ ˜ao de modelos de coerˆencia local para sum´arios multi-\n\ndocumento. PhD thesis, Universidade de S˜ao Paulo.\n\nElsner, M., Austerweil, J., and Charniak, E. (2007). A unified local and global model\nfor discourse coherence. In Sidner, C., Schultz, T., Stone, M., and Zhai, C., editors,\nHuman Language Technologies 2007: The Conference of the North American Chapter\nof the Association for Computational Linguistics; Proceedings of the Main Conference,\npages 436–443, Rochester, New York. Association for Computational Linguistics.\n\nFreitas, A. R. P. (2013). An´alise autom´atica de coerˆencia usando o modelo grade de\n\nentidades para o portuguˆes. PhD thesis.\n\nGrosz, B. J., Joshi, A. K., and Weinstein, S. (1995). Centering: A framework for modeling\n\nthe local coherence of discourse. Computational Linguistics, 21(2):203–225.\n\nHalliday, M. A. K. and Hasan, R. (1976). Cohesion in English. Longman.\n\nHoey, M. (2013). Textual interaction: An introduction to written discourse analysis.\n\nRoutledge.\n\nJurafsky, D. and Martin, J. H. (2024). Speech and Language Processing, chapter 23.\n\nDraft, 3 edition. Accessed: 2024-02-29.\n\nKoch, I. and Travaglia, L. (2003). A coerˆencia textual. Editora Contexto.\n\nLai, A. and Tetreault, J. (2018). Discourse coherence in the wild: A dataset evaluation\n\nand methods. In Proceedings of SIGdial, pages 214–223.\n\nLapata, M. and Barzilay, R. (2005). Automatic evaluation of text coherence: models and\nrepresentations. In Proceedings of the 19th International Joint Conference on Artificial\n\n\fIntelligence, IJCAI’05, page 1085–1090, San Francisco, CA, USA. Morgan Kaufmann\nPublishers Inc.\n\nLin, Z., Ng, H. T., and Kan, M.-Y. (2011). Automatically evaluating text coherence using\ndiscourse relations. In Lin, D., Matsumoto, Y., and Mihalcea, R., editors, Proceedings\nof the 49th Annual Meeting of the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 997–1006, Portland, Oregon, USA. Association\nfor Computational Linguistics.\n\nMann, W. C. and Thompson, S. A. (1987). Rhetorical structure theory: Description\nIn Natural Language Generation, pages 85–95.\n\nand construction of text structures.\nSpringer Netherlands.\n\nMikkelsen, L. F., Kinch, O., Pedersen, A. J., and Lacroix, O. (2022). Ddisco: A discourse\nIn Proceedings of the 13th Language Resources and\n\ncoherence dataset for danish.\nEvaluation Conference (LREC), pages 1234–1243.\n\nNaismith, B., Mulcaire, P., and Burstein, J. (2023). Automated evaluation of written dis-\ncourse coherence using gpt-4. In Proceedings of the 18th Workshop on Innovative Use\nof NLP for Building Educational Applications (BEA 2023), pages 394–403, Online.\nAssociation for Computational Linguistics.\n\nSagi, E. (2010). Discourse structure effects on the global coherence of texts.\n\nSeno, E. R. M. and Rino, L. H. M. (2005). Co-referential chaining for coherent summaries\nthrough rhetorical and linguistic modeling. In Proceedings of the Workshop on Cross-\ning Barriers in Text Summarization Research/RANLP, Borovets, Bulgaria. N´ucleo In-\nterinstitucional de Ling¨u´ıstica Computacional – NILC/USFCAR.\n\nThompson, I. (1986). Readability beyond the sentence: Global coherence and ease of\ncomprehension. Journal of Technical Writing and Communication, 16(1):131–140.\n\nVan Dijk, T. A. (1977). Text and context: Explorations in the semantics and pragmatics\n\nof discourse.\n\n\f"
        },
        {
            "titulo": "Anomaly Detection in Text Data: A Semi-Supervised Approach Applied to the Portuguese Domain",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31141",
            "idioma": "Inglês",
            "storage_key": "files/article_31141_30944.pdf",
            "autores": [
                {
                    "nome": "Fabio Masaracchia Maia",
                    "afiliacao": "USP",
                    "orcid": "http://orcid.org/0009-0008-3646-7338"
                },
                {
                    "nome": "Anna Helena Reali Costa",
                    "afiliacao": "USP",
                    "orcid": "https://orcid.org/0000-0001-7309-4528"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Anomaly detection, driven by advancements in machine learning and deep learning, has gained significant importance across various fields. However, its application to unstructured textual data, particularly in Portuguese, remains underexplored. In textual analysis, these techniques are crucial for detecting deviations within text collections. This paper investigates state-of-the-art methods for anomaly detection in Portuguese text corpora and introduces a new, flexible loss function designed to enhance detection across different contamination levels. By evaluating these methods on benchmark datasets, specifically in the contexts of hate speech detection and sentiment analysis, we address existing challenges and contribute to the development of more effective anomaly detection techniques for Portuguese text data.",
            "keywords": [
                "Anomaly detection",
                "Textual anomaly",
                "Transformers",
                "Pre-trained models",
                "Natural Language Processing"
            ],
            "referencias": [
                "Edgeworth, F. Y. (1887). Xli. on discordant observations. Philosophical Magazine Series 1, 23:364–375.",
                "Leite, J. A., Silva, D., Bontcheva, K., and Scarton, C. (2020). Toxic language detection in social media for Brazilian Portuguese: New dataset and multilingual analysis. In Wong, K.-F., Knight, K., and Wu, H., editors, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 914–924, Suzhou, China. Association for Computational Linguistics.",
                "Reimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China. Association for Computational Linguistics.",
                "Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S. A., Binder, A., Müller, E., and Kloft, M. (2018). Deep one-class classification. In Dy, J. and Krause, A., editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4393–4402. PMLR.",
                "Ruff, L., Vandermeulen, R. A., Görnitz, N., Binder, A., Müller, E., Müller, K.-R., and Kloft, M. (2020). Deep semi-supervised anomaly detection. In International Conference on Learning Representations.",
                "Sousa, R. F. d., Brum, H. B., and Nunes, M. d. G. V. (2019). A bunch of helpfulness and sentiment corpora in brazilian portuguese. In Symposium in Information and Human Language Technology - STIL. SBC.",
                "Souza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained bert models for brazilian portuguese. In Cerri, R. and Prati, R. C., editors, Intelligent Systems, pages 403–417, Cham. Springer International Publishing.",
                "Xu, H., Pang, G., Wang, Y., and Wang, Y. (2023a). Deep isolation forest for anomaly detection. IEEE Transactions on Knowledge and Data Engineering, 35(12):12591–12604."
            ],
            "artigo_completo": "Anomaly Detection in Text Data: A Semi-Supervised\nApproach Applied to the Portuguese Domain\n\nFabio Masaracchia Maia1, Anna Helena Reali Costa1\n\n1Escola Polit´ecnica, Universidade de S˜ao Paulo, S˜ao Paulo, Brazil\n\nfabio.masaracchia@gmail.com, anna.reali@usp.br\n\nAbstract. Anomaly detection, driven by advancements in machine learning and\ndeep learning, has gained significant importance across various fields. How-\never, its application to unstructured textual data, particularly in Portuguese,\nremains underexplored. In textual analysis, these techniques are crucial for de-\ntecting deviations within text collections. This paper investigates state-of-the-art\nmethods for anomaly detection in Portuguese text corpora and introduces a new,\nflexible loss function designed to enhance detection across different contamina-\ntion levels. By evaluating these methods on benchmark datasets, specifically\nin the contexts of hate speech detection and sentiment analysis, we address ex-\nisting challenges and contribute to the development of more effective anomaly\ndetection techniques for Portuguese text data.\n\n1. Introduction\n\nAnomaly detection refers to the identification of patterns in data that deviate from ex-\npected norms [Chandola et al. 2009]. Anomalies, often termed outliers or exceptions,\nare distinct from the majority of observations that define the “normal” pattern. While\nanomaly detection techniques have been extensively applied to structured data, such as\ncontinuous and categorical variables [Boutalbi et al. 2023], less attention has been given\nto unstructured data like text — the focus of this work.\n\nAnomaly detection began with statistical methods in the late 19th century\n[Edgeworth 1887] and has since expanded, with deep learning broadening its scope to\nunstructured domains like images and text [Chandola et al. 2009, Pimentel et al. 2014].\nHowever, its application to textual anomaly detection remains limited [Pang et al. 2019].\nDetecting anomalies in text data is particularly challenging due to the variety of linguistic\nlevels involved, such as spelling, syntax, and semantics [Xu et al. 2023b]. Leveraging\ndeep learning’s ability to model complex patterns has led to significant advancements in\nthe field.\n\nTraditionally treated as an unsupervised task due to the absence of ground\ntruth labels, anomaly detection has employed techniques like autoencoders and\nGANs [Pang et al. 2019].\nPopular approaches in this domain include DeepSVDD\n[Ruff et al. 2018] and Deep Isolation Forest [Xu et al. 2023a], both focusing on modeling\nnormal data and identifying deviations as anomalies. However, recent semi-supervised\nneural approaches, such as DevNet [Pang et al. 2019] and DeepSAD [Ruff et al. 2020],\nhave shown improved detection accuracy by integrating limited labeled anomalies into the\ntraining process [Xu et al. 2023b], bridging the gap between unsupervised and supervised\n\n\flearning. Despite recent advancements, there remains a significant gap in comprehensive\nresearch focused on anomaly detection in Portuguese text corpora.\n\nIn this paper, we extend neural network-based anomaly detection techniques to\nhandle these complexities in Portuguese text data. Furthermore, we propose a change\nin the loss function in order to establish a compromise between the samples that corre-\nspond to anomalies in relation to the others. Experiments show that this approach is quite\npromising. To effectively address the unique challenges of representing textual data, we\nemploy two pre-trained BERT-based models, checking the strengths and weaknesses of\neach representation in the different tasks.\n\n2. Methodology\n\n2.1. Problem Definition\n\nGiven a dataset X = {x1, x2, . . . , xN +K}, where U = {x1, x2, . . . , xN } is unlabeled data\nand K = {xN +1, . . . , xN +K} represents labeled anomalies (K ≪ N ), the goal is to train\na model to identify these rare anomalies. This task is challenging due to the imbalance\nbetween the large unlabeled set and the small labeled anomaly set. The process involves\ntwo key steps:\n\n1. Embedding Transformation: Data X is transformed into embeddings Z =\n\n{z1, z2, . . . , zN +K}, with each zi being a vector in Rd.\n\n2. Scoring Function: A neural network learns a scoring function ϕ : Z → R to\nensure that ϕ(zi) > ϕ(zj) when zi is an anomaly and zj is normal, minimizing the\nuse of labeled examples.\n\nWe adopted the DevNet model due to its demonstrated good performance obtained\nin studies considering textual domain [Xu et al. 2023b] along with its ability to effec-\ntively manage high-dimensional spaces, such as embeddings. Additionally, the model’s\ninterpretable loss function, based on a straightforward Z-score strategy, provides valuable\ninsights that can be later used to assess text identified as anomalies.\n\n2.2. DevNet for Anomalous Text\n\nThe DevNet algorithm [Pang et al. 2019] introduces a semi-supervised approach that\nlearns an interpretable outlier scoring function, ϕ(z; Θ), using a Z-score deviation loss.\nWhile the original formulation is based on raw data points x, we denote the embeddings\nas z to reflect the transformed data representations. This approach assumes a prior normal\ndistribution over anomaly scores, modeled with l random objects ri ∈ R sampled from a\nstandard normal distribution N (µR, σR):\n\ndev(z) =\n\nϕ(z; Θ) − µR\nσR\n\n,\n\n(1)\n\nwhere µR and σR are the mean and standard deviation of anomaly scores within the\nreference distribution. This deviation is then incorporated into a contrastive loss function\nto enhance the distinction between anomalous and normal samples, where y indicates\nanomaly status, and a ensures a minimum separation between classes [Pang et al. 2019].\n\nL(ϕ(z; Θ), µR, σR) = (1 − y)|dev(z)| + y · max(0, a − dev(z)),\n\n(2)\n\n\f2.3. Proposal\n\nTo provide flexibility, the parameter η ∈ [0, 1] is introduced in the DevNet loss function\ngiven in Eq. 2, controlling the balance between regular and anomalous samples,\n\nL(ϕ(x; Θ), µR, σR) = (1 − η)(1 − y)|dev(x)| + η · y · max(0, a − dev(x)),\n\n(3)\n\nThe η parameter adjusts the model’s emphasis on anomalies, allowing adaptation\nto varying levels of contamination (i.e., percentage of labeled anomalies) and data avail-\nability. We investigate different proportions of labeled anomalies to assess the robustness\nof the solution in various scenarios, aiming to determine the minimum amount of la-\nbeled data needed for good performance. Additionally, we employ two distinct text repre-\nsentation strategies: the monolingual BERTimbau model [Souza et al. 2020], specifically\ndesigned for processing Portuguese text, and the multilingual Sentence-BERT (SBERT)\n[Reimers and Gurevych 2019], which generates sentence-level embeddings across mul-\ntiple languages, including Portuguese. Our customized DevNet implementation, named\nη-DevNet, was evaluated against its original version using both representation strategies.\n\n3. Experiments and Results\n\n3.1. Experiments\n\nTo evaluate the performance of different representation methods and loss functions, we\nfirst tested η values ranging from 0.5 to 1 using the BERTimbau embedding strategy,\nwhere η = 0.5 corresponds to the original DevNet formulation. The other parameters\nwere adopted from the DevNet reference: a = 5, l = 5000, µR = 0, and σR = 1.\nContamination levels were adjusted by introducing between 5 and 1000 anomalies across\nthe experiments, with anomalies randomly selected. After identifying the optimal η value,\nwe applied it in subsequent experiments to compare both embedding strategies across\ndifferent datasets. The mean ROC-AUC values were calculated over 10 experimental\nruns for each scenario.\n\n3.2. Dataset\n\nWe evaluate our approach using two Brazilian datasets.\nThe first, Told-Br\n[Leite et al. 2020], contains 21,000 labeled instances of tweets tagged with hate speech,\ncategorized into themes such as homophobia, racism, and misogyny, with hate speech\nserving as the anomaly class. The second dataset, UTLC-Movies [Sousa et al. 2019],\ncomprises over one million movie reviews. From this dataset, we sampled 40,000 re-\nviews for sentiment analysis, where negative sentiment is treated as the anomaly class.\n\n3.3. Results\n\nFigure 1, shows performance and stability improvements as η is adjusted, with η = 0.7\nyielding optimal performance. This value was subsequently used for further analysis. The\nresults shown in Table 1 outline these results for both tasks, demonstrating that in most\ncases, the adapted loss function led to performance improvements. Reaching a reasonable\nlevel of accuracy requires a minimum threshold of labeled examples, which varies with\ntask complexity. In our experiments, sentiment analysis needed only 0.87% of labeled\nanomalies to achieve a ROC-AUC of 0.85, while hate speech detection required 2.59% to\n\n\fFigure 1. η Comparison across different η values with varying amounts of labeled anomalies using\nBERTimbau embedding strategy.\n\nreach a ROC-AUC of 0.73. This discrepancy likely arises from the greater complexity of\nhate speech detection, which involves subtle linguistic nuances and diverse expressions.\nAdditionally, pre-trained models may not fully capture slang and politically specific con-\ntexts, which are common in hate speech but may be underrepresented during training.\n\nTable 1. Comparison of ROC-AUC values across different scenarios and contamination levels for\nUTLC-Movies and Told-BR datasets when η = 0.7, where % refers to the contamination level.\n\nUTLC-Movies\n\nTold-BR\n\nNb. Outliers\n\n5\n10\n25\n50\n100\n250\n500\n1000\n\n%\n\n0.02\n0.04\n0.09\n0.18\n0.35\n0.87\n1.73\n3.41\n\nBη\n\n0.58\n0.62\n0.66\n0.70\n0.78\n0.82\n0.83\n0.85\n\nBD\n\n0.57\n0.58\n0.65\n0.69\n0.71\n0.76\n0.83\n0.84\n\nMη\n\n0.52\n0.54\n0.60\n0.64\n0.71\n0.75\n0.81\n0.81\n\nMD\n\n0.52\n0.57\n0.68\n0.66\n0.70\n0.69\n0.79\n0.75\n\n%\n\n0.05\n0.11\n0.27\n0.53\n1.05\n2.59\n5.05\n9.62\n\nBη\n\n0.46\n0.48\n0.57\n0.58\n0.66\n0.73\n0.75\n0.76\n\nBD\n\n0.48\n0.50\n0.57\n0.63\n0.63\n0.68\n0.73\n0.76\n\nMη\n\n0.50\n0.53\n0.54\n0.56\n0.61\n0.56\n0.50\n0.50\n\nMD\n\n0.50\n0.53\n0.60\n0.60\n0.60\n0.52\n0.51\n0.52\n\nAcronyms: BERTimbau η-loss (Bη), BERTimbau Devnet loss (BD), multilingual SBERT η-loss (Mη), multilingual SBERT Devnet loss (MD).\n\nOur results show that the BERTimbau representation [Souza et al. 2020] consis-\ntently outperformed the multilingual model across tasks. This advantage can be traced to\nBERTimbau’s specialization in Portuguese, allowing it to capture more intricate linguistic\nnuances, such as idiomatic expressions and regional variations.\n\n4. Conclusion and Future Work\n\nThis study shows that BERTimbau, tailored for Portuguese, consistently outperforms mul-\ntilingual models in anomaly detection, with the customized loss function providing no-\ntable improvements. These results highlight the potential of semi-supervised methods for\ntasks like harmful content detection and sentiment analysis in Portuguese contexts with\nlimited labeled data.\n\nFuture work may expand this approach to related tasks such as topic modeling,\nfake news detection, and fraud detection. Although some labeling effort is still required\nfor good performance, the small amount of labeled data needed makes this approach\nfeasible in resource-constrained scenarios. Furthermore, the promising advances in Large\nLanguage Models (LLMs) could not only serve as valuable tools for benchmarking but\nalso automate anomaly tagging, reducing manual effort and enhancing adaptability and\nscalability across various real-world applications.\n\n\fReferences\n\nBoutalbi, K., Loukil, F., Verjus, H., Telisson, D., and Salamatian, K. (2023). Machine\nlearning for text anomaly detection: A systematic review. In 2023 IEEE 47th Annual\nComputers, Software, and Applications Conference (COMPSAC), pages 1319–1324.\n\nChandola, V., Banerjee, A., and Kumar, V. (2009). Anomaly detection: A survey. ACM\n\nComputing Surveys, 41(3):71–97.\n\nEdgeworth, F. Y. (1887). Xli. on discordant observations. Philosophical Magazine Series\n\n1, 23:364–375.\n\nLeite, J. A., Silva, D., Bontcheva, K., and Scarton, C. (2020). Toxic language detec-\ntion in social media for Brazilian Portuguese: New dataset and multilingual analysis.\nIn Wong, K.-F., Knight, K., and Wu, H., editors, Proceedings of the 1st Conference\nof the Asia-Pacific Chapter of the Association for Computational Linguistics and the\n10th International Joint Conference on Natural Language Processing, pages 914–924,\nSuzhou, China. Association for Computational Linguistics.\n\nPang, G., Shen, C., and van den Hengel, A. (2019). Deep anomaly detection with devia-\ntion networks. In Proceedings of the 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, KDD ’19, page 353–362, New York, NY, USA.\nAssociation for Computing Machinery.\n\nPimentel, M. A., Clifton, D. A., Clifton, L., and Tarassenko, L. (2014). A review of\n\nnovelty detection. Signal Processing, 99:215–249.\n\nReimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using\nIn Inui, K., Jiang, J., Ng, V., and Wan, X., editors, Pro-\nSiamese BERT-networks.\nceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China. Association for Computa-\ntional Linguistics.\n\nRuff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S. A., Binder, A., M¨uller,\nE., and Kloft, M. (2018). Deep one-class classification.\nIn Dy, J. and Krause, A.,\neditors, Proceedings of the 35th International Conference on Machine Learning, vol-\nume 80 of Proceedings of Machine Learning Research, pages 4393–4402. PMLR.\n\nRuff, L., Vandermeulen, R. A., G¨ornitz, N., Binder, A., M¨uller, E., M¨uller, K.-R., and\nKloft, M. (2020). Deep semi-supervised anomaly detection. In International Confer-\nence on Learning Representations.\n\nSousa, R. F. d., Brum, H. B., and Nunes, M. d. G. V. (2019). A bunch of helpfulness and\nsentiment corpora in brazilian portuguese. In Symposium in Information and Human\nLanguage Technology - STIL. SBC.\n\nSouza, F., Nogueira, R., and Lotufo, R. (2020). Bertimbau: Pretrained bert models for\nbrazilian portuguese. In Cerri, R. and Prati, R. C., editors, Intelligent Systems, pages\n403–417, Cham. Springer International Publishing.\n\nXu, H., Pang, G., Wang, Y., and Wang, Y. (2023a). Deep isolation forest for anomaly\ndetection. IEEE Transactions on Knowledge and Data Engineering, 35(12):12591–\n12604.\n\n\fXu, Y., Gabor, K., Milleret, J., and Segond, F. (2023b). Comparative analysis of anomaly\n\ndetection algorithms in text data. pages 1234–1245.\n\n\f"
        },
        {
            "titulo": "Biases in GPT-3.5 Turbo model: a case study regarding gender and language",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31142",
            "idioma": "Inglês",
            "storage_key": "files/article_31142_30945.pdf",
            "autores": [
                {
                    "nome": "Fernanda Malheiros Assi",
                    "afiliacao": "UFSCar",
                    "orcid": "http://orcid.org/0000-0001-8820-964X"
                },
                {
                    "nome": "Helena de Medeiros Caseli",
                    "afiliacao": "UFSCar",
                    "orcid": "https://orcid.org/0000-0003-3996-8599"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Interactions with Generative Language Models like OpenAI’s GPT3.5 Turbo are increasingly common in everyday life, making it essential to examine their potential biases. This study assesses biases in the GPT-3.5 Turbo model using the regard metric, which evaluates the level of respect or esteem expressed towards different demographic groups. Specifically, we investigate how the model perceives regard towards different genders (male, female, and neutral) in both English and Portuguese. To achieve this, we isolated three variables (gender, language, and moderation filters) and analyzed their individual impacts on the model’s outputs. Our results indicate a slight positive bias towards feminine over masculine and neutral genders, a more favorable bias towards English compared to Portuguese, and consistently more negative outputs when we attempted to reduce the moderation filters.",
            "keywords": [
                "Natural Language Processing",
                "NLP",
                "Gender Bias",
                "GPT-3.5",
                "Generative Language Models",
                "Bias",
                "Regard Metric",
                "Moderation Filters",
                "Language Bias",
                "Large Language Models",
                "LLM"
            ],
            "referencias": [
                "Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., and Kalai, A. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings.",
                "Cambridge Dictionary (2024). Regard.",
                "Das, A., Selek, S., Warner, A. R., Zuo, X., Hu, Y., Kuttichi Keloth, V., Li, J., Zheng, W. J., and Xu, H. (2022). Conversational bots for psychotherapy: A study of generative transformer models using domain-specific dialogues. In Demner-Fushman, D., Cohen, K. B., Ananiadou, S., and Tsujii, J., editors, Proceedings of the 21st Workshop on Biomedical Language Processing, pages 285–297, Dublin, Ireland. Association for Computational Linguistics.",
                "Deshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., and Narasimhan, K. (2023). Toxicity in chatgpt: Analyzing persona-assigned language models. In Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1236–1270, Singapore. Association for Computational Linguistics.",
                "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T., editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. (2020). RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Cohn, T., He, Y., and Liu, Y., editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online. Association for Computational Linguistics.",
                "Gupta, S., Shrivastava, V., Deshpande, A., Kalyan, A., Clark, P., Sabharwal, A., and Khot, T. (2024). Bias runs deep: Implicit reasoning biases in persona-assigned llms.",
                "Kolomeets, M., Tushkanova, O., Desnitsky, V., Vitkova, L., and Chechulin, A. (2024). Experimental evaluation: Can humans recognise social media bots? Big Data and Cognitive Computing, 8(3).",
                "Liang, P. P., Wu, C., Morency, L.-P., and Salakhutdinov, R. (2021). Towards understanding and mitigating social biases in language models.",
                "Liu, Y., Zhang, W., Chen, Y., Zhang, Y., Bai, H., Feng, F., Cui, H., Li, Y., and Che, W. (2023). Conversational recommender system and large language model are made for each other in E-commerce pre-sales dialogue. In Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9587–9605, Singapore. Association for Computational Linguistics.",
                "Lucas, J., Uchendu, A., Yamashita, M., Lee, J., Rohatgi, S., and Lee, D. (2023). Fighting fire with fire: The dual role of LLMs in crafting and detecting elusive disinformation. In Bouamor, H., Pino, J., and Bali, K., editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14279–14305, Singapore. Association for Computational Linguistics.",
                "Nadeem, M., Bethke, A., and Reddy, S. (2021). StereoSet: Measuring stereotypical bias in pretrained language models. In Zong, C., Xia, F., Li, W., and Navigli, R., editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356–5371, Online. Association for Computational Linguistics.",
                "Nangia, N., Vania, C., Bhalerao, R., and Bowman, S. R. (2020). CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In Webber, B., Cohn, T., He, Y., and Liu, Y., editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1967, Online. Association for Computational Linguistics.",
                "Odbal, Zhang, G., and Ananiadou, S. (2022). Examining and mitigating gender bias in text emotion detection task. Neurocomputing, 493:422–434.",
                "OpenAI (2024). Gpt-3.5 turbo.",
                ".",
                "Orabi, M., Mouheb, D., Al Aghbari, Z., and Kamel, I. (2020). Detection of bots in social media: A systematic review. Information Processing Management, 57(4):102250.",
                "Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M., and Bowman, S. (2022). BBQ: A hand-built bias benchmark for question answering. In Muresan, S., Nakov, P., and Villavicencio, A., editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 2086–2105, Dublin, Ireland. Association for Computational Linguistics.",
                "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are unsupervised multitask learners.",
                "Rodrigues, G., Albuquerque, D., and Chagas, J. (2023). Análise de vieses ideológicos em produções textuais do assistente de bate-papo chatgpt. In Anais do IV Workshop sobre as Implicações da Computação na Sociedade, pages 148–155, Porto Alegre, RS, Brasil. SBC.",
                "Roy, K., Goyal, P., and Pandey, M. (2021). Attribute value generation from product title using language models. In Malmasi, S., Kallumadi, S., Ueffing, N., Rokhlenko, O., Agichtein, E., and Guy, I., editors, Proceedings of the 4th Workshop on e-Commerce and NLP, pages 13–17, Online. Association for Computational Linguistics.",
                "Rudinger, R., Naradowsky, J., Leonard, B., and Van Durme, B. (2018). Gender bias in coreference resolution. In Walker, M., Ji, H., and Stent, A., editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 8–14, New Orleans, Louisiana. Association for Computational Linguistics.",
                "Santana, B. S., Woloszyn, V., and Wives, L. K. (2018). Is there gender bias and stereotype in portuguese word embeddings?",
                "Sheng, E., Arnold, J., Yu, Z., Chang, K.-W., and Peng, N. (2021). Revealing persona biases in dialogue systems.",
                "Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. (2019). The woman worked as a babysitter: On biases in language generation. In Inui, K., Jiang, J., Ng, V., and Wan, X., editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3407–3412, Hong Kong, China. Association for Computational Linguistics.",
                "Shin, J., Song, H., Lee, H., Jeong, S., and Park, J. C. (2024). Ask llms directly, ””what shapes your bias?””: Measuring social bias in large language models.",
                "Stanovsky, G., Smith, N. A., and Zettlemoyer, L. (2019). Evaluating gender bias in machine translation. In Korhonen, A., Traum, D., and Màrquez, L., editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679–1684, Florence, Italy. Association for Computational Linguistics.",
                "Taso, F., Reis, V., and Martinez, F. (2023). Sexismo no brasil: análise de um word embedding por meio de testes baseados em associação implícita. In Anais do XIV Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana, pages 53–62, Porto Alegre, RS, Brasil. SBC.",
                "Wang, H., Wang, R., Mi, F., Deng, Y., Wang, Z., Liang, B., Xu, R., and Wong, K.-F. (2023). Cue-CoT: Chain-of-thought prompting for responding to in-depth dialogue questions with LLMs. In Bouamor, H., Pino, J., and Bali, K., editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12047–12064, Singapore. Association for Computational Linguistics.",
                "Zhang, Q., Naradowsky, J., and Miyao, Y. (2023). Ask an expert: Leveraging language models to improve strategic reasoning in goal-oriented dialogue models. In Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 6665–6694, Toronto, Canada. Association for Computational Linguistics.",
                "Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-W. (2018). Gender bias in coreference resolution: Evaluation and debiasing methods. In Walker, M., Ji, H., and Stent, A., editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15–20, New Orleans, Louisiana. Association for Computational Linguistics.",
                "Zhou, J., Liu, B., Acharya, J., Hong, Y., Lee, K.-C., and Wen, M. (2023). Leveraging large language models for enhanced product descriptions in eCommerce. In Gehrmann, S., Wang, A., Sedoc, J., Clark, E., Dhole, K., Chandu, K. R., Santus, E., and Sedghamiz, H., editors, Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages 88–96, Singapore. Association for Computational Linguistics."
            ],
            "artigo_completo": "Biases in GPT-3.5 Turbo model: a case study regarding gender\nand language\n\nFernanda Malheiros Assi1, Helena de Medeiros Caseli1\n\n1Computing Department – Federal University of S˜ao Carlos (UFSCar) – LALIC\nCaixa Postal 676 – 13565-905 – S˜ao Carlos – SP – Brazil\n\nfernanda.malheiros@estudante.ufscar.br, helenacaseli@ufscar.br\n\nAbstract. Interactions with Generative Language Models like OpenAI’s GPT-\n3.5 Turbo are increasingly common in everyday life, making it essential to ex-\namine their potential biases. This study assesses biases in the GPT-3.5 Turbo\nmodel using the regard metric, which evaluates the level of respect or esteem\nexpressed towards different demographic groups. Specifically, we investigate\nhow the model perceives regard towards different genders (male, female, and\nneutral) in both English and Portuguese. To achieve this, we isolated three\nvariables (gender, language, and moderation filters) and analyzed their individ-\nual impacts on the model’s outputs. Our results indicate a slight positive bias\ntowards feminine over masculine and neutral genders, a more favorable bias to-\nwards English compared to Portuguese, and consistently more negative outputs\nwhen we attempted to reduce the moderation filters.\n\n1. Introduction\n\nIn recent years, interactions with Generative Language Models (GLM) have become\na growing part of everyday life. Studies show that people are engaging with mod-\nels like OpenAI’s GPT [Radford et al. 2019] in a variety of ways, from using chat-\nbots for customer service and mental health support [Zhang et al. 2023, Das et al. 2022,\nto experiencing enhanced e-commerce through improved prod-\nWang et al. 2023],\nuct descriptions, attribute generation, and customer engagement [Zhou et al. 2023,\nRoy et al. 2021, Liu et al. 2023].\nOn social media, automated bot accounts are\nwidespread and are used to simulate human behavior, spread misinformation, pro-\nmote products, and engage with users [Orabi et al. 2020, Kolomeets et al. 2024,\nLucas et al. 2023].\n\nAs interactions between humans and GLMs become more frequent, it is increas-\ningly important to identify and mitigate the systemic biases these models may perpetuate.\nRecent studies have shown that language models frequently inherit, and replicate biases\nembedded in their training data [Sheng et al. 2019, Shin et al. 2024, Liang et al. 2021,\nGupta et al. 2024]. These biases reflect existing patterns of discrimination in society and\ncan reinforce harmful stereotypes and prejudices.\n\nBias in the context of language models refers to systematic differences in how\nthese models generate, evaluate or interpret text about different demographics (e.g., gen-\nder, race, sexual orientation) [Sheng et al. 2019]. A text can be said to exhibit bias if it\nportrays a demographic group in a way that causes people from this group to be perceived\nmore positively or negatively compared to others. Similarly, a model also exhibits bias\n\n\fif it consistently perceives a demographic group (such as men vs. women) more posi-\ntively or negatively than others. In this work, we specifically analyze bias in terms of the\nmodel’s perception of regard towards different genders.\n\nRegard, in this context, refers to the level of respect, esteem, or deference ex-\npressed towards an individual or group mentioned in the text. For example, a sentence\nlike “The woman is an excellent leader” conveys a positive regard towards the person\nmentioned, whereas “He is just lucky, not skilled” reflects a more negative regard. We\nused the regard metric to access potential biases in the GPT-3.5 Turbo model, specif-\nically looking at how it perceives individuals of different genders in both English and\nPortuguese.\n\nOur main goal was to determine if the model’s perception of regard differs across\ndifferent conditions and to identify any inherent biases. To achieve this, we isolated three\nvariables (gender, language, and firewall settings) to understand their individual impact\non the model’s output. We hypothesized that the regard for non-prototypical genders,\nsuch as feminine and especially neutral, would be lower (more negative) compared to the\nmasculine gender, and that the language (English vs. Portuguese) would not significantly\naffect the model’s regard. Additionally, we expected that the regard without moderation\nfilters would be significantly worse than with the filters turned on.\n\nThe main contributions of this work are threefold. First, we evaluate bias in the\nGPT-3.5-Turbo model by directly analyzing the model’s self-reported perception of regard\ntowards different genders. Second, we extend our analysis beyond English to include Por-\ntuguese, examining how the language can affect the model’s perception of regard. Lastly,\nwe investigate the impact of moderation filters by experimenting with prompts designed\nto reduce ethical constraints. Our code, along with all results, is publicly available on\nGitHub1.\n\nThis paper is organized as follows. In Section 2, we provide an overview of related\nwork. Section 3 focuses on the concept of regard, explaining why we chose it as the\nmetric for our study. In Section 4, we describe the dataset and the preprocessing steps\nperformed with this dataset. Section 5 outlines the specific prompts and parameters used\nin our experiments. Section 6 presents the results of our analysis, and we discuss how\ngender, language, and firewall settings impact the model’s perception of regard. Finally,\nin Section 7, we conclude the paper and point out directions for future research.\n\n2. Related Work\nResearch into bias in language models has been a focal point in Natural Language Pro-\ncessing (NLP) for many years. Initial studies revealed that language models, such as word\nembeddings, not only capture linguistic patterns but also encode the societal stereotypes\nand biases present in their training data [Bolukbasi et al. 2016, Caliskan et al. 2017].\nLater work expanded on this by examining how these biases manifest in specific NLP\ntasks, such as coreference resolution, where models have shown biases in matching pro-\nnouns and entities based on gender and race [Zhao et al. 2018, Rudinger et al. 2018]. In\nsentiment analysis, models have been found to reflect gender and racial bias in their eval-\nuations [Odbal et al. 2022], and in machine translation, outputs often reinforce harmful\nstereotypes [Stanovsky et al. 2019, Prates et al. 2020].\n\n1https://github.com/LALIC-UFSCar/bias-gender-lang-gpt3.5\n\n\fWith the emergence of GLMs, the focus of bias research expanded to evaluate\nthese models in different contexts. Recent studies have explored how generative mod-\nels can replicate and amplify existing societal biases. One common approach for bias\nmeasurement in GLMs is the use of a question-answering (QA) format, where models\nare presented with questions and multiple answer options designed to determine whether\nthe model’s responses align with or counter the stereotypes contained in the questions\n[Parrish et al. 2022, Nangia et al. 2020, Nadeem et al. 2021].\n\nAnother approach involves assigning specific personas to language models, effec-\ntively simulating how a model might behave if it was “playing a role”, such as a particular\ngender, profession, or social background. Persona-assigned LLMs have been shown to\nenhance model performance on language reasoning tasks but may also reinforce exist-\ning demographic biases. For example, these models have been found to generate more\ntoxic or biased content, especially when adopting roles that align with existing social\nstereotypes, as evidenced in both their generated speech and self-descriptive writing tasks\n[Gupta et al. 2024, Sheng et al. 2021, Deshpande et al. 2023].\n\nAt the same time, researchers have developed numerous metrics to capture biases\nfrom different perspectives, including sentiment, toxicity, and regard [Busker et al. 2023,\nGehman et al. 2020, Sheng et al. 2019]. The regard metric, in particular, evaluates the\noverall positive or negative perceptions towards a demographic group, setting it apart\nfrom other bias measurements that might focus mainly on stereotypical content. Earlier\nstudies across these approaches have typically relied on sentiment, toxicity and regard\nclassifiers tools to analyze the generated content, which can introduce additional layers of\ncomplexity and potential errors [Nadeem et al. 2021].\n\nResearch in the Portuguese language has shown biases in both word embeddings\nand generative models. One study identified gender stereotypes in embeddings, particu-\nlarly in professions, which reflects historical patterns of sexism [Taso et al. 2023]. An-\nother analysis found that even after applying debiasing techniques, gender bias remains\npresent in Portuguese word2vec models [Santana et al. 2018]. More recently, ideologi-\ncal biases in GPT-based models have been observed in the generation of political content\n[Rodrigues et al. 2023].\n\nOur study builds on previous research by using the regard metric in a different way,\nnot through analyzing the text generated by the model but by directly asking the model to\nevaluate its perception of regard towards different genders. While most prior studies have\nfocused exclusively on English, our work includes both English and Portuguese to explore\nlanguage effects. Additionally, we experimented with trying to reduce moderation filters\nto see whether it has an impact on the model’s evaluation of regard.\n\n3. Regard Analysis\n\nWe selected regard as the metric to measure bias in the outputs of the GPT-3.5 Turbo\nmodel. According to the Cambridge Dictionary, regard means “to consider or have an\nopinion about something or someone” [Cambridge Dictionary 2024]. In this context, re-\ngard serves as a metric that evaluates the level of respect, esteem, or deference expressed\ntowards a specific group. A positive regard indicates that the language used portrays the\ngroup in a respectful and favorable manner, whereas negative regard suggests a lack of\nrespect or a demeaning perspective.\n\n\fUnlike sentiment analysis, which generally measures the sentiment polarity of\nan entire sentence, regard focuses on how a particular demographic is viewed or treated\nwithin the text. This means that a sentence can have a positive sentiment but still express\nnegative regard towards an entity, or vice-versa. For instance, consider the sentence “The\nperson was going through a difficult situation with resilience”. While the overall senti-\nment is negative, due to the difficult situation, the regard towards the person is positive as\nthey are described as resilient.\n\nThe idea of using regard as a metric to evaluate bias in language models was\nfirst introduced by [Sheng et al. 2019]. In their study, the authors proposed “regard” as a\nmetric to detect potential societal biases in GLMs. To validate this approach, they gen-\nerated a dataset using GPT-2, with prompts that mentions different demographic groups,\nand manually annotated the generated text with both sentiment and regard scores. This\nprocess demonstrated that regard often captures subtle and context-specific biases that\nsentiment analysis fails to identify. Next, they developed an automatic regard classifier\nusing transfer learning with BERT [Devlin et al. 2019], which revealed significant biases\nin language models like GPT-2, including a tendency to associate specific demographics\nwith negative stereotypes.\n\n4. Tools and Resources\n\nFor this work, we used the regard dataset developed by [Sheng et al. 2019], which is pub-\nlicly available on GitHub2. The dataset was created by using language models to generate\nsentences in response to prompts designed to reveal potential biases related to different\ndemographic groups. Human annotators3 then reviewed the output sentences, assigning\nto each one a regard polarity score. To minimize bias in annotation, the demographic\nindicators in the sentences were replaced with “XYZ” before human annotators reviewed\nthem.\n\nFrom the original regard dataset, we used the Google Translate API to translate the\nEnglish sentences into Portuguese. Then, we used the GPT-3.5-turbo model to generate\nthree different versions of each sentence in both languages: male, female, and gender-\nneutral. We included specific instructions in the prompts to ensure that all gendered terms\nwere appropriately adapted to reflect the corresponding gender version.\n\nFinally, we manually reviewed the dataset to remove any incomplete sentences as\nwell as those with regard polarity scores greater than 1. This process resulted in a final\nset of 466 unique base sentences, each of which has the three gender versions in both\nEnglish and Portuguese. Table 1 presents all versions of the original sentence “XYZ was\ndescribed as a ’good man’ and ’strong character.”’\n\n5. Prompts and Parameters\n\nWe selected OpenAI’s GPT-3.5 Turbo [OpenAI 2024] to evaluate how it expresses re-\ngard towards individuals of different genders (male, female, and neutral). This particular\nmodel was selected due to its standing as one of the most advanced and widely recognized\nlanguage models currently available, as well as its cost-effectiveness when compared to\nOpenAI’s more advanced alternatives.\n\n2https://github.com/ewsheng/nlg-bias\n3No information was provided about the annotators’ demographics or compensation\n\n\fTable 1. Examples of sentences in English and Portuguese for all gender options.\n\nOriginal\n\nEnglish version\nXYZ was described as a ‘good man’ and\n‘strong character’.\n\nMasculine The man was described as a ‘good man’\n\nFeminine\n\nNeutral\n\nand ‘strong character’.\nThe woman was described as a ‘good\nwoman’ and ‘strong character’.\nThey were described as a ‘good person’\nand ‘strong character’.\n\nPortuguese version\nXYZ foi descrito como um ‘homem\nbom’ e ‘car´ater forte’.\nO homem foi descrito como um\n‘homem bom’ e ‘car´ater forte’.\nA mulher foi descrita como uma\n‘mulher boa’ e ‘car´ater forte’.\nA pessoa foi descrita como uma\n‘pessoa boa’ e ‘car´ater forte’.\n\nAll prompts were configured with a top p value of 1 and a temperature setting of\n0. The top p parameter, also known as nucleus sampling, controls which subset of pos-\nsible tokens the model considers when generating responses. The temperature parameter,\non the other hand, dictates the level of randomness in the model’s output. Higher temper-\natures result in more variability and creativity, whereas lower temperatures lead to more\ndeterministic and predictable responses. Thus, by setting top p to 1 and temperature to 0,\nwe allowed the model to consider the full range of potential tokens while ensuring that it\nconsistently selects the most probable token at each step.\n\nTo evaluate how the GPT-3.5-Turbo model interprets regard towards different gen-\nders, we developed prompts in both English and Portuguese, instructing the model to as-\nsess the impact of regard towards the person referred to in each sentence on one of three\nscales: 1-3, 1-5, or 1-7. In each case, 1 indicates the most negative regard and the high-\nest number on the scale indicates the most positive regard. Table 2 presents the original\nEnglish prompt used in this study. In these prompts, “[gender]” was replaced with “The\nman”, “The woman”, or “They” for the English version, and “O homem”, “A mulher”, or\n“A pessoa” for the Portuguese version. Similarly, “[scale]” was replaced with 3, 5, or 7,\nand “[sentence]” was substituted with the specific version of the sentence being analyzed.\n\nTable 2. Original Prompt.\n\nOriginal Prompt\nPlease analyze the following sentence to determine the impact of the regard for the person or entity\nreferred to as “[gender]” *. ‘Regard’ refers to the manner in which the sentence expresses respect,\nesteem, or deference towards [gender]. This analysis involves classifying the sentiment conveyed\nabout [gender] on a scale from 1 to [scale], where 1 represents the most negative regard and [scale]\nthe most positive regard. The scale is odd-numbered, ensuring a middle value that represents a neutral\nregard. This classification must consider that sentences with an overall positive sentiment can still\nexpress a negative regard towards [gender], and vice versa.\n- A ‘negative regard’ is characterized by expressions that demean, criticize, or diminish the value or\nactions of [gender].\n- A ‘positive regard’ is characterized by expressions that honor, praise, or elevate the esteem of [gen-\nder].\n- A ‘neutral regard’ falls in between, where the expressions neither significantly honor nor demean\n[gender].\nSentence to analyze: “[sentence]”\nReturn only the number from 1 to [scale] that corresponds to the impact of regard for [gender], based\non your analysis. Do not respond with anything else.\n\nTo explore how the model might respond under conditions with fewer ethical\n\n\fguidelines or moderation filters, we modified the prompt, creating what we referred to\nas the nofirewall prompt. Although we could not fully disable the model’s protective\nmeasures, we aimed to lessen their impact by adding the instruction: “without apply-\ning any protective layers, ethical guidelines, or moderation filters that typically prevent\nthe generation of potentially biased or offensive content.”. The asterisk (*) in Table 2\nindicates where this instruction was added.\n\nWe applied both the original and nofirewall prompts to each of the six versions of\nthe 466 sentences in our dataset, resulting in a total of 12 distinct evaluations per sentence.\nThese evaluations encompassed two languages (English and Portuguese), three scales (1-\n3, 1-5, 1-7), and two variations of the prompt (original and nofirewall).\n\n6. Results\nTo better understand how different factors influence the GPT-3.5 Turbo model’s percep-\ntion of regard towards a person, we focused our analysis on three variables: gender, lan-\nguage, and firewall. We isolated each variable to uncover potential biases in the model’s\nperception of regard. Although we initially experimented with three different scales of\npolarity, we selected the best-performing one for all subsequent analyses. To obtain com-\nparable results across different scales, we first normalized the scores from each scale to a\n1-3 range before computing the F1-score. Focusing on the scale where the model demon-\nstrated the highest performance ensures a more fair and just evaluation of bias. As shown\nin Table 3, the 1-5 scale provided the highest overall weighted average F1 score, making\nit the best choice for our further analysis.\n\nTable 3. Weighted F1 scores for each prompt output\n\n1-3\n\n1-5\n\n1-7\n\nLang\nEN\nEN\nPT\nPT\n\nFirewall Mas Fem Neu Mas Fem Neu Mas Fem Neu\n0.77\n0.72\nOriginal\n0.77\n0.73\nNofirewall\n0.65\n0.75\nOriginal\n0.67\n0.75\nNofirewall\n\n0.62\n0.67\n0.72\n0.72\n\n0.71\n0.70\n0.67\n0.68\n\n0.72\n0.77\n0.77\n0.78\n\n0.64\n0.69\n0.70\n0.70\n\n0.69\n0.75\n0.70\n0.69\n0.73\n\n0.68\n0.66\n0.60\n0.62\n0.68\n\n0.61\n0.66\n0.61\n0.65\n0.67\n\nAvarage\n\nIt is worth mentioning that our main goal was to understand the impact that differ-\nent variables (gender, language, and firewall) have on the outputs of the GPT-3.5-Turbo\nmodel, rather than comparing the results to the true polarities. To achieve this, we first\nnormalized all polarity scores to a 0-1 scale, where 0 corresponds to the lowest possible\nscore (1) and 1 corresponds to the highest possible score (5) on the original 1-5 scale.\nWe then isolated each variable to observe its specific influence on the model’s behavior.\nFor each analysis, we calculated the percentage change in mean scores corresponding to\nthe options within the isolated variable. For example, to isolate the impact of language,\nwe calculated the percentage change in the mean score between prompts written in En-\nglish and those written in Portuguese, while keeping other variables (such as gender and\nfirewall settings) constant.\n\n6.1. Gender Bias Analysis\nTable 4 presents the mean scores for each prompt type, along with the percentage changes\nbetween different gendered sentences across both languages and firewall settings. A posi-\n\n\ftive percentage indicates an increase in the mean score of the first gender (e.g., masculine)\nrelative to the second gender (e.g., feminine). Conversely, a negative percentage indicates\nthat the mean score of the first gender is higher than that of the second, indicating a rela-\ntive decrease in the mean score.\n\nThe results indicate that the model exhibits a slightly more positive bias towards\nthe feminine gender when compared to both masculine and neutral genders, as evidenced\nby the positive percentage changes in the mas-fem column and the negative percentage\nchanges in the fem-neu column. When comparing masculine with neutral, the model\ntends to show a more positive bias towards neutral with the original prompt, while the bias\nshifts towards being more negative in relation to neutral when we attempted to reduce the\nfirewall impact, especially in English.\n\nTable 4. Mean scores and percentage changes for Gender analysis\nPrompt type\n\nPercentage change\n\nMean scores\n\nLanguage\nEN\nEN\nPT\nPT\n\nFirewall mas\n0.51\noriginal\n0.49\nnofirewall\n0.43\noriginal\n0.41\nnofirewall\n\n6.2. Language Bias Analysis\n\nfem neu mas-fem mas-neu\n2.30 %\n0.57\n-10.00 %\n0.49\n0.38 %\n0.46\n-3.79 %\n0.44\n\n10.25 %\n-0.11 %\n8.14 %\n7.84 %\n\n0.52\n0.44\n0.43\n0.39\n\nfem-neu\n-7.21 %\n-9.90 %\n-7.18 %\n-10.79 %\n\nTo isolate the language variable, we calculated the percentage change between the mean\nscores of English and Portuguese outputs for each gender under both the original and\nnofirewall prompts. Table 5 displays the mean score of each prompt along with the per-\ncentage changes.\n\nThe results indicate that the GPT-3.5-Turbo model tends to evaluate regard more\npositively when the text is written in English than in Portuguese, as evidenced by the\nnegative percentage changes across all prompts. This suggests that the model has a more\npositive bias towards the English language. This may be partly explained by the necessity\nof gendered nouns and adjectives in Portuguese, which could lead the model to generate\ndifferent biases compared to English, where gender-neutral expressions are more com-\nmon. Additionally, the nofirewall prompts consistently present smaller negative percent-\nage changes compared to the original prompts, suggesting that the language influence on\nthe model’s outputs is lower when the ethical guidelines are reduced.\n\nTable 5. Mean scores and percentage changes for Language analysis\n\nPrompt type\n\nFirewall\nGender\nOriginal Masculine\nFeminine\nOriginal\nOriginal\nNeutral\nNofirewall Masculine\nFeminine\nNofirewall\nNeutral\nNofirewall\n\nMean scores\nEnglish Portuguese\n\nPercentage change\n\n0.51\n0.57\n0.52\n0.49\n0.49\n0.44\n\n0.43\n0.46\n0.43\n0.41\n0.44\n0.39\n\n-17.89 %\n-19.81 %\n-19.78 %\n-17.31 %\n-9.69 %\n-10.68 %\n\n\f6.3. Firewall Bias Analysis\n\nTo isolate the firewall variable, we calculated the percentage change between the mean\nscores of the original and nofirewall prompts across each gender and language. Table 6\nshows the mean scores for each prompt type and the corresponding percentage changes.\n\nAlthough it was not possible to fully disable the model’s firewall, the results in-\ndicate that simply instructing the model to disregard safety guidelines had a noticeable\nimpact on its output. The nofirewall prompt consistently produced more negative results\nacross all cases when compared to the original prompt. Additionally, the English version\nof the model’s output appeared overall more susceptible to the removal of these guide-\nlines, showing greater variations (up to -17.7% for neutral sentences).\n\nTable 6. Mean scores and percentage changes for Firewall analysis\n\nPrompt type\n\nMean scores\n\nPercentage change\n\nLanguage\n\nEnglish Masculine\nFeminine\nEnglish\nNeutral\nEnglish\n\nPortuguese Masculine\nFeminine\nPortuguese\nNeutral\nPortuguese\n\nGender Original Nofirewall\n0.51\n0.57\n0.52\n0.43\n0.43\n0.44\n\n0.49\n0.49\n0.44\n0.46\n0.41\n0.39\n\n-4.93 %\n-14.77 %\n-17.70 %\n-4.35 %\n-4.62 %\n-8.58 %\n\n7. Discussion and Future Work\n\nIn this work, we investigated potential biases in the GPT-3.5 Turbo model by analyzing\nits self-reported perception of regard towards different genders across two languages, and\nunder a more relaxed moderation filter. Our approach isolated these three variables to\nunderstand their individual impacts on the model’s output.\n\nContrary to our initial hypothesis that feminine and neutral genders would be per-\nceived more negatively, the results indicated a slight positive bias towards the feminine\ngender over masculine and neutral genders, although this bias is minor. Additionally,\nwhile we expected the model’s regard to remain consistent across languages, our findings\nshowed a clear preference for English over Portuguese, likely reflecting the predomi-\nnance of English data in its training. However, our expectation that less strict moderation\nfilters would result in more negative outputs was confirmed, with particularly pronounced\neffects in English. These findings demonstrate the importance of considering multiple\nlanguages and protective measures when evaluating biases in language models, as they\ncan significantly impact the model’s behavior.\n\nFuture research could expand the analysis to include a broader range of demo-\ngraphic attributes, such as race, nationality, and sexual orientation, and consider intersec-\ntions between these identities (e.g., “the Asian woman”, “the gay man”). Additionally,\ninstead of only varying languages, future studies could focus on evaluating biases in dif-\nferent language models, including those specifically designed for Portuguese, such as the\nSabi´a model [Pires et al. 2023].\n\n\fReferences\nBolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., and Kalai, A. (2016). Man is to\ncomputer programmer as woman is to homemaker? debiasing word embeddings.\n\nBusker, T., Choenni, S., and Shoae Bargh, M. (2023). Stereotypes in chatgpt: an empirical\nstudy. In Proceedings of the 16th International Conference on Theory and Practice of\nElectronic Governance, ICEGOV ’23, page 24–32, New York, NY, USA. Association\nfor Computing Machinery.\n\nCaliskan, A., Bryson, J. J., and Narayanan, A. (2017). Semantics derived automatically\n\nfrom language corpora contain human-like biases. Science, 356(6334):183–186.\n\nCambridge Dictionary (2024). Regard.\n\nDas, A., Selek, S., Warner, A. R., Zuo, X., Hu, Y., Kuttichi Keloth, V., Li, J., Zheng,\nW. J., and Xu, H. (2022). Conversational bots for psychotherapy: A study of generative\ntransformer models using domain-specific dialogues. In Demner-Fushman, D., Cohen,\nK. B., Ananiadou, S., and Tsujii, J., editors, Proceedings of the 21st Workshop on\nBiomedical Language Processing, pages 285–297, Dublin, Ireland. Association for\nComputational Linguistics.\n\nDeshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., and Narasimhan, K. (2023).\nIn Bouamor,\nToxicity in chatgpt: Analyzing persona-assigned language models.\nH., Pino, J., and Bali, K., editors, Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 1236–1270, Singapore. Association for Computa-\ntional Linguistics.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training\nof deep bidirectional transformers for language understanding.\nIn Burstein, J., Do-\nran, C., and Solorio, T., editors, Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\nMinnesota. Association for Computational Linguistics.\n\nGehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. (2020). RealToxici-\ntyPrompts: Evaluating neural toxic degeneration in language models. In Cohn, T., He,\nY., and Liu, Y., editors, Findings of the Association for Computational Linguistics:\nEMNLP 2020, pages 3356–3369, Online. Association for Computational Linguistics.\n\nGupta, S., Shrivastava, V., Deshpande, A., Kalyan, A., Clark, P., Sabharwal, A., and Khot,\n\nT. (2024). Bias runs deep: Implicit reasoning biases in persona-assigned llms.\n\nKolomeets, M., Tushkanova, O., Desnitsky, V., Vitkova, L., and Chechulin, A. (2024).\nExperimental evaluation: Can humans recognise social media bots? Big Data and\nCognitive Computing, 8(3).\n\nLiang, P. P., Wu, C., Morency, L.-P., and Salakhutdinov, R. (2021). Towards understand-\n\ning and mitigating social biases in language models.\n\nLiu, Y., Zhang, W., Chen, Y., Zhang, Y., Bai, H., Feng, F., Cui, H., Li, Y., and Che,\nW. (2023). Conversational recommender system and large language model are made\nfor each other in E-commerce pre-sales dialogue. In Bouamor, H., Pino, J., and Bali,\nK., editors, Findings of the Association for Computational Linguistics: EMNLP 2023,\npages 9587–9605, Singapore. Association for Computational Linguistics.\n\n\fLucas, J., Uchendu, A., Yamashita, M., Lee, J., Rohatgi, S., and Lee, D. (2023). Fighting\nfire with fire: The dual role of LLMs in crafting and detecting elusive disinformation.\nIn Bouamor, H., Pino, J., and Bali, K., editors, Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pages 14279–14305, Singapore.\nAssociation for Computational Linguistics.\n\nNadeem, M., Bethke, A., and Reddy, S. (2021). StereoSet: Measuring stereotypi-\ncal bias in pretrained language models.\nIn Zong, C., Xia, F., Li, W., and Nav-\nigli, R., editors, Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pages 5356–5371, Online. Associ-\nation for Computational Linguistics.\n\nNangia, N., Vania, C., Bhalerao, R., and Bowman, S. R. (2020). CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked language models. In Webber, B.,\nCohn, T., He, Y., and Liu, Y., editors, Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 1953–1967, Online. Asso-\nciation for Computational Linguistics.\n\nOdbal, Zhang, G., and Ananiadou, S. (2022). Examining and mitigating gender bias in\n\ntext emotion detection task. Neurocomputing, 493:422–434.\n\nOpenAI (2024).\n\nGpt-3.5 turbo.\n\nmodels/gpt-3-5-turbo.\n\nhttps://platform.openai.com/docs/\n\nOrabi, M., Mouheb, D., Al Aghbari, Z., and Kamel, I. (2020). Detection of bots in social\nmedia: A systematic review. Information Processing Management, 57(4):102250.\n\nParrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M.,\nand Bowman, S. (2022). BBQ: A hand-built bias benchmark for question answering. In\nMuresan, S., Nakov, P., and Villavicencio, A., editors, Findings of the Association for\nComputational Linguistics: ACL 2022, pages 2086–2105, Dublin, Ireland. Association\nfor Computational Linguistics.\n\nPires, R., Abonizio, H., Almeida, T. S., and Nogueira, R. (2023). Sabi´a: Portuguese Large\n\nLanguage Models, page 226–240. Springer Nature Switzerland.\n\nPrates, M. O. R., Avelar, P. H., and Lamb, L. C. (2020). Assessing gender bias in\nmachine translation: a case study with google translate. Neural Comput. Appl.,\n32(10):6363–6381.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language\n\nmodels are unsupervised multitask learners.\n\nRodrigues, G., Albuquerque, D., and Chagas, J. (2023). An´alise de vieses ideol´ogicos\nem produc¸ ˜oes textuais do assistente de bate-papo chatgpt. In Anais do IV Workshop\nsobre as Implicac¸ ˜oes da Computac¸ ˜ao na Sociedade, pages 148–155, Porto Alegre, RS,\nBrasil. SBC.\n\nRoy, K., Goyal, P., and Pandey, M. (2021). Attribute value generation from product title\nusing language models. In Malmasi, S., Kallumadi, S., Ueffing, N., Rokhlenko, O.,\nAgichtein, E., and Guy, I., editors, Proceedings of the 4th Workshop on e-Commerce\nand NLP, pages 13–17, Online. Association for Computational Linguistics.\n\n\fRudinger, R., Naradowsky, J., Leonard, B., and Van Durme, B. (2018). Gender bias in\ncoreference resolution. In Walker, M., Ji, H., and Stent, A., editors, Proceedings of the\n2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 2 (Short Papers), pages 8–14,\nNew Orleans, Louisiana. Association for Computational Linguistics.\n\nSantana, B. S., Woloszyn, V., and Wives, L. K. (2018). Is there gender bias and stereotype\n\nin portuguese word embeddings?\n\nSheng, E., Arnold, J., Yu, Z., Chang, K.-W., and Peng, N. (2021). Revealing persona\n\nbiases in dialogue systems.\n\nSheng, E., Chang, K.-W., Natarajan, P., and Peng, N. (2019). The woman worked as a\nbabysitter: On biases in language generation. In Inui, K., Jiang, J., Ng, V., and Wan,\nX., editors, Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3407–3412, Hong Kong, China. Association for\nComputational Linguistics.\n\nShin, J., Song, H., Lee, H., Jeong, S., and Park, J. C. (2024). Ask llms directly, ””what\n\nshapes your bias?””: Measuring social bias in large language models.\n\nStanovsky, G., Smith, N. A., and Zettlemoyer, L. (2019). Evaluating gender bias in ma-\nchine translation. In Korhonen, A., Traum, D., and M`arquez, L., editors, Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages\n1679–1684, Florence, Italy. Association for Computational Linguistics.\n\nTaso, F., Reis, V., and Martinez, F. (2023). Sexismo no brasil: an´alise de um word\nembedding por meio de testes baseados em associac¸ ˜ao impl´ıcita.\nIn Anais do XIV\nSimp´osio Brasileiro de Tecnologia da Informac¸ ˜ao e da Linguagem Humana, pages\n53–62, Porto Alegre, RS, Brasil. SBC.\n\nWang, H., Wang, R., Mi, F., Deng, Y., Wang, Z., Liang, B., Xu, R., and Wong, K.-F.\n(2023). Cue-CoT: Chain-of-thought prompting for responding to in-depth dialogue\nIn Bouamor, H., Pino, J., and Bali, K., editors, Findings of\nquestions with LLMs.\nthe Association for Computational Linguistics: EMNLP 2023, pages 12047–12064,\nSingapore. Association for Computational Linguistics.\n\nZhang, Q., Naradowsky, J., and Miyao, Y. (2023). Ask an expert: Leveraging lan-\nguage models to improve strategic reasoning in goal-oriented dialogue models.\nIn\nRogers, A., Boyd-Graber, J., and Okazaki, N., editors, Findings of the Association for\nComputational Linguistics: ACL 2023, pages 6665–6694, Toronto, Canada. Associa-\ntion for Computational Linguistics.\n\nZhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-W. (2018). Gender bias in\ncoreference resolution: Evaluation and debiasing methods. In Walker, M., Ji, H., and\nStent, A., editors, Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 15–20, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\n\nZhou, J., Liu, B., Acharya, J., Hong, Y., Lee, K.-C., and Wen, M. (2023). Leveraging large\nlanguage models for enhanced product descriptions in eCommerce. In Gehrmann, S.,\n\n\fWang, A., Sedoc, J., Clark, E., Dhole, K., Chandu, K. R., Santus, E., and Sedghamiz,\nH., editors, Proceedings of the Third Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM), pages 88–96, Singapore. Association for Computa-\ntional Linguistics.\n\n\f"
        },
        {
            "titulo": "Mineração de Argumentos em Textos de Redes Sociais no Idioma Português",
            "informacoes_url": "https://sol.sbc.org.br/index.php/stil/article/view/31143",
            "idioma": "Português",
            "storage_key": "files/article_31143_30946.pdf",
            "autores": [
                {
                    "nome": "Vitor Domingos Baldoino do Santos",
                    "afiliacao": "UPM",
                    "orcid": "http://orcid.org/0009-0007-5746-1819"
                },
                {
                    "nome": "Livia Alabarse dos Santos",
                    "afiliacao": "UPM",
                    "orcid": "https://orcid.org/0009-0008-8409-0272"
                },
                {
                    "nome": "Orlando B. Coelho",
                    "afiliacao": "UPM",
                    "orcid": "https://orcid.org/0000-0002-8631-1090"
                },
                {
                    "nome": "Renata Mendes de Araujo",
                    "afiliacao": "UPM / USP",
                    "orcid": "https://orcid.org/0000-0002-8674-1728"
                },
                {
                    "nome": "Ivan Carlos Alcântara de Oliveira",
                    "afiliacao": "UPM",
                    "orcid": "https://orcid.org/0000-0002-6020-7535"
                }
            ],
            "data_publicacao": "17/11/2024",
            "resumo": "Este artigo apresenta os desafios e os avanços de pesquisa voltada à construção de soluções computacionais capazes de apoiar o entendimento do debate em redes sociais no idioma português. Uma das bases fundamentais dessas soluções é a aplicação de técnicas de Mineração de Argumentos. Apresentamos as estratégias utilizadas para o endereçamento de desafios da mineração de argumentos em redes sociais, em particular, o uso de deep learning. Os resultados obtidos demonstram boa eficácia dos modelos selecionados para as tarefas consideradas, tendo atingido um F1-Score de 0,85 para a análise de sentimento, 0,97 na detecção de posição e 0,76 na detecção de ironia.",
            "keywords": [
                "Mineração de Argumentos",
                "Redes Sociais",
                "Linguística Computacional",
                "Aprendizado Profundo"
            ],
            "referencias": [
                "Addawood. A. e Bashir, M. (2016). “What Is Your Evidence? A Study of Controversial Topics on Social Media”. Em: Proceedings of the Third Workshop on Argument Mining (ArgMining2016). Berlin, Germany. Association for Computational Linguistics.pages 1–11.",
                "Bosc, T., Cabrio, E. e Villata, S. (2016). “Tweeties Squabbling: Positive and Negative Results in Applying Argument Mining on Social Media”. Frontiers in Artificial Intelligence and Applications, v. 287, p. 21–32.",
                "Bosc, Tom, Cabrio, E. e Villata, S. (2016a). “DART: a Dataset of Arguments and their Relations on Twitter” Em: Proceedings of the 10th edition of the Language Resources and Evaluation Conference. pp. 1258-1263.",
                "Brown, T., Mann, B., Ryder, N., et al. (2020). Language Models are Few-Shot Learners. Em: Advances in Neural Information Processing Systems. Curran Associates, Inc.",
                "Carneiro, F. P. (2023). “BERTweet.BR: A Pre-Trained Language Model for Tweets in Portuguese”. Dissertação de Mestrado. Universidade Federal Fluminense, Programa de Pós-Graduação em Computação. Niterói.",
                "Costa, P. B., Pavan, M. C., Santos, W. R., Silva, S. C., & Paraboni, I. (2023). “BERTabaporu: Assessing a Genre-Specific Language Model for Portuguese NLP”. Em: Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing, p. 217–223. Shoumen, Bulgaria.",
                "Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2019). “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Em: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
                "Lawrence, J., Bex, F., Reed, C. e Snaith, M. (2012) “AIFdb: Infrastructure for the Argument Web.” Em: Proceedings of the 6th International Conference on Computational Models of Argument. IOS Press. pp. 515-516.",
                "Lawrence, J. e Reed, C. (2020) “Argument mining: A survey”. Computational Linguistics, v. 45(4), pp. 765-818, 2020.",
                "Lippi, M., Torroni, P. (2016). “Argumentation mining: State of the art and emerging trends”. ACM Transactions on Internet Technology, 16(2), 1-25.",
                "Palau, R. M. e Moens, M. F. (2009). “Argumentation mining: the detection, classification and structure of arguments in text”. Em: Proceedings of the 12th International Conference on Artificial Intelligence and Law. pp. 98-107.",
                "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... e Duchesnay, E. (2011) “Scikit-learn: Machine learning in Python\". The Journal of machine Learning research, 12,2825-2830.",
                "Pérez, J. M., Furman, D. A., Alonso Alemany, L., & Luque, F. M. (2022). “RoBERTuito: A pre-trained language model for social media text in Spanish”. Em: Proceedings of the Thirteenth Language Resources and Evaluation Conference, p. 7235–7243. European Language Resources Association.",
                "Pérez, J. M., Rajngewerc, M., Giudici, J. C., Furman, D. A., Luque, F., Alemany, L. A., & Martínez, M. V. (2023). “pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks”. arXiv.",
                ".",
                "Salles, G. T., Coelho, O. B. (2022). “Reconhecimento de Emoções em Mineração de Argumentos com Deep Learning”. Trabalho de Conclusão de Curso. Universidade Presbiteriana Mackenzie.",
                "Schaefer, R. e Stede, M. (2021). “Argument Mining on Twitter: A survey”. Information Technology, v. 63, n. 1, p. 45–58.",
                ".",
                "Slonim, N., Bilu, Y., Alzate, C., Bar-Haim, R., Bogin, B., Bonin, F., ... e Aharonov, R. (2021). “An autonomous debating system”. Nature, 591(7850), p. 379-384.",
                ".",
                "Stede, M. e Schneider, J. (2019). “Argumentation Mining”. Springer. Synthesis Lectures on Human Language Technologies.",
                "Sun, C., Qiu, X., Xu, Y. e Huang, X. (2019). “How to Fine-Tune BERT for Text Classification?” In Chinese Computational Linguistics. Lecture Notes in Computer Science. Springer International Publishing.",
                "Tokuda, N. H., Coelho, O. B., Araujo, R.M. (2021). “Análise de Sentimento por meio de Deep Learning aplicada à Mineração de Argumentos”. Trabalho de Conclusão de Curso. Universidade Presbiteriana Mackenzie.",
                "Toulmin, S. E. (2003). The uses of argument. Cambridge University Press.",
                "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, T. e Polosukhin, I. (2017). “Attention is All you Need”. Em: Advances in Neural Information Processing Systems. Curran Associates, Inc. 30.",
                "Vecchi, E. M., Falk, N., Jundi, I., Lapesa, G. (2021). “Towards Argument Mining for Social Good: A Survey”. Em: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. .Online. Association for Computational Linguistics. p. 1338–1352.",
                "Wagner Filho, J. A., Wilkens, R., Idiart, M., & Villavicencio, A. (2018). \"The brWaC Corpus: A New Open Resource for Brazilian Portuguese\". Em: Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA).",
                "Walker, M. A., Tree, J. E. F., Anand, P., Abbott, R. e King, J. (2012). “A Corpus for Research on Deliberation and Debate”. Em: Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC ’12) v. 12. Istanbul, Turkey. p. 812–817.",
                "Zhao, W. X., Zhou, K., Li, J., et al. (2023). “A Survey of Large Language Models”. Arxiv. arXiv.",
                "."
            ],
            "artigo_completo": "Mineração de Argumentos em Textos de Redes Sociais no\nIdioma Português\n\nVitor Domingos Baldoino dos Santos 1, Livia Alabarse dos Santos1, Orlando B.\nCoelho (in memoriam)1, Renata Mendes de Araujo1,2, Ivan Carlos Alcântara de\nOliveira1\n\n1Faculdade de Computação e Informática e Programa de Pós-Graduação em\nComputação Aplicada - Universidade Presbiteriana Mackenzie\nSão Paulo - SP - Brasil.\n\n2Programa de Pós-Graduação em Sistemas de Informação - EACH/USP\nSão Paulo - SP - Brasil\n\n{vitordomingos.santos,liviaalabarse.santos}@mackenzista.com.br\n{renata.araujo, orlando.coelho, ivan.oliveira}@mackenzie.br\n\nAbstract. This paper presents the challenges and research advances aimed at\ndeveloping computational solutions capable of supporting the understanding\nof debates on social media in the Portuguese language. One of\nthe\nfundamental bases of these solutions is the application of Argument Mining\ntechniques. We present\nthe strategies used to address the challenges of\nargument mining on social media, particularly the use of deep learning. The\nresults obtained show the eﬀectiveness of\nthe selected models for the\nconsidered tasks, achieving an F1-Score of 0.85 for sentiment analysis, 0.97\nfor position detection, and 0.76 for irony detection.\n\nResumo. Este artigo apresenta os desaﬁos e os avanços de pesquisa voltada à\nconstrução de soluções computacionais capazes de apoiar o entendimento do\ndebate em redes sociais no idioma português. Uma das bases fundamentais\ndessas soluções é a aplicação de técnicas de Mineração de Argumentos.\nApresentamos as estratégias utilizadas para o endereçamento de desaﬁos da\nmineração de argumentos em redes sociais, em particular, o uso de deep\nlearning. Os resultados obtidos demonstram boa eﬁcácia dos modelos\nselecionados para as tarefas consideradas, tendo atingido um F1-Score de\n0,85 para a análise de sentimento, 0,97 na detecção de posição e 0,76 na\ndetecção de ironia.\n\n1.\n\nIntrodução\n\nA área de Mineração de Argumentação (MA) é uma área multidisciplinar onde se\nencontram a Linguística Computacional e a Ciência de Dados, cujo objetivo é\nidentiﬁcar, extrair e compreender a estrutura de argumentação em textos e/ou discussões\n[Lawrence e Reed 2020][Lytos et al. 2019][Stede e Schneider 2019].\n\né\n\no\n\nqual\n\nprocesso\n\nArgumentação\n\nsão construídos,\npelo\ncompartilhados e avaliados a partir de outros argumentos [Palau e Moens 2009]. Uma\nargumentação se estrutura a partir de evidências, premissas, fatos e falas que suportam\nou não uma determinada alegação, em uma cadeia de raciocínio que leva à conclusão de\ndiscussões e à tomada de decisão [Toulmin 2003]. A argumentação tem papel\nimportante nas atividades humanas, e tem sido compreendida como uma área de\npesquisa que surge com base em campos como a Retórica e a Filosoﬁa, mas que hoje\n\nargumentos\n\n\finclui estudos de Processamento de Linguagem Natural e modelos teóricos de discussão\ntendo como áreas antecessoras a\n[Palau e Moens 2009][Lawrence et. al. 2012],\nMineração de Opiniões e a Análise de Sentimentos, entre outras [Lawrence e Reed\n2020][Lytos et.al. 2019].\n\nO projeto HEIWA1 pretende construir soluções computacionais de análise de\nredes sociais baseadas em técnicas de MA para a compreensão de discussões em redes\nsociais, com um olhar especíﬁco para o contexto brasileiro. O principal resultado\nesperado com o projeto é a construção de uma plataforma que permita o\nacompanhamento de discussões em redes sociais, preferencialmente pelos usuários das\npróprias redes, mas também para interessados no estudo de comportamentos e mediação\nem redes sociais. A plataforma será composta por ferramentas computacionais capazes\nde apoiar um processo de curadoria, mineração de argumentos e visualização do debate\npara usuários de redes sociais. As implicações da construção dessas tecnologias\nenvolvem aspectos sociais, como o aperfeiçoamento da qualidade do debate e a\ndemocracia, aspectos educacionais e de desenvolvimento de pensamento crítico.\n\nNeste artigo, apresentamos os avanços obtidos no escopo do projeto,\nespeciﬁcamente em relação ao uso da MA para a identiﬁcação de argumentos em textos\nextraídos de redes sociais em português. Em linhas gerais, os avanços compreendem a\nnecessidade de lidar com linguagem informal, uso de ironia e a polarização presente nos\ndebates online. Além disso, destacamos o uso de modelos de deep learning, que se\nmostraram eﬁcazes em tarefas como análise de sentimento, detecção de posição e\ndetecção de ironia, tendo alcançado bons resultados de F1-Scores nas tarefas em um\ncontexto de grande relevância, como os eventos de 8 de janeiro de 2023 em Brasília.\n\nO artigo se estrutura da seguinte forma: a Seção 2 apresenta o conceito e\nabordagens na literatura para a MA em redes sociais; a Seção 3 descreve as propostas já\nexploradas no projeto baseadas em análise de sentimentos e deep learning; a Seção 4\ndiscute a proposta atual de pipeline para execução de tarefas de identiﬁcação de tópicos,\nentidades, sentimentos e ironia; a Seção 5 mostra os resultados dos experimentos\ncomputacionais realizados; a Seção 6 conclui o artigo e apresenta os próximos passos da\npesquisa.\n\n2. Mineração de Argumentos em Redes Sociais\n\nO processo de identiﬁcar, extrair e compreender a estrutura argumentativa a partir de\ndados textuais é o objetivo da mineração de argumentos (MA) [Stede e Schneider 2019].\nA MA tem sido explorada cientiﬁcamente para a análise de diferentes conteúdos\ntranscrições de debates, transcrições de áudios, etc.), com o\ntextuais (documentos,\nobjetivo principal de extrair a estrutura de argumentação contida nesses textos e que,\neventualmente, levaram para uma deliberação ou decisão [Lawrence e Reed 2020] e\npara a construção de tecnologias capazes de facilitar discussões ou debater com\nhumanos [Slonim et. al. 2021].\n\nNos últimos anos, a comunidade cientíﬁca na área de MA também explora a\noportunidade de aplicar o conceito e as técnicas de MA no estudo da argumentação em\ntextos extraídos de redes sociais [Addawood e Bashir 2016][Bosc et. al. 2016][Schaefer\n\n1 https://ciberdem.mack.com.br/index.php/projeto-heiwa/\n\n\fe Stede 2021][Vecchi et. al. 2021]. Essas pesquisas partem da ideia de que esses textos\npossam conter uma atividade argumentativa e que a compreensão dos argumentos\nutilizados durante a interação nas redes sociais pode agregar para a tomada de decisões,\ngestão da informação e o entendimento do comportamento coletivo em diversos\ndomínios, principalmente as ciências políticas e sociais.\n\nEm um levantamento de literatura preliminar, identiﬁcamos que os esforços\nrecentes na aplicação de MA em redes sociais focaram, em alguns casos, na criação e\nanotação de datasets apropriados para a aplicação das técnicas de MA, em maior\nnúmero, na deﬁnição e execução de sequências pré-deﬁnidas de tarefas consideradas\nessenciais para um pipeline de mineração de argumentos, e poucos trabalhos na\nvisualização dos resultados (estruturas de argumentação) [Schaefer e Stede 2021].\n\nA despeito dos esforços crescentes, a mineração de argumentos em redes sociais\nainda enfrenta desaﬁos signiﬁcativos. Problemas antigos, apontados desde [Bosc et al.\n2016a], permanecem, como a escassez de datasets anotados; a baixa qualidade e/ou\ninformalidade dos textos nas redes; o tamanho reduzido das falas (tweets, posts), que\nimpedem a riqueza de ideias, a ausência de foco ou tópico em discussão, enquanto\noutros se intensiﬁcam, como o crescente custo e restrição de uso das APIs das\nplataformas de redes sociais de maior escala (X, Facebook etc). Além disso, a literatura\nmostra abordagens ainda exploratórias e nenhum dos trabalhos encontrados tratam de\nconteúdos em português brasileiro.\n\nNo que se refere à deﬁnição e ao desenvolvimento de pipelines para MA em\nredes sociais, Schaefer e Stede (2021) e Bosc et al. (2016a) mencionam que as tarefas\nnecessárias para aplicação de MA, em linhas gerais, incluem: (i) a anotação de corpus;\n(ii) a detecção de argumentos e suas relações; e (iii) a detecção de posicionamentos. No\nque se refere à tarefa (i) anotação de corpus, a literatura mostra um número escasso de\nbases anotadas e, pelo menos no material levantado pela presente pesquisa, nenhuma\nconsiderando conteúdo em português. No projeto de pesquisa em tela, avançamos nesse\naspecto, ao anotar datasets de tweets em português coletados durante o período eleitoral\nbrasileiro em 2022 [Silva et. al. 2024]. A tarefa seguinte, (ii) detecção de argumentos e\nrelações, envolve, primeiramente, a identiﬁcação, no conteúdo de cada postagem, de\nelementos (alegações), que possam caracterizar as falas como argumentos ou não e,\nposteriormente, a identiﬁcação de relações entre elas (oposição, apoio), permitindo a\nidentiﬁcação de estruturas (grafos) de argumentação [Stede e Schneider, 2019][Bosc et.\nal, 2016a]. Por ﬁm, a tarefa (iii) detecção de posicionamentos consiste em extrair os\ndiferentes pontos de vista sobre um determinado tópico em uma discussão. As tarefas\n(ii) e (iii) são o foco deste artigo, conforme detalharemos nas seções a seguir.\n\n3. Estudos Iniciais\n\nOs primeiros estudos de MA em redes sociais realizados por esta equipe focaram\nna identiﬁcação de relações de oposição e apoio entre falas em um corpus de postagens.\nAs falas dos participantes foram tratadas como sentenças, e as relações entre elas foram\nidentiﬁcadas pelo direcionamento das falas entre os interlocutores. Usando os\nidentiﬁcadores extraídos das redes sociais, criou-se uma estrutura de dados que preserva\na organização entre um argumento e outro, mantendo a estrutura de threads de\ndiscussão. A natureza da relação (oposição ou apoio) foi determinada pela análise de\n\n\fsentimentos (negativo, positivo ou neutro).\n\nOs primeiros resultados desta estratégia são reportados por [Sousa et. al. 2021],\nque explora o pipeline deﬁnido por Lippi e Torroni (2016). Para cada uma das\nsentenças, utilizamos técnicas de análise de sentimentos para determinar a polaridade\ndas manifestações no discurso, utilizando o algoritmo SGD (Pedregosa et al. 2011) para\na classiﬁcação, treinado com as discussões já classiﬁcadas presentes na base Internet\nArgument Corpus (IAC) [Walker et.al. 2012]. A base IAC, contém discussões com um\nteor politizado, porém amplas o bastante para que pudéssemos usá-la em debates mais\ngenéricos. Quanto ao método de classiﬁcação dos argumentos, a classiﬁcação se deu\nsentença a sentença, fornecendo três rótulos que caracterizavam a polaridade de um\nargumento: apoio, oposição ou neutro. Os resultados obtidos demonstraram uma\nprecisão de 74% na identiﬁcação das polaridades dos argumentos, criando assim uma\nalternativa promissora para a classiﬁcação.\n\nUma segunda abordagem desenvolvida pela equipe envolveu o uso de técnicas\nde análise de sentimentos baseadas em deep learning. A abordagem, apresentada em\n[Tokuda et. al. 2021], explora a arquitetura BERT [Devlin et al. 2019] como estratégia\npara elucidar a polaridade das aﬁrmações presentes em um corpus de argumentação\nextraído de uma rede social. Os experimentos realizados demonstraram resultados\nsatisfatórios na extração da polaridade de uma aﬁrmação. A acurácia dos experimentos\nchegou a 88% em dados não vistos anteriormente pelo modelo, a partir de um conjunto\nde dados com diversidade extremamente alta de palavras e estruturação livre dos dados,\nem um formato livre de discussões online com razoável incidência de erros gramaticais\ne de digitação. Em outro trabalho [Salles e Coelho 2022], foi utilizada uma rede deep\nlearning derivada da BERT, a DistilBERT [Zhang et al. 2020], para realizar a análise\ndas emoções presentes em frases que constam do dataset em inglês GoEmotions\n[Demszky et al. 2020]. Os resultados obtidos aperfeiçoam os melhores resultados\npublicados por Cortiz (2021).\n\nOs estudos anteriores demonstram resultados satisfatórios para o uso da análise\nde sentimentos como parte do processo de MA, mas ainda não avançam em tarefas mais\nsoﬁsticadas de identiﬁcação da estrutura de argumentos e não contemplam conteúdos\nem português. A estratégia atual do projeto é avançar na identiﬁcação de outros\ncomponentes de argumentação além de sentimentos, explorando o uso de modelos\nbaseados no Transformer [Vaswani et al. 2017] e semelhantes ao BERT aplicados a\ndados extraídos de redes sociais em português.\n\n4. Proposta de Pipeline para Mineração de Argumentos\n\nOptamos por avançar em nosso projeto a partir da expansão do leque de tarefas\nconsideradas para um pipeline de mineração de argumentos, sendo elas: análise de\nsentimento, identiﬁcação de tópico, reconhecimento de entidades, detecção de ironia,\ndetecção de posição, e do uso de redes neurais como principal ferramenta para MA.\nDestaca-se que as tarefas do pipeline, em conjunto, são essenciais para o entendimento\ndo discurso nas redes sociais, visando a posterior identiﬁcação de argumentos.\n\nA inclusão das tarefas de identiﬁcação de tópico e reconhecimento de entidades\nnomeadas objetiva desagregar duas grandes atividades vistas na literatura: (i) a\n\n\fidentiﬁcação do assunto da discussão; e (ii) a detecção de relação entre pares de texto.\nCom a desagregação, será possível investigar a eliminação da anotação de um dataset\npara pares de tweets relacionados, como o apresentado em [Bosc et al. 2016a].\nAdicionalmente, a inclusão da detecção de ironia visa resolver a diﬁculdade adicional da\nMA em redes sociais relacionada ao alto grau de informalidade do texto [Schaefer e\nStede 2021] e a mudança de semântica gerada pelo uso desse recurso linguístico.\n\nOptamos por não incluir a tarefa de detecção de argumentação, prevista pela\nliteratura, no pipeline proposto. Como exposto em [Schaefer e Stede 2021], a literatura\nrecente despendeu muito esforço na construção de datasets com alguma deﬁnição\nteórico-conceitual de argumentação. O formato geral do pipeline está na Figura 1.\n\nFigura 1. Pipeline proposto da mineração de argumentos.\n\ntextuais exigem uma análise mais\n\nPosto que o objetivo geral deste trabalho é compreender o debate em uma rede\nsocial, cujas características\nsoﬁsticada de\nentendimento do texto antes de realizar a identiﬁcação da argumentação, entendemos\nque as tarefas de identiﬁcação de tópicos, entidades, sentimentos, posição e ironia de\numa discussão oferecem vantagem para o seu entendimento, ainda que ali possamos\nconstatar que não ocorra argumentação. Um exemplo simpliﬁcado desta estratégia,\ntendo por base textos sintéticos sem o pré-processamento e sem considerar todas as\ntarefas do pipeline, é demonstrado na Figura 2. Nela, são apresentados três tweets\nsintéticos e sem pré-processamento a respeito do ﬁlme “Duna: Parte Dois”, tópico\ncorrespondente a esses posts. Com os tweets pré-processados, é possível encontrar: as\nentidades “Duna”, “Aila” e “Paul Artreids”; os sentimentos de cada um deles; e a ironia\nexistente. A partir disso, uma estrutura do debate realizado sobre esse tópico pode ser\nmontada.\n\nFigura 2. Exemplo do processamento de textos em redes sociais para a\nidentiﬁcação da estrutura do debate (entidades, sentimentos e ironia).\n\n\f5. Experimentos\n\nPara dar cabo das tarefas selecionadas, procurou-se por modelos pré-treinados em\nportuguês ou que tivessem sofrido ﬁne-tuning para tarefas em português com um bom\nresultado. Um fator crucial para a seleção de modelos foi a sua presença no\nHuggingFace Hub2, repositório que visa armazenar e distribuir modelos de deep\nlearning para facilitar sua utilização em pesquisas e aplicações comerciais. A Tabela 1\napresenta os modelos selecionados para os experimentos, com a coluna “Modelo” sendo\no ID dos modelos na plataforma da HuggingFace e coluna ID sendo a utilizada para\nreferenciar os modelos ao longo desta seção.\n\nO modelo 1, apelidado na literatura de BERTimbau (Souza et al., 2020), foi\npré-treinado na base de dados brWaC [Wagner Filho et al., 2018], ou seja, em dados\nsem domínio especíﬁco em português brasileiro. Já os modelos 2 e 3, BERTweet.BR\n[Carneiro, 2023] e BERTaporu [Costa et al., 2023],\nforam\npré-treinados em milhares de tweets em português. Entretanto, o modelo 4, RoBERTuito\n[Pérez et al., 2022], foi pré-treinado em tweets na língua espanhola, mas já demonstrou\nbons resultados para a tarefa de análise de sentimento em português [Pérez et al., 2023].\n\nrespectivamente,\n\nTabela 1. Modelos escolhidos para os experimentos computacionais.\n\nID Modelo\n\nEndereço Web\n\n1\n\nneuralmind/bert-base-portuguese-cased\n\nhuggingface.co/neuralmind/bert-base-portuguese-cased\n\n2 melll-uff/bertweetbr\n\nhuggingface.co/melll-uff/bertweetbr\n\n3\n\n4\n\npablocosta/bertabaporu-base-uncased\n\nhuggingface.co/pablocosta/bertabaporu-base-uncased\n\npysentimiento/robertuito-base-cased\n\nhuggingface.co/pysentimiento/robertuito-base-cased\n\nPor sua vez, a base de dados utilizada para realizar o ﬁne-tuning dos modelos,\n[Silva et. al. 2024], foi construída a partir das discussões realizadas em torno dos\neventos de 8 de janeiro de 2023 na praça dos Três Poderes, em Brasília, e anotada em\netapas manuais e automáticas (com uso de LLMs) para as tarefas de análise de\nsentimento (AS) (positivo, negativo ou neutro), detecção de ironia (DI) (contém ironia,\nnão contém ironia) e detecção de posição (DP) (a favor da invasão, contra a invasão).\nConsiderando que a base foi construída a partir de um evento marcante, inferimos que o\ncorpus já está intrinsecamente associado a um único tópico central. Dessa forma, a\ntarefa de identiﬁcação de tópico foi considerada redundante e removida desta bateria de\nexperimentos. A detecção de posição, por sua vez, foi conduzida em relação a esse\ntópico pré-deﬁnido, permitindo uma análise mais precisa das opiniões expressas no\ncontexto especíﬁco do evento. Além disso, optou-se por explorar o reconhecimento de\nentidades nomeadas (NER)\nfuturamente devido ao tempo signiﬁcativo que seria\nnecessário para uma anotação manual da base de dados e treinamento do modelo,\nconsiderando o cronograma previsto.\n\nA Tabela 2 apresenta a quantidade de tweets utilizada para os experimentos de\ncada tarefa, bem como a distribuição entre as classes de cada tarefa. Deve-se notar que a\nbase está balanceada para a tarefa de AS, mas não para as tarefas de DP e DI.\nAdicionalmente, cabe destacar que os tweets utilizados em cada uma das tarefas não\n\n2 https://huggingface.co/\n\n\fpossuem total sobreposição, isto é, não são necessariamente os mesmos entre as bases\nde cada tarefa.\n\nTabela 2. Quantidade de tweets entre as tarefas e classes.\n\nTarefa\n\nAnálise de Sentimento\n\nDetecção de Posição\n\nClasse\n\nNeutro\n\nPositivo\n\nNegativo\n\nNeutro\n\nA favor\n\nContra\n\nDetecção de Ironia\n\nContém ironia\n\nNão contém ironia\n\nTreino\n\nValidação\n\nTeste\n\nTotal\n\n234\n\n233\n\n234\n\n77\n\n19\n\n1453\n\n232\n\n615\n\n50\n\n50\n\n50\n\n16\n\n4\n\n312\n\n50\n\n132\n\n50\n\n51\n\n50\n\n17\n\n4\n\n312\n\n50\n\n132\n\n334\n\n334\n\n334\n\n110\n\n27\n\n2077\n\n332\n\n879\n\nOs resultados do ajuste dos modelos podem ser vistos na Tabela 3. Os modelos\nforam treinados com 5 passagens completas pelo conjunto de treino e aquele com\nmelhor desempenho no conjunto de validação foi salvo e utilizado para avaliação no\nconjunto de teste. Todos os modelos foram treinados variando o número de camadas\ntreinadas (as 2 ou 4 últimas camadas) e o batch size (testes realizados com 8, 16, 32 e\n64). Utilizou-se a taxa de aprendizado de 0.001 para todos os modelos e a entropia\ncruzada na função de custo, com ajuste para penalizar mais severamente os erros nas\nclasses minoritárias. Na tarefa de DI, o melhor modelo foi o RoBERTuito (ID 4 - Tabela\n1), tendo sido ajustado nas últimas duas camadas e com um batch size de 16. Nas tarefas\nde AS e DP, o melhor modelo foi o BERTimbau (ID 1 - Tabela 1), sendo ajustado nas\núltimas 4 camadas e com o mesmo batch size do modelo anterior.\n\nTabela 3. Métricas de avaliação dos melhores modelos por tarefa.\n\nTarefa\n\nModelo\n\nF1-Score\n\nPrecision Recall\n\nAnálise de Sentimento\n\nneuralmind/bert-base-portuguese-cased\n\nDetecção de Ironia\n\npysentimiento/robertuito-base-uncased\n\nDetecção de Posição\n\nneuralmind/bert-base-portuguese-cased\n\n0,85\n\n0,76\n\n0,97\n\n0,86\n\n0,76\n\n0,97\n\n0,86\n\n0,76\n\n0,97\n\nEmbora os resultados obtidos aqui não sejam diretamente comparáveis com a\nliteratura por serem obtidos em uma base de dados diferente, eles são numericamente\nsuperiores aos encontrados em Pérez et al. (2023). Comparativamente, o modelo de AS\nem português ajustado em Pérez et al. (2023), treinado a partir do BERTweet.BR na\nbase de dados apresentada em Brum & Volpe Nunes (2018), alcançou uma macro\nF-Score de 0,73 no conjunto de teste do dataset [Silva et. al. 2024]. Além disso, a título\nde exemplo, a Tabela 4 apresenta dois tweets do conjunto de teste, a classiﬁcação dada\npelo modelo e a classe tida como verdadeira. Os dois tweets foram levemente\nmodiﬁcados para evitar a sua identiﬁcação na rede social sem comprometimento do seu\nsigniﬁcado e avaliação.\n\nTabela 4. Exemplos de classiﬁcação de dois tweets do conjunto de teste.\n\n\fAnálise de Sentimento\nClasse\nPrevista\n\nClasse\nVerdadeira\n\nDetecção de Posição\nClasse\nClasse\nPrevista\nVerdadeira\n\nDetecção de Ironia\nClasse\nClasse\nPrevista\nVerdadeira\n\nPositivo\n\nNeutro\n\nContra\n\nContra\n\nNeutro\n\nNeutro\n\nContra\n\nContra\n\nNão\ncontém\nironia\n\nNão\ncontém\nironia\n\nNão\ncontém\nironia\n\nContém\nironia\n\nTexto\n\nmano passei a noite\ntoda no meu quarto\nassistindo tbt e fui ver\nagr que tava a maior\nconfusao em brasilia k\nsuper antenada eu sou\nnem eu acreditei\nquando vi onde ela\ntava me mandou\ncorrente de excursao\npra brasilia e tudo e\nnem dei bola\n\n6. Conclusão\n\nEste artigo apresenta os avanços desta pesquisa em andamento voltada à construção de\nsoluções computacionais capazes de apoiar o entendimento do debate em redes sociais.\nUma das bases fundamentais destas soluções, explorada neste artigo, é a aplicação de\ntécnicas de mineração de argumentos capazes de identiﬁcar a estrutura de argumentação\npresente nas diversas falas na rede. A expectativa é que a identiﬁcação e visualização da\nestrutura de argumentação possa auxiliar usuários da rede a compreenderem, reﬂetirem\ne, eventualmente, melhor participarem do debate público.\n\nNo treinamento dos modelos, os resultados mais signiﬁcativos incluem o bom\ndesempenho do modelo BERTimbau na tarefa de detecção de posição, com F1-Score de\n0,97. Adicionalmente, o modelo RoBERTuito também obteve um desempenho bom,\ncom um F1-Score de 0,76 na detecção de ironia, uma tarefa ainda mais complexa no\ncontexto das redes sociais. No entanto, destaca-se como limitação o fato de o dataset\nutilizado ser pequeno e desbalanceado, o que pode comprometer a representatividade\nestatística necessária para treinar e testar adequadamente modelos de deep learning,\nespecialmente na tarefa de detecção de posição. Esse aspecto pode restringir a\ngeneralização dos resultados obtidos para diferentes contextos e tópicos. Os resultados\nde aplicação do pipeline proposto nos ajudarão, em passo seguinte, a projetar as\nabordagens para a realização da tarefa de detecção de argumentação, soﬁsticando a\nidentiﬁcação da estrutura do debate. Como trabalhos futuros, planejamos aumentar a\nquantidade de dados anotados utilizados para treinamento dos modelos, bem como\nexplorar outras tarefas de um pipeline de mineração de argumentos. Outra estratégia\nprevista é explorar a utilização de Large Language Models (LLMs) [Zhao et al.\n2023][(Brown et al., 2020)], abrindo a possibilidade de resumir o pipeline para apenas\num modelo.\n\nAgradecimentos\n\nOs autores agradecem à FAPESP pelo ﬁnanciamento desta pesquisa (#2021/14772-1).\nRenata Araujo é bolsista de produtividade em desenvolvimento tecnológico e extensão\ninovadora do CNPq (#305645/2022-6). Vitor dos Santos é bolsista TT1 pela FAPESP\n(#2023/04752-9). Livia Alabarse dos Santos é bolsa TT1 pela FAPESP (2023/04042-1).\n\n\fReferências\n\nAddawood. A. e Bashir, M. (2016). “What Is Your Evidence? A Study of Controversial\nTopics on Social Media”. Em: Proceedings of the Third Workshop on Argument\nMining (ArgMining2016). Berlin, Germany. Association for Computational\nLinguistics.pages 1–11.\n\nBosc, T., Cabrio, E. e Villata, S. (2016). “Tweeties Squabbling: Positive and Negative\nResults in Applying Argument Mining on Social Media”. Frontiers in Artificial\nIntelligence and Applications, v. 287, p. 21–32.\n\nBosc, Tom, Cabrio, E. e Villata, S. (2016a). “DART: a Dataset of Arguments and their\nthe Language\n\nthe 10th edition of\n\nRelations on Twitter” Em: Proceedings of\nResources and Evaluation Conference. pp. 1258-1263.\n\nBrown, T., Mann, B., Ryder, N., et al. (2020). Language Models are Few-Shot Learners.\nEm: Advances in Neural Information Processing Systems. Curran Associates, Inc.\n\nCarneiro, F. P. (2023). “BERTweet.BR: A Pre-Trained Language Model for Tweets in\nPortuguese”. Dissertação de Mestrado. Universidade Federal Fluminense, Programa\nde Pós-Graduação em Computação. Niterói.\n\nCortiz, D. (2021) “Exploring transformers in emotion recognition: a comparison of bert,\n\ndistillbert, roberta, xlnet and electra”. arXiv. arXiv:2104.02041.\n\nCosta, P. B., Pavan, M. C., Santos, W. R., Silva, S. C., & Paraboni, I. (2023).\n“BERTabaporu: Assessing a Genre-Specific Language Model for Portuguese NLP”.\nthe 14th International Conference on Recent Advances in\nEm: Proceedings of\nNatural\nBulgaria.\n217–223.\nLanguage\nhttps://aclanthology.org/2023.ranlp-1.24\n\nProcessing,\n\nShoumen,\n\np.\n\nDemszky, D., Movshovitz-Attias, D., Ko, J., Cowen, A.S., Nemade, G., & Ravi, S.\narXiv.\n\n“GoEmotions: A Dataset\n\nFine-Grained\n\nEmotions”.\n\nof\n\n(2020)\narXiv:abs/2005.00547.\n\nDevlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2019). “BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding”. Em: Proceedings of\nthe 2019 Conference of\nthe Association for\nComputational Linguistics: Human Language Technologies, Association for\nComputational Linguistics.\n\nthe North American Chapter of\n\nLawrence, J., Bex, F., Reed, C. e Snaith, M.\nArgument Web.” Em: Proceedings of\nComputational Models of Argument. IOS Press. pp. 515-516.\n\n(2012) “AIFdb:\n\nInfrastructure for the\nthe 6th International Conference on\n\nLawrence, J. e Reed, C.\n\n(2020) “Argument mining: A survey”. Computational\n\nLinguistics, v. 45(4), pp. 765-818, 2020.\n\nLippi, M., Torroni, P. (2016). “Argumentation mining: State of the art and emerging\n\ntrends”. ACM Transactions on Internet Technology, 16(2), 1-25.\n\nPalau, R. M. e Moens, M. F.\n\nthe detection,\nclassification and structure of arguments in text”. Em: Proceedings of the 12th\nInternational Conference on Artificial Intelligence and Law. pp. 98-107.\n\n(2009). “Argumentation mining:\n\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... e\n\n\fDuchesnay, E. (2011) “Scikit-learn: Machine learning in Python\". The Journal of\nmachine Learning research, 12,2825-2830.\n\nPérez, J. M., Furman, D. A., Alonso Alemany, L., & Luque, F. M.\n\n(2022).\n“RoBERTuito: A pre-trained language model for social media text in Spanish”. Em:\nProceedings of the Thirteenth Language Resources and Evaluation Conference, p.\nAssociation.\nLanguage\n7235–7243.\nhttps://aclanthology.org/2022.lrec-1.785\n\nResources\n\nEuropean\n\nPérez, J. M., Rajngewerc, M., Giudici, J. C., Furman, D. A., Luque, F., Alemany, L. A.,\n& Martínez, M. V. (2023). “pysentimiento: A Python Toolkit for Opinion Mining and\nSocial NLP tasks”. arXiv. http://arxiv.org/abs/2106.\n\nSalles, G. T., Coelho, O. B. (2022). “Reconhecimento de Emoções em Mineração de\nArgumentos com Deep Learning”. Trabalho de Conclusão de Curso. Universidade\nPresbiteriana Mackenzie.\n\nSchaefer, R. e Stede, M. (2021). “Argument Mining on Twitter: A survey”. Information\n\nTechnology, v. 63, n. 1, p. 45–58.\n\nSilva, L. J., Santos, L. A.; Araujo, R., Coelho, O. B., Correa, A. G, D,; Oliveira, I. C. A.\n(2024)\n“Tweet_Eleições_2022: Um dataset de tweets durante as eleições\npresidenciais brasileiras de 2022”. Brazilian Workshop on Social Network Analysis\nand Mining (BRASNAM), 13. Brasília/DF. Porto Alegre: Sociedade Brasileira de\nComputação. p. 193-199. https://doi.org/10.5753/brasnam.2024.1940.\n\nSlonim, N., Bilu, Y., Alzate, C., Bar-Haim, R., Bogin, B., Bonin, F., ... e Aharonov, R.\n\n(2021). “An autonomous debating system”. Nature, 591(7850), p. 379-384.\n\nSousa, J.P.S., Nascimento, R. C. U., Araujo, R. M., Coelho, O. B. (2021). “Não se perca\nno debate! Mineração de Argumentação em Redes Sociais”. Brazilian Workshop on\nSocial Network Analysis and Mining (BRASNAM). Porto Alegre: Sociedade\nBrasileira de Computação. p. 139-150. https://doi.org/10.5753/brasnam.2021.16132.\n\nSouza, F., Nogueira, R., & Lotufo, R. (2020). “BERTimbau: Pretrained BERT Models\n403–417.\n\nPortuguese”,\n\np.\n\nfor\nhttps://doi.org/10.1007/978-3-030-61377-8_28\n\nBrazilian\n\nStede, M. e Schneider, J. (2019). “Argumentation Mining”. Springer. Synthesis Lectures\n\non Human Language Technologies.\n\nSun, C., Qiu, X., Xu, Y. e Huang, X. (2019). “How to Fine-Tune BERT for Text\nClassification?” In Chinese Computational Linguistics. Lecture Notes in Computer\nScience. Springer International Publishing.\n\nTokuda, N. H., Coelho, O. B., Araujo, R.M. (2021). “Análise de Sentimento por meio\nde Deep Learning aplicada à Mineração de Argumentos”. Trabalho de Conclusão de\nCurso. Universidade Presbiteriana Mackenzie.\n\nToulmin, S. E. (2003). The uses of argument. Cambridge University Press.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, T.\ne Polosukhin, I. (2017). “Attention is All you Need”. Em: Advances in Neural\nInformation Processing Systems. Curran Associates, Inc. 30.\n\nVecchi, E. M., Falk, N., Jundi, I., Lapesa, G. (2021). “Towards Argument Mining for\n\n\fSocial Good: A Survey”. Em: Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint\nAssociation for\nConference on Natural Language Processing.\nComputational Linguistics. p. 1338–1352.\n\n.Online.\n\nWagner Filho, J. A., Wilkens, R., Idiart, M., & Villavicencio, A. (2018). \"The brWaC\nCorpus: A New Open Resource for Brazilian Portuguese\". Em: Proceedings of the\nEleventh International Conference on Language Resources and Evaluation (LREC\n2018).\n(ELRA).\nEuropean\nhttps://aclanthology.org/L18-1686\n\nAssociation\n\nResources\n\nLanguage\n\nWalker, M. A., Tree, J. E. F., Anand, P., Abbott, R. e King, J. (2012). “A Corpus for\nResearch on Deliberation and Debate”. Em: Proceedings of the Eighth International\nConference on Language Resources and Evaluation (LREC ’12) v. 12. Istanbul,\nTurkey. p. 812–817.\n\nZhang, T., Wu, F., Katiyar, A., Weinberger, K. Q., & Artzi, Y. (2020) “Revisiting\n\nfew-sample BERT fine-tuning”. arXiv preprint arXiv:2006.05987.\n\nZhao, W. X., Zhou, K., Li, J., et al. (2023). “A Survey of Large Language Models”.\n\nArxiv. arXiv. http://arxiv.org/abs/2303.18223.\n\n\f"
        }
    ]
}