# STIL-2024

Este projeto visa a realizaÃ§Ã£o de uma sÃ©rie de atividades de processamento de linguagem natural sobre o corpus criado com os 24 artigos da conferÃªncia principal dos Anais do SimpÃ³sio Brasileiro de Tecnologia da InformaÃ§Ã£o e da Linguagem Humana (STIL).

## ğŸ“š Etapas Realizadas

### 1. [TokenizaÃ§Ã£o](tokenizaÃ§Ã£o/)
- Utilizamos o modelo **BERTimbau** para a tokenizaÃ§Ã£o dos artigos.
- Cada artigo tokenizado foi salvo em uma nova chave no corpus chamada `artigo_tokenizado`, contendo uma lista de tokens.

### 2. [POS Tagging (Part-Of-Speech Tagging)](pos_tag/)
- Utilizamos o modelo fine-tunado **`lisaterumi/postagger-portuguese`**, disponÃ­vel no GitHub, para a tarefa de POS tagging.

#### Tags Utilizadas
| Tag | DescriÃ§Ã£o |
|:----|:----------|
| ADJ | Adjetivo |
| ADV | AdvÃ©rbio |
| ADV-KS | AdvÃ©rbio conjuntivo subordinado |
| ADV-KS-REL | AdvÃ©rbio relativo subordinado |
| ART | Artigo |
| CUR | Moeda |
| IN | InterjeiÃ§Ã£o |
| KC | ConjunÃ§Ã£o coordenativa |
| KS | ConjunÃ§Ã£o subordinativa |
| N | Substantivo |
| NPROP | Substantivo prÃ³prio |
| NUM | NÃºmero |
| PCP | ParticÃ­pio |
| PDEN | Palavra denotativa |
| PREP | PreposiÃ§Ã£o |
| PROADJ | Pronome adjetivo |
| PRO-KS | Pronome conjuntivo subordinado |
| PRO-KS-REL | Pronome relativo conectivo subordinado |
| PROPESS | Pronome pessoal |
| PROSUB | Pronome nominal |
| V | Verbo |
| VAUX | Verbo auxiliar |

### 3. [EstatÃ­sticas do Corpus](Estatistica/)
Foram extraÃ­das as seguintes informaÃ§Ãµes:
- NÃºmero total de sentenÃ§as
- NÃºmero mÃ©dio de sentenÃ§as por artigo
- NÃºmero total de tokens
- NÃºmero mÃ©dio de tokens por artigo
- Top 10 tokens mais frequentes
- Bottom 10 tokens menos frequentes
- NÃºmero de substantivos
- NÃºmero de verbos
- NÃºmero de preposiÃ§Ãµes

> ğŸ”¹ [As estatÃ­sticas gerais foram salvas em um arquivo `.json`.](Estatistica/estatisticas_gerais_stil.json)
> ğŸ”¹ [As estatÃ­sticas por artigo foram armazenadas em um arquivo `.csv`.](Estatistica/estatisticas_individuais_stil.csv)

### 4. [LematizaÃ§Ã£o](LematizaÃ§Ã£o/)
- Utilizamos o framework **spaCy**, com um modelo prÃ©-treinado para portuguÃªs.
- Iteramos sobre as sentenÃ§as do corpus para lematizar cada token.

### 5. [AnÃ¡lise de DependÃªncia SintÃ¡tica](analise_de_dependencia)
- TambÃ©m com o **spaCy**, realizamos a anÃ¡lise de dependÃªncia sintÃ¡tica.
- Para cada token, foi gerado um dicionÃ¡rio contendo:
  - A palavra analisada
  - A relaÃ§Ã£o sintÃ¡tica (`dep_`)
  - A palavra da qual o token depende (`head`)

## ğŸ› ï¸ Tecnologias Utilizadas
- [BERTimbau](https://github.com/neuralmind-ai/portuguese-bert)
- [lisaterumi/postagger-portuguese](https://github.com/lisaterumi/pos-tagger-portuguese)
- [spaCy](https://spacy.io/)

## ğŸ—‚ [OrganizaÃ§Ã£o dos Dados no Corpus](corpus.json)
- **TÃ­tulo do Artigo**: `titulo`
- **Link para as informaÃ§Ãµes do artigo**: `informacoes_url`
- **Idioma do Artigo**: `idioma`
- **PDF do artigo na pasta files**: `storage_key`
- **Nome, AfiliaÃ§Ã£o e Orcid dos Autores**: `autores`
- **Data que foi publicado**: `data_publicacao`
- **Resumo do Artigo**: `resumo`
- **Palavras Chaves**: `keywords`
- **ReferÃªncias**: `referencias`
- **Artigo completo em formato de string (Texto Cru)**: `artigo_completo`
- **Artigo Tokenizado**: `artigo_tokenizado`
- **Artigo Separado em SentenÃ§as**: `artigo_sentenÃ§as`
- **POS Tag**: `pos_tag_artigo`
- **Lemas**: `sentenÃ§as_lematizadas`
- **DependÃªncias SintÃ¡ticas**: `chave analise_dependencia`
---


